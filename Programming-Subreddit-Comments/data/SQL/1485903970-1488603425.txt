You can absolutely reference the same temp table in both outer and inner queries...
This post is about spaces, not emojis. :P &gt; Others have done a really good job in describing these issues. I see these as: bad support in tools, applications, and db, difficulty in writing, difficulty in reading. Do you see this as correct? &gt; probably has to do with added complexity I would be interested to know. There's as much difference in a unicode 'j' or a smiley, as a ascii 'a' or 'b', from a data perspective, and databases aren't responsible for displaying.
This particular case is on Oracle. AccountAssignmentDim &gt; "AccountAssignmentDim" AccountAssignmentDim is the table and "AccountAssignmentDim" is a view. The view is actually an aggregate of the AccountAssignmentDim. So if I wanted to query the "AccountAssignmentDim" view I would have to write my query as select whatever from "AccountAssignmentDim" while this is the actual table select whatever from AccountAssignmentDim Again, "AccountAssignmentDim" is an aggregate and should not be confused with AccountAssignmentDim. If the code factory where this was in a mindset where this was bad practice they may have actually come up with a slightly different name for "AccountAssignmentDim" that makes the database easier to use. Something like AccountAssignmentAgg would be prefect. So many defects are caused by code sloppiness and poor readability. This especially becomes important when things start to grow in complexity. It's not because these more obscure ways of doing something won't work. It's because it's inherently more difficult to troubleshoot. Especially when you have to pass you code off to others. This has a simple error. with "Days in 2016" as (select day_short_name as "Weekday",weekend_flag as "Is A Weekend",1 as "Day Counter" from TIME_DIM where time_key between to_date('2016-01-01', 'YYYY-MM-DD') and to_date('2017-01-01', 'YYYY-MM-DD')) select sum("Day Counter") as "Number of Weekend Days", sum(case when "Weekday" = 'Sat' then "Day Counter" else 0 end) as "Number of Saturdays", sum(case when "Weekday" = 'Sun' then "Day Counter" else 0 end) as "Number of Sundays" from "Days in 2016" where "Is a Weekend" = 'Y' This is the same query with the spaces replaced with _ and the " removed. It runs without a problem with Days_in_2016 as (select day_short_name as Weekday,weekend_flag as Is_A_Weekend,1 as Day_Counter from TIME_DIM where time_key between to_date('2016-01-01', 'YYYY-MM-DD') and to_date('2017-01-01', 'YYYY-MM-DD')) select sum(Day_Counter) as Number_of_Weekend_Days, sum(case when Weekday = 'Sat' then Day_Counter else 0 end) as Number_of_Saturdays, sum(case when Weekday = 'Sun' then Day_Counter else 0 end) as Number_of_Sundays from Days_in_2016 where Is_a_Weekend = 'Y' Edit: And a properly formatted version with Days_in_2016 as ( select day_short_name as Weekday, weekend_flag as Is_A_Weekend, 1 as Day_Counter from TIME_DIM where time_key between to_date('2016-01-01', 'YYYY-MM-DD') and to_date('2017-01-01', 'YYYY-MM-DD') ) select sum(Day_Counter) as Number_of_Weekend_Days, sum(case when Weekday = 'Sat' then Day_Counter else 0 end) as Number_of_Saturdays, sum(case when Weekday = 'Sun' then Day_Counter else 0 end) as Number_of_Sundays from Days_in_2016 where Is_a_Weekend = 'Y' 
Are you creating the DSN using the 32 or 64 bit applet? Is the application using the DSN 32 or 64 bit?
**TL;DR:** Type in ¯\\\\\\\_(ツ)\_/¯ for proper formatting Actual reply: For the ¯\_(ツ)_/¯ like you were trying for you need three backslashes, so it should look like this when you type it out ¯\\\_(ツ)_/¯ which will turn out like this ¯\\\_(ツ)\_/¯ The reason for this is that the underscore character (this one \_ ) is used to italicize words just like an asterisk does (this guy \* ). Since the "face" of the emoticon has an underscore on each side it naturally wants to italicize the "face" (this guy (ツ) ). The backslash is reddit's escape character (basically a character used to say that you don't want to use a special character in order to format, but rather you just want it to display). So your first "\\_" is just saying "hey, I don't want to italicize (ツ)" so it keeps the underscore but gets rid of the backslash since it's just an escape character. After this you still want the arm, so you have to add two more backslashes (two, not one, since backslash is an escape character, so you need an escape character for your escape character to display--confusing, I know). Anyways, I guess that's my lesson for the day on reddit formatting lol ***CAUTION: Probably very boring edit as to why you don't need to escape the second underscore, read only if you're super bored or need to fall asleep.*** Edit: The reason you only need an escape character for the first underscore and not the second is because the second underscore (which doesn't have an escape character) doesn't have another underscore with which to italicize. Reddit's formatting works in that you need a special character to indicate how you want to format text, then you put the text you want to format, then you put the character again. For example, you would type \_italicize\_ or \*italicize\* in order to get _italicize_. Since we put an escape character we have \\\_italicize\_ and don't need to escape the second underscore since there's not another non-escaped underscore with which to italicize something in between them. So technically you could have written ¯\\\\\\\_(ツ)\\\_/¯ but you don't need to since there's not a second non-escaped underscore. You ***would*** need to escape the second underscore if you planned on using another underscore in the same line (but not if you used a line break, aka pressed enter twice). If you used an asterisk later though on the same line it would not work with the non-escaped underscore to italicize. To show you this, you can type _italicize* and it should not be italicized.
&gt;"AccountAssignmentDim" is an aggregate and should not be confused with AccountAssignmentDim. OK, let's back up a second here, because you are confused on two fairly complicated matters. The first is with your own object names, which are are actually "AccountAssignmentDim" (the aggregate VIEW, which is impossible to access without quotes) and "ACCOUNTASSIGNMENTDIM" (your table, which you are referencing in a non-quoted manner) Those quotes are extremely important, because they enable case-sensitive behavior by the *interpreter*. When you specify an object name *without* quotes, most SQL interpreters will automatically covert all characters to UPPERCASE *before* evaluating the query. If you put the quotes on, the object is created with the exact "Literal Casing" that you provide. Surprising fact: The SQL engines with this behavior are actually case sensitive all the time, you just don't realize it because CREATE TABLE MyTable actually creates a table named "MYTABLE" – you just don't realize it when you write queries like SELECT * FROM myTablE. (See what I did there?) That also means when you create an object with quoted names like your aggregate view "AccountAssignmentDim" - it becomes inaccessible to people who don't use quotes (or use quotes with incorrect casing). When you are interviewing with job applications and they say something that doesn't make sense, maybe you should ask them about it. They might even teach you something you did not know. EDIT: This is how your query is being converted by the interpreter before the object names are evaluated for a match. Notice I upper case all table names, aliases and functions ("object names") - I did not upper case operators like CASE or BETWEEN or the keyword WITH because I don't truly know how those work (likely all cast to upper case as well, not accessible with quoted identifers at all). Your example query doesn't work because "Is a Weekend" is not equal to "Is A Weekend". with DAYS_IN_2016 as ( select DAY_SHORT_NAME as WEEKDAY, WEEKEND_FLAG as IS_A_WEEKEND, 1 as DAY_COUNTER from TIME_DIM where TIME_KEY between TO_DATE('2016-01-01', 'YYYY-MM-DD') and TO_DATE('2017-01-01', 'YYYY-MM-DD') ) select SUM(DAY_COUNTER) as NUMBER_OF_WEEKEND_DAYS, SUM(case when WEEKDAY = 'Sat' then DAY_COUNTER else 0 end) as NUMBER_OF_SATURDAYS, SUM(case when WEEKDAY = 'Sun' then DAY_COUNTER else 0 end) as NUMBER_OF_SUNDAYS from DAYS_IN_2016 where IS_A_WEEKEND = 'Y' Final comment: "Most" SQL engines are Oracle and DB2 for sure, I think likely Postgres as well. SQL Server is a totally different beast, where case sensitivity is always literal (you don't have to quote one during a CREATE statement) but usually ignored (case-insensitive collation when matching objects). I'm not familiar with configuring SQL Server to treat objects in a case sensitive manner.
How do you mean? I don't think any RDBMS developer is going to stop adding features and competing with the other platforms in favour of waiting for the glacial pace of ANSI standards to tell them what to do, and are hampered by the standard being entirely silent on a lot of really important parts of real-world SQL use: programmability (e.g., flow control), indexing, use of file systems, nulls/three state logic, date/time data type definitions to name a few. And it's a moving target, the standard is still changing and evolving. So to be ANSI compliant and nothing more they'd have to throw a bunch of functionality away, and going beyond the standard will eventually result in the platform-specific features being made into the standard and a whole bunch of weird shit needing to be done to keep everything complying to the new standard while being backwards compatible. So it's very patchy as to how ANSI-standard any RDBMS is. Postgresql is probably the truest ANSI implementation. MSSQL is generally pretty good, albeit there's a lot of behaviour that use server options to determine whether it uses ANSI. MySQL is just plain weird (backticks and major changes of behaviour between versions annoy me greatly).
I can second T-SQL fundamentals. I had been writing SQL for my job (stumbled into it) for a while before reading through this book, and it was still helpful.
"We can never use special characters, reserved keywords or extended character lengths in all of our object names" Or "We should write queries in a case-sensitive manner that most databases rely on" Whichever one works for you, I guess. Double quotes + writing case sensitive code (that is, ANSI standards) is the truly portable way. And you should really go back and examine your previous examples. Every single one of them fails because you refuse to acknowledge case sensitivity.
That's actually a problem I've yet to work out. I'm new to SQL and didn't know that you can't store lists in a field, unless the list is really just a string. Maybe each user could have a column that contains a reference to another table which lists the foods they like, one per row?
The problem with your original query is that you're counting distinct categories instead of items. Try: select distinct a.category, count(*) from item a, premium_item b where a.id = b.id group by category having count (*) &gt; 1; If you use this answer in a school assignment you are academically obliged to include a link to this comment in the bibliography.
That perfectly answers my question. Thanks
&gt; I don't think any RDBMS developer is going to stop adding features &gt; So to be ANSI compliant I mean, standardizing the existing feature sets and syntax that's been around for a while, in the general sense, unrelated to ansi compliance, like your example of the quoting. There's no reason quoting should be different. &gt; MySQL is just plain weird Is there a motivation to *not* be standard/interoperable with the other databases, to make transition difficult?
MSSQL has been moving towards ANSI in leaps in bounds, but with setting options so as not to break legacy code. Postgresql is as close as it's ever going to get, Oracle is pretty good but with weird oddities (being as old and widely used as it is it can't just change to ANSI immediately). There *is* a reason backticks exist, and that's because it needs to accomodate the legacy stuff and non-legacy with different rules around case sensitivity. I don't think the ANSI standard actually addresses that difference but I might be wrong. I think there would be some resistance to standardisation to encourage vendor lock in, but Microsoft who are the most notorious for vendor lock in have made the most headway towards ANSI wheras MySQL, completely open source, isn't heading that way at all, so I doubt that motivation has much impact.
So `rank()` and `row_number()` are very similar but also different. Lets take a case where you have some duplicate data and you need to filter it for an analysis. | Parent | Child | RqstID | Nbr_of_Rqsts | Rqst_Date | | :--- | :--- | :--- | :--- | :--- | | Client A | Subsidiary A | 1001 | 6 | 2016-01-01 | | Client A | Subsidiary B | 1001 | 6 | 2016-01-03 | | Client A | Subsidiary C | 1001 | 6 | 2016-01-07 | | Client A | Subsidiary D | 1001 | 6 | 2016-01-08 | | Client B | Subsidiary A | 1002 | 14 | 2016-01-01 | | Client B | Subsidiary B | 1002 | 14 | 2016-01-01 | | Client B | Subsidiary C | 1002 | 14 | 2016-01-06 | | Client B | Subsidiary D | 1002 | 14 | 2016-01-07 | So depending on a variety of situations maybe you want to transform this and make it so only the first subsidiary is given credit for the number of requests. You could write something like this: select * , case when X = 1 then Nbr_of_Rqsts else 0 end as Mod_Rqst_Nbr from ( select row_number() over(partition by RqstID order by Rqst_Date) as X , * from table ) a This will create a column (x) where it counts each request id starting with the first request date, and when its equal to 1 it will take the original request number, otherwise make it zero, e.g.: | Parent | Child | RqstID | Nbr_of_Rqsts | Rqst_Date | X | | :--- | :--- | :--- | :--- | :--- | :--- | | Client A | Subsidiary A | 1001 | 6 | 2016-01-01 | 1 | | Client A | Subsidiary B | 1001 | 0 | 2016-01-03 | 2 | | Client A | Subsidiary C | 1001 | 0 | 2016-01-07 | 3 | | Client A | Subsidiary D | 1001 | 0 | 2016-01-08 | 4 | | Client B | Subsidiary A | 1002 | 14 | 2016-01-01 | 1 | | Client B | Subsidiary B | 1002 | 0 | 2016-01-01 | 2 | | Client B | Subsidiary C | 1002 | 0 | 2016-01-06 | 3 | | Client B | Subsidiary D | 1002 | 0 | 2016-01-07 | 4 | Rank is a little different. If there are two cells that share the same date, so the criteria is exact, then it will rank them equally, e.g.: | Parent | Child | RqstID | Nbr_of_Rqsts | Rqst_Date | X | | :--- | :--- | :--- | :--- | :--- | :--- | | Client A | Subsidiary A | 1001 | 6 | 2016-01-01 | 1 | | Client A | Subsidiary B | 1001 | 0 | 2016-01-03 | 2 | | Client A | Subsidiary C | 1001 | 0 | 2016-01-07 | 3 | | Client A | Subsidiary D | 1001 | 0 | 2016-01-08 | 4 | | Client B | Subsidiary A | 1002 | 14 | 2016-01-01 | 1 | | Client B | Subsidiary B | 1002 | 14 | 2016-01-01 | 1 | | Client B | Subsidiary C | 1002 | 0 | 2016-01-06 | 3 | | Client B | Subsidiary D | 1002 | 0 | 2016-01-07 | 4 | edit: In the above example with `row_number` it will not be a deterministic result because you're simply telling it to order by date, and there are two identical dates, so it will somewhat "arbitrarily" pick which one of the two to give credit to. If you order by a primary key this won't happen. You could order by the name, but then it will simply give credit based on alphabetical order. This might not matter, but depending on your business needs (like if commissions are involved) you might need to go a layer deeper and find out how to appropriately distribute the total number of requests. Perhaps you need to then go a step further and divide the requests between any two subsidaries that share a rank of 1, etc. 
Some books I like for laymen Data Science for Business by Forrest Provost Keeping up with the Quants by Thomas Davenport Mastering Metrics by Angrist and Pischke Visual Display of Quantitative Information by Tufts Storytelling with Data by Knaflic And the Big Three How to Win Friends and Influence People by Dale Carnegie The 48 Laws of Power by Robert Greene Elements of Style by Strunk and White
I had to review a few reporting tools to hook into a database a while back. One of them was perfect except that one of the columns in the database had a space in and it did not encapsulate them so it could not be used. There are plenty of great tools out there that might not handle them and its not a thing that is likely to be documented... what if a space in the column name stopped you using tools like redgates sql toolkit. It's bad practice as it requires unnecessary characters which do not necessarily add to readability and adds a thing to remember which if forgotten will cause issues. You might remember each time but the new guy might not know. You should code in a way that does not create 'gotchas' and forces people to write in a certain way unless there is a good reason which makes that way better. In this case there is no good reason why you should have spaces in column names. Either create a view and add the space in the results set there or change the column name in application code. There is no technical reason why you should need a space in a column name and doing so can create issues therefore don't. Besides - you should be following naming conventions or development standards and if your place of work enforces spaces instead of say underscores in column names then whoever wrote your naming conventions needs to be fired. A quick google brings many results discussing the issue and out of the several I looked at not one person advocated spaces in column names and all implied it was poor practice at best. 
It might be worth preventing the bad data from being entered with a `check constraint`.
Select is exactly as it implies, it is what you are selecting. Performance wise, it's the same to say, give me everything vs give me one or two things. It's best to specify what you want, because if you don't need everything, you're going to pull a lot of network time trying to pull more data across. Also, it can break things. Example: My beginning point has 5 columns and so does my end point, I get sloppy, I just say, hey, just grab everything from my beginning point, insert it into my end point. What if I add a column in the future? It will break your code because the end point has 5 columns, but you added a 6th column to your beginning point and told it to grab everything. For joins, practice. Lots of practice. This covers joins well: https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/ Here are some books I'd highly recommend: [First book to read](https://www.amazon.com/Microsoft-Server-T-SQL-Minutes-Yourself/dp/0672337924/ref=sr_1_fkmr2_2?ie=UTF8&amp;qid=1485958935&amp;sr=8-2-fkmr2&amp;keywords=10+minute+dba+sql) [Second book](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X/ref=sr_1_9?ie=UTF8&amp;qid=1485958888&amp;sr=8-9&amp;keywords=sql+dba) Go download SQL 2016 developer, almost the same as enterprise and free. They have a huge DB for it that you can practice in.
 SELECT process_date, sum(sys_amount), sum(r_amount), sum(total_amount), ac_id, trans_type FROM TransactionTable GROUP BY process_date, acc_id, trans_type This will work so long as acc_id and trans_type are the same across the multiple rows.
I agree with this sentiment also - the things you learn in 2016 will generally be useful in 2014/12, but there might be some features that are new. If you specifically take an older exam you are just missing out on learning about new features.
MCSA 2016 takes 2 exams. MCSA for 2012/2014 takes 3 exams.
Answer 1.a. Yes. For this specific example, a combination of 'exists' conditions most likely is going to be more efficient. An index (especially bitmap one) might help. Better server hardware is always a valid answer too. Answer 1.b. "OR" conditions are generally inefficient - 'exists' can help but there will be a limit to where this will be effective and it's a less 'generic technique. Count( distinct) is obviously worse for performance vs simple count() - normalizing your data set should help. String comparisons are less effective than integer comparisons usually - so switching to a reference table with instruments and having only the IDs in your musician table should be more performant on larger sets of data. Answer 2. Yes, it can be achieved, look @ /u/lbilali's comment for the general approach. The way it is working right now (by returning a random row values per group) is MySQL specific interpretation/hack. Answer 3. 'Group By' changes granularity of the data - it reduces your data set to a single record for a combination of values of attributes in your group by. To simply select data at the original level look to #2. Answer 4. I don't quite get what are you saying in your first condition ("instruments that are not present...") but regardless - you can literally brute force all possible combinations of musicians and apply any relevant filters to that data set (including "the least amount of Names").
SQL is a **specification**. Nothing but a piece of paper. MySQL, Oracle, MS SQL are software - called [RDBMS](https://en.wikipedia.org/wiki/Relational_database_management_system) - **implementing** SQL. Implementations **differ** from one RDBMS to another. Why ? Because when SQL was standardized, some RDBMS already did exist. I found a course for beginners, which is not too bad : http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
The latest driver I can find is Microsoft SQL Server Native Client Version 11.00.6538 Is there a better one?
Client app is a proprietary app - I've been on contact with their tech support but they haven't been able to help so far. They say that using the same settings on their end works.
There are a ton of useful datasets on data.gov (at least for now...). I did a quick search and found a few pharmaceutical databases like [RxNorm](https://catalog.data.gov/dataset/rxnorm) which normalizes all of the different names that drugs go by, [Lactmed](https://catalog.data.gov/dataset/drugs-and-lactation-database-lactmed) which catalogs possible drug interactions with pregnant and breastfeeding women, etc. I didn't get anything searching for "contraindication" but you might be able to find something useful if you look for "drug interactions" or some such. It feels like something that CDC or HHS should have available, but maybe not. Assuming you wind up with some data files, you'll need to load them into a database engine. SSMS will query a SQL Server database, not the data files directly. You can get [SQL Server Express](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) for free, and you can use SSMS to import data into SQL Server.
NULL is a very special case that is worth reading more about to gain experience in SQL. Nothing EVER is equal to NULL. Even SELECT CASE WHEN NULL = NULL THEN 1 ELSE 0 END will return 0. SQL uses the reserved word "IS" for comparisons to NULL. So you can keep your first syntax but change the = to IS: SUM(CASE WHEN .... AND ColumnA = '1' OR ColumnA IS NULL Note also that you don't need the single quotes around NULL.
Thanks, looks pretty solid What do you think about stuff like Crystal Reports? I've heard that thrown around at work but heard it's a nightmare to work with Also, do you know of any good ones that are pretty with charts and all that stuff? Dbeaver looks very good but sort of plain. Price isn't a huge concern, company would pay it
Thank You! That fixed the issue! 
&gt;That column actually has several possible values but I'm only interested in having it included in my SUM, if it has values X or specifically says the word "NULL", as opposed to being blank. You're confusing me now - are you looking for records where the value is actually a null, or records that contain the word 'null'? If the former SUM(CASE WHEN .... AND ColumnA = '1' OR ColumnA is null)
If you are using MS SQL server, it's not a bad idea to stick to their software. I haven't used crystal, but SSRS is way better than business objects. There's sort of a gap between applications to build reports vs applications that are for database management and development.
Look into pivoting data.
I hate have to [type] in [brackets for] all the columns. It annoys me
Where does the ID in table B come from?
What if we'd like to use it to update tables too? Would MS SQL Server Manager suffice or is something else needed?
Yeah, I figured, was hoping for an all inclusive package I've just heard them say E2 is running on SQL 2012, not sure if that equals MS SQL Server 2012. Would we just need to download MS SQL Server 2012 Express and get our IT company to connect it to our database?
You're looking for SQL Server Management Studio. You can get it as a stand alone package too now. That will allow you to view databases, their components, run queries and depending on your security, manage perms too. https://msdn.microsoft.com/en-us/library/mt238290.aspx
Sure, why not? 
Why?
Drop the `@` in argument names, it denotes a user defined variable ([more info](http://stackoverflow.com/questions/1009954/mysql-variable-vs-variable-whats-the-difference)). [Works fine then](http://i.imgur.com/LmBXtz1.png). Of course I inserted '123' into the table prior to executing that.
Insert them
I thought all RDBMSs were welcome in this sub?
Always single employee, single status shift. Can't start one til the other stops.
I can't get that to work. Im going to have to take courses.
And being the guinea pig! 
Is there a list of these qualifiers for a beginner? Some "best practices" guide of what type of structure / query to use based on data? 
This has already been extremely helpful, thank you!! :) Should I be looking for .mdl files? Also is it true that I need to separately install SSDT to import the database? 
It reads like an academic exercise to me. Implementing a CS-101/102 data structure in an RDBMS is...odd.
&gt;If the only options are 2016 or 2014/2012, which do you think would be the best to obtain a cert in? I’m happy to install SQL Server express in my PC at home for practising on, for either of the two versions. If you install Express Edition, you probably won't be able to perform all the tasks required for the certifications. Although with 2016 SP1, all the "programmatic" features are available on all editions - you just won't have things like Agent and SSIS (I don't know if they're on the exams or not). The good news for you is that SQL Server 2016 Developer Edition is **free** and includes everything.
Are you a reddit developer?
How do you store a list of lists?
No, I used the term "we" to generically mean the company where I work. I am the DBA there. 
Interesting. Thanks for a link to the article. 
You've gotten solid answers as regards differences in RDBMS (Relational Database Management Systems), so I'll address the second part. To learn SQL there are a few excellent resources: * For super introductory SQL instruction you might try [Code Academy's Learn SQL tutorial](https://www.codecademy.com/learn/learn-sql). It is short, to the point, and interactive. (And don't be fooled, you can do all the core lessons for free.) * To learn SQL with a little more depth and explanation try [Head First SQL](http://shop.oreilly.com/product/9780596526849.do). * To learn more about RDMS generally try the [Stanford Introduction to Databases Course](https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about). Those should give you the grounding you need to pursue the topic for MS SQL or any other RDMS you are interested in. In regards to your question about what gave the best understanding of SELECT and JOIN clauses, for me the answer was experimentation. The nice thing about learning in programming/computer science is that often when you have a question about how something works you can write some code and immediately test your hypothesis. This is true for SQL too. You can either create some small test tables or make use of dummy tables and UNION functions to create some small sample datasets for yourself and then write queries against them. See if the results are what you expected. If they're not, figure out why. Repeat until you have a fairly intuitive understanding of how those concepts work. Anything you play with, you'll understand better.
plus, COALESCE is standard SQL, so it works in many more databases besides SQL Server 
You can try the free http://www.heidisql.com/
I too would like to know this. I use SSMS at work to run (read-only) queries all day long but I use it at home to work on some project DBs I have. I often use the editor to seed the first few rows as I figure out what I want the table layout to be and almost never after that. Is it just bad practice?
What is the error?
They are historically buggy and can potentially causes performance issues. It's also no good for bulk operations or performing updates or deletes involving joins. You can insert or update large numbers or records far more efficiently via regular queries. 
This is so important to understand with SQL Server 2016. If you are only using a database for learning - use the Developer Edition. The license is free and it includes almost all of the features available in enterprise edition of SQL server. When using express edition, your DB size is limited (among other things) - which means you can't play with fun/real datasets like the stack overflow DB.
There are more than five records total, but they need to be inserted in groups of 5 if that makes sense.
It's simply auto-incrementing with an initial seed so it doesn't come from anywhere in particular :P
We do something similar with some of our tables.
Long live ROW_NUMBER!
If you have MS Office with Access or can get Access, then that's the best in terms of versatility. You can create a working database in Access, pull in your tables from SQL via ODBC and use the GUI in Access to query it. It also has a very powerful reporting module for presenting your data. You can also create forms and automate data tasks with macros. Tons of how to videos on Youtube on how to create stuff. Its a great end-user tool for people that need to manipulate stuff in SQL Server.
Are you doing some independent learning or are you in an actual class. If you're in an actual class, have you been paying attention in that class? All fields in the select need a comma separating them. You missed the one between department name and salary. While it is still correct syntax to put your join conditions in the where, it's better form to put them into a formal join condition because then you can easily separate what's putting the tables together (the join conditions) and what's specific to the problem you're trying to solve (the where condition) A LIKE is a very specific thing where you only know a subset of what you're looking for in a field, so if you only knew that you were looking for last names starting with SM, you'd use like: EMP.ENAME LIKE 'SM%' When you're saying you need an exact match (like the department number) you need an = SELECT EMP.ENAME , DEPT.DNAME , EMP.SAL FROM EMP INNER JOIN DEPT ON EMP.DEPTNO = DEPT.DNEPTNO WHERE SAL BETWEEN 1000 and 2000; 
 SELECT e.ENAME, d.DNAME, e.SAL FROM EMP e JOIN DEPT d on e.DEPTNO = d.DEPTNO WHERE SAL BETWEEN 1000 AND 2000; Not sure what your problem is because you didn't show the result and/or example rows. You shouldn't use "like" unless the match isn't perfect.
problem is, row_number ignores ties, which, in my humble opinion, is **semantically wrong** when what you're after is the highest value
no -- this does not return "the **row** with the highest value" -- it gives two highest values, which might come from different rows
Good spot on the comma, seems it eluded me. Following SQLcourse2 I understood that in most cases you don't need the 'inner join'? However looking at w3schools it seems a formal join statement is easier to keep track of and use. Cheers to both of you for using that syntax, I'l be switching over. Anyhow, after I corrected the spelling error for dept.dneptno it worked as intended. Thanks for giving me some pointers!
If you want ties, just change `row_number` to `rank`. Or `dense_rank`, if that's your jam.
http://stackoverflow.com/questions/343402/getting-around-mysql-cant-reopen-table-error .... to whoever is downvoting me you need to brush up on SQL 
Okay, I've never seen the OVER clause before. Thanks!
 SELECT EMP.ENAME, DEPT.DNAME, EMP.SAL FROM EMP JOIN DEPT ON EMP.DEPTNO = DEPT.DEPTNO WHERE SAL BETWEEN 1000 AND 2000;
Thanks, got it
I run MSSQL 2016 Server (Dev edition) on a i7-3770K with 32GB RAM and Win10 on SSD, as well as the db core installation and tempdb, data files (dbs, logs) are on HDD. From this I would guess you should not have any problems with a similar configuration. My CPU is already 3 generations old, for example. As price is a consideration: i5 processor will do it, consider a larger SSD (500GB min) and at least 2 HDDs, should total at ~$500
Are you planning on having apps hit your SQL instance 24/7? For what you are asking, an Azure SQL Database might meet your needs. Up when you want it, down when you don't. Only paying for storage, uptime, and outbound data transfers by the GB. Might be worth looking into.
I think what it is, is there is a table with a bunch of values in the _SN column, he wants to grab 5 and insert them into another table that has 5 individual columns. Sounds like the ID column of the second table is an identity so it would go like, grab 5, insert, grab 5, insert, grab 5, insert.
Deleted my previous solution simply for the fact that I was assuming the NameID field and the ID fields were somehow related, if you are just batch inserting into an identity column its easier. This should get you there: DECLARE @TABLE TABLE (SN INT) DECLARE @SN1 INT, @SN2 INT, @SN3 INT, @SN4 INT, @SN5 INT INSERT INTO @TABLE (SN) SELECT SN FROM dbo.TableA LEFT JOIN dbo.TableB ON SN1 = SN OR SN2 = SN OR SN3 = SN OR sn4 = SN OR sn5 = SN WHERE SN1 IS NULL OR SN2 IS NULL OR SN3 IS NULL OR SN4 IS NULL OR SN5 IS NULL WHILE (SELECT COUNT(*) FROM @TABLE) &gt; 0 BEGIN SELECT TOP 1 @SN1 = SN FROM @TABLE DELETE FROM @TABLE WHERE SN = @SN1 SELECT TOP 1 @SN2 = SN FROM @TABLE DELETE FROM @TABLE WHERE SN = @SN2 SELECT TOP 1 @SN3 = SN FROM @TABLE DELETE FROM @TABLE WHERE SN = @SN3 SELECT TOP 1 @SN4 = SN FROM @TABLE DELETE FROM @TABLE WHERE SN = @SN4 SELECT TOP 1 @SN5 = SN FROM @TABLE DELETE FROM @TABLE WHERE SN = @SN5 INSERT INTO dbo.TableB ( SN1, SN2, SN3, SN4, SN5 ) VALUES ( @SN1, -- ID - bigint @SN2, -- SN1 - bigint @SN3, -- SN2 - bigint @SN4, -- SN3 - bigint @SN5 -- SN4 - bigint ) END SELECT * FROM dbo.TableB
That's not a whole lot of data. I'd suggest anything with a quad core and go crazy on the RAM. You could *likely* get away with a solid state drive if you really only have 90GB of data.
It's really impossible to suggest other ways without knowing the contents of said text file. Could you give us an example?
What does "clean up" the text file mean? You would use a flat file source to import the data from the text file and select the columns you want to use. Then use a data conversion / other transformation steps to manipulate the data prior to inserting it into SQL
You are a saint. I will tinker around with this and try to make it work. Thank you so much! EDIT: It works, I can never thank you enough! Thank you thank you thank you!! :D
SQL express only utilises 1gig of ram
Idk why everyone is throwing these high specs around when you are using sql express (software prevents full use of hardware). Can you upgrade to SQL developer? I have a 200gb database that runs fine on a 8 year old Pentium and 8gb of RAM, and a single SSD. It also does nothing besides SQL though.
This, I'm running 2016 developer at home, it's been really useful because I can try things that were impossible with express. Edit, developer 2016 is free for non production use.
Well yes, it does know that cte is recursive and it executes it in a certain way.: on the first iteration, the non recursive part runs, then that data set is fed into the recursive part, the result of which is fed into recursive part, the result of which ... etc. This goes on until an empty result set is returned or the maximum recursion depth is reached.
This might be a little more helpful: https://msdn.microsoft.com/en-us/library/ms186243.aspx Specifically: &gt;The semantics of the recursive execution is as follows: 1. Split the CTE expression into anchor and recursive members. 1. Run the anchor member(s) creating the first invocation or base result set (T0). 1. Run the recursive member(s) with Ti as an input and Ti+1 as an output. 1. Repeat step 3 until an empty set is returned. 1. Return the result set. This is a UNION ALL of T0 to Tn. But basically, yeah, it keeps track of itself so it knows better than to start back from 1.
SSIS is definitely the best way. You can scan each column in each line and run it against C# or VB functionality before importing it into SQL server. Otherwise I would just import the file contents into an Import/Holding table, then use sql to "clean" those values to my needs.
organize by table (i.e. matching columns and data types), merge all spreadsheets in to .csv, and then load into ssis to put into a sql table. If you're game and want to handle looping through files and understand how to handle skipping header rows and identifying truncation issues and errors there are a ton of tutorials online. here's [one](http://help.pragmaticworks.com/dtsxchange/scr/FAQ%20-%20How%20to%20loop%20through%20files%20in%20a%20specified%20folder,%20load%20one%20by%20one%20and%20move%20to%20archive%20folder%20using%20SSIS.htm) 
this is nifty. thank you sir for sharing.
How many rows and columns are we talking about here? How complex is the data? How many different purposes is the data used for? I only ask because if it is currently doable in excel then you are probably adding needless complication by uploading it into a database and then getting it out again. Power BI can read directly from excel files, and there is nothing I can think of that you could do to manipulate the data in SQL that you couldn't do in excel. The real value of databases is when you have an enormous volume of data, data that accrues automatically, or when you need to connect multiple different data sources to analyse together. As context, I manage BI for a small online marketplace company. Every transaction is stored in our database and I use SQL to get the data out and Power BI to visualise it, but all financial budgeting, modelling and forecasting is done in excel. If I had the financials in excel already then I don't think I would spend time uploading them to a db.
Agreed SSIS is the way to go. Even if you just use the "Import data" wizard within SSMS, it's definitely the way I'd do it. Watch out for data type issues though. Specifically memberIDs being converted to scientific notation if they're long enough.
Good Point. It also allows for insert into table (column) select column from other_table
What you mean by "staging tables" and "architect a real database"? Are you saying not to simply import my current data tables into SQL as is? I currently have several worksheets in a workbook with data such as operating expenses, the different expense accounts, project actuals and forecast, headcount names, headcount roll-up, and so on. It totals to 19 worksheets. However, in total it would be about 1200-1300 rows. Would you say I'd benefit from migrating the data into SQL? Edit: The financials are being fed from corp finance in our company and they retrieve these numbers from SAP. How do I go about using SQL to retrieve these financials directly from SAP? Would I just need to point the server to it?
 DELETE FROM TABLE WHERE ID IN (SELECT ID FROM TABLE WHERE Code IN('X3','X4')) That will probably do the work 
from the top of my head I'd suggest to get that, do an aggregation on your returned dataset by extending the CTE again, like this for total days: WITH DateRanges AS ( SELECT ApplicationID, StatusID, MIN([Date]) as StartDate, MAX([Date]) as EndDate FROM $table GROUP BY ApplicationID, StatusID ) ,ApplicationStatus AS ( SELECT StatusID, Order FROM APPLICATIONSTATUS ) ,StatusDays AS ( SELECT d.ApplicationID, d.StatusID, DATEDIFF(d, d.StartDate, d.EndDate) as DaysAtStatus FROM DateRanges d INNER JOIN ApplicationStatus s ON s.StatusID = d.StatusID ) -- total number of days over all applications for each StatusID SELECT SUM(DaysAtStatus) AS TotalDays, StatusID FROM StatusDays GROUP BY StatusID and for average days: WITH DateRanges AS ( SELECT ApplicationID, StatusID, MIN([Date]) as StartDate, MAX([Date]) as EndDate FROM $table GROUP BY ApplicationID, StatusID ) ,ApplicationStatus AS ( SELECT StatusID, Order FROM APPLICATIONSTATUS ) ,StatusDays AS ( SELECT d.ApplicationID, d.StatusID, DATEDIFF(d, d.StartDate, d.EndDate) as DaysAtStatus FROM DateRanges d INNER JOIN ApplicationStatus s ON s.StatusID = d.StatusID ) -- average number of days applicants spend in each StatusID SELECT AVG(DaysAtStatus) AS AverageDays, StatusID FROM StatusDays GROUP BY StatusID
 SELECT ID, Code FROM TABLE_NAME WHERE Code IN ('X1','X2') AND ID NOT IN ( SELECT ID FROM TABLE_NAME WHERE Code NOT IN ('X1','X2') )
IN is for a list of values in the same field. you could also use IN in the first query.. &gt;SELECT yr, subject, winner FROM nobel WHERE subject IN('literature') AND yr &gt; 1979 AND yr &lt;1990 but it's more "expensive" than just using = *edit* and bad practice
Not looking to delete records, just exclude them from the results.
That's a good read. It explains in nice detail several performance-boosting techniques that are used in Views, Subqueries and CTEs.
&gt; bad practice wut
 bad practice to use IN instead of = for just 1 value
IN is not an ordered list. When comparing your table to the values in IN you're forced to do a "table scan" which means you compare it to every row in the table. Depending on how big your table is and how complex things can get, you should avoid using IN and use some type of JOIN or EXISTS. But in smaller data sets you should be fine.
it works in MS-SQL with the test data you gave; is your actual data significantly different? run this as is and it returns what you're looking for: WITH TABLE_NAME AS ( SELECT 'AB1' AS [ID], 'X1' AS [Code] UNION SELECT 'AB1' AS [ID], 'X2' AS [Code] UNION SELECT 'AB2' AS [ID], 'X1' AS [Code] UNION SELECT 'AB2' AS [ID], 'X2' AS [Code] UNION SELECT 'AB2' AS [ID], 'X3' AS [Code] UNION SELECT 'AB2' AS [ID], 'X4' AS [Code] ) SELECT ID, Code FROM TABLE_NAME WHERE Code IN ('X1','X2') AND ID NOT IN ( SELECT ID FROM TABLE_NAME WHERE Code NOT IN ('X1','X2') )
Staging tables would be all data that you receive now, and from this you design a real database structure. From your edit regarding the origin for the input: you want to get as close to the original data source as possible. My best guess is that data is queried from SAP directly, so if you get an access (to build a connection string) and the queries your company uses to retreive data, you could set up a "linked server" connection on MSSQL and query directly into staging tables on your local server to start a database design for your purposes.
okay, why? because i disagree 
[removed]
I need a little help for my Opensoure project. http://queryit.purepix.net/ Its' an SQL Query/Analyzer Toolkit with an plugin Interface. Anyone using SQL frequently is welcome to beta test the Application and any C# developer is welcome to join the project if you want to contribute. I am also on Kickstarter to raise some funds for code signing certificates and test environments. https://www.kickstarter.com/projects/dennismittmann/queryit-sql-developer-toolkit-for-windows feel free to contact me if you have questions regards Dennis
You are probably asking about attaching and detaching a database. http://dba.stackexchange.com/questions/30440/how-do-i-attach-a-database-in-sql-server https://msdn.microsoft.com/en-us/library/ms190209.aspx
 SELECT * FROM `table` WHERE `column2` LIKE CONCAT( '%' , id , '%' )
Thanks for the feedback. I think you're right. I'm adding unnecessary complication to what already works perfectly fine. As I said before, I'm not working with a lot of data and the pairing of Power BI and my spreadsheets are pretty seamless. However, I don't think it'd be a bad idea to learn SQL on the side.
Did you just remove the variable @item on purpose? Because I need the variable
I found the solution, it was not related to this! (I posted it above)
If you're getting the data from SAP, IMO you really should be doing the calculations in SAP and have a report or something with the final results emailed to you. Making a database to store data that came from a different DB, is generally not a great idea.
It's not likely 
What are any tech certs good for? To prove you know something. Same thing can be said about degrees. They really aren't required. Its just much easier to prove you know a topic when you haven't got the work experience yet to back the claim on your resume. It would be useful if you want to manage dbs someday, but not really required. If its free and you don't know the topics or want to know more, then I would recommend taking it.
I'm not sure but it may be a prerequisite to a more advanced cert.
Love your work btw. Read all over "Use the Index Luke" and impressed my co-workers with my ability to read and optimize Oracle query plans.
In my experince the value of the cert is not the cert itself. I had to revise like a MF to pass because it was expensive and I did not want to lose out. So I actually learned a lot. It does look good on a CV, increases your SQL knowledge. No brainer really
This is an MTA (Microsoft Technology Associate) cert as opposed to an MCSA (Microsoft Certified Solutions Associate) cert. To my understanding, it is a lighter certification that wouldn't carry the same weight as the MSCA's. Do study up and take the certification test, though. It will not only look good but it will prepare you for the MCSA tests, should you decide to move forward with SQL.
What?
If you want to set up a multi-select parameter easily, you'll need to re-format your data. Having multiple values in a single column violates [1NF](http://www.1keydata.com/database-normalization/first-normal-form-1nf.php), and this is a great example of why those rules are important. Instead, your data should have multiple rows for "Sweatshirt", one row per color value. Then you can easily set up a multi-select parameter. 
Thank you! I'm working through this, but I'm getting an error: Could not find stored procedure black I replaced "Exec @a" with "Exec @b", but then I get: Could not find stored procedure '' This looks like a syntax problem on my end; I'm probably not knowledgable enough and/or overthinking your answer. This SQLFiddle shows what I'm doing: http://sqlfiddle.com/#!3/defe8/1 *[Edited for formatting]*
Probably has something to do with technology site Glenn quotes near the + signs. You need to exit out of them properly. I'll test it in an hour or 2 Edit: lol damn mobile... I just saw what I wrote.
syntax error will be the exec bit, it needs brackets Exec (@a) and it's also executing the colour parameter not the dynamic SQL, so should be @b. Doesn't need dynamic SQL either, where clause can be like '%' + @b + '%'
In case you don't see the other comments, /u/pix1985 's suggestion below did the trick for me. I'm keeping the @Color as a string -- thank you! -- and leaving the select statement unescaped, but with the filter for '%' + @Color + '%'. Thank you for your help!
No worries, all good -- I am grateful for your help! (Not sure it was your syntax at fault, either, as I went in a slightly different direction...) I agree with the other comment about normalization, but it's not my data and not really an option in this case. (To strain the metaphor, the stock checkers are adding and removing colors as stock changes. Having one option per color would mess up other stats -- e.g., # of different models in stock.)
I can't answer this as fully as I'd like because I'm away for the weekend and on mobile, so maybe this can point you in the right direction and someone else may be able to help with detail but this has been open for a hour or so without any replies so it might get you started. Apologies for any formatting issues. If I've understood your question correctly, you have the option of running the ODBC Data Sources configuration in two modes - either 64 bit or 32 bit. In each case, you get the appropriate Data Source Names (DSNs) set up within each mode. You can't display a mixture of each - the data sources are either set up with a 32 bit or a 64 bit ODBC driver and each dialog can only list the appropriate connections matching the drivers it's able to use. What you _might_ have is an appropriate connection to each database in each particular setup, e.g.: * The 32bit database listed in the 32bit instance of the ODBC Data Sources as a DSN * The 64bit database listed in the 64bit instance of the ODBC Data Sources as a DSN So, if you want to access the 64bit SQL Server database using the 32bit application, you need to create an entry for it in the 32bit ODBC Data Sources dialog. Whether you can do this or not depends on whether you have a valid ODBC driver or not (if it's SQL Server then you most likely will). This works vice-versa in that you can create a 64bit connection to a 32bit database, _if_ you have a 64bit ODBC driver that supports it. To add the SQL Server database it should simply be a case of clicking 'Add...' in the System DSN tab, choosing an appropriate SQL Server driver that's listed (I think it's _'SQL Server Native Client'_, or similar) and typing in the necessary details to access (Instance and DB, credentials etc.). If you don't have this information then you may need to speak to someone who can supply this for you, but ultimately that should get you going. I've done this in the past a number of times for Wherescape RED, a 32bit application to connect to 64bit SQL Server databases. Once the data source is set up and tested you might need to restart the application you want to connect with and it should appear next time you try. Good luck and sorry I couldn't flesh this out a bit more to help you further.
You need 64 bit and 32 bit versions of the driver installed and you'll need to use the appropriate 32 or 64 bit version of odbc to access system dsns using the 32 or 64 driver. I have seen more than one vendor put up there 32 bit drivers as 64 and vice versa. I don't know why. I assume that most of their apps only support the 32 driver and people have been selecting 64 bit apps to download for a while and I think they assume if it doesn't work you'll just figure it out otherwise you downloaded the 32 bit one thinking you needed the 64 bit and you aren't doing anything advanced.
[removed]
I see, that makes more sense. Thank you, I guess i need to think more outside. 
I think you might be talking about the session ID. Commonly known as a SPID. When you have your file open and connected to the database, look at the status bar at the bottom. That number is your Session ID. You can use it for a few things, including wait stats and KILL. https://technet.microsoft.com/en-us/library/ms189535(v=sql.110).aspx https://msdn.microsoft.com/en-us/library/ms173730.aspx
Well if you leve leave out the sql and memcache part. What you have here is a concurrency issue. What's the straight goto solution? Locking! Where you want to maintain a lock is up to you. I would probably use the database for it. 
I thought int datatypes were already very size efficient? I don't think you would be saving very much space at all by converting to binary - it's basically already binary, no? I guess varbinary would allow you to save some space where the values were significantly smaller than the max size of bigint (if it's half the value of bigint it'd be one bit smaller, one quarter then two bits, maybe?). I'm not actually 100% sure on all of that, but one thing to bear in mind for certain that as soon as you're doing joins via a function (including cast) it's no longer [sargable](https://en.wikipedia.org/wiki/Sargable) and you get no benefit from any indexing: in other words, as soon as you join your tables together your db has to do a full scan and convert of every one of your four billion keys rather than just looking up indexes. Doesn't sound like a good plan to me. I do think it's possible though, certainly in MS SQL, to set a constraint that includes a function.
If you want to go trough the entire table, I'd use a subquery and row_number, or just a having clause. If you filter for only a few of the products, I'd use an outer apply (select top 1 (...) order by date desc) What exactly is your problem thou ? 
&gt; First question: &gt; Are you using ANY type of session handling? The only solution to this concurrency issue is to predetermine who gets what and then deliver it to them. Describe what you have going and I can elaborate. Yes of course. Every user has a unique session handler so I can tell who is who. &gt; Second question: ...is this a telemarketing application? No absolutely not. 
&gt; Implement a separate endpoint that gets n numbers, add flags field to numbers so each client sets the number to "in-use" before selecting / using So I make a column called "inuse" and let's say I use a bit, 0 not in use and 1 in use. Two people come along and do a query against the table and get 5 records (the same records), and then they both do an update to set inuse to 1... how is that different than what I'm doing now? &gt; I'm guessing this is a dialler of some sort. If it is please stop, those things are annoying as heck! Nope, not a robo-dialer, not telemarketing. 
&gt; store the referenced BIGINT from the parent table as a binary or varbinary type for the love of god, WHY????
Let's say you're a Tax Filing company -- in the old days you would fill out 1 form for each person (like an Excel spreadsheet). Pretty sound you will have hundreds of thousands of these files. Now, you all that data can be housed in a single SQL DATABASE. You can bring up anyone's tax filing and view it -- rather than manually searching through hundreds of thousads of physical or online forms.
Do not return your results until the result is successfully added to the memcache. Do not allow an entry to add to the memcache if it's already present. Even if two users hit the query at the exact same millisecond, one of them will reach the point of adding to the memcache first. So you loop through * select the next phone number (where not present in memcache). * insert that phone number into the memcache (where not already present in memcache) Loop until you've allocated 5 numbers 
TO_DATE is an Oracle function, not SQL Server. In SQL Server, string-to-date conversion is implicit. If your strings-of-dates (*why?*) are all valid, you shouldn't need to convert anything.
hey thanks for the reply - I'm actually using intersystems cache...I just put ms sql cause I figured it was going to make me put something....oracle would have probably been better. heres the [to_date](http://docs.intersystems.com/latest/csp/docbook/DocBook.UI.Page.cls?KEY=RSQL_todate) documentation. 
Ohhh this sounds promising.
This might help: W3Schools - http://www.w3schools.com/sql/
google "premature optimization is the root of all evil"
hackerrank has a good collection, so does codewars
You could try this to make sure it's actually not being referenced anywhere: EXEC sp_dropsubscription @pulication='YOUR_PUBLICATION_NAME', @article='all', @subscriber='BAD_SUBSCRIBER' EXEC sp_dropdistpubliser @publisher='BAD_SUBSCRIBER' Then retry: sp_serveroption 'BAD_SERVERNAME', 'sub', 'off' sp_dropserver 'BAD_SERVERNAME, 'droplogins'
[removed]
RANK() is the wrong function to use. Use ROW_NUMBER() OVER (PARTITION BY COD_PROD ORDER BY DATE_MVT DESC)
Tried sp_dropsubscription already, just tells me the subscription doesn't exist (because it doesn't) Can't really use sp_dropdistpublisher right now, I have a half-dozen other publications and probably 30 other push subscriptions on the server. Hopefully it won't come to doing that.
I can't quite say what you're saying applies here. I was looking for a functionality that would have saved space that costs more than the ability to occasionally compute these values. I have statistics based on how populated the table is, and the savings would have been far from "evil," while the extra computations that must take place would have been "anti optimization."
Correct, bigint(20) is just a hint for display size but it's basically meaningless without also setting the ZEROFILL property: CREATE TABLE foo ( bar INT(20) ZEROFILL ); INSERT INTO foo (bar) VALUES (1234); SELECT bar from foo; +----------------------+ | bar | +----------------------+ | 00000000000000001234 | +----------------------+
&gt; So I make a column called "inuse" and let's say I use a bit, 0 not in use and 1 in use. Not where I was going but suppose it could work... &gt; Two people come along and do a query against the table and get 5 records (the same records), and then they both do an update to set inuse to 1... how is that different than what I'm doing now? You've not thought this through, the separate endpoint is the one that likely in-memory fetches an unlocked record or polls the db to get more numbers not in it's list; it doesn't update the DB per client connecting to it. That is how it is different.
[Did you try searching for that?](http://lmgtfy.com/?q=mssql+query+output+to+file) Seriously, top 3 links in results all have viable options. Also, learn to format your query, otherwise, you'll be ignored, due to your massive wall of text... SELECT incident_num AS "event_no" ,'gty' AS "COUNTRY_CODE" ,call_type AS "call_type_code" ,source AS "call_source" ,'LAPD' AS "agency_code" ,ent_datetime AS "datetime_rpt" ,city ,'GTY' AS "county_code" ,'CO' AS "State" ,dispatchers_id AS "dispatcher_id" ,dispo_code ,primary_unit AS "pri_unit_id" ,officer_1 AS "pri_officer_id" ,location AS "common_location" ,reporting_district AS "district_code" FROM dbo.gtycore WHERE ent_datetime &gt; '2017/01/01' AND primary_unit LIKE 's45%' FOR xml path ('test'), root ('test1')
Wouldn't it be best to load all the data into a staging table, and then use a MERGE statement to load any changes or new records into your final table? You can match the MERGE statement on your parent/sub store PK combo and used MATCHED / NOT MATCHED to control your UPDATE, INSERT, DELETE statements.
Hi there! Thank you for the respondse. I did not think you could simply run an INSERT within a function. If I try that I receive the error &gt; Invalid use of a side-effecting operator 'INSERT' within a function. Any suggestion?
Without knowing anything of the schema it's hard to provide any material help, but generally I'd say the query you run when a user logs in should look something like this, SELECT u.id, u.username, u.&lt;whatever_else&gt;, ot.value FROM users u LEFT JOIN othertable ot ON ot.userid = u.id WHERE u.username = 'foo' Then you can test the contents (or nullity) of `ot.value` and proceed from there.
Oh, my bad! On SQL Server, you can't do any DML whatsoever from within a user defined function. UDF's are restricted to "read only" operations, which is annoying. Looks like you'll have to stick to (shitty) stored procedures.
ok following that example how would I write basically a if then statement?
Really your question is way too vague and depends more on the system that is managing logins - at a guess I would say that system is actually where this logic should be being handled, rather than the database. Your application should read the value using a statement like /u/shaunc's, decide based on the value returned what to do with the login, and then write the necessary value back to the db using another statement. But if you were doing this purely from the database with no logic from the client side you'd do this with two statements wrapped in some logic - and because logic and flow statements are very platform specific there's no way for us to give this to you without knowing your database platform.
Do you know what this would look like as a stored proc? I need to be able to pass in the job_id from the application this will be going over.
This is not an SQL problem.
Just to clarify, your result set should include only 2 records, X1 and X2 for ID AB1 correct? Because you don't want any IDs that have a Code X3 or X4 anywhere in the result set right? I would do as /u/jacobmross said and join the table to itself, flagging IDs that have a bad code and excluding them from the select. 
Hooray! 
&gt; how many times that Customer.ID times shows up in the table Orders. the answer is **zero** for all customers reason: there is no customer id in the orders table, just order id and fruit
I see it as a powerful to with a poor user interface. Yeah, I can generate reports from multiple databases, however the learning curve is precipitously high. I liken it to the d3.js visualization library. It can generate practically any visualization you want, but it will take a while if you aren't anything short of an expert. 
Have you tried something like: PROC SQL SELECT a.ID , a.Date , a.another_unrelated_var FROM mytable a INNER JOIN ( SELECT ID , MAX(Date2) mdate FROM mytable GROUP BY ID) b ON a.ID = b.ID WHERE a.Date &gt;= b.mdate GROUP BY a.ID, a.Date, a.another_unrelated_var
It's not difficult to convert the data itself, but it's the applications that use the database that you really need to check out in more detail to see how or if they can connect to a SQL backend. [This might be a starting point for working out a plan](http://sqlmag.com/database-administration/15-steps-convert-access-data-sql-server).
Short answer: no Longer answer: depends. Access actually has a tool to help you get the tables and the raw data into SQL. Called 'upsize'. The depends part comes in with how you are using it. Alot of access dbs don't have a foreign keys and other constraints set up correctly which will slow things down no matter what the DBMS. I use to be in a role where I would frequently upsize access dbs into SQL Server, leaving the front end still in access while pointing to the new SQL instance instead. This worked out ok for short term projects but eventually you will see you have to get away from access all together if it's used as a business critical application. Good luck to you. I learned so much in my career starting down this very path you face.
I'm making some assumptions on your data but I believe this would work for what you need. You may need to convert it to your particular RDBMS. Also, I didn't attach the result data to a calendar table to provide for if a day has 0 calls then it is returned with 0. SELECT calls.call_dt , COUNT(calls.Call_Instance_ID) num_calls , SUM(web.web_pst_36_hr) num_calls_with_web FROM ( -- Get calls and their dates. SELECT Call_Instance_ID , TO_CHAR(Call_Create_TS, 'YYYY-MM-DD') call_dt FROM cust_calls GROUP BY Call_Instance_ID , call_dt ) calls INNER JOIN ( -- Flag calls if they have had a customer log in 36 hours prior to a call. SELECT cc.Call_Instance_ID , MAX(CASE WHEN wl.CUSTOMER_ID IS NOT NULL THEN 1 ELSE 0 END) web_pst_36_hr FROM cust_calls cc LEFT JOIN web_log wl on cc.Customer_ID = wl.Customer_ID AND wl.Web_TS BETWEEN cc.Call_Create_TS - INTERVAL '36' HOUR AND cc.Call_Create_TS GROUP BY cc.Call_Instance_ID ) web on calls.Call_Instance_ID = web.Call_Instance_ID GROUP BY calls.call_dt ORDER BY calls.call_dt DESC;
A count by itself will return 1 for customers with 0 orders.
&gt; Not all customers have placed an order but I need a query that runs through the list of Customers.CustomerNumber and lists how many times that Customers.CustomerNumber times shows up in the table Orders. Sounds like orders per customer, including those without orders. That means a simple left join and count will be incorrect.
Try just referencing it as Continent instead of using its ordinal position.
&gt; That means a simple left join and count will be incorrect. wut
jesus, no SELECT Customers.CustomerNumber , COUNT(Orders.CustomerNumber) AS orders FROM Customers LEFT OUTER JOIN Orders ON Orders.CustomerNumber = Customers.CustomerNumber GROUP BY Customers.CustomerNumber i guess you don't realize that aggregate function skip nulls, yeah?
As others have pointed out, there are tools to get the database moved over. I have had good success with running Access as the UI with a SQL Server back end, but I've built the Access applications to work with SQL from the start. Converting the application side can be a big job depending on how things have been done and the size of the system.
Assuming you already have a SQL server set up, you can follow these quick steps to get started. 1. Download SQL Server Management Studio (free) * Create a new database (I'd call it whatever your access file is currently named, but whatever is fine.) * Right click on the new database and select Tasks &gt;Import Data * Click Next on the opening screen * On the next screen select Microsoft Access (Microsoft Jet Database Engine) * Browse to the access file, if the file is protected enter in the username and password. * Select Next to see more options or select Finish to import This will get your data into a SQL database. You may have to link tables together again on your own if your access DB was relational. This should get you started, at least.
It's Postgres (not mysql, sorry) but the Schemaverse is an interesting way to force yourself to get familiar with a horribly documented, but entertaining, schema. www.schemaverse.com 
I know exactly where you're coming from and it's something I struggled with as well. My job *technically* didn't involve creating queries because the program did that, it just did it poorly. I wasn't able to ask for training because it was outside of my job description, everyone was super busy with other stuff, live data was sensitive/protected, and the testing database copies would have taken forever to manually populate with understandable data. Anywho, I'd recommend building out tables yourself. It'll help you get familiar with setting and looking for primary keys and if you try to build it out like your org's database, you'll get better at finding the relationships between the tables. Maybe you could even generate a database table map design from a production or test database and then take that home to create tables with! Good luck!
Pass `NULL` as the value for your identity / auto-increment column: insert into customer values(NULL,'Gerald','McMillan','555-555-5555', '281-552-6241','3553 Bird Spring Lane','Houston','TX','77077'); 
Would be difficult to convert this data to this format but I believe I have a solution thanks to /u/ihaxr. Thank you. 
[removed]
Doing things with implicit columns in sql is generally bad form. (see multiple rants on ["SELECT *"](http://stackoverflow.com/questions/321299/what-is-the-reason-not-to-use-select) various places on the internet) You don't need to have an INSERT statement for every record, you can pass many records in the VALUES clause of the statement. INSERT INTO CUSTOMER (FirstName, LastName, PhoneNumber1, PhoneNumber2, Address, City, State, Zip) VALUES ('Gerald','McMillan','555-555-5555', '281-552-6241','3553 Bird Spring Lane','Houston','TX','77077'), ('Guy','Number2','555-555-5555', '281-552-6241','3553 Bird Spring Lane','Houston','TX','77077'), ('Gerald','Number3','555-555-5555', '281-552-6241','3553 Bird Spring Lane','Houston','TX','77077'); This prevents the issue if your schema changes down the road, your inserts can still know where the data is supposed to end up within the schema. If you change your table and reorganize the columns or add new ones etc... your above inserts with the NULL value in the first column might fail, while the explicitly defined inserts will succeed (unless you've dropped one of the defined columns, then it would fail regardless. 
google FOREIGN KEY ON UPDATE CASCADE 
I am trying to get that users MAJORID from another table They both have an ID column 
Shady how? Ok, donning your manager's hat, and going only by what you've written: 1. are there trust/relationship issues? What you are describing clearly affects your work performance (" queries I struggle to come up") yet you have chosen to seek help of "internets" rather than talking to your direct manager. 2. You clearly have access to the (a?) database and you already have some knowledge about it. Why seek some other source if you can look up/learn whatever methods/techniques you need and test/try it on the relevant data source? In other words, what would AdventureWorks teach you about your data? 3. Why are you badgering your DBA for "stuff" instead of the AppDev team, unless he/she wears 2 hats? Our developers come up with a different table or change semantics of the data all the time. I wouldn't expect my DBA to keep up with these. 4. Why did you need to buy courses on your own without vetting it first with your manager? Are you in the same situation as /u/ScrollButtons (i.e. you are not really supposed to be accessing DB/writing queries)?
I guess if we're asking questions, why are you so defensive on behalf of OPs DBA? Help out or keep it to yourself, that's literally what this sub is for. Not everyone went to college. Not everyone has time for college. Not all companies have single-service roles that can take time to help teach others new skills. Not all companies have a CE budget. For some people, curiosity and learning on personal time are the only ways to pick up new skills to move on or around the company. I have the DBA's blessing to muck around all I want but all the resources stay at the office. I'm a single parent, I can't pull all-nighters at the office to muck around, I have to be home. I suspect OP is in a similar situation and these things happen in small shops. So, just be helpful if you can and not so darn possessive over databases that you've never even sneezed on. You might find yourself in a similar situation sometime soon; "today you, tomorrow me", right?
As /u/hypo11 states, you do not need to add your Constraint if it's already added. If you need to run the script as is, just make sure you remove the constraint first. Also, you are missing a semicolon after your DBMS Output line, which is throwing off the compilation. It should be DBMS_OUTPUT.PUT_LINE('blanks'); END; It's not finding that first semicolon, so it's replacing the END with one, then your END is missing.
I understand your suspicions and it's my fault for not giving enough detail, though I didn't think it necessary at first. I started learning SQL back around July or August of last year and landed a job in September that involves quite a bit of querying in SQL. I made it clear to my employers that I was still learning SQL and I was more than willing to continue learning, which they accepted. The issue is that many of the tutorials/resources available online (at least the ones I've come across) consist of bare bones exercises that get you acclimated with things like syntax, aggregate functions and the like. So, while certainly serving as good introduction, it may not necessarily offer enough practice because they're purposely meant to be stripped down to better help one understand the fundamentals as opposed to more advanced querying. This is contrary to the database I work with at my place of employment. It *is* offering me great practice, but due to the fact that I am only able to access it at work and the fact that it's a challenging database for me personally to work with, I came on here to see if anyone else could recommend a complex database available for free somewhere online that I could use to practice at home. I'm trying to accelerate the growth of my abilities as quickly as I can as I stumble often at work given that I'm still learning some things. So, I suspect that because I'm still new, both as an employee and as someone learning SQL, and because of some of the sensitive information contained in the database, I don't have access to it at home. I'm not using or trying to use alternative databases to learn more about the data I work with at my job because clearly that isn't going to work. I'm doing it so I can practice how to approach building more complex queries and explore more advanced topics. Again, while technically possible at work, I don't have all the time during the work day to do this, which is why I wanted an alternative resource. As for the DBA at my work, he wears more than just two hats. I work at a startup where there are just 6 of us and he and I are the only ones that know any SQL. He has a multitude of other responsibilities as well which is why sometimes I can get his help and other times I can't. 
Not sure I'm completely following, but row_number could be what you're looking for? Row number will basically break out all of the records for one field, then "rank" them by another value. After that, you would just need to set it to return the 1st value for each fruit (the cheapest). It'd work something like this: select * from (select *, row_number () over (partition by fruit_name order by price) as row_num from fruit_table) q where q.row_num = 1
Ack. &gt;&lt; Thank you!
That happens when you try to put something big into something small.
Do what Ptad suggests but perhaps use (forgive my pseudo code I'm on mobile) Rank() Over Partition by x Where Rank = 1 
Laying in bed and on the phone so can't type it all out. Assign the value of 1 to each record and the where sum is two and Course IN('xxx','yyy') Edit: Since this is a course the method should already have been covered. Don't use one of our answers if they don't use something already taught. There are multiple ways to solve a problem. 
An alternative to using COUNT() would be using INTERSECT. Though if you're doing this for many course codes then there's less to write using COUNT(). SELECT StudentNum FROM Student_Course WHERE CourseCode = 'DP036' INTERSECT SELECT StudentNum FROM Student_Course WHERE CourseCode = 'DP070' 
It would go along the lines of this: USE [Relevant_Database] GO CREATE PROCEDURE [dbo].[insert_proc] ( @job_id int --comma separated list of all your parameters, you can also specifcy static defaults as well ) AS BEGIN INSERT INTO [dbo].[table](job, value, type) (SELECT @job_id, value, type FROM [dbo].[default_table]); END
This times 100!
when you say "join two tables" do you mean JOIN or UNION???
if you look at the data supplied by OP, it's pretty clear OP wants to return `stories.id` and not `names.id`
Can you post the real query you are making and then it will be easier to explain/solve the problem.
I missed the ID column, thanks!
Yes it is possible. You can link tables using ODBCs and that can be from any source. I already used SQL Server, Postgre, Firebird and MySQL with Access. In older Access version (before 2013) has the ADP option where the project is directly connected to a database, here where I work has an ERP where the front-end is all in Access as ADP with SQL2014 as DB.
I agree with the last part about having @EDate as '2017-02-28 23:59:59.000' not being the best practice. While it is small, basically anything that happens between 2017-02-28 23:59:59.001 and 2017-02-28 23:59:59.999 is lost and not would not be included in any month. If I have have anything that is Start1 to End1, Start2 to End2 and so on, I'll usually always have End1=Start2, End2=Start3 and so on. Makes it easier to read and prevents any chance of orphan data that falls between end1 and start2. 
so each customer is an employee? and each employee is a customer? because that's the only way the join will work
Totally disagree. I cannot STAND working on code that uses or attempts to use meaningful aliases. First thing I do is strip them out and go with sequential single letter variables + append comments at the top as to what they actually are/do. It is so much easier to deconstruct and rewrite a process for optimization from my perspective. edit: I do name CTE's meaningfully, but then give them a single letter alias, e.g.: with raw_data as (), monthly_totals as (), monthly_medians as () select from monthly_totals A left join monthly_medians B on B = A If/when I need to, its easy to write out equations on paper to look at how A is joining to B... it becomes much more mentally cmplex for me to do this with names, especially when you can have 10+ names that are similar (e.g. reqd, reqdis, reqdrm, etc.)
I agree with your disagreement! 
You can... but it might not be the best solution for you. With powerquery or powerpivot, you can pull the data directly into Excel. But some other questions for you: 1. How are you planning on entering data into the database? Is it loaded through a general ETL process or just uploaded, or entered in via a form? 2. What types of data manipulation is needed with this data? 3. How much data are you talking about? 
I agree with aliasing but they should be relevant to whats being aliased. WITH raw_data AS (), monthly_totals AS (), monthly_medians AS () SELECT * FROM monthly_totals mt LEFT JOIN monthly_medians mm on mt.key = mm.key Unless you how some restriction on space: WITH H AS(SELECT i=0 UNION ALL SELECT i+1 FROM h WHERE i&lt;99) ,N AS(SELECT i=ROW_NUMBER()OVER(ORDER BY(SELECT 1))FROM H,H A) SELECT*FROM N 
Yeah, what I'm saying is that when I see mt and mm I want to shoot someone. It is incredibly annoying with. A is my first set, then B... then C... etc. You end up with a dozen two letter acronyms that start with m and try to write out an equation representing joins to visualize the set and you'll be constantly going, "what was mm again? oh yeah..." 
the "hours" part of this is tricky. Assuming that they just want the Computerization project hours reported (and not the total hours the employees worked including Computerization) then I believe this will work: select distinct e.ssn, e.lname, wo.hours from employee e, works_on wo, project p, dependent d where e.ssn = d.essn and e.ssn = wo.essn and wo.pno = p.pnumber and p.pname = 'Computerization' and wo.hours &lt;=10 
If there are only two identical columns, just loop over the returned dataset. First time ID appears is from the select..FROM table, second time ID appears it will be from the joined table. Can make it explicit by typing select a. *,b.* from a inner join b on a.id=b.id
We are going to cheat a little and use distinct (look into this function) when looking for more than 1 dependent. We are also assuming essn and ssn are the same I like to create basic queries then piece them together, one approach not the only one. First write a query that retrieves a list of employees ( this will be the base query) Select ssn, lname from employee then write another query that will retrieve employees who worked less than 10 hours and on compterization project Select essn, hours from works_on where pno = 10 -- pno = 10 based on data from table project (Select pnumber from project where pname = 'Computerazation') and hours &lt;= 10 -- filter out anyone over 10 hours then final query, find out dependant select distinct essn from dependent -- distinct will not show duplicates on ssn, you can also do a count but it is a bit more complex and something you should definately familiarize yourself with. Now its just putting everying together. I merged the first two queries assuming essn and ssn are the same. then i added a subquery to control for dependants. Select e.SSN, e.lname, wo.hours from employee e, works_on wo where e.ssn = wo.essn and wo.hours &lt;= 10 and wo.pno = 10 and e.ssn IN ( select distinct essn from dependent)
That came up with no results. edit: I had not loaded the dependent table, so now it works fine :) thanks man. 
Thank you for the explanation, however like the other guys it's not showing up with anything once I run it. Could you run this using the table and tell me if it shows up anything for you? I'll come back and edit this post after I look through the table trying to see what should be returned. edit: It looks like only 1 result should be returned Franklin. He has 3 dependents. I'm not sure why it's not showing up for me. Could it be because I need to use Where dno =(... dno = dnumber...) type nested query? If so I don't understand why that's the case. 
YMMV, but it is considered to be the best practice to not rely on implicit column list/ordering (in 'select *' and 'insert' scenarios). 
Thank you very much, I had not loaded the dependent table. It now loads the correct row! Can I see how it is done with count instead of distinct though? 
Don't read too much into it. I was basing my solution off of the past problems we were solving and trying to convert it over. I was just wanting to list out the names of people who worked on the computerization project. Can you give me an example of using count instead of distinct? 
less than 10 hours is also a condition. It should be Wong. 
As /u/ihaxr says, changing Where e.ssn = (Select essn..... to Where e.ssn in (Select essn..... fixes the syntax error, BUT this will return everyone who has worked on either one of the two products you've given, not people who have worked on both. Since this is homework I'm not going to give you the answer outright, but your approach is *almost* correct. Think about how having two subqueries in your where clause might fix your problem. 
Thank you very much for the explanation. I'll give that join a try. edit: It's not returning people who work on both, I changed the or to and, and it doesn't return anything. I'm guessing that's because it can't check for both things. How could I fix that? edit2: got it. 
I got it. Thanks for the help :) 
In your where clause, couldn't you also do where (e.ssn = ... and e.ssn = ...) ?
This right here is the million dollar question, specifically #2 and #3. 
Yep I just tried it on my server. This is using the adventure works DB. select count(loginID) from HumanResources.Employee where (ManagerID = 21 and MaritalStatus = 'M') The results returned were 8. 
Have them point to the listener ex: AGLIST_Test and it will create the db on the primary server for the sake of getting them in the habbit of always referring to it that way. When they are done you will have to sync the db to the secondary server (this makes it h.a.) don't forget to clone any server level accounts and their roles from the primary to the secondary, otherwise when you fail over the accounts will only exist on the database level and they won't be able to use them. And when I say "clone" I mean use a script that can use the same SID's so they automap to the database level accounts that are already being replicated. I happen to set up two more of these things today.
Interesting, didn't think of this. It seems to a lot faster this way. Appreciate it.
Why are you doing SUM()? If you do select sale, sum(whatever) you need a group by clause since it's an aggregate function.
Thanks for the response. Generally the AG's won't be at the application DB level so it wont be every time that I need to create a new AG, as in, we will probably be grouping different apps together. I'll be looking into the login sync as we go but generally there are internal groups that are server level apps and then the vendors / apps will have their own DB level permissions / accounts.
Thanks for the response. It def sounds like I'm heading down the correct path. One question you may be able to answer. The DB's themselves syn permissions / accounts, does that apply to DB owners / creators? So our current process for a vendor app is: 1. We have a custom role that only has the DBCreate permission 2. We create an account for the vendor / app applied to the role 3. The app creates the DB's, which automatically applies the DBO role to the account So I guess the account itself will still need to exist at the server level? or would the auto-sync (using the AG) pull over that DBO role / user? I'll be testing further today, just thought you might have the answer off the cuff!
I learned the hard way that it only replicates stuff that resides within the database: things like database level accounts and permissions, objects etc. But things which exist outside the database will not replicate over: server level accounts and roles, sql jobs. The fewer accounts and jobs you have and the fewer changes to these things you have the better. So the account at the database level will replicate across along with the db owner permission. But until you clone the corresponding server level account: they won't be able to use it.
Gotcha. Good thing that I asked that because that is 90% of our use cases at the moment!
Could you elaborate more on this: "Mixing attributes with different semantics into one column is almost always a bad idea." The end data ends up supporting pivot tables and dashboards in excel for the most part. If each currency was it's own column - the measure in the dashboard would have to be swapped out depending on what currency the user wants to see. With it all in the same column, the dashboard can be built out and adjusted via a filter on currency. Curious to hear more on the main reasons for avoiding. I have a similar dilemma with other data that breaks out sales be probability of closing. I always go back and forth of keeping sold revenue in one column, potential revenue in another or combing and adding a % to close field if that makes sense. 
Although it would be ideal, asking to set aside some time just for practice is difficult at the moment and perhaps even for the foreseeable future. Because I'm juggling several projects at the moment, all my time at work *must* be dedicated to completing the assignments I've been given. So my interaction with the database as it stands is less for my own practice and more for completing what I've been tasked with. This is the current state of my job and the company, for better or worse. As I've mentioned before, we're an extremely small startup and each of us has more than enough work to handle individually, so there is virtually no time to set aside for practice during the work hours. Whether this is an issue or not is a separate matter all its own and already far outside of the advice I came seeking here. As I've explained previously, since I cannot access the database at home, I came to this forum for guidance on obtaining another database, identical in structural complexity, so that I can practice while at home given that I can't access the database there. That's all there is to it--nothing more. 
I'd like to try answering my own question haha, I think I figured it out. In the first code bit, WHERE is referencing an alias for the result set which has yet to exist. In the second code bit, WHERE (the outer one) is referencing the column from the result set which does exist at that point.
[removed]
That's about the size of it - check out SQL order of operations for more info, here's a quick [link](https://www.bennadel.com/blog/70-sql-query-order-of-operations.htm). Basically the WHERE executes before the SELECT, so cost does not exist at that point in time. Another thing to consider - you've based your predicate (and ORDER BY) on a calculation, which means indexes can't be used - it has to be calculated for every row to return the rows that match. It may be inconsequential for your current data set, but it's a bad pattern in general.
No, we haven't changed any users on windows or the database, the only change was a new schema. 
Thank you all for your help this ended up working for me *sourced from a friend who didnt post on reddit* select "StudentNum" from "STUDENT_COURSE" where "CourseCode" in ('DP036', 'DP070') group by "StudentNum" having count(*) = 2
&gt;It looks like I need to normalize the color name, so I can have a display name column Dark Café which gets normalized to dark-cafe, is that &gt;correct? Ideally, I'd like to use the normalized name in the resource url. Standardized! Always standardize. Normalising is a good practice (in my opinion for this situation). You don't have to though. &gt;The user might enter a typo in color display name, and then correct it later. This means that the composite key of that row will change later, &gt;which also means that any other table rows referencing to this row need updating. Is this a problem? No, but yes. What I'd advice is use a third table **Product_has_Color** ('Product_idProduct', 'Colors_idColor'). And in this table use a composite key of those two columns. This way you work around typo's because in the colors table you have a *PK* and a *Name* column. The name gets updated when the typo is corrected but the key stays the same so no cascading is needed. &gt;What if the color name is kind of big, say more than 100 or 200 chars. does that affect the performance? The more data you have the slower it gets. Period. Though you probably won't notice the difference.
Are you already settled on Oracle accessed via access? And does your company have any intranet already built? Any chance you have sharepoint? My only reason for asking is that Oracle for this type of project seems overly complex. There are many simpler solutions that'd probably solve your issues and still give you functionality.
[removed]
Nope, haven´t started yet. An Intranet is existing and we do have SharePoint here in use (Never seen it, but I´ve heard about it). Would MS SQL be the better solution then ? I´d prefer some SQL solution just to be future proof, because when I`m changing to a DB solution the amount of users will increase, until now it´s just a handful but I´ve integrated a lot of statistics already (into the Excel solution) and it seems to be interesting for a lot of people, so I can predict that it could be widely used in no time. 
Well, SQL Server would probably be better if you already have that environment. My reason for suggesting Sharepoint is that it seems the things you want are already built into it. Further, it's meant for multiple users. Everything in sharepoint is based around a list. AND you can still view or export data into excel to do your stats. Things you mentioned earlier that work easily in sharepoint. * Inputting data via a form - these care auto-created with every list, and can be customized. * Add attachment with zip file - has this feature and is easy to use, using SQL server or Oracle, it'd be difficult to implement without another application. * Version control - this is a setting, but it can be implemented, and keep track of everything. * Workflows - the best part of sharepoint is workflows that can send emails, reminders, directly to your email accounts, and track work. Now, i know that you seem excited about doing something with SQL, but i think converting a spreadsheet into a database is a bit more complex. 
&gt; My goal is to find where Active = 0 and they do not have Active = 1 SELECT UserId FROM daTable GROUP BY UserID HAVING SUM(Active) = 0
Assuming UserId has an index, you can probably remove DISTINCT from cte to prevent an extra group operation. In the second query replace UserId NOT IN with NOT EXISTS (SELECT 1 FROM cte where cte.UserId = Table1.UserId). Instead of checking the entire cte to ensure a non-match, the query can eliminate a row as soon as it finds just one match from the cte. 
Use ROW_NUMBER, RANK, or DENSE_RANK then a predicate to exclude duplicates.
This - use both pieces (removing distinct and using 'not exists').
Because if you use two tables you will always have duplicate data.
Whenever I remove ph.rate from groupby I get throw this message "Column 'HumanResources.EmployeePayHistory.Rate' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause." Am I using the aggregate function AVG wrong? 
&gt; I should use a composite primary key, composed of product_id and color name. this is essentially the third table, the many-to-many relationship table between products and colors your color table in this case would have color name as PK... some people do not bother declaring this table if they don't care about which colors get added to the database
I love running my mind with ""pseudocode"" (~~english~~, part way to code,~~code~~) when developing a solution: select patid, case when datediff(disch1, admit2) &lt;= 30 then null else N+1 end as Num_SeqAttacks (I format weird)
&gt; the many-to-many relationship It's a one to many relationship. My question is not about a 3rd table at all.
can a product have multiple colors? pretty sure it's yes can a color have many products? in other words, are you saying that there will only ever be one single red product? if not, then it's many-to-many, whether you implement it with a proper stucture or not
Is there an importance to the rate change or does your query only need to use the most current rate? If only using the latest one then you should limit the rate selected from that table to only that rate. If you only have one rate per department you won't get the dups. Also, lol for the dick column. 
 SELECT foo FROM tableA INNER JOIN tableB ON tableB.AccountNumber = tableA.AccountNumber AND tableB.transdate BETWEEN tableA.StartDate AND tableA.EndDate
Products A has color `{name: "red", price: 10.99}`, product B has color `{name: "red", price: 999.99}`. Yes, everything is fine.
I think the basis of your question is why learn SQL to query a database; knowing SQL means you can build databases, create tables and relationships etc. If you only want to get simple data from a SQL database then MS Query will help you with that, but even trying to pull more complicated data will require an understanding of joins and alike. 
avg is fine. you're not using any aggregate on the DeptYrlyBudget column
Thanks so much!
I guess it really depends on your job function and role. This question is of a similar nature to why learn to write JS, CSS, and HTML when you can use a page builder. Or any such question about any programming language that has a "drag and drop" style interface available. I'll admit I don't know MS Query, but I would assume from the description that some one on the backend used SQL to build the tables and define the relationships that power It's ability to write those queries. Being limited to just querying data is only one small aspect of the SQL language and if that is all you use, then great. But for me I would rather have the ability to build Stored Procedures, Functions, indexes, and keys, and understand how to optimize it all by tweaking. 
Well, if the query builder meets your needs, then you don't need to learn it. I know that for me, I needed to learn the nuts and bolts because query builders didn't exist back in the day. Knowing SQL has helped me with query optimization (Oracle) and troubleshooting problems.
Bravo. I love stepping through insane stores procs. 
A query that runs once versus a query that runs on a regular basis has entirely different needs in terms of optimization The query generators don't typically understand how to optimize a query or how to optimize the database to help queries 
That is very true. People like to think that all these systems in use at companies are new and shiny when in reality they are not. I know the companies I have worked for always use a variety of software and systems that span a couple decades and are interfaced together on a prayer. 
Not familiar with overlay, but is this what you're looking for: [OVERLAY Function - PostgreSQL to Oracle Migration](http://www.sqlines.com/postgresql-to-oracle/overlay).
&gt; using dynamic SQL in a terribly convoluted way Personally, I consider dynamic SQL of any kind a code smell. In my admittedly limited experience, its always used trying to do too much. 
How are your SSRS skills? I have to imagine that SSRS + Finance ---&gt; business intelligence job. Bank IT and application support would be another option. Have you considered doing client support for financial software? That would get your foot in the door for development.
http://www.w3schools.com/sql/default.asp 
There's more intricacies of SQL than what MS Query is able to represent. It'll do some standard queries fine, sure - but then at the same time someone who knows the syntax and has some code completion can probably type those same queries in 1/10th the time it takes you to drag and drop bits around in your GUI. Most complex queries I write on my production systems need to make use of temporary tables for intermediate result sets, so as not adversely affect business users. Or they end up being multiple levels of nested subqueries. Or they have flow logic and loops. None of these can be done with MS Query.
What is not enough? And it's not enough for what? How do I get my foot in the door somewhere using SQL? My buddy who works as a BI Analyst said I should be looking for BI roles. 
Read this: https://www.reddit.com/r/excel/comments/5p3ptr/what_good_jobs_can_you_get_with_strong/
I've never heard of client support. I'll look into it. Thank you. 
Wait so you don't know sql but you already got the job?
What is the job title and where are you located? I can't find a job and I know SQL and other languages...
&gt;right joins That's how you can be 99% sure it wasn't written by a human.
&gt;GROUP BY t1.id, t2.fieldA, t3.fieldB, fieldC I could be missing something, but MS SQL doesn't allow you to GROUP BY on a label (fieldC)
I guess I get fascinated as the stuff people can pull off in a stored procedure. But your right about dynamic sql. I've worked with some people who used it WAYYYY too much. No fun when you have to work through that. 
Ideally I'd say you should be looking for a junior position to get your foot in the door and start building on the job experience. We're recruiting a 3rd line sql dba and we won't give an interview unless the person has a few years on the job experience. Look for a junior role, get the experience and then look to progress from there. Doing labs at home will only get you so far and unfortunately they won't make you stand out from other applicants.
Right, done that myself a few times -- hence the 99%. I've yet to meet anyone who used them more than that.
What do you mean by "overused"? Multiple updates to the same table in separate statements? If so, I agree. Though, relatively low on my list of "what really grinds my gears" compared to the ALTER TABLE statements I found inside of a transaction in a stored proc put out by our CRM vendor. Yes, I'm still salty. 
Look to get an entry level certification like this: https://www.microsoft.com/en-us/learning/mta-certification.aspx
Have you tried contract work via a recruiter? TEk Systems, the Computer Merchant, 4 Corners, Ciber, Catalyst, etc. there are a ton around and they all: 1) are paid to help you find a job, so they don't have a motive to waste your time or theirs 2) have an "In" with the companies they deal with so their candidates are among the first reviewed. 3) can usually give you a long contract or contract to hire position I had great experiences as a contractor when I first started. Pay was great and the experiences were wonderful. When I'm looking to build a team, I usually go contract first because they don't require on-boarding or benefits costs. This also allows me to give them the boot if they are not working out without much of a fuss. The ones I like, I offer a full time position to. The positions I would hire you for with your skill set are: -System Analyst -Reporting Analyst (I wouldn't care if you knew SSRS or Crystal, Microstrategy, etc. That is trainable) -database developer -Data integration specialist/analyst -.NET developer
It doesn't. I used some short cuts in the sample that won't work in all databases just to keep the query a little easier to read and pick up. I know a lot of people on here are in Oracle/MS SQL shops and that query wouldn't run on either but there are some where it would. However, the OP was asking about MS Query which can connect to any ODBC database. 
What!?!? Why would you be altering the table in a set procedure??
Merge statements are *extremely* powerful if a developer knows how to use them. You can even use them implement a type two slowly-changing-dimension in pure SQL, without any outside tools. 
I'm not sire they are more efficient. In fact I think on the msdn it says it most cases it is not, but I could be wrong.
There are situations where it's required, but it's safe to be critical of any dynamic SQL until its real purpose is found out.
I have also hit the cant convert data to int issue. I'm not sure I follow your answer though. EDIT: Of course it was right after I caved into asking for help that it clicked. 
Well, it depends. If you've got some stored procedure that can be run in "Mode A or B" and you have a predicate like WHERE ( ( MODE = A AND ColumnA = Foo ) OR ( MODE = B AND ColumnB = Bar ) ), that sort of thing actually will confuse the optimizer and lead to worse performance. In those cases you would be better off with dynamic SQL. So I think, besides the situations where you HAVE to use it, there are also some where you don't need it but it's still the best thing to do.
I'm stuck on the second part. I've read over this and I've looked at a ton of pages about how inner join on works. What am I missing? EDIT: Of course it was right after I caved into asking for help that it clicked.
What in the world are you talking about? "overused"? And since you said "especially" if they're overused, I take it you don't like them in general, overused or not. Why not?
You know about the Save feature of Reddit, yes?
I was on mobile on break. I didn't want to spend time finding where the save option is in mobile.
I'd probably find just writing SQL to be boring too, but my employers never seem to run out of new hats they want me to wear. Some of the more interesting things I've been able to do are machine learning with Python, Jupyter, and sklearn. Data integration/APIs with Node, Koa, KnexJS. Web UI with React, Redux. Spatial visualization with D3.js, leaflet, mapbox. They're all involving my SQL and BI work to some degree, just branching out. Keeps things interesting at least!
That makes sense.
Nice, thanks for the reply. I appreciate it. Was looking into RMAN scripts a little yesterday. I'll be learning more about them this week coming up and in my homework this weekend. This is some good insight on what I'll be learning. thank you,
I'm content. I enjoy writing queries and find it rewarding. I dabble in bi and find formatting reports putzy. I can do it and it's not terrible but it's also not as satisfying as getting a complex query working. At my current role, I really miss having a business analyst to translate what the business people want into technical language. I find myself rewriting a lot of code because I did what they asked, not what they wanted it that the goal Keeps evolving.
Well, that's a good counterpoint and really the toe-dipping into a larger discussion, because *my* counterpoint to that would be arguing readability and extensibility over performance, but *then* we get into the discussion of "is clean dynamic SQL as readable and extensible as vanilla SQL" and really the answer to all of these, as with nearly anything in SQL, is "it depends." But yes, I agree with your last statement. What I don't like is when dynamic SQL is used not as a refactoring tool used with an eye to future use, but to just like make small sections dynamic, and then copy-pasting *that* code throughout the entire thing...you might be working with more competently-written dynamic SQL, I work with code literally written in the 90s.
&gt; The positions I would hire you for with your skill set are: -System Analyst -Reporting Analyst (I wouldn't care if you knew SSRS or Crystal, Microstrategy, etc. That is trainable) -database developer -Data integration specialist/analyst -.NET developer Okay. Great. Thank you for helping me see where i fit into a job market. Where are you located? I am in the midwest.
Well, I got rejected from a Database Analyst position because I didn't have enough data warehousing experience. I found the decision to be strange because one of the people interviewing me had a degree in psychology and had only worked with SQL for 6 months prior to interviewing me. I just want to find out what job I should be looking for and how to get that job. I work on my own projects and practice daily. I just need to know what I am doing wrong and what job to apply.
Are you saying you prefer MERGE over UPDATE for performance reasons, or that stylistically you would rather track historic changes, disable the current record and insert a new one?
Web development and web design are different fields and skillsets entirely. Web devs usually get html from a designer then add the code behind for buttons etc. You rarely do both unless you're a full stack engineer in a start-up.
I'm relatively new to the game. Went to school for Econ with an interest in market research. Graduated 5 years ago. I've been messing with SQL for a little over two years in analyst roles, though I miss being closer to the operations side of things. Right now I'm starting a new gig that requires a lot of SSRS, and I'm hoping that I can turn it into a more operations and analytics related job, though I'm not sure what I skills I should be focusing on to get there. Overall, it's been pretty rewarding, but I see it more as a stepping stone to something bigger. 
I'd have to look at your logs but I've seen cases where the change data capture and triggers documenting updates to CDC conflict. The replication might just be a misnomer 
No, just using update statements for the purpose of data manipulation and not actually updating of records. See edited post.
how do you deal with long, nested queries where shit is just joined and nest-seelected from like 100 tables? I think im okay @ SQL, meaning I can do basic select statements and joins and whatnot but when it comes to big queries I feel lost. it makes no fucking sense and i get frusterated :(
its pimp when you have no responsibilities aka single and no kids to take care of 
As a person who deals with very convoluted stored procs/queries, it's all about finding a pattern, imo. I've found long queries to have a certain pattern that builds. Am I querying on tables that all have info that I am particularly interested in? What do these tables share that I am particularly looking to pull? Once you can find that, then you can build on editing/updating the query. 
So you think I can get any of those jobs?
If you are talking about looking at someone else's code, sure if it isn't documented (well) then it is gonna take some real onion peeling to get through it. When you write it from the start though it just builds incrementally. You don't have to understand the whole thing then write it down, you build it.
that was the problem..no documentation and the abbreviations that were used for the tables were confusing as hell. the nested part made it worse and hard to follow. 
A way to store a query is a database view. It's basically exactly that. You make a view that has any filtering, grouping, or joins, then you just query the view as of it is a table. I haven't used sqllite, but you're definitely looking for a view. You could also use a stored procedure of you want to pass parameters. https://www.tutorialspoint.com/sqlite/sqlite_views.htm
Does this also apply to = with OR?
https://www.youtube.com/watch?v=cDGlN6mluGA
&gt;[**No sir, I don't like it [0:06]**](http://youtu.be/cDGlN6mluGA) &gt; [*^Triox*](https://www.youtube.com/channel/UCnCwLB42iReE0bTJ-AKnujw) ^in ^Film ^&amp; ^Animation &gt;*^441,161 ^views ^since ^Oct ^2011* [^bot ^info](/r/youtubefactsbot/wiki/index)
As someone who's interested in this career path, what are some recommendations you would give to. Newbie sql learner ? Major in business decision science (business management analyst)
You might not be doing anything wrong. The job search process is not a straightforward endeavor and you don't have insight into what's going on in the mind of the person across the table. Maybe they hired someone else because of nepotism. Maybe they didn't like the color of your tie. There's no telling. That said, there are probably things you could be doing better (which is true for everyone), but don't start with the assumption that you're doing something wrong. That can be counterproductive. There are many options to [volunteer](https://www.freecodecamp.com/) outside of churches. (Though there are churches where you could volunteer without being a Christian anyway.) So, /u/admiralwaffles point about volunteering to build out your portfolio of completed DB projects is a good one. And volunteering is a good way to stay sane while going through the grueling and lonely process of the job search, because it provides the opportunity to work on interesting things and interact with other humans. If you're interested, pick something that you like and want to get better at and you'll be enhancing your skills at the same time you proving them and you'll create professional references you can include in your job applications. You mentioned you have built projects with your new skills. Do you have them listed in your resume or cover letter? And do you have a way to show them to potential employers? Being able to demonstrate an interesting portfolio of work communicates much more to a hiring manager about your actual skills than saying that you completed some online courses. For general job search advice, check out some of Donald Asher's books, like: * ["Cracking The Hidden Job Market: How to Find Opportunity in Any Economy"](https://www.amazon.com/Cracking-Hidden-Job-Market-Opportunity/dp/158008494X), or * ["How to Get Any Job"](https://www.amazon.com/How-Get-Any-Job-Re-Launch/dp/158008947X) He has some good insights into how to be more efficient at the search process and to find job opportunities that aren't immediately obvious.
Log table where every row is a version of a row in the main table. Look up slowly changing dimension. 
Should I consider making one table with all 6 (this number will increase in future) categories of data, or should I make a separate table for each group of data? Also, is it considered okay if the number of rows gets massively increased? Should I worry about it, and if I should, should I be splitting that table in more tables (for example one for each year or something like that)?
Weekend is nearly over, so had any luck?
Since we're working a lot with historical data, I am looking to optimize it as much as possible. I have to ask, is the term log table/log timestamp something specific or is it just a normal table? 
normal table. 
Ok, thanks a lot! 
For what it's worth - I don't think of it as the 'WHERE IS' operator, I think of it as the 'WHERE' &amp; 'IS NULL' operators.
I have never seen IS used with anything else other than NULL and NOT NULL.
Pickybacking everyone here, the IS operator is used in conjunction with NULL or NOT NULL. A few notes on your SQL. WHERE FirstName LIKE 'James' and WHERE FirstName = 'James' are the same as the LIKE statement doesn't include a wildcard so therefore it will act just like an equals sign. Also, a little trick if you want to do a search on multiple columns (and are not necessarily worried about index performance you can use COALESCE. COALESCE returns the first non NULL result in a series such as FirstName = COALESCE (@FNVar, FirstName). In this case, if @FNVar is NULL then FirstName = FirstName thus no filter in the result. Very handy for scripts and SPs.
.Net application would be a good next step. A lot of people are going to web apps and staying away from your traditional desktop .net app. I would almost recommend you look into visual studio Lightswitch, but then again I cannot recommend it because there hasn't been much movement from Microsoft on this in a couple of years. Lightswitch was the stepping stone for me to develop my skills and move into a full Angular web app.
Sample databases, like Adventureworks, are not intended to teach relational database design or SQL database design. They're intended to showcase a product. In this case, it's SQL Server. [Here's a StackOverflow answer](http://stackoverflow.com/a/5471265/562459) that's about a somewhat similar table structure.
Thanks. Do companies normally store customer and staff details in the same table? Wouldn't that be a security issue?
I appreciate the effort! I look forward to hearing back.
It makes sense not to have two tables storing the same type of information (name, address, personal details etc) when all you need to distinguish the two (customers and staff) is a simple typeID column. Sounds like that is what the PersonType column is doing. You'd then have a PersonType table with one column for ID and another for the type of person, IE customer, staff etc. Makes a lot of sense actually.
I supposed BusinessEntityID could correspond with a CustomerID, IE all customers have a BusinessEntityId but not all BusinessEntities are customers in the case they are vendors, suppliers, or anything else that might not necessarily be a "customer". The BusinessEntityID column in the people table allows multiple People to be related to a single BusinessEntity so you can have multiple people in the People table that are all related to a single BusinessEntity and/or Customer and not clutter up the BusinessEntity or Customer tables with a potentially endless list of People. Then again I've never looked at this database before, just going on my own personal experience.
This should set you in the right direction: https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:49818662859946 1. create a logon trigger 2. reference the current user with the *USER* keyword
Better yet, put all subqueries into temp/volatile tables and join to them instead. Depending upon the database and configuration, this usually also improves performance by freeing up resources by breaking it up into separate runs. I'll then usually put that into a stored procedure with a error handling template if it has to run daily. That allows me to find what specific step that is failing in order to resolve data issues. If the job errors out on a given day, I run the procedureagain to the failing step in order to generate the temp tables in my session and can debug. If a set query is an absolute requirement, such as for a BI report where you decide you want to avoid building a data model and/or want to ensure that the database does most of the processing, I'd go the indentation a clean alias route. Worst case scenario was on time where the query of another user was so mess and long that I printed it out and highlighted the different parts. Would be great if there was an editor that could automatically change text color based on each subquery. Started in CRM software 10 years ago, became a BI report dev then data modeler, now do almost entirely just SQL and Python for financial audit. Starting to get involved in NoSQL platforms as well. Got an MBA along the way to manage a team, but never stop writing my own stuff. 
what exactly do you need help with? the plsql? (just create a function in the DB that selects the table has an 'if' and an update statement) the running this when the user logs on? build a trigger on user logon and run that function do you have a problem accessing the user connecting? theres a constant for that named USER I dont understand what you mean by returns a value? returns the value to where? to me this sounds like a simple procured that does this and is launched on user logon by trigger.
you should select the max of price_history_id beforehand. put it in a variable. then use the variable to insert. also item_id is coming from nowhere what about using old or new? 
I've been trying to work with the PATINDEX to figure that out (but replacing every number with a space). I got that far, and then was trying to figure out a function that separated them. The functions are still entirely new to me, so that's been challenging to wrap my mind around. I should mention that the passwords don't have letters in alphabetical order just mixed in with numbers. It's much more complicated. So for example, one password is 62f0a68a4179c5cdd997189760cbcf18. The longest alphabetical sequence in that is "cd". Some of them don't even have an alphabetical sequence of more than one letter (e.g., 1c3a8e03f448d211904161a6f5849b68), in which case I'll go with the character that's closest to "a" in the alphabet, I suppose...
I need help with creating the function because there is a system set up that I can tell it to run a function when a user logs in. This is for a front end system that connects to the database. &gt; I dont understand what you mean by returns a value? returns the value to where? I want it to return a value because I need it to redirect them to a certain web page if it could just redirect them to the page based on the value such as 1 = mail 2=google etc.
The Stanford Intro to Databases Course is really good especially if you want to get an intro to database theory. But, I wouldn't consider it especially good for learning SQL. They touch on SQL. But, it doesn't focus on it so they can cover other concepts like relational algebra, XML, JSON, etc.
The item_id in the trigger's insert is what is wrong. Remove it and try to compile the trigger.
Is there a reason you're not using identity to create primary keys? I've never seen anyone go down the path you're headed and come out successful. Identity exists in SQL for the exact purpose you're talking about. If you must keep doing it the same way, you could create a function that would select the max ID value (using the datetime only) and if it's the same as the primary key you just generated, wait a sec, and try again? This is such a dirty way of doing it... I feel dirty even typing it out but sometimes you have to do what you have to do.
Yes I plan on sending it user ID for it to locate and check if they have been redirected before I am just having some trouble coming up with the function.
I have been a DBA for about 4 months. I love it. I've been doing help desk work for the past 10 years. It's nice to have a focus. I love puzzle solving. My company pays for Pluralsight, so I have good training material. I have good mentors, which is the biggest thing for me. 
looking at this I absolutly hate myself cos I always scream on people who dont know ho to identation. so now I hate myself.
I've actually found that maintenance on dynamic SQL is much higher than any repetitive code, and it tends to get quite ugly very fast. Any of the tools I've used in the past (extended events, SQL Sentry, etc) also tend to provide little or no insight into performance issues where dynamic SQL is being used either, as SQL Server just doesn't treat those statements going through the sp_executesql proc the same as traditional statements. If you have the option you could do it in a different language - either in a CLR stored proc, or outside the database in an entirely different layer (Entity Framework, Knex.JS, etc) though that's not always an option.
Okay.. accepting that you can't rework the system you're using, can you set up a memory table? I'd expect that the memory table could be limited in size because you're never creating new records with yesterday's timestamp. ALTER FUNCTION [dbo].[ipf_fn_getKey](@inDate DATETIME, @SlNo INT) RETURNS NVARCHAR(20) AS BEGIN DECLARE @NewKey BOOLEAN DECLARE @myKey NVARCHAR(20) SET @NewKey = False IF @SlNo &gt; 999 SET @SlNo = 1 -- Stops * appearing WHILE NOT @NewKey BEGIN SELECT @myKey = CONVERT(VARCHAR, @inDate, 112) + REPLACE(CONVERT(VARCHAR, @inDate, 114), ':', '') + CONVERT(VARCHAR(3), @SlNo) IF EXISTS ( SELECT 1 FROM MemoryTable WHERE KeyField = @MyKey ) BEGIN SET @SlNo++ END ELSE BEGIN SET @NewKey = True END IF @SlNo &gt; 999 SET @myKey = NULL -- Failure condition, all 999 have been used. BREAK END IF END RETURN @myKey END 
Hmm, I haven't worked with memory tables much - out of curiosity, how do you expect the "MemoryTable" to get a record with the value of "KeyField" of a "used"/returned @myKey? 
If you are busy this one is well designed : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Everything for free.
Are you saying that MS SQL will allow you to insert a record into a memory table inside a function? 
interesting, i got into management about 10 years into my career and hated it because I no longer got to code my own stuff. I left that job to become a straight up developer and never looked back. ps. Would love to start looking at NoSql. I'm hearing good things. 
With terrible requirements like that are you sure this is a company you want to work for? 
Without going into a bunch of detail that would bore you, there is some reasoning why this question ended up on the "test". Although it still simultaneously feels slightly torturous 😉 I don't actually need this depth of SQL knowledge to do the job. It's a little reassuring to know this question is actually challenging, though, rather than me just being a complete dolt.
or use dbms_pipe
can you post what each table / query looks like and what you want the end result to look like?
It's all coming from one table. Each query is pulling all of the sums, but with different where clauses. Query 1 has two indicators, that when they both have certain values, all of values of one column is summed. Same with the second column, but the indicators are different. The third query is joining the data outputs from the first two queries and doing a division (the sum of query 1 / the sum of query 2), which creates my final answer. Sorry if that's a little vague. I'm very green in all this.
 select to_char(to_date('22-07-1985','DD-MM-YYYY'),'DAY'),to_char(to_date('22-07-1985','DD-MM- YYYY'),'MON'),to_char(to_date('22-07-1985','DD-MM-YYYY'),'YEAR'), ROUND(MONTHS_BETWEEN(SYSDATE,to_date('22-07-1985','DD-MM-YYYY'))/12) as age from dual; Obviously replace your date variable in the query. 
"For example, one password is 62f0a68a4179c5cdd997189760cbcf18. The longest alphabetical sequence in that is "cd"." Why is it not cbcf?
From what I understand, I need to find series of numbers all in alphabetical order. "cbcf" would not count because "b" comes before "c". It's alphabetical order, not just alphabetical characters. (Because that would make it too easy, right? 🙄) It's also unclear whether they have to be in immediate sequential order (e.g., "bcde" but not "bce"), or if a letter just has to be alphabetically after the letter in front of it (e.g., "bcde" works and so does "bce", because they are alphabetical in an ordinal sense). I'm finding this to be a pretty frustrating task, as well. I apologize I can't give clearer information.
Agreed, order of columns in a composite index is important.
That's pretty much exactly the issue - he was a front end developer that got roped into building the back end applications, so he didn't know how to properly interface with the next level deeper. I could have forgiven that. His refusal to learn - or even accept that he might be wrong - was the nail in the coffin. 
Making a view will still have it run for 15 minutes. Make a permanent table in a different DB or at least a different schema in your existing DB. Ideally you'd have this table in a reporting server. Call a stored procedure that will truncate that table and repopulate it. Set up a sql job that will run your new proc at the frequency you need. Your new table will be a datamart.
A view is constructed when it is queried, and so it's always "up to date" and never needs to be "refreshed," but it has the coding benefits of being able to reference it like a table. From an end-user perspective (or report writer) it's just like any other table. I work a university, and we have a lot of "daily" sort of reports. Some of them take like 20 minutes to run. But nobody runs them. We have jobs that kick off every morning and run these reports. It's kind of stupid that they take that long, but, it's not like anybody's waiting on them. So, what exactly is the 15 minute hold up? Is it the queries take that long to run? Is it that there's a process that just takes that long to complete? I can't really recommend one way or the other without knowing more about the process here, but as a general statement automation is your friend. Views are great because nobody needs to "refresh" a temp table, it just deals with itself when called. On the other hand, if you're hitting this all day, then you don't want to wait for that, but you could automate the refreshing of temp tables using batch scripts (or whatever the MS SQL version of a "job" is, I use Oracle so I'm not the expert). 
SELECT TOP 1 is returning ""at most one record can be returned by this subquery"". Is this because it's finding more than one TOP 1? Hmm. I understand that it can only take one, but not sure why it would be finding more than 1 .. Unless there are exact duplicates. edit :looks like I fixed it by specifying more in the ORDER BY. SELECT t1.*, t2.* FROM Orders AS t1 LEFT JOIN Calls AS t2 ON t1.clientid = t2.clientid AND t1.notedate &lt; t2.notedate WHERE t2.notedate &lt; (SELECT TOP 1 notedate FROM Calls WHERE clientid = t2.clientid AND notedate &gt; t2.notedate ORDER by casenoteid ASC) ORDER by t1.notedate; Thank you again for all your help gnieboer! Appreciate it tons -- this is so helpful -- it's replacing a ridiculously messy INDEX MATCH in excel and this is elegant and beautiful. Cheers!
This was basically in line with my first idea -&gt; automate the build out of some sort of shared data asset (table or whatever) at 2am or something when nobody is on. Then those tables / assets can be used as needed. The challenge is that the environment we work in updates based on events / triggers. So that 2am update job is out of sync / irrelevant as soon as someone else does something that triggers an update. IT has sort of tied our hands regarding options hence trying to turnover rocks until I find something that might work. One guy said stored procs run at the top of all reports will save the day. Another said views will save the day. I think both could solve our standardization issue but neither fix the time leaks. I think I may just have the client eat the time leak until IT is willing to play ball and stand up something more robust. 
You must have at least Windows server 2012. I'm doing this exact setup right now and have a Windows Server 2008 that I'll have to upgrade. https://msdn.microsoft.com/en-us/library/ff878487.aspx 
Looks kind of strange to be honest. That error loop they have is fairly standard for doing an update. But for creating a view it's completely unnecessary. In the view all you can do is check for expected data and handle unexpected data. If the view fails there isn't anything that needs to be rolled back.
Ahh wait, the update is from the sample code they provided, OK I was confused. So your actual change to the DB is just changing a view definition? What is it that they want to log? Queries of the view, or the actual processing of modifying the view. If they want to log queries, then all the queries that access the would need the logging code, not the code that is modifying the view definition. 
SQL is similar between databases like English is similar between the US, England, Australia, and even Wales. Glad to help.
Yes, you don't need it. Once it's successful manually do an insert statement in the log saying what you did :)
[removed]
I don't either. Seems like a huge waste of time.
It seems like it's providing more than one match (since there are x possible matches it supplies them all). So it's just ignoring the SELECT TOP SELECT t1.*, t2.* FROM CUSTOMER AS t1 RIGHT JOIN CALL AS t2 ON (t1.notedate &lt; t2.notedate) AND (t1.clientid = t2.clientid) WHERE (t2.notedate = (SELECT TOP 1 notedate FROM CALL WHERE clientid = t2.clientid AND notedate = t2.notedate ORDER by casenoteid DESC)) AND ((Abs(DateDiff("d",[t1].[notedate],[t2].[notedate]))&lt;=180) ORDER BY t2.casenoteid; I'll tweak it and look again -- seems like a lot is dependent on how things are Ordered.
Statements are correct, with a caveat. The temps have to get recreated when either a new session is created / restored or when the underlying data is changed. I think the time is from the sheer volume of the data (millions of rows) and how many joins are needed to compile the data. It's a clone of a production environment so the table structure branches quite a bit.
Yeah, realistically, you could try to automate this so that the "runtime" issue isn't as bad, because everything just sorta happens, without someone sitting and staring at it. But, you're not going to get around the basic problems. 
Are you familiar with the pl/sql block structure, and variables? [This](https://www.techonthenet.com/oracle/functions/to_date.php) should help you with the date formatting. You'll want to use dbms_output to write your results to the output buffer. 
Thank you! This will help a bit. It's not too hard, but can be quite difficult to set up and get to run
I think you need to use pass through queries, so the queries are handled by the DB server end and not MS Access. If you combine them with an ODBC connection it should work fine. https://support.microsoft.com/en-ca/help/303968/how-to-create-an-sql-pass-through-query-in-access
Hey quick question. Is this going to insert a row each time someone calls the view, or only once a week when they materialize (what I suspect?)
/r/learnSQL/ https://www.w3schools.com/sql/ is a pretty good site for basic SQL knowledge. There's also the wiki page which lists some good resources /r/SQL/wiki/index although these are going to be much more comprehensive.
Only when the script is called, so only when the view is created.
A late thanks for the link and optimization note! I was on mobile earlier when I saw your reply and forgot to respond.
You may get better traction on /r/businessintelligence I'm curious what people will say as I'm supposed to learn cognos soon.
Khan academy has a SQL series that is a solid intro and has you write statements as you go
&gt; Why is it giving me the "missing right parenthesis" error? Because you are missing a **)** in your statement. More precisely in your WHERE clause you do not correctly close the SUBSTR() befor you check if it equals 'a'. It does not belong at the very end of your statement (after 'Europe' you have two brackets, only one is needed to enclose the IN correctly).
I appreciate the response. WHERE SUBSTR() (country_name, -1, 1) = 'a' AND (region_name IN 'Americas', 'Europe') ; Is this correct? I'm still receiving the error. Is it neccesary to put the () after SUBSTR as you did? 
Try SUBSTR(country_name,-1,1)='a' AND region_name in ('Americas','Europe');
Thanks heaps guys! 
Thankyou so much for the information, guy above me had a correct fix but this information is super helpful too. Enjoy your gold!
Thanks for the gold dude! I didn't expect a gold for this. :) 
easier for what? it all depends what you want to do. python and SQL are 2 different languages for solving 2 different problems. 
[removed]
Database administration, is there any books you recommend for basics of SQL?
Just a tip that helps me, I will take my sql out of whatever I'm using (Sql mgmt, toad, whatever) and paste it into a blank notepad++ document and enable SQL language identification. It really helps you identify pairs of parenthesis and other potential problems.
I like the Month of Lunches books by Don Jones. Here is one on [Database Administration](https://www.amazon.com/Learn-Server-Administration-Month-Lunches/dp/1617292133/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1487165732&amp;sr=1-1&amp;keywords=sql+sql+month+of+lunches). I am going through the Powershell book now. 
Using w3 ATM, making my SQL understanding a little better, ;)
I would say it is. It's the highest-level programming language on the market. Basically everything is abstracted away from you into a few different keywords you can use. But to be good in the field, you need to understand what goes on under the hood, and be able to see past the magic.
Could you write a compiler using nothing but ANSI SQL? Surprisingly, the answer is 'possibly'. The addition of recursive CTEs gave SQL a sort of quasi control flow. A better question is - would you choose SQL to write a compiler, rather than an actual programming language (Python/Java/C++ etc)? I suspect the answer would be 'no'. That was never a design goal for SQL, and while it is theoretically possible, it would be extremely difficult in practice. There are Turing complete *extensions* to SQL (T-SQL and PL/SQL for example), but these are proprietary extensions, and are not part of the ANSI standard. By itself, SQL is primarily designed to manipulate database objects. It can be used for other tasks, but in nearly all cases a 'real' programming language would be far easier to use.
You are asking a pretty broad question. If you want to get into DBA sort of stuff, SQL is certainly a place to start, but there is a LOT more involved.
In addition to all the responses, go to class.
T-SQL and SQL are different things, though. T-SQL is an actual programming language designed as an extension to SQL by Microsoft. It encapsulates SQL, but it's not SQL by itself. T-SQL does support actual, declarative control flow (loops and such), but SQL does not. It's possible to emulate that using the functional aspect of recursive CTEs in nothing but pure SQL, but I certainly wouldn't recommend it.
I disagree with a few of your points. I think python is one of the worst languages to start out on for beginners. I think Javascript or C# is the way to go depending on what your goals are. JS is easy to see the 'effects' of your code and C# teaches really good syntax control. 
JS is a good beginner language as well. I tried C# and followed a tutorial where I made a basic text RPG with button control, but overall I felt Python and JS are easier to grasp from the get go. C# didn't click for me but that's because I need to spend more time with it. Python has minimal setup to begin doing things in comparison to C# and it felt more like powershell to me. JS lets you see those changes you make and I agree on those points. 
Also doesn't click for me lol. That being said everyone I know that started out on it have absolutely the best understanding of how control flow works.
I learned SQL before doing C++ classes and had difficulty wrapping my head around. I like my whitespace
Maybe table valued functions would be better?
SQL is a very different language from scripting languages. I found SQL easy to pick up once I wired my head around how a database stores data across tables. If you have any interest in database work or programming that will interact with a DB, go and learn some SQL. I'll add it uses set based logic. Some people pick up on that easily and I've worked with a few really bright people that could not wrap their head around that concept. 
Wonderful! I will give this a shot, thank you!
LDF file will need to go into the TLog folder fyi 
It's gotta be one of your two set clauses. You are either getting more than one TXID from inserted values or you are getting more than one txDesc from inserted values that match the TXID. So it seems to me like it's going to be software behavior. If you wanted to resolve the issue at the risk of missing data, adding top 1 to those both I believe would resolve the issue. I'm wondering if the software app is accidentally trying to process the same record twice simultaneously. 
It's set-based which requires a different mode of thinking if you only have previous experience with procedural programming languages. Being able to visualize how n different sets of data can interact with each other takes a bit of time.
SQL is declarative, which is different than almost every other popular language that exists. Most languages are imperative, meaning they generally execute code in the same order it is written, following a list of instructions in order. Code literally executes line 1 first, then line 2, then onto the next - something you can see in action one step at a time using a debugger. SQL, on the other hand, lets you describe what you're looking for - all of what you're looking for in one big declarative statement (the query). But then it creates &amp; runs its own execution plan to retrieve the results. In theory, execution plans should not matter as long as the result is correct. In practice, poor execution plans happen all of the time &amp; can take a lot of time to figure out why a query takes so long to run. There is no opportunity to "step through" a query using a debugger, because that's not at all how the language works. In a more general sense, SQL's declarative nature makes it difficult to understand how a query runs. A subquery looming at the bottom of a long query may be the most important part (or the first to be evaluated) of the whole thing. There's a lot to it that is counter-intuitive, *especially* if you've learned another language first.
&gt;administration arguably shouldn't be a first step Really? Most DBAs I know are pretty awful coders &amp; never really were into that aspect to begin with. But I like your idea of learning to use the language first.
 Using "##" to indicate a global temporary table is a feature of MS SQL. Is your platform really MySQL? If you're on MS SQL, simply check if the table exists and drop/recreate it at the 'starting point' of your procedure/workflow. 
Have you tried indexed views?
Yeah thats what I saw too after looking at it. I had to modify the trigger to do X or Y depending on if multiple records were being inserted into the table at the same time. If 3 records hit that table at the same time it would blow up. Thank you for you insight. 
IT's MS SQL - Typo! Dropping the tables doesn't always work properly. But I think table-valued functions may solve my problem
what do you mean by "Dropping the tables doesn't always work properly"? One of the great (or infuriating, depends on who you ask) features of MS SQL is that DDLs are included in transaction context.
## temp tables are global and persist across sessions. I believe they persist even after a session ends, until the server is restarted. That being said, I'd perform a drop on every temp table at the start of your proc. And wrap that drop in an IF OBJECT_ID() statement so that the drop only executes if the table exists.
ok - can you give me an example? I have drops @ the start and end of the main proc. But I'm not sure I follow the last statement about wrapping it in an IF OBJECT_ID statement. EDIT: Like This...IF OBJECT_ID('tempdb.dbo.##TempTbl', 'U') IS NOT NULL DROP TABLE ##TempTbl I'm already doing this, but still having intermittent issues with SQL complaining that a temp table exists when it doesn't 
Global temp tables persist until all connections that reference it have been dropped. If only one connection ever references it, then it only lives for the duration of that connection. But since any connection can see the temp table, it is also possible for it to be passed (in hot potato like fashion) from connection to connection for seemingly forever (aka server restart).
If you try to drop a table that doesn't exist, you will get an error. You should check to see if the table exists, then attempt to drop it. IF OBJECT_ID('tempdb..##temptable') IS NOT NULL DROP TABLE ##temptable or if you have mssql 2016 DROP TABLE IF EXISTS ##temptable
I am curious about if this global temp table is potentially being referenced by multiple concurrent processes. Imo, I would look into whether a global temp table is actually necessary, or can the scope be reduced to a local temp table. 
my guess is the endpoint is using a transaction and it is not committing?
It sounds like you have a problem with commitment..
Essentially I have a bunch of smaller queries i need to bring together for a scorecard for upper management. I thought throwing the results of each query into a ##TempTable and then using a union would be the easiest...maybe not?!? After playing with table-valued functions for a bit they seem seriously slow. Guess I need to either figure out this issue with dropping the temp tables or find another way.
No other connections are referencing these tables. 
Worth noting, probably a typo, but this is MSSQL, not MySQL as per your title. 
I mean that even after executing drop statements for all the tables, SQL still complains that a table already exists. Doesn't happen regularly...seems random, but happens enough to be annoying. 
Check out [this page](https://dev.mysql.com/doc/refman/5.7/en/commit.html) from the MySQL manual. You've either got autocommit turned off, or your app is explicitly beginning a transaction that it never commits.
Doesn't look like a backup to me, looks like they are attaching a MDF file vs trying to restore a backup. I think [this](https://sqlserver-help.com/2014/05/22/solution-the-file-mdf-is-compressed-but-does-not-reside-in-a-read-only-database-or-filegroup-the-file-must-be-decompressed/) will walk through OP and fixing it though.
https://sqlserver-help.com/2014/05/22/solution-the-file-mdf-is-compressed-but-does-not-reside-in-a-read-only-database-or-filegroup-the-file-must-be-decompressed/
Oh, hah. I didnt even consider that they tried to do that. 
Thank you, this one helped!
I guess it depends on the shop. If they have third party tools for administration and it's such a large shop to where they are literally just administrating and they may see or write a query once in a blue moon because of the tools and duties, I could see that. I do agree with you that dev experience first is way better to start with.
If you're using Oracle, you can look at APEX. It comes with Oracle, and it's kinda like a website builder which allows you to show queries, or enter data into the database through the GUI front end (or website). Not sure if this is what you're thinking tho!
You should be able to pass the number 0 then. 0 should be interpreted as 1900-01-01 00:00:00.0000000 for datetime2, 1900-01-01 for date, and 1900-01-01 00:00:00.0000000 +00:00 for datetimeoffset. Personally, if you had to pass a value, I'd pick one of the above or just use 0. 
you my friend are a genius, seems obvious to me now to replace the 0000-00-00 with 1900-01-01 but for the last two hours it was like trying to figure out rocket surgery 
It is very likely if you give it '0', it will auto fill 1900-01-01. 
ok...won't create table also fail if the table exists?
It depends what it represents...but usually fine, sure you should use `numeric(4,4)` and store them like 0.9876 If you want to store 0.98765 then use `numeric(5,5)` If you want to store 199.99% (1.9999) then use `numeric(5,4)`
I firmly disagree with this technique, though it is everywhere in the industry. If all zeros indicates lack of date data, then NULL is a better choice as NULL represents lack of data and 1900-1-1 is a specific value. 
... it will, that's why you're dropping it if it exists on the first step
Generally, if the percentage is derived from other data, you don't want to store it or you have to make sure to update it when the other data changes. Depending on your RDBMS, you can embed the calculation in the table definition itself so you don't have do the calculation manually: https://technet.microsoft.com/en-us/library/ms191250(v=sql.105).aspx Creating it as a computed column means it's still updated on the fly, but you can reference it just like an ordinary table field.
That seems like a bad idea. If you start doing date arithmetic, these values are going to yield strange (and most likely incorrect) results. A NULL is a better idea, since date interval functions on a NULL will always return NULL.
but in mysql for example you can't have null on a datetime field for example... right?
I am not a MySQL expert, but I believe it does and a quick Google backs up that belief. Looks like some people have problems with datetime and PHP with MySQL, but MySQL will allow this. Assuming the column is configured as nullable.
With a correlated subquery, the subquery is re-evaluated for each row. Depending on the data, your join condition, and the RDBMS, a correlated subquery can either perform *much faster* or *much slower* than other alternatives. Personally, I'm partial to correlated subqueries to accomplish anti-joins. For example, you can return all Customers without an Order in 2016 by doing this: SELECT * FROM Customer c WHERE NOT EXISTS ( SELECT 1 FROM Order o WHERE o.CustomerId = c.CustomerId AND o.OrderDate &gt;= '2016-01-01' AND o.OrderDate &lt; '2017-01-01' ) I think that's easier to understand than this: SELECT c.* FROM Customer c LEFT JOIN Order o ON o.CustomerId = c.CustomerId AND o.OrderDate &gt;= '2016-01-01' AND o.OrderDate &lt; '2017-01-01' WHERE o.OrderId IS NULL Or this: SELECT c.* FROM Customer c LEFT JOIN ( SELECT OrderID FROM Order WHERE OrderDate &gt;= '2016-01-01' AND OrderDate &lt; '2017-01-01' ) o ON o.CustomerId = c.CustomerId WHERE o.OrderId IS NULL Most RDBMSs will execute those the same, but those that are less advanced will execute them differently. 
You'll want to use a left join to the second table.
I think that making use of an "inner join" is the best option. The statement should look like this: SELECT MIN (p.price) ,AVG (p.price) ,MAX (p.price) FROM product p INNER JOIN vendor v ON p.v_code = v.v_code;
look into exists condition
Thank you I had no idea INNER JOIN was a thing. This is making a lot of the other questions make a lot more sense.
If the database "populates" in SSMS then there's a SQL server instance running somewhere. Might be LocalDB. You can figure out which files are used if you right click on the DB, select Properties and look around. 
You're welcome! I just re-read your slogan and I realized that the query might look like this: SELECT v.v_code AS vendor ,MIN (p.price) AS minimum_price ,AVG (p.price) AS average_price ,MAX (p.price) AS maximum_price FROM product p INNER JOIN vendor v ON p.v_code = v.v_code GROUP BY v.v_code; ^PS: ^The ^*EXISTS* ^condition ^could ^be ^useful ^if ^you ^were ^not ^interested ^in ^retrieving ^data ^from ^the ^"vendor" ^table.
I agree, but their table is set to non null, although I'd prefer to change the table value, they may not be able to easily. 
Yeah it's looking like I'm going to be needing EXISTS for this next question. List the product code, price, vendor name, vendor contact, vendor area code and telephone number for all products that have an invoice date newer than January 15, 2014. Or at least using it would be helpful.
Left join table A to table B. [Venn diagrams](https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins) always helped me when determining joins. 
HTML is probably one of the easiest languages to learn. I would suggest that you learn C to get the basic fundamentals of 3GL programming. Its a bare-bones type of language. You need SQL for DBA work though. Don't learn a language just because it's easy. You will be frustrated in your career if you just try to get by the easy way. 
Are you dropping and recreating the table multiple times in the procedure? Are the temp tables being used/created in loops or cursors? Temp tables don't work well in those situations because of how SQL compiles the procedure. Remember that SQL is a declarative language, not a procedural one. It might have basic flow control, but it doesn't necessarily run your query line by line. Your code only describes what you want and SQL tries to optimally return your request. Also, is there a reason why you use global temp tables over regular temp tables?
What's your error? Can you post the question verbatim and table structures? You definitely don't need to count customer balances. I think what you need to do is select the columns you mention and only include customers with a balance over $100 in your where clause. I usually don't just give people the answer but it's late, I feel your pain. I'm just taking a stab here because I don't have all the information I need. Also, I'm making an assumption that both tables have customer number for the join.. if not replace that with the right key. SELECT i.INV_NUMBER, c.CUS_CODE, c.CUS_LNAME, c.CUS_INITIAL, c.CUS_LNAME, i.INV_DATE FROM invoice i JOIN customer c on i.CUS_NUMBER = c.CUS_NUMBER WHERE c.CUS_BALANCE &gt; 100; 
No group by? And ,* AFTER ITA Here's my take without knowing the schema and going by your code. SELECT C.CUS_LNAME,C.CUS_INITIAL,I.INV_DATE,I.INV_NUMBER FROM INVOICE I INNER JOIN CUSTOMER C I.CUS_ID = C.CUS_ID WHERE C.CUS_BALANCE &gt; 100 Sorry for the format, using potato.
Why not create an oldb connection in access to SQL server? You can use SQL to select the data and then disable the connection with the data in place. ?
Doesn't that return each record in table A only one time? Can I do joins with LIKE statements like this: SELECT A.Acctnum, B.Street, B.Zip FROM A LEFT JOIN B ON "*" &amp; A.StreetNum &amp; "*" LIKE B.Street AND A.Zip = B.Zip
This website explains subqueries quite well. The different sort of subqueries are explained through examples. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
One thing I think everybody missed is customers may have more than one invoice. This means you need a total of invoices by customer. You will need to sum the customer invoice amount. In other words, a customer with 2 $99 invoices won't show if your using previous code, but it will if you don't assume each customer only has a single invoice. 
I would not store it. There will probably be some rounding involved in creating it. Those lost digits could be useful. If you ever need metrics covering more than one row (say you want the % calculated from all rows in the table) the individual percentages won't do you any good. Basically, I see this as a display value, which could always be calculated on the fly.
- A left join can return records in A more than once if the join conditions are met more than once to B - Your proposed solution works; however, I would avoid multiple wild cards for performance reasons. - He is syntax in tsql: &gt; SELECT &gt; A.Acctnum, &gt; B.Street, &gt; B.Zip &gt; FROM A &gt; LEFT JOIN B ON &gt; B.Street LIKE '%' + A.StreetNum + '%' AND &gt; B.Zip = A.Zip 
This should work so long as ErrText is a varchar as well. SELECT * FROM TABLE1 JOIN TABLE2 ON TABLE2.ErrText = (CONVERT(VARCHAR,TABLE1.ID) + TABLE1.Err_code)
SELECT * FROM Table1 JOIN Table2 ON ErrText = (CONVERT(varchar(8),ID) + Err_Code) *edit MSSQL
From the video, what I gather is that we are getting all of the information from the table regarding the person with the highest salary. While looking at that table, I say, hey, who has the highest salary, then I'm going to count that salary in the table, turns out only one person has that salary in the table which is the highest. Then based on that count, because we want to know who has the highest, the single (1 in the where clause) highest paid persons information should be returned. I personally would have written it differently because that's super inefficient. I'd just return all data based on the max salary which is just using the max function. So they did it more as a simple demonstration of correlated subqueries, I prefer these two articles and think they explain it better. http://stackoverflow.com/questions/17268848/difference-between-subquery-and-correlated-subquery http://www.databasejournal.com/features/mssql/article.php/3485291/Using-a-Correlated-Subquery-in-a-T-SQL-Statement.htm
Let me ask you this. If I have 2 identical copies of the database, Copy_A currently being used by the web project, and Copy_B being edited by SSMS, is it possible for Copy_A to be replaced with Copy_B? Or is there any special thing I need to do to switch them out? 
So I figured out which files it was. And I moved and replaced those files and it totally grenaded everything. 
you neglected to mention your platform (see sidebar), so here's the solution for MySQL SELECT * FROM Table1 INNER JOIN Table2 ON Table2.ErrText = CONCAT(Table1.ID,Table1.Err_code) 
if you're interested the following code: DATEADD(wk, DATEDIFF(wk, 6, GETUTCDATE()), 5) Is checking for the number of whole weeks between day 6 (7th of January 1900) and today (found using GETUTCDATE(), you can also use GETDATE()). You add this number of weeks to day 5 (6th of January 1900, which is a Saturday) to get the date of the previous Saturday.
Not sure what platform you are on, but generally speaking joining tables on expressions rather than columns will not perform well. But if that's what you've got, then you have little choice (unless you want to preprocess one of the tables to ensure your datatypes match before you perform the join).
So you've described the model for the data tables. What do you want to accomplish with the indexes? "build out processes" and "implement a set of objects" are fairly vague needs. 
Thanks. What I think threw me off is that I am not used to a number being put inside a WHERE clause to force the inner query to equal to that number. Does it mean that when a query generates results, the count starts from the top and returns as many items as the query was made equal to? 
Probably not. SQL is pretty efficient when it comes to joins, mainly because that's how a relational database is utilized properly--by joining the dependent tables to get the required data. But you could always test it out and see if there's a difference. Pretty much getting a baseline performance, making a change, getting a new baseline, and continuing on is going to be your best bet to determine if something adds performance or not. Assuming your `ID` column is unique in the second table... SQL will perform the same execution plan for both a `WHERE ID IN ...` and a `INNER JOIN ... ON a.ID = b.ID`.
Install the driver?
Yeah like a suitable one or something.
Your query is fast and looks pretty optimized to me. I would NOT simplify that query any further in the sake of performance. You're hitting a few tables and it appears to always be on Primary Key or Unique Constraint columns... It's gonna be fast.
Have you tried turning it off and on again?
Appreciate the feedback! 
This is close but not quite what I was looking for. It did give me inspiration for a solution. Heres what I came up with: COUNT(DISTINCT CASE WHEN os = 'win_7' THEN employee_machine_id ELSE NULL END) AS os_win_7 I dont mind manually setting up for all os' since there are a small number of them. Thanks for the help.
&gt;I am joining item 6 different times for 6 different columns, is this efficient? It's as efficient as you're gonna get. The fact the joining 6 times seems intense is just a side effect of proper DB design, which from the surface appears like you got it right. 
What about using AWS to restore the database? Then extract the table and restore using SSIS. Then you can scrap the AWS ec2 instance. 
Thanks for your time! 
Just chiming in to agree with everyone else - as long as your tables are properly keyed and indexed you're almost certainly best to do this all server side in one big statement, and trust the DB optimiser to do what it does.
And then docker the driver. Or expose your port.
Oh I had forgot to use DISTINCT in my second query, you are right. Glad I could help!
Using "WHERE IN" is almost definitely worse than a join. If you are worried about performance, consider looking into indexing your tables. Especially if they don't change much. 
Here is the query I came up with. CTE1 being the table that only holds RowNum and PartNumber. CTE2 groups them together. WITH CTE AS ( SELECT (RANK() OVER (ORDER BY BatchID ASC, BatchSequence ASC)-1)/25 RowNum, bom.PartNumber FROM ... ), CTE2 AS ( SELECT RowNum, PartNumber, COUNT(PartNumber) as Qty FROM CTE GROUP BY RowNum, PartNumber ) SELECT PartNumber, MAX(CASE WHEN RowNum = 0 THEN Qty ELSE 0 END) as [1 to 25], MAX(CASE WHEN RowNum = 1 THEN Qty ELSE 0 END) as [26 to 50], MAX(CASE WHEN RowNum = 2 THEN Qty ELSE 0 END) as [51 to 75], MAX(CASE WHEN RowNum = 3 THEN Qty ELSE 0 END) as [76 to 100], MAX(CASE WHEN RowNum = 4 THEN Qty ELSE 0 END) as [101 to 125], MAX(CASE WHEN RowNum = 5 THEN Qty ELSE 0 END) as [126 to 150], MAX(CASE WHEN RowNum = 6 THEN Qty ELSE 0 END) as [151 to 175], MAX(CASE WHEN RowNum = 7 THEN Qty ELSE 0 END) as [176 to 200], MAX(CASE WHEN RowNum = 8 THEN Qty ELSE 0 END) as [201 to 225], MAX(CASE WHEN RowNum = 9 THEN Qty ELSE 0 END) as [226 to 250], MAX(CASE WHEN RowNum = 10 THEN Qty ELSE 0 END) as [251 to 275], MAX(CASE WHEN RowNum = 11 THEN Qty ELSE 0 END) as [276 to 300], SUM(Qty) as Total FROM CTE2 GROUP BY PartNumber
[removed]
Can you post your SQL? Are you using nolocks? It's a bit crude but perhaps adding option recompile?
Along the lines of nolocks, its possible that there is an open transaction on the table that your query is waiting for. If you catch the problem in real time, run DBCC OPENTRAN and cross reference with [sp_whoisactive](https://www.brentozar.com/archive/2010/09/sql-server-dba-scripts-how-to-find-slow-sql-server-queries/). If you can't add the proc, you can turn it into a script by removing the create proc statements and modifying the param directly.
Not so much that older books would be useless. The basics; INSERT, SELECT, UPDATE, DELETE, and things like JOIN, WHERE, etc are pretty much unchanged. Most of what you'll see changing over time are new features that make those basics easier, like MERGE to do a combined insert, update, delete, or things like support for json as a data type. Having the stuff that's been there nearly all along under your belt will only help you understand and appreciate the improvements.
&gt; So has SQL evolved so much that a book from 2002 would be wrong information Probably not wrong information (unless it was wrong in 2002 ;) &gt; or so outdated to be a complete waste? Some approaches have changed because there are now more convenient ways to solve many problems. In other words: those books might present examples that would be solved differently today. I think this is indeed a risk so that you might learn something "wrong" (in quotes, because those solutions still work, they are just not start-of-the art anymore). If you are interested to see WHAT has changed over those years, I invite you to my website modern-sql.com. In particluar, you might want to skim through the slides to see when some of those features appeared. E.g. for the "OVER" clause: http://de.slideshare.net/MarkusWinand/modern-sql/104
You dont know what youre doing?
If it's just a general SQL book, then I think it will be still useful, however if it's more specific to a particular implementation like 'SQL Server 2005', then I would recommend against it, as it may teach you workarounds for things that have been fixed in the specific TSQL implementation of SQL, or other things which are no longer best practise.
I was so close! As soon as I saw your query it just clicked. Your explanation also really helped a lot. In my pivot, I was COUNT(PartNumber) instead and my result set would just be 25 in each column and the part number disappeared. Count the rank instead. Duh! Thanks a lot!
I second this. If you ever have to do a count of dates or something like that, you will get bad results.
DB structures - heaps, index trees, transactions, etc... no not really... some newer stuff (column based structures), but easy to pick up incrementally... the only "new" stuff would be things like XML/JSON support, but they're not really doing anything fancy, so they'd be easy to pick up (they're just blobs with support functions). - the only "new" stuff that would actually be of interest, would be the geo stuff... geography (points on a sphere, aka lat/long) and geometry (shapes defining areas on a sphere, aka boundaries of country/state/city/etc)... not super difficult or anything, but there's some cool stuff you can do if the data types are applicable to your use. NoSQL - skip over this stuff for now... it's new (and thus won't be in the books anyway), but it's worth knowing about real databases as a reference point (specifically, knowing about things like atomic transactions and consistency) OLAP / cubes - no notable changes... if anything they're being deprecated/ignored in favor of in-memory processing. table structuring (OLTP vs OLAP, etc) - this stuff is still just as valid today as it was then, with the exception of NoSQL (given the design changes, and again, skip it until you're familiar with traditional RDBMS)
While SQL has added a handful of really cool features, the fundamentals of CRUD, joins, aggregates, subqueries, indexing &amp; table design have not changed since 2000 (or quite likely a long time before that.) I would strongly encourage you to read those nearly free but old books. If you master 80% of what's in them, you'll be delighted to learn about newer features that have come out since then. (CTEs, recursion, windowing functions, etc.)
Haha. That all sounded like Middle Earth Elvish to me. But I've saved this post so I can reference/research it once I get some background knowledge under my belt. :) Thanks for the obviously well-thought out reply!
Wow - I never knew that Pervasive *was* Postgres. We acquired a smaller company at a previous job, and all I ever heard from the BA's who were working on it was that "Pervasive DB is a nightmare." But if it was really just Postgres, joke's on them. I offered to help a dozen times...
If you find that your team are re-using queries look at ways in which these can be parametrised and converted into stored procedures. If there are no obvious ways to parametrise or stored procedures are not an option you could look at an SVN repository (tortoise SVN for example) to store versioned sql scripts.
Thanks.
Thanks. I was not aware of Tortoise SVN. I'll look into it.
You could search within a circle by defining a radius and origin and then filtering by checking the Cartesian distance. Not at a computer or I'd provide a sample. 
A circle isn't much better than a square. I'd like to be able to search within a polygon, like find all datapoints within NY state (outlined by my polygon).
Rewrite query to update all 18k records in a single shot.
how do i do that?
That depends. You'll have to provide more information. Where are those 18k records coming from? How do you determine the values of columns you are setting? Where's the `id` coming from? Basically give us as much context as possible.
Thanks alot for the feedback!! I managed to learn some of the basics and am trying to learn vba first since they threwe into excel duty but i want to get deeper into sql when i can! 
18000 update statements take 100 hours? This shouldn't take that long, do you have any indexes on this table or is it a heap? Secondly, are there any periodic transactions running against this table that could be locking you out? 
Thanks for the links. I'll be sure to reference them for post-basic concepts that obviously wouldn't exist in 2002.
Just truncate the results of the integer/60 then append the modulo of the integer/60. 
**Regarding Indexes** You are going to get a few different kinds of advice in here about indexes. Some people might tell you to disable indexes, others will tell you to create indexes. What's going on? - Disable indexes: This is because indexes must be maintained when the data changes. So you are not just updating data in a table, you are also writing to one or more indexes associated with the table. It's possible that a single update can require many index writes to several indexes. - Create index: This is because in simple terms an update has at least two parts. First you have to find the row to update. Then you have to write the change. (More to it than that but all we need to talk about for now.) So if your update is slow because it is taking a few seconds to find the affected row, then you probably need to create an index to help you find the row to update. Questions for you: - How big is the table you are updating? - How many indexes does the table have? - Do you have an index on the "id" field? I would assume so because it looks like a primary key. But if you do not have an index on "id" you should create one. It may be taking too long to find the affected rows. 
[Edit: I misread the OP. They are doing UPDATES, not INSERTS. Some of the comments in this post are still applicable.] For some quick math: * 100 hrs = 360,000 seconds. * 18,000 inserts per 360,000 seconds = 1 insert per 20 seconds. Something is catastrophically wrong with your database. Check the following: * Is the WHERE clause filtering on an index column? If not, add an index. This alone will reduce your runtime to a few seconds. * If the columns in your WHERE clause are already indexed, can you disable the index prior to insert, then reenable after your insert? * Are you running an application which performs INSERTs in a loop? If yes, ensure that you open your database connection before your loop, and close your connection when you are finished. Opening/closing a database connection shouldn't take very long, but the overhead of doing many 1000s of times in a loop will slow down your application, *especially* if you have a slow network connection. * Are you positive the bottleneck is the INSERTs, rather than some other part of your application? If not, consider the possibility that your application may be doing some other processing, lookups, http requests, or database operations in addition to your inserts; that extra work doesn't happen for free. If none of this helps, try the following: 1) Re-write your insert statement to insert multiple records at once. For example, in SQL Server: insert into destination (col1, col2) values (val1, val2) insert into destination (col1, col2) values (val3, val4) Is equivalent to: insert into destination (col1, col2) values (val1, val2), (val3, val4) Alternatively, if you can query your source table with the data that you intend to insert into your destination table, your insert simplifies considerably: insert into destination (col1, col2) select col1, col2 from source where foo = bar 2) Insert the data using a bulk insert. See documentation for [SQL Server](https://msdn.microsoft.com/en-us/library/ms188365.aspx), [Oracle](http://www.dba-oracle.com/oracle_news/news_plsql_forall_performance_insert.htm), [PostGres](https://www.postgresql.org/docs/current/static/sql-copy.html), [MariaDb](https://mariadb.com/kb/en/mariadb/how-to-quickly-insert-data-into-mariadb/).
Correlated subqueries can also be quite fast, however. As always, it depends on the specific example.
Seems like this is a question of data modeling but the title says B-Tree Indexing. 
How selective is this: &gt; WHERE A.PLANT='1100' Will that return 90% of Table A or 10% or 1% or 0.001%? And is A.PLANT a number field or character field? 
[removed]
Define "SQL". The ANSI or ISO standard? Or a flavour like T-SQL, MySQL or Postgre? If, for example you're talking T-SQL then it's fundamentally very similar but practically very different, i.e. core concepts are unchanged but the script you'd write back in the day to accomplish something could be pretty different to what you'd write now. I don't think it would be counterproductive, but there's probably going to be a lot of stuff in those books that's not relevant. There's a reason they're so cheap now, and running through a course at CodeAcademy or similar will probably adequately cover the fundamentals. Just, ahem, *acquire* more recent books if you're really that hard up. With regard to staring at a screen for hours... I can't say I can relate, but it's very nice to be able to fart out a bit of script to practically demonstrate stuff you're reading about, so if your learning style is anything like mine I'd keep a server within reach.
Your indents, other tabulation and comments are great. Far more readable SQL than much of what my colleagues, and likely myself, produce. I would suggest using leading commas instead of trailing but it's a personal preference. I agree with /u/jacobmross regarding a date table as best practice. Joins are fast as shit compared to date functions. But if the performance you're getting is acceptable and you don't manage the DB then it's not a big deal, just something that might be nice to raise with whoever does. With regard to logical errors when flattening the query, try to look at each subquery/related join logic, think about exactly what those joins are doing, and figure out what's happening. It might help to just throw each subquery's returns into Excel and emulate the join logic in there for each level of aggregation (or rewrite portions of your script to show the same).
Holy shit, are you altering the WHERE clause for every value of id? What is common to the records you're applying this to? Figure that out, and replace that clause with WHERE id IN (&lt;subquery returning your target set&gt;). If there's no pattern to it then just include the list of id values in your script.
Look at the MCSA for SQL Server 2012-2014. 20461 and 20462 cover advanced T-SQL querying, performance tuning, indexes and other DBA stuff. 20463 covers data warehouse design and implementation. There should be plenty of training courses on offer for all of those, and you can get a recognised certification if you care to sit the exams. I realise it's outdated, but I don't like that they've split the 2016 MCSA into streams and it seems like you can cover the 2014-2016 feature gap by reading a few articles. With regard to specific training providers, I'm afraid I can't advise.
Just create a Team Foundation Server instance (on prem, online is Visual Studio Team Services) and throw them into a Team Foundation Version Control or Git repo. Of course, this may involve educating people on how to use repos. Not really any way around that if you want centralised archiving and version control.
Unfortunately I only have a rudimentary knowledge of Bash, though I expect this would be a better means of extracting the metadata. This is likely pretty weak, but I'd try to write a .sh to extract each file's metadata to filename_metadata.csv using grep/sed and assemble a manifest.txt of *.csv using sed and ls. You could then BULK INSERT manifest.txt into a temp table and iterate through the .csv files to get values for INSERT. However, someone far more familiar with Bash will hopefully be along shortly.
If you have an excel just make concacenations to build the insert statements 
EXISTS and IN are very different. Also NOT IN and NOT EXISTS are even more dissimilar. Not sure why people are trying to make them sound the same here.
With regards to formatting, I just punched my scrambled code into [Instant SQL Formatter](http://www.dpriver.com/pp/sqlformat.htm). I haven't yet figured out the most readable conventions so I'm happy to outsource that. :D Regarding flattening the query - I'll do that. That said - what I'm hearing if that it's *possible* to make it flatter given my current constraints (no date table yet)? Which is to say, I don't need to wrap the StartOfWeek function (to force the calculation(?)) before I use the datepart functions? I could do that on the same level?
[removed]
To answer your first point, I can imagine that pre-checking a join condition in a subquery is superfluous, but it depends on the overall query logic. It might have been a planner hint or a copy/paste from writing the subquery. On the second point, EXISTS is asking "ignore the content of this query, does it return any rows?", whereas IN is asking "does my value match any of the results in this query (or value list)?". For equivalent expressions, a query planner will probably treat them as equivalent, but I think its generally clearer to use IN when I have a static list of values, and EXISTS when I have a subquery.
[removed]
[removed]
So from what research I just did, I really don't believe that pervasive is derived from Postgres. I believe that back in 2005-6 they had a separate product that was a fork of postgres.
Nobody else mentioned it so far but maybe using a prepared statement would help. It's kind of like a temporary function which saves time spent planning query execution. Link for postgres but all dbs would have something similar: https://www.postgresql.org/docs/current/static/sql-prepare.html
I would go with visual studio online (tfs) it's free for 5 users and if you have msdn it's also free. 
Right, his question is a bit weird to begin with, so he probably is lacking a lot of understanding. But his second question is a valid curiosity, because of those instances where they can be used interchangeably. EXISTS is not there as a replacement for IN, but since EXISTS can be used to more efficiently handle a very common use of IN, it makes sense to talk about these situations. 
[removed]
what if you use double quotes around the $SQL = " .... " 
I had that at first, but didn't change anything. I switched it to single to see it made a difference, but both give same error.
I'm really sorry, I'm not a PHP expert. Just trying to help. [here](http://www.sourcecodester.com/tutorials/php/9221/inserting-data-using-php-mysql-database.html) they use a slightly different syntax. (please note that they also use double quotes around the query variable) Perhaps that will work? {$f_name}
&gt; I know it is probs pretty simple it is ;o) string and date values need to be delimited so instead of 2008-11-11, 555, gasdfgfdfsgdfgdfgd you need '2008-11-11', 555, 'gasdfgfdfsgdfgdfgd'
OP is doing updates, not inserts.
"The application needs Sysadmin access to the DB server."
Hopefully they're not on the same array! The Podcast is Sql Server Radio. It's a fun one to nerd out to.
My favorite (And they named the episode after it) "Oh shit, I'm on the prod server."
&gt; Hopefully they're not on the same array! That was kind of implied :) &gt;Sql Server Radio. I've listened to that one a few times, but it's not in my regular rotation.
We had an SCCM add-on that not only needed admin privileges for the install, the installer had to be given the `sa` account credentials (couldn't just be "any admin user").
Actual words spoken by our helpdesk guy to our DBA: "The prod database server stopped communicating with the application server, some error about no listeners, but don't worry because someone in the sales group fixed it by editing some of the config files." Even scarier response from the DBA: "Oh, so it's working again? Great, I'll close the ticket."
Holy shit.
Boom. Same here.
I forgot a where clause...
'Opps...' 
[decent thread about it](https://social.technet.microsoft.com/Forums/en-US/126ac62a-77f4-4f36-b7b8-6ee94c3dd980/sccm-2012-and-sql-permissions?forum=configmanagergeneral)
Ugh. Why are so many of Microsoft's server platform products so hostile to each other?
*shiver*
also he threw the ball...
App Developer during morning SCRUM... *Hey wish_theyd_done_it, yeah, we don't need to worry about that stored proc, I just had the ORM map to the tables, and did the aggregation in code.* *OK, I'd like to see the query after the standup.* .... sometime later .... App Developer: *Here's that query, 'select * from table1'* Me: *OK so how are you aggregating?* App Developer: *I'm just looping through the collection, and ...* Me: *&lt;insert Janeway facepalm&gt;* .... six months later .... SCRUM Master: *Hey there wish_theyd_done_it, users are complaining that app1234 is really slow - they think it's the database* Me: *nope, it's an app problem.* 
`The upgrade on our production server failed and no backup.`
Haha yep. I can only assume they do that due to time constraints. But even they say that the questions posted to them should be ones that yield short answers. "Everything else should be posted on StackExchange." The good thing is that Erik and Tara hang out on SE and will actually go in and answer questions.
How do I rollback a truncate table?
[Was that... production?](https://redd.it/2tzlnr)
We had that too. We were like...I mean, we can give you that for the install, but even that's pushing it *hard.* And they responded, no, they definitely need sa til the end of time. Needless to say, that vendor has been an absolute fuckin' nightmare to work with. Completely incompetent.
This is how ORMs can get a bad reputation :(.
Makes sense. None of our rogue applications have a network connection, but to get around the firewall, there are XML data feeds to cloud-based servers, and users access the app through their web browser. Currently we're running about half of the company from rogue applications, everything from logistics to purchasing, CRM, production scheduling and quality control. I'm also the only one that supports all of those, so they are going to have a lot of fun if I ever leave the company. IT doesn't even have access to most of those servers, and I'm not sure they know what's out there. Half of those apps were custom-built and my boss tells me not to worry about writing documentation. ¯\\\_(ツ)_/¯ Our IT group should probably be a little worried.
I have a vendor like that. They can't document for us what permissions they need on which tables (I want to give them the minimum possible; they admitted to me that they don't do documentation of most stuff) after installation, so their solution is "just make it an admin account."
That's just the thing, understanding the proper way to use the tools at your disposal. I'm a Sr DB developer - data is what I do - and in a past life I was also an application developer; in fact I moved *from* app development to DB development so I do fully understand the compulsion to do everything in code, but taking a code-first approach to every project often leads to poorly performing database designs. Learning enough about SQL to be dangerous is deceptively easy, and I think that some app devs suffer from overconfidence in their db skills, and an ORM can greatly exacerbate the issue. Now, if we're talking about the Microsoft stack, then source control should not be an excuse for embedding queries in code, or replicating database functions in code, e.g the nested loops example you gave. Sql Database Projects in Visual Studio work like any other project, and build deployable dacpacs that can be used with virtually any deployment solution. The approach I take with regards to business rules is this, *if you have to recreate a database function in your application code to accommodate a business rule, then you're doing something wrong. In that case the rule needs to go into the database.*
This is why none of our developers have *write* access to the production environment. The only way something gets executed in production is if it's done by DevOps, and only after it has been developed and executed in DEV, tested in QA when appropriate, and code reviewed by another developer. 
You couldn't roll back the transaction at that point?
This is the part where I admit I was a total newb and didn't use one. 
Cursor.
Look up the recent gitlab disaster, it's full of sentences that would make a dba cry
I'm also a Sr application dev who transitioned into a data role. Spent my entire career having never used an ORM until I came to my current shop. Here the devs use entity framework and its database generation and migration functionality. They're not the worst DB designs I've ever seen, but they're still pretty bad. Missing FK constraints, and sometimes entire lookups, over reliance on surrogate keys, no unique constraints whatsoever, using 1:M when a 1:1 was needed, on and on. I don't mind ORMs when used appropriately, but the skills of even decent DB designs are atrophying due to abuse of them imo. 
Can I add: In place upgrade, no rollback plan.
That's a good point. I appreciate them taking the time to answer questions, I just wish they'd delve deeper into some topics. But I guess that's what the blog is for.
I only had 10 words but yes that would be much worse...
I have been given a sandbox database to build reports for my team. Can I ask what is the issue with views within views, and what would you suggest instead? 
On one of our more recent projects a team of app developers designed and built an entire application and database platform using the MVC model, and one of the .Net ORMs. Every table had a GUID as the primary key, and all the tables were linked via junction tables - imagine a junction table with four columns of GUID FKs, and a GUID PK. There was no business logic in the database, it was all in the .Net layer; they had essentially reinvented the core functions of RDBMS in c#. I stared in disbelief at the ERD for a good 5 minutes trying to figure out how this thing could possibly work. The shit people come up with is astonishing.
How about "where did the data in the table go?" Usually followed after they did a delete....
You absolutely can, it's dangerous in the wrong hands as it breaks the backup chain. If you find yourself needing to truncate in an emergency, always reinitalise the backup chain afterwards by performing a full or even differential backup. Simple mode of course truncates the log every so often because you explicitly chose not to want to back it up.
&gt; Instead of a LEFT JOIN try using EXCEPT oh goodie, when was the last time you got that to work in MySQL? 
you were pretty close... SELECT a.gid , a.name FROM ( SELECT g.gid , g.name FROM miRNA AS m INNER JOIN targets AS t ON t.mid = m.mid INNER JOIN gene AS g ON g.gid = t.gid WHERE m.name LIKE "%let-7c%") AS a LEFT OUTER JOIN ( SELECT g.gid , g.name FROM miRNA AS m INNER JOIN targets AS t ON t.mid = m.mid INNER JOIN gene AS g ON g.gid = t.gid WHERE m.name LIKE "%miR-16%") AS b ON b.gid = a.gid WHERE b.gid IS NULL
We had this same problem. What we ended up doing was locking down QA and prod so they had to do their work in dev, every release prod gets pushed down to QA, there is a folder on the QA server that holds restore scripts, once it's restored, scripts are applied so we don't lose any changes currently in QA. We're still trying to get people to keep up with SQL source control.....
Anything someone says when they call you at midnight. Just because I know I'm going to be up for 3 hours trying to run down some obscure problem and rerunning what failed and making sure it succeeds. What podcast was this? Just curious.
The podcast was SQL Server radio. I'm going back through old episodes as I have time. 
[removed]
You can insert the results of dir command into a table. I've had to do this many times, and as mentioned, going the SSIS route would be better, but sometimes the business doesn't want to spend the money haha. Example: http://www.sqlteam.com/forums/topic.asp?TOPIC_ID=94064 (4th post down -yes this article is old but still relevant)
Thank you so much. I appreciate this. It worked!
&gt; We have to downgrade from SQL Server Enterprise to Standard At least that one's a little less painful with the new licensing model, no?
"It turns out we haven't taken a viable backup in years."
[removed]
 WITH label AS ( SELECT * FROM ( VALUES ( 1 , 'Very Weak'), ( 2 , 'Weak'), ( 3 , 'Somewhere in the middle'), ( 4 , 'Strong'), ( 5 , 'Very Strong') ) AS label (Value,Description) ) SELECT label.description,mytable.multiple_choice FROM label LEFT JOIN mytable ON label.Value = mytable.multiple_choice So now you can use the same Count expression to find the number of each label and use label.description as the category instead of multiple_choice. 
Am I not going to run in to the same problem, though - that because the count expression returns zero for each of the first two categories, it won't be represented in the chart?
"Can you check if the query I **ran** is correct?" Sadly we only have the production server &gt;_&gt;
&gt; imagine a junction table with four columns of GUID FKs, and a GUID PK. !!!
I would suggest this is not a job for SQL, but rather a scripting language (choose one, almost any of them) that reads the files and then prepares &amp; executes "INSERT" statements in a loop.
The trick here is that you have explicitly listed all the possible labels in the label CTE. The reason the other 2 categories weren't showing up as categories is that there were no occurrences of those categories. |multiple_choice |--- | 1 |1 |1 |1 |1 | 5 | 5 | 5 | 5 | 5 2,3 and 4 just don't show up so it can't display them Now you are going to get something like the below: description |multiple_choice ---|--- Very Weak| 1 Very Weak |1 Very Weak |1 Very Weak |1 Very Weak |1 Weak |NULL Somewhere in the middle| NULL Strong| NULL Very Strong| 5 Very Strong| 5 Very Strong| 5 Very Strong| 5 Very Strong| 5 Weak, Somewhere in the middle and Strong are now in the list but there are no occurances of them against the category. So the missing categories still show up but with no records against them. So when you do a count you will still get 0 but the categories will appear. Its the power of the "left join" 
I think you are talking about ANSI-89 vs ANSI-92 right?
I did something similar a few months ago, but I did mine in a server 2012 r2 VM with sql server 2016 using hyper-v. My host machine was my laptop running windows 10. Is this similar to your use case?
Um I guess! I'm just using a Virtual Machine setup to Windows 2008 Server, and then I'm using SQL 2008 Express, there's a lot of numbers and I don't get half of them. :[ 
Sure thing, sorry! http://prntscr.com/ebajv7 The server errors seem to change around a lot. And here's my protocols for the server: http://prntscr.com/ebak63
http://prntscr.com/ebaq47 I get these ones when I change it to TCP/IP
Yeah my apologies. 1. Neither - using VirtualBox by Oracle 2. I can connect to the server via management studio yeah, cause I was restoring some databases onto it. But I don't know how to test that server? Like how how do I know if it's online or my ping to it or anything? Does the server need to be online for ODBC to work? 3. Not sure what that means! 4. Yeah this is just my PC that I'm using that is running the VM. 5. Not sure how I figure that out, sorry
1. No experience with that software 2. You are not being clear, you have management studio instaledl on your host computer (not the VM) and you were succesfully able to connect? 3. Virtual switches determine the exposure level of VM to network traffic 4. Sounds good, is your host computer windows also? 5. Open powershell and type ipconfig on host computer and vm and read ipv4 address (should start with a 192 or 172 or 10) * Is the SQL Server service running in the VM?
Sorry, sorry. 2. Everything is on the VM. I have added Databases to the Databases menu http://prntscr.com/ebb5aa and the next step is to connect ODBC to them. Here's more context if this helps at all: http://prntscr.com/ebb5s5. 3. Oh right, so where would I find the menu for my virtual switch? Or on what program rather? 4. Yeah, Windows 10 on my home PC. 5. http://prntscr.com/ebb6hp for the VM and http://prntscr.com/ebb6s5 my personal. 6. It is indeed. Sorry, I know I'm not making this easy. I really appreciate the help though. :) 
I gave that a try, got a new set of errors. http://prntscr.com/ebb7am - it's that bottom one that concerns me most. I'm pretty sure it does exist since it's selected from the drop down, so perhaps the denied access is the issue? Also thanks a lot for your time! I realise helping a total newbie out isn't the greatest.
This would be very easy with the out of the box tasks in SSIS. You can run a for each file loop, which will write the name of each file found into a variable. With that file name you can run a SQL task to check for that file in your table. Then with some logic move that file if it doesn't exist. 
An open work order with no open task is open in error (job is complete and work order needs to be closed) or lost work (job is incomplete and dispatch has lost visibility). The last non-cancelled task is probably the last site visit, knowing who did that would tell me who should have closed the work order. Thanks.
While I totally get you asking here instead of just googling, I recommend searching through older posts in this sub and /r/learnsql, you'll find lots of solid, relevant answers from people you might be able to identify with.
Why are you using an OS and database that are in Extended Support, get minimal if any patches from Microsoft, and will be EOL in 2 years?
Ok, so... It'd be a bit easier with some sample data. Like, if you could mock a few work orders, this would probably be pretty easy. select * from workOrders w WHERE EXISTS (select * from tasks t where w.id = t.wid and status like 'c%'); select * from workOrders w where not exists (select * from tasks t where w.id = t.wid and status like 'a%'); select eid, wid from tasks t where task_no = (select max(task_no) from tasks k where status not in ('cca', 'aca') group by k.wid); I'm not sure about any of these. Some sample data would cement my certainty. eid := employee_id wid := work_order_id Please pardon anything stupid. I'm an MSSQL programmer, not Oracle.
I guess it's OK if you're not one of those 2129 people...
This is by far the most context-free question I have seen in a long time.
https://www.techonthenet.com/sql/index.php has a lot of good sql info with examples. I've referred to it very often over the years.
wasnt really a question so much as a shocked statement who the hell keeps cc numbers in plain text along side card holder info and expiration date 
Yes
If you just want new customers in January.... Select * From IFSAPP.CUSTOMS_INFO Where cre_date BETWEEN '1/1/2017 AND '1/31/2017' edited date* assuming cre_date is when the customer record was created.
Since you're using To_Date I'm assuming Oracle? Start with a literal string, and move into using a variable later, so does SELECT * FROM IFSAPP.CUSTOMS_INFO WHERE cre_date &gt;= TO_DATE( '2017-01-01', 'YYYY-MM-DD') Get what you're looking for? If that works, try taking the quotes off your variable.
Assuming that is the correct table, maybe something like this: SELECT * FROM IFSAPP.CUSTOMS_INFO WHERE YEAR(cre_date) = '2017' AND MONTH(cre_date) = 'January' The WHERE condition here finds the year and the month of the date. And it looks like you are only looking for new customers for only the month of January. I'm not that great at SQL either so there could be a better way of writing this. 
I tried this before, but I get error stating: ORA-00904: "CRE_DATE": invalid identifier 
&gt;!!! Exactly, and they fought us tooth and nail on making any changes - saying things like; "It's just a database", "we don't care about any of the DB functions, we're going to do it all in code", and my favorite was, "It'll scale just fine, we have a team of top developers, and we know what we're doing". Then I populated the db with a million rows of test data just to drive the point home.
Thanks, this is a real helpful tip. I should have included I'm using Toad for Oracle. Column Names are: CUSTOMS_ID NAME CREATION DATE ASSOCIATION_NO DEFAULT_LANGUAGE DEFAULT_LANGUAGE_DB COUNTRY COUNTRY_DB OBJID OBJVERSION EDIT: So I swapped out from my original query cre_date, to creation_date. Here's what I have so far: SELECT * FROM IFSAPP.CUSTOMS_INFO WHERE creation_date &gt;= TO_DATE( '2017-01-31', 'YYYY-MM-DD') It runs the query, however I'm getting no results. 
[removed]
&gt; "It's just a database." "You're right! It is just a database, which happens to be my profession. So... consider taking the advice I'm being paid to give you?"
http://www.sqlservercentral.com/ has it all, check it out.
So.. SELECT MAX( CREATION_DATE) FROM IFSAPP.CUSTOMS_INFO Find out what the most recent creation date is in the table you're looking at. Looking at the list of fields you've got listed, I don't think we're looking at CUSTOMer data, but are looking at CUSTOMS data, i.e. something to do with the Customs office when moving items in and out of the country. You may need to find someone who can point you at documentation for the database schema you're supposed to be querying from so that you can find the tables you need to be getting data from.
Very true, but so is Ozar. They do some side stuff, but they are mostly Microsoft based, so I assumed OP would use similar systems if they are reading through that blog. (Although they do have some general advice and tips too.) 
sigh... once again, a reminder that /r/SQL **≠** /r/SQLServer
Not a stored procedure, but I built a lightweight function recently that creates and manages table partitions. It's triggered every time an insert is made on the parent table. Edit: Probably worth noting that this is PostgreSQL CREATE FUNCTION &lt;schema&gt;.create_partition_and_insert() RETURNS trigger LANGUAGE 'plpgsql' COST 100.0 AS $BODY$ DECLARE partition_date TEXT; table_name TEXT; partition TEXT; min_date TEXT; max_date TEXT; BEGIN partition_date := to_char(NEW.utc_event_dtm,'yyyy_MM'); table_name := TG_RELNAME || '_' || partition_date; partition := 'fact_partition.' || TG_RELNAME || '_' || partition_date; min_date := date_trunc('MONTH', NEW.utc_event_dtm::DATE); max_date := date_trunc('MONTH', NEW.utc_event_dtm::DATE) + INTERVAL '1 MONTH'; EXECUTE 'INSERT INTO ' || partition || ' (col_1, col_2) ' || 'VALUES (' || quote_nullable(NEW.col_1) || ', ' || quote_nullable(NEW.col_2) ||')' || ' ON CONFLICT (hash_value) DO NOTHING;'; RETURN NEW; EXCEPTION WHEN SQLSTATE '42P01' THEN --Catch table doens't exist error RAISE NOTICE 'RELNAME %', TG_RELNAME; IF NOT EXISTS (SELECT relname FROM pg_class WHERE relname = table_name) THEN EXECUTE 'CREATE TABLE ' || partition || ' (check (utc_event_dtm &gt;= ''' || min_date || ''' and utc_event_dtm &lt; ''' || max_date || ''' ), LIKE &lt;schema&gt;.' || TG_RELNAME || '_physical INCLUDING ALL) INHERITS (fact.' || TG_RELNAME || '_physical)'; RAISE NOTICE 'A partition has been created %', partition; END IF; EXECUTE 'INSERT INTO ' || partition || ' (col_1, col_2) ' || 'VALUES (' || quote_nullable(NEW.col_1) || ', ' || quote_nullable(NEW.col_2) ||')' || ' ON CONFLICT (hash_value) DO NOTHING;'; RETURN NEW; END; $BODY$; 
Company I worked for changed domain name and asked for a set of scripts to do a general find / replace to update any occurrences of the old domain. One guy handled text files (HTML pages, etc) and I got tasked with the database side of things. It was a huge company and I had no ability or permission to view even the schemas of any databases the script could potentially run on. So I ended up making a stored procedure that using the INFORMATION_SCHEMA tables, scanned entire databases for any text based columns (varchar, char, text, etc) and then did replace calls on them. Also required to make this script available for MySQL, SQL Server, Oracle, and Postgres, so I had to rewrite it to handle the nuances of each of those systems. Trickiest part was halfway through when we told told a specific subset of URLs had to remain the same (old domain). So the stored procedure had to change to modify only occurrences of the domain name that were not part of specific list of URLs. It was truly awful especially since this task was being given to me and not being handled by individual DBAs of these different systems. I ended up finishing all of the scripts after testing them on locally deployed instances of each database server. To this day I only know of one DBA who actually attempted to run it. I honestly hope nobody else did. The scripts worked as intended in my many tests, but general catch-all scripts like that should NEVER be run by someone who doesn't have direct control of the system they are scripted for. All that being said, it was a really fun stored proc to write lol.
I run a website focused on database developers, not DBAs. It's got a lot of Oracle information, but also general database and SQL information like database design, normalisation, and other concepts. http://www.databasestar.com.
Stored procedure still used that accepts two table names as arguments and alters every field in the second table to be as large as the sister fields in the first if they aren't already larger. Accounts for nvarchar(max) data types and whatnot. Very useful if we've bulk inserted data into a temp table and, before inserting those results into our target table, want to make sure the receiving table's fields are large enough to receive the data.
There is no performance or recovery advantage to putting them on separate virtual disks with the same underlying physical disk. Your benefit to separate virtual disks for the database / log files is if SQL fills one of them up, it won't take down your entire volume. If this is critical data being stored on here, I would strongly suggest splitting the SQL data and SQL log files onto separate physical disks for recovery purposes. You can recover with 0 data loss if either of those physical disks die, but you will have data loss if they both die. If you can stand to lose a few minutes/hours (depending on your transaction log backup frequency) then feel free to put them on the same physical drive.
So are the two people referenced by the OP. Perhaps that's what he's looking for?
So a company I worked for didn't want to pay for enterprise edition of oracle. But they had huge tables and they needed partitions. I had coded several functions that got sql statement and distributed them over pre made tables just like partitions. It was cool and lame at the same time.
That sounds very risky and could potential replace other strings not intended to be replaced by the new domain name string but I'm pretty sure youre an expert and got it figured out. Wait, if you had to perform a replace all on the whole infracture, how long did that take since you said your company is also big
Over time wouldnt this not work anymore cause it would get too big? You saved your company money , ask for a raise lol.
I'm not familiar with Oracle's DB but we were able to do it in MSSQL by just querying the system tables based on the fully qualified table name, so I'm not sure how you'd do that with a separate DB system because I don't know what table metadata you'd have access to.
I made a linear solver that would attempt to either hit a target inventory level or ship a target amount of inventory. It took into account store specific size biases, store inventory, available inventory by size, and comparative store performance. It also had the ability to apply a smoothing factor to store performance to help normalize some of the outlier stores that waaay outsold the rest of the chain. It would do up to 100 iterations trying to hit the target number, would stay within a given percent of the size bias for a store (i.e. dont let size 7 get more than 10% more than its size biasing would indicate should be there). There were actually 5 sprocs that would be called with branching logic based on which variables someone has selected. The final sproc was the cool one tho that took the variables set up by the other 4 and actually did the loop for the linear regression. Ill have to email my old DBA and see if he can send me a copy of it. Would be a nice portfolio piece.
Or better yet, point out to them the cost of an appropriate licence vs. the cost of all the extra time spent creating, maintaining, fixing and documenting this work around.
What did you use for the loop? 
Documenting? What documenting? :) It was like 3 or 4 years ago who knows what happened to that. It was a contract that my company won via tendering (TIL this word in english) to do for a specific office at the state. I think they won by the teeth because it was super super cheap. But the process i built there was pretty complex. It would periodically detect tables that are growing and would repartition them to new tables etc.. They brought me as an expert to build it and when it was done I was replaced with a junior developer. I pity his soul as there was zero documentation to that.
Care to share?
my modification to the pattern above is looking for %[char(0)-char(31),char(160)]% so it's looking for char(0) to char(31) or char(160). Patterns in SQL are similar to regex (but not the same). The square brackets do indicate sets, in the usage above they are for single characters. [0,1,2,3,4,5,6,7,8,9] is the same as [0-9] or [0-8,9]. You use the dash to indicate ranges and the comma single characters. If you put something outside the square brackets like %[0-8]%,%9% you are trying to find a pattern of the character [0-8] followed by the character 9. Why change it to a longer variable then it needs to be and use RTRIM(@pattern)? Using @pattern as char(11) and: @Pattern = '%['+ CHAR(0)+'-'+ CHAR(31) + ',' + char(127) + ',' + char(160) +']%' should accomplish your goal above of finding the unprintable characters?
Probably not the most complex I've written, but this function does exactly what our business wants: create function [dbo].[ZGF_Firms_horizon] ( @SP date, @firm varchar(50)) returns TABLE as return select firm,dateadd(day, (CASE WHEN (SELECT datepart(WEEKDAY, @SP)) = 2 THEN [mon range1 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 3 THEN [tue range1 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 4 THEN [wed range1 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 5 THEN [thu range1 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 6 THEN [fri range1 start] ELSE 0 END), @SP) as start1 ,dateadd(day, (CASE WHEN (SELECT datepart(WEEKDAY, @SP)) = 2 THEN [mon range1 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 3 THEN [tue range1 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 4 THEN [wed range1 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 5 THEN [thu range1 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 6 THEN [fri range1 end] ELSE 0 END), @SP) as end1 ,dateadd(day, (CASE WHEN (SELECT datepart(WEEKDAY, @SP)) = 2 THEN [mon range2 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 3 THEN [tue range2 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 4 THEN [wed range2 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 5 THEN [thu range2 start] WHEN (SELECT datepart(WEEKDAY, @SP)) = 6 THEN [fri range2 start] ELSE 0 END), @SP) as start2 ,dateadd(day, (CASE WHEN (SELECT datepart(WEEKDAY, @SP)) = 2 THEN [mon range2 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 3 THEN [tue range2 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 4 THEN [wed range2 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 5 THEN [thu range2 end] WHEN (SELECT datepart(WEEKDAY, @SP)) = 6 THEN [fri range2 end] ELSE 0 END), @SP) as end2 from ZGT_firm where firm = @firm I wrote this some years ago, if I would have to write it now, I would just use the weekday-int as input instead of selecting the datepart over and over again. I also have made a database for a webgame at home (which I wrote for fun/myself) when I was learning SQL Server 2012. One of the stored procedures uses tons of windowed functions, but the names are all in Dutch so I would have to translate them for you guys when I get home... Not that it would improve the readability that much.
I don't want a link back to where I work or anything. I can tell you it's just from querying the system tables for the column names, sizes, and datatypes and then mapping the size differences into a dynamic sql query.
I finally had a little time to test this on an actual server (MS SQL Server 2012. CREATE FUNCTION dbo.udf_RemoveUnprintableCharacters (@string VARCHAR(8000)) RETURNS VARCHAR(8000) AS BEGIN DECLARE @IncorrectCharLoc SMALLINT, @Pattern CHAR(38) SELECT @Pattern = '%[' + CHAR(0)+CHAR(1)+CHAR(2)+CHAR(3)+CHAR(4) + CHAR(5)+CHAR(6)+CHAR(7)+CHAR(8)+CHAR(9) + CHAR(10)+CHAR(11)+CHAR(12)+CHAR(13)+CHAR(14) + CHAR(15)+CHAR(16)+CHAR(17)+CHAR(18)+CHAR(19) + CHAR(20)+CHAR(21)+CHAR(22)+CHAR(23)+CHAR(24) + CHAR(25)+CHAR(26)+CHAR(27)+CHAR(28)+CHAR(29) + CHAR(30)+CHAR(31)+CHAR(127)+CHAR(160) + ']%', @IncorrectCharLoc = PATINDEX(@Pattern, @String) WHILE @IncorrectCharLoc &gt; 0 SELECT @string = STUFF(@string, @IncorrectCharLoc, 1, ''), @IncorrectCharLoc = PATINDEX(@Pattern, @string COLLATE SQL_Latin1_General_CP1_CI_AS) RETURN @string END GO 
Fair enough. Thought it might be generic. Hope you're having a good day.
If you can provide a sample data set with a table variable and T-SQL, I could poke it. Off the top of my head, why not windows functions? I'm sure what I'm pasting below will be wrong, but it may give you an idea or show some syntax for you. SELECT InvoiceToTP, Grade, SUM(PRMQTY) OVER (PARTITION BY grade,period,customer) AS summedprmqty , FiscalYear, PeriodNumber FROM [TIMEDET].[dbo].[Det_Actual] where FiscalYear in ('2016','2017') and PeriodNumber in ('1','2','3') order by InvoiceToTP, FiscalYear, Grade
Here's a lazy &amp; quick hack, hoisting some of your WHERE predicates up into SUM / CASE expressions: select sum(case when e.SksAcceptedDelay &lt;= 20 then 1 else 0 end) as under20, sum(case when e.SksAcceptedDelay &gt;=21 and e.SksAcceptedDelay &lt;= 30 then 1 else 0 end) as under30, sum(case when e.SksAcceptedDelay &gt;=31 and e.SksAcceptedDelay &lt;= 45 then 1 else 0 end) as under45, sum(case when e.SksAcceptedDelay &gt;=46 and e.SksAcceptedDelay &lt;= 60 then 1 else 0 end) as under60 -- ... and so on from dbo.eCSRStat e where e.SkillsetName like '%KR%' and e.ContactTypeName = 'Voice' and e.OriginatedStamp &gt;= '2017-01-01 00:00:00' and e.OriginatedStamp &lt; '2017-02-01 00:00:00';
I think that would require dynamic sql and I don't even know if you can use dynamic SQL in a function.
you need to convert the selection to a comma delimited string, my quick and dirty code for this would be: Set oVar = Selection Dim passme As String For Each cell In Selection passme = passme &amp; ",'" &amp; cell.Value &amp; "'" Next passme = Right(passme, Len(passme) - 1) With oCM .ActiveConnection = oDB .CommandText = "SELECT gsc_materials.matnonzr, gsc_materials.matdesc, gsc_inventory_bymaterial.stockqty " &amp; _ "FROM gsc_materials LEFT JOIN gsc_inventory_bymaterial ON gsc_materials.matcod = gsc_inventory_bymaterial.matcod " &amp; _ "WHERE gsc_materials.matnonzr IN (" &amp; passme &amp; ");"
Wow, this is exactly what I was looking for. I had a feeling that the range variable needed work, but wasn't sure how to approach. Thanks, I really do appreciate this!
You basically have CONVERT(decimal,x/y,5). The problem is that it will do x/y first which if they are both integers it will assume the product is an integer, so it is converting 0 to 0.00. You can get away with it by just converting one SUM alone, then SQL should assume the rest. Try these out to get a hang of what SQL is doing: select 20 / 100 select convert(decimal,20 / 100,5) select convert(decimal,20,5) / 100 ALSO, don't know if "thing1" can ever be null so the count is 0 because you would run into division errors. If that could be the case then you need to do some ISNULL checks depending on how you want to proceed, but if the column never allows null you should be ok.
I once rolled my own auditing system on SQL Server 2005 because upgrading to 2008 (to get CDC) was not an option at the time. I had a procedure that created triggers on tables and columns you'd specify. Then another proc for doing all kinds of reporting on the gathered data. I had to put this together in 3 days over one weekend, because a customer "forgot" they had to be [Sarbox](https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act) compliant and the auditors were coming in on Monday.
For those interested, for shits and gigs I ran both these statements through one of my large tables. SELECT COUNT(*) FROM dbhpbx2 WHERE calldate BETWEEN '2017-01-01 00:00:00' AND '2017-01-31 23:59:59' Returned in 1.70s SELECT COUNT(*) FROM dbhpbx2 WHERE YEAR(calldate)=2017 AND MONTH(calldate)=1 Returned in 1.69s
A SP that creates a new randomly named Agent job that calls the SP. 
I would start by grabbing the SUM values individually so you can diagnose SUM(Debits), SUM(Credits) Then use those values for your subtraction SUM(Debits)-SUM(Credits) I'd also isolate an employee and view their individual transactions so you can confirm your SUM values are correct.
Perfect. Always good to catch your own errors. Happy resolving.
Your current attempt at the case doesn't make sense to me, can you explain what you're trying to do? At the moment, looking at it, you're going to get RE_RW_Start_Date and RE_RW_End_Date for when your SFRRSTS_RSTS_CODE has RE or RW in, should this be the same? Following this, if SFRRSTS_RSTS_CODE doesn't have RE or RW in, you'll get a NULL in both returns. The situation is the same for your next 2 case statements. select student_name , test_score , case when test_score &gt; 95 then 'A' when test_score &gt; 90 then 'B' when test_score &gt; 85 then 'C' when test_score &gt; 80 then 'D' else 'Fail' end as 'grade' from test_1_scores
'All' is a string and it is not a number. What are you trying to do with the condition of INVOICE_ID = 'all'? Is 'all' an actual value in that column? If you are trying to get all the results, regardless of INVOICE_ID, remove that condition and leave it as SELECT INVOICE_ID FROM IFSAPP.CUSTOMER_ORDER_INV_HEAD WHERE INVOICE_DATE &gt;=TO_DATE('2016-12-01', 'YYYY-MM-DD') AND INVOICE_DATE &lt;=TO_DATE ('2016-12-31', 'YYYY-MM-DD') 
I want to display SFRRSTS_START_DATE in a column where the SFRRSTS_RSTS_CODE has RE or RW, and the SFRRSTS_START_DATE in other columns where the RSTS_CODE equals other values. Is pivot not the correct solution?
 SELECT INVOICE_ID FROM IFSAPP.CUSTOMER_ORDER_INV_HEAD WHERE INVOICE_DATE BETWEEN '2016-12-01' AND '2016-12-31' What is "lab" in this situation, are you wanting only the invoices where something = lab?
Are you using Oracle or MSSQL? Also, if you are just looking for a count, then.... SELECT COUNT(INVOICE_ID) FROM IFSAPP.CUSTOMER_ORDER_INV_HEAD WHERE INVOICE_ID = 'all' AND INVOICE_DATE BETWEEN '12/1/2016' and '12/31/2016'
Your all caps post made me wonder momentarily if this was a /r/totallynotrobots post. Total number of inventories for Dec '16 by lab sounds more like you want: SELECT LAB_ID, COUNT(INVOICE_ID) AS 'InvoiceCount' FROM IFSAPP.CUSTOMER_ORDER_INV_HEAD WHERE INVOICE_DATE between '2016-12-01' AND '2016-12-31' GROUP BY LAB_ID COUNT() is an aggregate function that either stands only with other aggregate functions in the select list, or with any or all of the fields specified in GROUP BY. In my example, the query would return one line or record per LAB_ID value and the InvoiceCount column would show the number of INVOICE_ID values. If you use a column inside COUNT() that has non-unique values, such as if you instead wanted the number of labs a given customer used, you'd group by CUSTOMER_ID and use SELECT CUSTOMER_ID, COUNT(DISTINCT LAB_ID) AS 'LabsUtilized'. The word DISTINCT there ensures that duplicate values are only counted once. You could also include a total from a value in the invoice: SELECT LAB_ID, COUNT(INVOICE_ID) AS 'InvoiceCount', SUM(INVOICE_AMOUNT) AS 'InvoiceTotal' Of course, it's often more complex than that and for the invoices I deal with I'd end up with something more like SUM(INV_DEBIT) - SUM(INV_CREDIT) AS 'InvoiceTotal' and/or something to do with a reversal column in some circumstances. Also, repeating what others have asked, why do you include INVOICE_ID = 'all' in the WHERE clause? If you don't want to limit the results to a specific INVOICE_ID value, don't include that in the WHERE clause.
nevermind I just used self joins, it did exactly what I wanted
THIS WORKED! Thank you very much. First, how are you getting the code to display in the box? Second, I have another one here I'm trying to run. This query is: "Total # of co’s for last quarter of 2016 (oct-dec) by lab" This is what I got, I'm trying to work off of the format for the last query. SELECT CONTRACT, COUNT(ORDER_NO) FROM IFSAPP.CUSTOMER_ORDER_LINE WHERE ORDER_NO BETWEEN TO_DATE('2016-10-01', 'YYYY-MM-DD') AND TO_DATE ('2016-12-31', 'YYYY-MM-DD') GROUP BY CONTRACT; Now I'm getting error that literal does not match format string. ORDER_NO is data type VARCHAR2. Is there like a really in depth YouTube video or something that explains all this?? I took CodeAcademy course and I was thrown into these queries to learn. But it's a lot to take in at once so I'm kind of lost at the moment
So I finally figured it out. Heres what I used to get the info: SELECT CONTRACT, COUNT(ORDER_NO) FROM IFSAPP.CUSTOMER_ORDER_LINE WHERE DATE_ENTERED BETWEEN TO_DATE('2016-10-01', 'YYYY-MM-DD') AND TO_DATE ('2016-12-31', 'YYYY-MM-DD') GROUP BY CONTRACT; I got one more to run, but I might hold off till tomorrow. Thank you very much for the assistance.
Oh no problem at all... it's that whole "teach a man to fish" thing. I think this does it. Select m.employee As Employee, sum(m.Revenue) AS Revenue FROM (SELECT distinct(GeneralLedgerTransaction.GLTID), Employee.LastName AS Employee, (GeneralLedgerTransaction.CreditAmt - GeneralLedgerTransaction.DebitAmt) AS Revenue FROM Employee AS Employee_1 INNER JOIN Customer INNER JOIN BasicPolInfo ON Customer.CustId = BasicPolInfo.CustId INNER JOIN Employee ON Customer.Prod1Code = Employee.EmpCode ON Employee_1.EmpCode = Customer.CsrCode INNER JOIN GeneralLedgerTransaction INNER JOIN GeneralLedgerDepartment ON GeneralLedgerTransaction.GLDepartment = GeneralLedgerDepartment.GLDeptCode INNER JOIN GeneralLedgerBranch ON GeneralLedgerTransaction.GLBrnchCode = GeneralLedgerBranch.GLBrnchCode INNER JOIN InvoiceTransaction ON GeneralLedgerTransaction.InvTPId = InvoiceTransaction.InvTPId INNER JOIN Company ON InvoiceTransaction.CoCode = Company.CoCode INNER JOIN PrCode ON InvoiceTransaction.TranType = PrCode.Code ON BasicPolInfo.PolId = GeneralLedgerTransaction.PolicyId WHERE (GeneralLedgerTransaction.GLNumber between'400000000' and '499999999') AND (GeneralLedgerTransaction.TranDate BETWEEN DATEADD(mm, - 14, GETDATE()) AND DATEADD(mm, - 2, GETDATE())) AND (BasicPolInfo.Status &lt;&gt; 'D')) AS m Group BY m.employee
Hint: add four spaces at the beginning of each line or just select the entire query and press that `&lt;&gt;` button above the textbox to format your query as code. Like: Select m.employee As Employee, sum(m.Revenue) AS Revenue FROM (SELECT distinct(GeneralLedgerTransaction.GLTID), Employee.LastName AS Employee, (GeneralLedgerTransaction.CreditAmt - GeneralLedgerTransaction.DebitAmt) AS Revenue FROM Employee AS Employee_1 INNER JOIN Customer INNER JOIN BasicPolInfo ON Customer.CustId = BasicPolInfo.CustId INNER JOIN Employee ON Customer.Prod1Code = Employee.EmpCode ON Employee_1.EmpCode = Customer.CsrCode INNER JOIN GeneralLedgerTransaction INNER JOIN GeneralLedgerDepartment ON GeneralLedgerTransaction.GLDepartment = GeneralLedgerDepartment.GLDeptCode INNER JOIN GeneralLedgerBranch ON GeneralLedgerTransaction.GLBrnchCode = GeneralLedgerBranch.GLBrnchCode INNER JOIN InvoiceTransaction ON GeneralLedgerTransaction.InvTPId = InvoiceTransaction.InvTPId INNER JOIN Company ON InvoiceTransaction.CoCode = Company.CoCode INNER JOIN PrCode ON InvoiceTransaction.TranType = PrCode.Code ON BasicPolInfo.PolId = GeneralLedgerTransaction.PolicyId WHERE (GeneralLedgerTransaction.GLNumber between'400000000' and '499999999') AND (GeneralLedgerTransaction.TranDate BETWEEN DATEADD(mm, - 14, GETDATE()) AND DATEADD(mm, - 2, GETDATE())) AND (BasicPolInfo.Status &lt;&gt; 'D')) AS m Group BY m.employee 
&gt; "Total amount ($) invoiced AND number of invoices for December 2016 by lab" I would expect it to look something like this: SELECT CONTRACT, COUNT(INVOICE_ID), SUM(GROSS_AMOUNT) FROM IFSAPP.CUSTOMER_ORDER_INV_HEAD WHERE INVOICE_DATE BETWEEN TO_DATE('2016-12-01', 'YYYY- MM-DD') AND TO_DATE ('2016-12-31', 'YYYY-MM-DD') GROUP BY CONTRACT; As I understand your instruction, we aren't looking to count how many gross_amount entries there are (that's what count() does), but instead, trying to find the total of the amount entered for that column. So, we would want to use sum() for that. We (typically) don't want to GROUP BY a column that you are aggregating- GROUP BY specifies which columns are not aggregate columns. 
Maybe something like this, although I'm not sure if I understood your requirement correctly. SELECT InvoiceToTP AS Customer , Grade , PeriodNumber AS Period , SUM(IIF(FiscalYear = 2016, PRMQty, 0)) AS Qty2016 , SUM(IIF(FiscalYear = 2017, PRMQty, 0)) AS Qty2017 FROM TIMEDET.dbo.Det_Actual WHERE FiscalYear IN (2016, 2017) AND PeriodNumber IN (1, 2, 3) GROUP BY InvoiceToTP , Grade , PeriodNumber ORDER BY InvoiceToTP , Grade , PeriodNumber 
Awesome, this might be just what I need.
I am going crazy.... I am using this and it is saying I have a syntax error near the first CASE where it says 2016.... I am clueless as to why... SELECT InvoiceToTP, GRADE_BASIS, FiscalYear, sum(CASE WHEN FiscalYear = '2016' THEN PRMQTY ELSE 0 END) AS 2016pounds, sum(CASE WHEN FiscalYear ='2017' THEN PRMQTY ELSE 0 END) as 2017pounds, sum(CASE WHEN FiscalYear = '2016' then Credit else NULL end) as 2016dollars, sum(CASE WHEN FiscalYear = '2017' then Credit else NULL end) as 2017dollars, PeriodNumber FROM [MPMTIMEDET].[dbo].[Det_Actual] where FiscalYear in ('2016','2017') and PeriodNumber in ('1') group by InvoiceToTP, FiscalYear, PeriodNumber, GRADE_BASIS order by GRADE_BASIS, FiscalYear, PeriodNumber
1) backup table. Your approach is going to be causing performance issues, you're going to blow your buffers out of the water and unless you have file_per_table grow your ibdata files beyond what's necessary. You're better off doing a full snapshot with innobackupex (non-blocking) and loading it onto another server in case you need to restore. 2) The best approach is to partition your table. One partition for the rows you want to keep, and one or more for rows you want to delete. Then once partitioned, drop the partitions you don't want. Now there's limits to what you can partition - the presence of foreign keys for example will prevent you from partitioning in the first place. As far as your stored procedure goes - that can behave very badly depending on the condition in your WHERE clause (for example if you are selecting on unindexed columns). 
Thanks for the reply. I apologize in advanced for any lack of understanding. 1) Ok, I see there is a "mysqldump" command that can be used on the command line to backup databases and/or tables. Problem is, the machine that runs this database disabled the command line. Not to go too into it, but the DB isn't owned by us, so some access is limited. I couldn't find an alternative "mysqldump" that could be done in the SQL editor. &amp;nbsp; 2) Tables do have foreign keys so partitioning is not possible. The delete condition I will be using is a date - each record had a date of when it was entered into the table. The date column does not have an index on it. I'm guessing the issue with my procedure is even though I LIMIT it to 10000, it still does a full table scan because it doesn't have an index? I may be in over my head with this =/
Give this a try. It may have some issues on Oracle though. SELECT x.load_week , y.mnth_num FROM ( SELECT DISTINCT load_week , TO_CHAR(load_week, 'YYYYMM') load_mnth FROM table WHERE load_week &lt; SYSDATE ) x INNER JOIN ( SELECT TO_CHAR(load_week, 'YYYYMM') load_mnth , ROW_NUMBER() OVER ( ORDER BY TO_CHAR(load_week,'YYYYMM') DESC ) mnth_num FROM table WHERE load_week &lt; SYSDATE GROUP BY TO_CHAR(load_week, 'YYYYMM') ) y ON x.load_mnth = y.load_mnth ORDER BY 2;
Instead of backing up the data in a table and than deleting the rows in the original table. Why don't you just load only the records you need into the bckp_table and after verifying the data rename the tables such as the bckp_table gets the original name. Once you are done you can drop the old table. Inserts are much faster than deletes. 
What you are doing is partitioning by month and ordering by load_week meaning the Dense_rank will generate a new number for every unique Load_week in a month. partitioning by month means the dense_rank will reset to 1 for every month. What you need is no partitioning (at least from you example) and ordering by month. so give a try to: Dense_Rank() Over(Order By EXTRACT (MONTH FROM Load_Week) Desc) 
1) mysqldump is a blocking operation. Not what you want. I'd go with what /u/lbilali suggested instead - pick rows, insert into backup table, delete from original. 2) Can you use a different column? Row id, for example might be a good candidate &gt; I may be in over my head with this =/ Have you discussed this with your boss? If it's mission critical data and you're not confident enough with your approach then you may be better off contracting a dba to do this once and document the process (after which you can do it the next time yourself)
Can't you organise a maintenance window to perform the delete? 7 million rows really isn't that much. Should only take 5 to 10 minutes on a decent box
Thanks! I need to check why cannot I use the WITH function, maybe it's different SQL setup?
Q1 you use a temp table and a join. If you google both those terms I'm sure you will figure it out. Q2 you use the count function in the same query as the query with the group by Q3 &amp; Q4 you will probably find most of what you need to answer the question in this post on stack overflow. http://stackoverflow.com/questions/13031850/how-to-find-the-average-value-in-a-column-of-dates-in-sql-server
Are you using MS SQL Server or MS Access? They use different dialects of SQL and have different features.
I guess that's sort of what I was going for with step (1). You're saying, the change would be to add a WHERE condition on the records I need. So instead I would: 1\. CREATE TABLE newtable LIKE oldtable; INSERT newtable SELECT * FROM oldtable WHERE date &lt; 'XXXX-XX-XX'; 2\. Verify data 3\. DROP oldtable TABLE; Because the "date" column isn't indexed, won't this run into the same performance issue as doing DELETE? or will the INSERT performance minimize the issue?
From my understanding, MySQL does not have an equivalent RowID similar to Oracle. I did let them know I wasn't confident with this, but yes I will have to have another discussion and let them know this may be a bit too complicated for me. 
This worked perfectly! Thank you very much for the help. I'd upvote you twice if I could.
not sure what is the expected result
don't forget the steps (between steps 2 and 3 ) a. rename oldtable to del_oldtable and b. rename newTable to oldtable :) The idea was that you wanted to backup the whole table and than delete the rows and in he end if all OK delete the backup table Instead of doing that you can load only the rows you want to keep and than rename tables and drop the old one. Now this will be faster because 1. worst case scenario there is the same time to move data from old table to new table in both cases but with a filter should be a bit faster as less rows have to be inserted 2. renaming 2 tables is way faster than deleting some rows from a table (drooping the old table is fast but still you do not care as you do after the new table has taken it's place) 
Taking things to extreme -- what if I were to have 100 views (all select *s) and write a long procedure which joins to all 100 of them. Are you 100% confident that there would be no loss to performance? That's essentially what I'm asking.
+1 all this. We've gotten to the point of having views that use views that use views. Trust me its hell to troubleshoot the source of these things. Just to restate a few things differently If the view has a WHERE clause filtering data, eh maybe the view adds value as a consistent data set used by multiple systems / reports so one doesn't accidentally forget to filter the data. You can index/materialize the view so it improves performance - but if its a select * with no where clause then you are getting no benefit for twice the work / storage - basically its two copies of the table you are maintaining.
Performance largely depends on the size of the database, if it is small enough (&lt;100mb), bad practices will likely not matter when a small tables can be stored entirely in memory.
This is what I'm looking for, and what the other commenter's code does: MONTH_NUM MONTH_CNT 2-2017 1 1-2017 2 12-2016 3 11-2016 4 10-2016 5 9-2016 6 8-2016 7 7-2016 8 6-2016 9 5-2016 10 4-2016 11 3-2016 12 2-2016 13 1-2016 14 12-2015 15 11-2015 16 10-2015 17 9-2015 18 8-2015 19 7-2015 20 6-2015 21 5-2015 22 4-2015 23 3-2015 24 2-2015 25 1-2015 26 12-2014 27 11-2014 28 10-2014 29 9-2014 30 8-2014 31 7-2014 32 6-2014 33 5-2014 34 4-2014 35 3-2014 36 2-2014 37 1-2014 38
Oh as I see you need to order by year and month give a try to this: Dense_Rank() Over(Order By EXTRACT (YEAR FROM Load_Week) Desc, EXTRACT (MONTH FROM Load_Week) Desc) 
We're talking about tables that are all &gt;10M rows.
&gt;+1 all this. We've gotten to the point of having views that use views that use views. Trust me its hell to troubleshoot the source of these things. Just to restate a few things differently We are in this place as well, and it is cold and scary, and I want to rearchitect the entire process... but that is going to take time, requirement gathering, etc., -- it will likely be a 'mini-project' So I thought I'd come here and make sure of myself before moving forward. Good discussion. &gt;If the view has a WHERE clause filtering data, eh maybe the view adds value as a consistent data set used by multiple systems / reports so one doesn't accidentally forget to filter the data. Nope, just a * &gt;You can index/materialize the view so it improves performance - but if its a select * with no where clause then you are getting no benefit for twice the work / storage - basically its two copies of the table you are maintaining. This is where I am confused. If I have an index on the table, will adding an index to the view do anything?
Yes, we run into this wall every other week or so. Lots of fun. Meta-data is one of my least favorite words.
Thanks! **Q1** Working like charm **Q2** Used Query Builder tool, got this query &gt; SELECT Tweets.screenName, Count(Tweets.screenName) AS CountOfscreenName &gt; FROM Tweets &gt; GROUP BY Tweets.screenName; Working great as well! **Q3** No, I did not realise it's the number of days since 1900, but my professor hinted that it would be a decimal number since the date/time is stored as an integer timestamp in the database. It does exactly what it's supposed to be doing so I'll leave it like this for now ^^ **Q4** So I wanna create a query that creates a new table called “New_Tweets” that only contains those tweets (with all attributes like retweetcount, screenName) that were created after this average time of 42779 days since 1900. But this must be a single query that is executed on the original table. 
I believe what you are looking for is the PIVOT function http://stackoverflow.com/questions/15931607/convert-rows-to-columns-using-pivot-in-sql-server Note: If this is indeed what you are working for it isn't a native function in MYSQL only SQL server. Next time post your datasource type!
That looks right. Thank you!
In MS SQL you can create a store procedure that runs other stored procedures, something like: CREATE PROCEDURE dbo.RunThese @param int AS EXEC Procedure1(@param) EXEC Procedure2() EXEC Procedure3() EXEC Procedure4() EXEC Procedure5() GO 
Typically for complex tasks like this in SQL Server, you would do in a C# assembly. Why do you need 5 stored procedures rather than combining everything into 1 procedure? To do what you are describing, the temporary table will need either a global temporary table or it will need to be writted to written to a database making it not temporary.
I guess I don't know how/didn't know that was possible. I'll look into that.
I have a more complicated suggestion than Pivot, if that doesn't seem like it would work for you (for whatever reason.) select name, sum(group_a) as group_a, sum(group_b) as group_b, sum(group_c) as group_c from (select name, case when group = group_a then number_ else 0 end as group_a, case when group = group_b then number_ else 0 end as group_b, case when group = group_c then number_ else 0 end as group_c from table_name ) group by name ; 
Nope. It's just a * and the reason they exist is because of downstream dependencies (i.e. Tableau data sources) So for example when things were smaller, someone might have created a view such as, dbo.tableauview: select a.*, b.*, c.* from a join (select * from d where x) b on b.id = a.id join c on b.id = c.id join d on a.id = d.id where x Then over time people would request more and more columns, while at the same time the sources were growing. This resulted in the views going from 2-3minutes execution time, to 1-2hrs, and to fix this the view would be rewritten as a stored procedure, where the data would be put in a table such as, dbo.tableautable. Then to prevent all of the workbooks which used dbo.tableauview as a datasource from breaking or having to be modified, they simply changed the logic of the view to a * which points to the new table. 
 select count(*) from table a where (select count(*) from table b where a.value1=b.value1 and a.value2=b.value2 and a.value3=b.value3 and a.value4=b.value4 and a.value5=b.value5) &gt;3
That works too! This code takes roughly the same time to run as the other commenters code, and doesn't do any joins on my huge table. Thanks for the help! I really appreciate it. Here are the results from your Dense_Rank: MONTH MONTH_COUNT FEB-17 1 JAN-17 2 DEC-16 3 NOV-16 4 OCT-16 5 SEP-16 6 AUG-16 7 JUL-16 8 JUN-16 9 MAY-16 10 APR-16 11 MAR-16 12 FEB-16 13 JAN-16 14 DEC-15 15 NOV-15 16 OCT-15 17 SEP-15 18 AUG-15 19 JUL-15 20 JUN-15 21 MAY-15 22 APR-15 23 MAR-15 24 FEB-15 25 JAN-15 26 DEC-14 27 NOV-14 28 OCT-14 29 SEP-14 30 AUG-14 31 JUL-14 32 JUN-14 33 MAY-14 34 APR-14 35 MAR-14 36 FEB-14 37 JAN-14 38
If you can get the boss man to agree, you could recreate the views with schema binding (MS SQL). Then any time the developers try to alter the table structure the server will prevent it unless they drop the view first. Sure its a little PITA, but it would keep things in sync.
I would say go with sum of abs( sign( value[i] - value[i+1])) and compare to whatever is needed (1 in this specific case). Sprinkle with ifnull() if nulls are present.
One smaaall correction: 'PIVOT' is not a "function" of MS SQL - it's a syntactical sugar (i.e. a shortcut) for a &lt;pivot variables&gt;, &lt;agg_function&gt;(case &lt;pivot variables&gt; then &lt;expression&gt; end) as &lt;pivot value&gt; .... group by &lt;pivot variables&gt; with some extra conversion/convention logic pieces thrown in. Sorry - it is a pet peeve of mine when people refer to pivot/unpivot functionality as something special of MS SQL.
Oh, I plan on doing a lot of things as I mature in the position and grow into it. For now just sorting out some things and adding ammo to the belt.
Will this roll back if any of the procedures fail ? 
I want to run a query that creates a new table with New Tweets, would that be possible?
* Doesn't matter, its what you are familar with, MySQL is typically more integrated with PHP and Python than C#. * You may want to look into some SSIS equivilant in mysql to do this as well
I think I understand now. In that case, your filter should return records if the *parameter* is null: ((CAST(POCostDetail.WBS1 AS VARCHAR) LIKE '%' +CAST(@custProjectLookup AS VARCHAR)+ '%') OR @custProjectLookup IS NULL)
Thanks so much for your help, it's definitely starting to all come together. I changed all the WHERE statements to your first example's styling OR POCostDetail.WBS1 IS NULL and the report returned all 7 sonicwalls - YES! But, when I tried searching by Project Number, it returned every PO in the database - NO! So I switched the formatting to your second example OR @custProjectLookup IS NULL and the Project Number search now works correctly - YES! But now it's back to only returning 1 sonicwall. I then tried mixing and matching the formatting with the same result.
That suggests to me that the problem is indeed records that don't have matches in some of the tables you've got in your filter. To fix this I would change your where clause as a test, going through with just one filter at a time. You can test the parameters by listing them all at the start and setting a value manually, e.g., DECLARE @custProjectLookup varchar(30) SET @custProjectLookup = 'Value' DECLARE @custItemDescriptionLookup varchar(30) SET @custItemDescriptionLookup = 'Value' SELECT .... [your statement goes here]. Then rerun your query with just one field being checked in your WHERE clause. This should tell you which tables don't have a match for all seven sonicwalls. Once you've worked that out you can decide what you want to happen when there is no match.
[removed]
Would it not be something like: set @startdate = dateadd(week, datediff(week, 0, getdate()) - 1, 0) set @EndDate = dateadd(day,-1,dateadd(week, datediff(week, 0, getdate()), 0)) this would give you the start of monday and start of sunday If you wanted the end of sunday then the outer would be either minute -1 or ms -3 depending on if its smalldatetime or datetime to get the end of the range
Thank you so much! Completely missed the "into" command, and it's working great now, thanks a lot for all the help
&gt; only if fake news! it'll work just fine as written without the parentheses
I dont know if this is possible, but if the room has designated blocks to be reserved (10-12, 12-1, 5-7, etc) then create a temp table eith each of those values as a datetime associated with a primary key, then you just just ID the keys as open/close based on the form input (IE time 4 is booked but time 7 is open). If the room input range isnt static (10-12 or 10-1), I would not be sure how to do that. 
This would work: CREATE PROC spDistributions ( @TableName NVARCHAR(128), @ColumnName NVARCHAR(128), @CastType NVARCHAR(128) ) AS BEGIN DECLARE @SQLString NVARCHAR(MAX) SET @SQLString = N'SELECT cast(' + @ColumnName + ' AS ' + @CastType +') AS [' + @ColumnName + '], CT = COUNT(*), COUNT(*)*1.0/SUM(COUNT(*))OVER() FROM ' + @TableName + ' GROUP BY ' + @ColumnName EXEC sp_executesql @SQLString END
If you're on the right version of SQL Server, you could use try_cast() instead of cast()
You can have a look at this response on [stackoverflow](http://stackoverflow.com/questions/451415/simulating-group-concat-mysql-function-in-microsoft-sql-server-2005). In MySQL there a built-in function called `GROUP_CONCAT` which is doing something close to what you wish. In the link I mentioned to you, it is suggested a way to simulate that function in SQL Server. 
Can you post the structure of your tables?
I have a lot of tips I've learned over the years that I could share, and it's probably an incomplete list. * Use ANSI joins (using the join type and ON clause) where possible instead of Oracle-joins (the (+) symbols) as they are clearer and easier to read. * Use column aliases for complicated column names. * Use table aliases for all tables in your query, and keep them short where possible. * Format your queries consistently (e.g. indentation, case) * When inserting, specify the columns to insert into rather than leaving them out. This helps future-proof your code and avoid errors if table structures change. * For a DELETE or UPDATE statement, SELECT the rows you're going to change first to double-check the query is correct. * Use the right function for the query, and avoid many layers of nested functions if you can. * Only SELECT the columns you actually need to use. * Use the WITH clause for complicated queries or queries you might need to run more than once. * Learn the shortcuts of your IDE, whether it's Toad, SQL Developer, PL/SQL Developer, or something else. * Learn to read an EXPLAIN PLAN so you can understand where bottlenecks in your queries are, especially if you're writing a large query. * If you need to delete every record in a table and don't need to ROLLBACK, consider using the TRUNCATE statement as it usually performs better. * Learn how to use analytic functions. They can help you get the answer you need in many cases without using nested subqueries. I run a blog that focuses on advice for Oracle database developers at www.databasestar.com and I have written many articles over the years. I don't have a central "Oracle tips" post but these are tips that I've learnt during my career and usage with Oracle. Hope this helps!
The cool things I miss out on by being on 2005 still :-(
Thanks - also appreciate the Kevin Durant reference :)
I've used this in SQL2008: ...for Monday of Last Week: SELECT DATEADD(wk,DATEDIFF(wk,7,GETDATE()),0 ...and Sunday of Last Week: SELECT DATEADD(wk,DATEDIFF(wk,7,GETDATE()),6) Then to search by a range on a field with time, I've eliminated the time by casting both as a date. Something like this: select * from shipment_header where cast(ACTUAL_SHIP_DATE_TIME as DATE) between CAST(DATEADD(wk,DATEDIFF(wk,7,GETDATE()),0) AS DATE) and CAST(DATEADD(wk,DATEDIFF(wk,7,GETDATE()),6) AS DATE) 
Use a "not exists" clause in your query. It was made for this exact type of thing. This will produce a list of all room numbers that are available for the time slot provided: SELECT distinct roomnumber FROM reservationTable A WHERE NOT EXISTS (SELECT null FROM reservationTable B WHERE A.roomnumber = B.roomnumber AND ( (B.starttime &gt; x and B.starttime &lt; y) OR (B.endtime &gt; x and B.endtime &lt; y) OR (B.starttime &lt;= x AND B.endtime &gt;= y) ) Edit: I left off one condition that I needed. Query fixed.
I believe this varies depending on the database vendor. The effect of changing the underlying table has a different impact in Oracle vs MS-SQL.
What is the query that you attempted to use? Please provide the table structures
you shouldn't have an array of IDs as a value of a column (breaks the 1NF). If you do, how you handle that will greatly depend on the environment you're in and the way the array is captured.
This sounds like we are doing someone's homework.
Does this work if I don't have two tables? My roomTable and reservationTable are just two columns in the same table.
when you say "SQL database" do you mean Microsoft SQL Server? try sticking a leading 0 in front of the 5 and 7
In addition to /u/r3pr0b8's question &amp; suggestion, you need to explain what "isn't working" means. Does it give invalid results? No results? An error message? &gt;they were using things I had never seen before like DECLARE and convert() Did you Google the terms you didn't understand? Both of these are pretty straightforward, especially `declare`.
here's the solution for a single room... suppose we designate -- X = inputted start time Y = inputted end time S = startTime column E = endTime column now let's visualize all possible overlaps -- X Y | | | | 1. S------E | | | | | | 2. S------E | | | | | 3. | S------E | | | | | 4. S----------------------E | | | | 5. | S------E | | | | 6. | | S------E okay, the overlaps are cases 2 through 5 above now consider this SQL WHERE clause -- WHERE E &gt;= X /* eliminates case 1 */ AND S &lt;= Y /* eliminates case 6 */ run a query with the above WHERE conditions, and if any results are returned, you have an overlap if no results are returned, you can go ahead and book the new reservation X to Y expanding this to multiple rooms is left as an exercise for the student
don't you hate dipshits who delete their question after they got the answer?
Ya I tried to scale it up and then I ran into this problem. I was checking for those 5 conditions in the last line of my SQL statement.
Yes, it should. Query updated.
There are 3 more scenarios that you did not list. So I added them below as numbers 7, 8, and 9. You forgot the cases where the Start and End times exactly match X or Y. X Y | | | | 1. S------E | | | | | | 2. S------E | | | | | 3. | S------E | | | | | 4. S----------------------E | | | | 5. | S------E | | | | 6. | | S------E | | | | 7. S-----E | | | | | 8. | S-------E | | | | 9. S----------E | | 
thank you... please notice i used **&gt;=** and **&lt;=** in my WHERE conditions so, 7 is a subcase of 2, 8 is a subcase of 5, and 9 is a subcase of 3 i guess seeing them separate on the diagram might help, but in that case i'd renumber everything ;o)
hey, happy cakeday! did you know that ORs perform poorly as compared to ANDs... this is because ANDs can use an intersection of indexes... which is why i prefer my WHERE conditions ;o)
i am not familiar with the "getting stuck" error message 
Comparing it to the 9 possible scenarios it should: 1. Include 2. Exclude (because endtime is &gt; X and &lt; Y) 3. Exclude (because starttime is &gt; X and &lt; Y but also because endtime is &gt; X and &lt; Y) 4. Exclude (because starttime is &lt;= X and endtime is &gt;= y) 5. Exclude (because startime is &gt; X and &lt; Y) 6. Include 7. Include 8. Include 9. Exclude (because starttime is &lt;= X and endtime is &gt;= Y) But if a room had ANY overlapping reservation (like one of # 2,3,4,5,9) then the entire room would be left out (even if it has non-overlapping reservations also).
 SELECT * FROM l WHERE l.id % 10 = ROUND(RANDOM()*10) LIMIT 100
Why do you have to delete all your posts, /u/RioGirl ? We come in here, help you out for free, and then you run as soon as you get an answer! I spent some time on this (for fun) as did /u/r3pr0b8 and it's kinda rude to not even say thanks. Worse, you removed all the context for the answers. Anyone who comes across this later will miss out on the original question. 
Much more. It's a full turing complete programming language. EDIT: T-SQL is at least.
Right() needs a string as the first argument. You can cast l.id as text and it should work.
but that's an assumption not applicable to all kinds of reservations hotel rooms, for example, need a clean-up time between reservations but i guess if it really matters, you can change to strict inequalities
Ohh, okay. Thank you! Knowing just what I listed in the main post, is that enough SQL knowledge for a Business Analyst? (if you're familiar with that role)
I'm a BI Manager and 99% of the SQL my team uses is data manipulation. I put SQL on my CV too, but it's an analyst / data scientist / business intelligence CV, not a developer CV. So you're fine. I'd say that the following is good enough: - Know the clauses and what they're for, including different join types - Know a few flavours, like MySQL, Oracle, SQL Server - Know how to use an IDE like SSMS or Toad - Know more advanced operations like subqueries, partitions, 'with' clauses - Know the more OLAP-side of SQL like rollup and pivot (I think they're pretty naff but whatever) - Know a bit about a procedural extension like PL/SQL - Maybe get to know the reporting packages, e.g. Crystal Reports, SSRS, Business Objects I also really like to see R or Python on a CV but it probably crosses the line between Business Analyst and Data Analyst. I think even SQL kind of crosses the line but there is a lot of overlap these days so that view is probably a bit old fashioned.
Ok, sounds good, ty! I will put that stuff in my notes.
I spent many years as a BA and now I'm a BI developer. I think it's a good thing if a BA has the SQL skills you mention, or at least a basic understanding of relational theory, but it's not a requirement. I expect a BA to get requirements from the business and give me solid specifications about what the business wants. I'll ask questions such as "what if the Invoice field for a customer is NULL?" or "what if a customer appears more than once in the result set?" and if you're a technical BA it's nice if you thought through those kinds of things beforehand. Being a BA is an excellent way to become a subject matter expert as well as determine your career path. I chose the technical route, but if I had to do it over again I may have chosen the Project Manager route. Project Managers (at least in my experience) seem to have values as individuals to the enterprise, while IT developers are seen as an expensive cost center.
Thank you! That makes sense - I doubt I will be doing much of the 'administration' process with SQL/etc. Probably just some 'data manipulation' occasionally. The job description relates more to SDLC and requirements gathering and that entire process, minus any actual 'programming' or dev (other ppl do that)... but it did mention SQL once, so I thought I'd ask here to get some clarification. I don't have any other questions for now, but ty for offer!
Thanks for the info! I will make a note of that 2nd paragraph. BA is really the exact job I want (i start in 4 months), but my thing is, I am willing to be as technical as needed, *without* actually having to program/code. And to clarify, as that means dif things to certain people, is that I don't mind stuff like SQL and queries, but actual programming code like C C# C++ python VBNET and all those things (sry, I'm not that familiar with all the code language names). Anyway, I like the technical side up to that point, but I do think eventually I will go the PM route, but at least for now, I'd like to remain a BA for a long time before switching.
As someone also learning basic SQL querying to eventually transition into a data analyst role, this has been very helpful. Also, what are your thoughts on power pivot/bi? I am also learning it on the side. Once I reach a decent understanding of sql/power pivot, what would you recommend next as priority? R, Python, VBA, or something else?
I asked this below also but what are your thoughts on Power Pivot/BI? I am learning that on the side as well as SQL querying. Worth it? Or move to R/Python?
The parent mentioned [**Business Analyst**](http://legaliq.com/Definition/Business_Analyst). Many people, including non-native speakers, may be unfamiliar with this word. **Here is the definition**^\(In ^beta, ^be ^kind): **** A business analyst is someone who analyzes an organization or business domain (real or hypothetical) and documents its business or processes or systems, assessing the business model or its integration with technology. The International Institute of Business Analysis (IIBA) describes the role as "a liaison among stakeholders in order to understand the structure, policies, and operations of an organization, and to recommend solutions that enable the organization to achieve its goals." The role of a systems analyst can also be defined as a bridge ... [[View More](http://legaliq.com/Definition/Business_Analyst)] **** ^(**See also:**) [^Mis](http://legaliq.com/Definition/Mis) ^| [^Database](http://legaliq.com/Definition/Database) ^| [^Job ^Hunting](http://legaliq.com/Definition/Job_Hunting) ^| [^Systems ^Analyst](http://legaliq.com/Definition/Systems_Analyst) ^| [^Nonprofit ^Organization](http://legaliq.com/Definition/Nonprofit_Organization) ^| [^Business ^Analysis](http://legaliq.com/Definition/Business_Analysis) ^(**Note**: The parent poster ) ^\(tempy4) ^can [^delete ^this ^post](/message/compose?to=LawBot2016&amp;subject=Deletion+Request&amp;message=cmd%3A+delete+reply+t3_5w5gch) ^| [^**FAQ**](http://legaliq.com/reddit)
deleted due to fuckup with the title. will repost.
Cool, that makes sense! Yeah a refactor is definitely needed; definitely looking forward to getting rid of that array column. 
I would (you may not, please feel free to enlighten me and everyone else who reads this) lump all of koi-koi's list into "Administration" as in "DataBase Administration" (DBA). There's sort of a separation of concerns around this at the enterprise level, since DBA is a full-time, on-call concern and the app development, data science/analytics are disciplines where your concern is usually around business logic and the most efficient SQL to get data. All the normalization, indexing, pooling, etc. is done for you--now you just need to make sure you understand how to use it all. 
Every single bit of backend logic in the systems I work with is written in T-SQL (and I've seen the same in Oracle + PL/SQL and Postgres + PL/pgSQL). Hundreds of stored procedures, thousands upon thousands lines of code. While you could argue it all comes down to "querying the database", it's a bit of oversimplification.
I don't know if anyone's going to help you, because your database design is completely against any and all standards. You shouldn't store those multiple loot values in columns, you should have many-to-many relationship tables, something like: CREATE TABLE npcobject ( guid INT(11) PRIMARY KEY -- whatever else are the attributes of an npc object ) CREATE TABLE item ( guid INT (11) PRIMARY KEY -- whatever else are the attributes of an item object ) CREATE TABLE itemstat ( guid INT(11) NOT NULL, item_guid INT(11) NOT NULL, -- not sure if I get the stats right ) CREATE TABLE loottable ( npc_guid INT(11) NOT NULL, item_guid INT(11) NOT NULL -- composite npc_guid, item_guid PK, FKs to respective tables ) This lets you specify however many items for each npc (one can have 5, the other 19271) and query them easily, recursively if needed (although MySQL doesn't support recursive CTEs, or CTEs at all, so I'm not sure how). With your current design, joining on multiple columns, some of which are bound to be useless, is going to be the exactly pain the ass you're experiencing right now. 
That's a sum. If you would like to show the sum on a separate row, look up the cube functions for your particular RDBMS
A data viz / communication platform like Power Pivot, Tableau or Qlik, I'd say this is almost a prerequisite - I say 'almost' because they're quite intuitive and shouldn't take as much time to learn as SQL. I'm also a fan of just raw excel skills too. How's your stats knowledge? Can you explain what a linear regression is, why it's useful, how to test for model quality, how to be sure that you can reject your null hypothesis, how it can be used for inferential and predictive statistics? What about classification? ML? Clustering? You don't need to worry - I'm pretty bad at stats, and we're not data scientists. But Google 'data scientist venn diagram' for reasons why I think it's important to at least have some knowledge here. It's the classic phrase 'people who know enough to be dangerous'. R or Python - take your pick. In my team it's R, with a bit of Python if needed. I feel R has a slightly higher learning curve because every package is tooled differently, as a language it's more functional than imperative, and it's syntactically a bit weird. I like it, though.
Make me a stored procedure that creates a new view. Can you do that?
yeah it's the way the game stores the data in glorified custom made zip files which then contain individual *.db files with all the data in another arbitrary binary format ( the .db files are what is now a database table each since we basically just re-did the extractor and imported the whole thing into a mysql ) - so it's not my own design but by the company that makes the game in the first place - it's essentially raw data the game client loads and uses :/ We just create a player-based item / game data overview since all official sites of this kind are long gone. So far I managed to get most of the data with rather simple queries or a join here and there, but this task really got me stuck since the necessary data is spread across so many columns instead of having it in a single one :) for the record (or rather "the lulz") this is what one table looks like fully since we essentially only did a raw import: http://pastebin.com/aMFVHNKh a lot of the columns are maybe used once across 10K monster entries, loot tables are even worse. (tales from the data crypt: for some fairly basic world drops you can find internal references from one loot table id to another 10 levels deep (just to generate weighted chances since table1 has a 20% chance and tablea1 within it 15% and table aa1 within that one 10% and so on) where it then contains lvl 2 arrows with a 0,002% chance on the item itself... and the best part? since the chance to drop is also saved along the item and anything past accumulated 100% dropchance is automatically ignored ... that stuff does not even do anything but pollute the data. I'll gladly take suggestions on redoing the structure since as you already wrote it how it currently is is simply just a pain in the ass. I'll try with your basic layout for a start, it should not be too hard to populate it with the most common item data and see where that leads me then. I guess it's about time that we redo the whole thing and write the necessary used data into our own structure and just keeping the raw data archived to make use of once we find new relations. Most of the time the issue really only shows when it comes to loot - other problems are more of the "okay, which column represents THIS attribute on $item in the first place and what is it related to?" 
Procedural sql is a full programming language. But many databases requires the user to only execute these statements as stored procedures. So more advanced than just sql dml statements. 
Select 842 + 4 + 94245 + 91 + 53
 select sum(column_name) from table_name; [Here's the T-SQL documentation on the function](https://msdn.microsoft.com/en-us/library/ms187810.aspx)
^this SELECT SUM(field_name) FROM table; 
If that is correct as you say, then that makes me feel better lol. I remember doing normalization in my database course, and to begin with I understood it and it was easy, and then I think we got into all the normalization forms like 1 2 3 4 or w/e they're called, and that's when it went over my head. I think it was something simple that messed me up, but I'll just have to review that info as I'd like to learn it regardless. But yeah.. if all that is done for me, and I just need to do queries/etc, that's good w/ me :)
You can use a **union**, a **subselect** and the *dual* to add a TOTAL to the end of records. select item, amount from inventory union all select 'TOTAL', (select sum(amount) from inventory) from dual; [Working example.](http://sqlfiddle.com/#!9/65c3f3/4)
Table 3 has multiple events of different types for one msg, and you want the most recent one per msg? A subquery of table 3 where you qualify for the most recent event for each message should do it. Let me know if my understanding of your problem is correct 
Seems like you understand - I was using Max(EventDate) which was correctly returning the most recent event for each msg. It's not until I also try to get the associated EventId that there's a problem. 
Select * From table3 Qualify row_number() over (partition by eventmsg_id order by event_date desc) = 1 Would be how you'd write your subquery in Teradata. Not sure what db you're using. What you are describing appears to be a window function. I.e For each msg, find the most recent event. Sorry for the format I'm on my phone. Edit: check this out https://drill.apache.org/docs/ranking-window-functions/
For free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can submit exercises online too with feedback.
That seems like it might do the trick, I'll give it a try tomorrow. Thanks!
[removed]
[This](http://stackoverflow.com/questions/2334712/update-from-select-using-sql-server) is what you need. Let me know if the example provided is proving too hard to translate what you've got. I'm envisioning something like: Update A Set Thing = X From A as Part2 join Part2 on A.PrimaryKey = Part2.PrimaryKey Something like that.
Nope, I think I got it... now lets see if there's a PK involved. 
It doesn't need to be a PK, but that's definitely the best field to join on. As long as there is a unique identifier to each row, you're good. If there's not, then... slap the designer.
Yeah... if not then I need to join on every field. 
Can you modify the table? Can you make a new table? Are we dealing with tons of data? I'd add my own column and make a PK or insert into a new table or variable or temp table with a PK that auto increments with the insert and make my own.
I'm not sure what Total is, but I'm assuming it's the total sales. Is it possible there are duplicate stores in the sales table? -- Distinct numbers of stores in sales table Select count(distinct store) from sales -- Distinct numbers of stores in stores table Select count(distinct store) from stores -- Distinct numbers of stores in both tables that they have in common select count(distinct a.store) from stores b inner join sales a on a.store = b.store -- How many stores have more than one address listed select count(store), store_address from sales group by store_address having count(store) &gt; 1 
That was actually thought right after thinking of joining to everything, but in this case we do have a PK, just verified it.
Nice, that will make it way easier.
&gt; Select distinct count(store) &gt; from sales i just threw up in my mouth a little
&gt; You could create a stored procedure that could scan the history table to find out where a car is currently located Or a View with a windowed function like Rownumber, or a persisted table with a trigger on CarHireHistory that updates CurrentCarDepot.
I started with that and it was OK, especially if I wasn't in front of a computer and I was just starting out and new to SQL. But eventually you'll want to get your hands dirty and do some actual practice problems. Recommendations from previous posts are in the wiki, which can be found on the sidebar. Don't get too focused on what's the "best resource" or "which SQL implementation (SQL Server, PostgreSQL, MySQL, etc)". 
thanks (sarcasm intended) for editing your initial post luckily, i quoted your error in my comment, eh 
Try this: SELECT stores, store_address FROM stores WHERE store_address IS NULL; should give you any stores that are missing addresses. 
along that thought, bonus SQL to find stores that have the same address as other stores, or have no addresses SELECT store , store_address FROM stores WHERE EXISTS ( SELECT 1 FROM stores stores_in WHERE stores.store != stores_in.store AND stores.store_address IS NOT NULL AND stores.store_address = stores_in.store_address ) OR stores.store_address IS NULL ORDER BY store_address , store
I would say there's stores in B that are not in A. A select distinct store on A then B might explain why the join has fewer records.
I think you misunderstood my point. I didn't mean that SQL as a language is incapable of logic. But the logic is all accomplished through querying databases. I was referring to the SQL language specifically sine that's what OP asked about, not to an RDBMS broadly, or complementary tools that come bundled with an RDBMS like SSIS. I was simple trying to explain to a new developer that SQL is not a general purpose programming language. People aren't going to be using it to write the front end of a website.
I know that, you know that, It's entirely possible /u/Rehd did not know that or it's also entirely possible it was a mistake. Don't know, don't really care. But that's why I emphasized the word just. Because I was fine with you being a dick as long as you point out why. Had I caught it first I likely would have said something just as dick-ish but I would also have said "Having a distinct in a count(*) is pointless since you're already counting everything into one result." People come here for help and to learn. Being a dick is fine as long as it's not just for the sake of being a dick.
But you're still wrong. The phrase "the logic is all accomplished through querying databases" is not accurate. I can write a stored procedure that is passed in a value through a parameter, that parameter value is then sent through an IF branching statement that then in turn outputs something different depending on that input value I passed in. At no point in this process am I querying any tables or doing anything at all with data. &gt; People aren't going to be using it to write the front end of a website. And I'm contending that T-SQL and the PL/SQL are general purpose programming languages. You wouldn't write the front end of a website in C either. Would you say that isn't a general purpose programming language? 
I'm assuming you have a table that has a column of movie titles and then a column of actors/actresses that appear in them. In this case it'd be something like SELECT tablename.movietitles FROM database.tablename WHERE tablename.actor = "Brad Pitt"
Does #Louisiana2 include columns named [Mo] or [Yr]? You could try two things: 1. try executing the query without the Mo and Yr columns in the SELECT projection list and the GROUP BY. See if that doesn't fail. 2. Assuming you're trying to project the pivoted Mo and Yr values, Try fully qualifying them in SELECT and GROUP BY with LAPivot.Mo and LAPivot.Yr and see if that helps. Post the schema of the #Louisiana2 table. Also, I'm assuming this is MSSQL by the error message format. Please clarify if this is not correct.
T-SQL is an abbreviation for "Transact SQL" which is the dialect of SQL that is used by Microsoft SQL Server (MSSQL) and one or two other RDBMS's out there. All SQL is based on ANSI SQL for the most part. But each different RDBMS has its own implementation of SQL to try to leverage functionality that's specific to that RDBMS.
I understand what you are saying, and where we disagree. I don't think of T-SQL and PL/SQL as general-purpose programming languages as you do. In my opinion, general-purpose are languages such as C, Python, C#, Java. Languages such as SQL and R are meant to be used for more specific applications, SQL being even a little more narrow-scoped than R. And its totally fine to disagree. I don't lean towards or see many systems out in the wild developed with T-SQL that don't involve tables at tall, which is why I am considering T-SQL to be a DDL and DML specific language. 
This depends entirely on your schema. Suppose you have these tables: **movies** id | name --|-- 1 | Face/Off 2 | Leaving Las Vegas 3 | Con Air 4 | The Rock 5 | Cinderella Man **actors** id|name --|-- 1 | Nicolas Cage 2 | Sean Connery 3 | Renee Zellweger **movies_actors** movie_id|actor_id --|-- 1| 1 2| 1 3| 1 4| 1 4| 2 5| 3 To find all movies that Nic Cage is in, SELECT movies.name FROM movies JOIN movies_actors ON movies_actors.movie_id = movies.id JOIN actors ON actors.id = movies_actors.actor_id WHERE actors.name = 'Nicolas Cage' To find "an actress/actor who appears in every film," SELECT actors.name FROM actors JOIN movies_actors ON movies_actors.actor_id = actors.id GROUP BY actors.name HAVING COUNT(actors.name) = (SELECT COUNT(1) FROM movies) That would return no results using the tables above, because no actor is in all of the movies. If you delete movie ID 5, it would return Nicolas Cage.
As others said.. it depends on your table. Run this in sql plus where tablename is the name of your movie table. DESC tablename; This will show the columns available to you. From there it will be a WHERE condition on the Actor column (whichever one is that in your DESC). Example (assuming the information is all in the same table.. no joins required) select * from tablename where actorcolumn = 'Jackson'; Hope that helps. Let me know if I can explain it a little better for ya. Edit formatting. Edit this is also assuming oracle sql for any syntax. 
Well, when\if you can, post the schema for the temp table or the SELECT..INTO query, but add the data types. There might be something that stands out. I'm interested to see what happens with this.
SELECT dbo.HoursMins(SUM(CASE WHEN analysis.FeeEarnerRef = 'username' AND analysis.year = 'current Year' THEN analysis.ChargeableTime END)) as col1, dbo.HoursMins(SUM(CASE WHEN targets.FeeEarnerRef = 'UserName' and targets.year = 'CurrentYear' THEN targets.chargeabletime END)) as col2
OK, this looks so much neater! Do you mind explaining exactly what is happening here?
apologies, I should have read the entire question... you can reduce the year portion of your query like this: where users.Department = 'IP' and users.FullName not like 'Z%' and analysis.year = CASE WHEN MONTH(GETDATE()) IN (1,2,3,4) THEN YEAR(GetDate()) - 1 ELSE YEAR(GetDate()) END But am I wrong in that you always want to look back 4 months? So maybe you want to do: where users.Department = 'IP' and users.FullName not like 'Z%' and analysis.year = year(DATEADD(mm, -4, getdate())) 
ah ok, ty
Awesome! Thank you so much
and I am guessing you only have month and year, no full date column?
Thanks, what does a `With` clause do and how would it help with my syntax?
It just lets you alias a query with a special name so you can easily reference it elsewhere. Technically it doesn't "do" anything, since you could also have these as aliased subqueries and then join those subqueries to each other. It's just a cleaner way to represent that concept, because now it's happening in the query the same logical order you're thinking about it - I'm creating one or more row-column sets, and *then* I'm doing things to them like they were tables. So in your case, if you needed to, you could combine them in such a way where you do it in one query and e.g don't hit alldatainput twice. But in context I have a bunch that performance isn't especially a concern for you, and it might suffice for you to know that SQL already has quite good support for the idea of referencing query outputs like they're tables. (For the pedants - yes, there are things like recursive CTEs such that it's not exactly the same as just directly doing the subqueries in the FROM clause. I do think it's a fair general description that a CTE is mostly just a method to very cleanly do what you can also do directly in the FROM clause in a less readable way)
`WITH` defines the CTE (Common Table Expression) https://www.simple-talk.com/sql/t-sql-programming/sql-server-cte-basics/ What flavor of SQL are you using? AFAIK, MySQL _still_ doesn't support CTEs...
I tried with "YTDSales" as (select dp.Superhero, (count(sd.Sale_Date) filter (where extract(year from sd.Sale_Date) = 2017 and sd.Order_ID= 'GAP' )) - (count(sd.Cancelled) filter (where extract(year from sd.Cancelled) = 2017 and sd.Order_ID = 'GAP')) as "Net for 2017" from allsalesdata sd left join facttable ft on ft.salesdatafk = sd.pkSalesdata left join alldatainput dp on dp.alldatainputpk = ft.alldatainputfk and sd.Order_ID = 'GAP' group by rollup((dp.Superhero))) Error: ERROR: syntax error at end of input 
Thanks ihaxr, I added another line to the script `group by rollup((AllTime.Superhero))` and when I ran the query it showed all ZEROS for `Net For 2017` and ONES for `RPG Inventory`. Any idea why this is?
Sure. The temp table is: Column Name Type NDC varchar Name varchar Brand varchar Paid numeric Units numeric PpU numeric Rate decimal Type varchar GCN char AAC decimal Mo int Yr int Eff_Dt datetime
This is so helpful, thank you! Just so I understand what you're doing, you're counting the number of appearances for every actor and checking if it is equal to the number of columns?
Correct! I have three tables: one with movies, another with actors, and a third with, say theaters, that contains the primary keys from movies and actors. So my issue is that I know how to find the films with Brad Pitt, like you've described above, but I'm not sure of how to get any actors who're in all of the films in my table. Thanks again!
Thanks! I have three tables: one with movies, another with actors, and a third with, say theaters, that contains the primary keys from movies and actors. Now my goal is to return any actors who appear in all of the movies. 
Sorta. I completely understand your query above, but I don't know how to reverse it to retrieve an actor who appears in every film (i.e. present in every column of my table)
I'm guessing you are using Microsoft SQL Server based on the terminology you have used. It sounds like your database is in FULL recovery model but you don't have any log backups set up. If you run: Select name, log_reuse_wait_desc from sys.databases You should see the reason the log isn't being cleared down. If this query says log backup then that's your issue and you need to configure a job to run log backups periodically. Let me know if you need anymore assistance. 
&gt;10:30 Tell wife goodnight and go to my office to play video games. I have not felt a woman's touch in a long time, I am married. Dude, right in the ticker... 
20 minute commute? I seethe with envy.
shitpost on reddit
I would be able to make more money if I went to work for a larger company, but being able to ignore pretty much everything after 7:00pm is worth it. I only hear from my boss if it is a legitimate database issue that requires my immediate attention and that is rarely because I don't entirely suck at my job.
&gt; Look at email on phone do determine the scope and magnitude of the cluster fuck I am walking into that day This really is for anyone in operations or any tech support of ANY capacity. It took me about 6 months of being outside of direct support to realize I no longer do a threat assessment before I walk out of the door every morning like I'm going into a battle field. 
Inner join the three tables together and then you can isolate films with a specific actor using a WHERE clause 
if you want to set the column to 0 on all records in the table you can issue an UPDATE statement to accomplish that. Only update rows that have a creditlimit set, using the WHERE clause: UPDATE Data1.dbo.SLcust SET creditlimit = 0 WHERE creditlimit &lt;&gt; 0 
Plot revenge/harm against the development teams.
Because you can't drop it if it doesn't exist.
by dropping all the indexes and doing a backup job during business hours muahahha 
You don't strictly need to if you're sure that it's a completely fresh install and that the user isn't going to accidentally try to run the same script twice, but on the other hand there's no reason not to do that and it's good practice to make your scripts as error proof as possible. Doing so also means that the same script can be used as an update script.
Azure sql is just a public connection string with sql with. You can also acl the external connections 
You need a day to go alone with the month and year, as they're not an optional component. Then you can use the following select convert(datetime, '01/' + datecol, 103) The 103 indicates the date format that it's converting. [Documentation here](https://www.w3schools.com/sql/func_convert.asp)
"Poor planning does not constitute an emergency." It's a fucking mantra.
&gt; a query to organize all the rows what do you mean, organize?
not sure about any missing keyword, but it appears you have the PK referencing itself with the FK2_ITEM constraint
Damn dude. Are you me?
Surf reddit
There are some alternatives if you don't want to store it as a date with a fake day. For example, you can store it as two int columns: month and year. The right answer really depends on how this data will be used.
 SELECT userid , lessonid , videoid , COUNT(*) as dupes FROM thetable GROUP BY userid , lessonid , videoid HAVING COUNT(*) &gt; 1 
Thanks, I'll give this a try.
I would recommend asking more specific questions about SQL here and directing questions about displaying your data to a web dev subreddit. This post is so vague that I'm not sure if you need help with SQL syntax, recommendations on a front-end web framework, or both. These are very different questions.
semicolon in front of the FROM clause also, your table aliases are inconsistent with their use in the ON clause
I wrote some CLR stored procedures. Inside the CLR is a dBase/Foxpro datareader that gets passed to a SQLBulkCopy object. It sucks the files in pretty quickly and can be incorporated into our automation because we get these files on a regular basis. Last year alone I imported 31,247 files in 22 separate formats (according to my log table) so automation and repetition is a must.
does that mean I got the link wrong between the tables? Is it employee id instead but that comes up as an invalid identifier. :) ty
That'd do it. Thanks!
You can create a new DB by right-clicking 'Databases' in SSMS. What is &lt;hitting all sorts of permission issues&gt;? What are you trying to do by moving the mdf file? Where to start with understanding the Sql Server - microsoft has some basic tutorials here: https://msdn.microsoft.com/en-us/library/jj590844.aspx 
I did try that but it does not filter out the out-of-range orders. The query as you have written it outputs: +--------------+---------------+ | product_name | quantity_sold | +--------------+---------------+ | Apple | 99 | | Banana | 27 | | Carrot | 29 | | Tomato | 9 | +--------------+---------------+ I tried changing the second join to INNER but that filters out anything with no sales.
'place it in a spot ssms could see/access' - are you talking about your solution (project) file? if you are able to access some folder you should be able to navigate to that via 'Open...' dialogue. Then you're bringing in 'mdf' files which is an extension for db files for the SQL server DB engine. SSMS is _NOT_ an application to access and modify these files directly. SSMS is an interface (with many different capabilities built-in) to various services/engines of SQL server. You CAN use SSMS to move (by detaching/attaching) files relevant to a database from one DB engine instance to another. FYI, this operation ALSO happens via a service on the DB engine instance NOT in SSMS itself - so, even if you on the machine where SSMS is running can browse to a folder where your mdf/ldf files are located, but the DB engine cannot - you will not be able to attach the DB files to the instance.
I am using "SQL Server Management Studio" from Microsoft SQL Server 2012. &amp;nbsp; When I say access/see I should state: When I'm in the SSMS and going through the "add database" steps I can not see anything for the current user, I cannot open the project database because it is on the current user's desktop. I can't explain why it can't see the directories for this user, I've tried uninstalling and reinstalling but it doesn't work. &amp;nbsp; &amp;nbsp; I am going through the tutorial you provided as well. 
What you are looking for is another join on products and a COALESCE for the quantity to get products a your baseline shown if I understand you correctly. Taking the query from u/r3pr0b8 and changing to INNER JOIN it would look like this: SELECT p2.product_name , COALESCE(SUM(quantity),0) AS quantity_sold FROM products p INNER JOIN order_items i ON i.sku = p.sku INNER JOIN orders o ON o.order_number = i.order_number AND o.purchase_date BETWEEN '2013-01-01' AND CURRENT_DATE RIGHT OUTER JOIN ( SELECT product_name FROM products ) p2 ON p2.product_name = p.product_name GROUP BY p2.product_name
Just glancing at this it seems like you're grouping by everything not being summed up, so there'd only be one row of data to add up for each row in your output table...
I guess the issue for me is that you're using terms/words very loosely but ask for some specific functionality at the same time - for example, there is no 'add database'. There's 'new database' where you create a completely new (empty) database and you'll need to provide names for the new files to be created. There's also 'attach' where you add files by pointing DB engine to relevant DB files you've detached previously (or received from someone who detached those) from another instance. As I mentioned, DB engine can see only what it can see - for example, if it runs on a different machine it wont see files on your local drive; if it runs under a user account that does not have access to personal folders, it will not see files in your personal desktop folder; etc.
As u/ichp explained, the SQL Server runs as a service. You'll need to connect to the service on the user's machine and detach the database before you can move it. Optionally you can create a full_backup and restore that into your machine.
Right then. I cannot attach the project database because none of the user directories for the current user show up. Which is why I am ultimately why I am trying to create an entirely new database in the first place. Because I can't really reach the first one. &amp;nbsp; &amp;nbsp; I feel like I am explaining this extremely incorrectly. If you have access to teamviewer I will gladly show you what exactly I am looking at. 
12,4 then 10,2?
Everything I am doing is local.
Here's an instruction how to move a database via detach/attach: https://msdn.microsoft.com/en-us/library/ms187858.aspx Make sure you're following the instruction, and please report back if any steps fail for you.
12, 4 means there are a total of twelve characters, four of which are decimal places. So if you want to remove two decimal places but have the same maximum number, you take two off both numbers.
I am certain it's all self contained. Eventually it will be put on the web, I'm sure but for now it's all in one VS project, and all self contained. 
Will do! 
Will the steps be any different for a DB that is contained within a VS project?
I may be misunderstand the question. But does this help? - SELECT * FROM INFORMATION_SCHEMA.TABLES - SELECT * FROM INFORMATION_SCHEMA.COLUMNS
When you have a **M-M** relation, then you need an **additional table**, which is called [join table](https://en.wikipedia.org/wiki/Associative_entity). That table contains the FK of the two tables. In **1:1** you do not need any additional table, then you only deal with **one** table.
In 1:M you have one table referencing to the other. In other words, one table contains the FK of the other. E.g. *Department-Employee* is a 1:M relationship. The Employee table contains *Department_ID* as FK.
Sounds like BIML could help you out with both automating the discovery and the ETL process. When you run the scripts you can identify new objects, add them to your metadata store and then with metadata logic decide if you want to build and deploy new ssis packages or whatever for handling. Takes some time to learn but will be very valuable as your project expands or runs for a long time. To check out if this is for you try the free versions and build your own metadata store or get a licence for BIML studio (there is also a free trial for something like 2 weeks): https://varigence.com/bimlstudio
The primary key of a table is that records identifier. A foreign key is that primary key in another table/record. The foreign key identifies the relationship from one record to another in the same or different tables. A table can consist of solely foreign keys to match records (tuples) togethers. (Essentially) Any record that relates to another record requires a primary key and a foreign key, otherwise how would you know they are related...! 
You group by identifiers that are at the level of grouping required In your example you want totals for *Units* and *Paid* for **NDC**, so group by **NDC** If you wanted totals for *Units* and *Paid* for **NDC** and **Rate Type**, you would group by **NDC** and **Rate Type**
Table for each item with PK, title, description, other fields on a product page. Additional tables for categories, sub-categories, and so on. Another table for seller and price.
Google indicates that there are lots of [shopping cart examples](https://www.zen-cart.com/wiki/index.php/Developers_-_Database_Schema) and applications. https://tarikguelzim.files.wordpress.com/2007/12/simple-shopping-cart.pdf
hey, thanks! I think it's the most performant variant, and since you need all products as your baseline (and MySQL has not CTEs) the subquery is unavoidable.
You shouldn't be concerned because SSMS just has a visual hard limit on the number of characters that can be displayed in the UI. You can verify the data by exporting it out to a flat file if that's a concern.
Thanks, this is a little beyond what I know but makes sense. Will look into it. cheers
Transaction log. How you are handling your backups?
recovery mode is simple. The transaction log file is only 24MB. Is there something maybe I'm not understanding here? I've not really ever worked with something in simple recovery mode
I like this. Thanks, this will be a valuable tool for me. Unfortunatley it does confirm what I already had seen in that this also reports the DB size as 139GB or so
There are two settings in SSMS that might be affecting you. Connect a new query window to your database and click Query (menu) -&gt; Query Options. Results -&gt; Grid -&gt; Maximum Characters Retrieved Results -&gt; Text -&gt; Maximum number of characters displayed in each column Turn both of those way up. 
The other side of this is that Simple mode will still grow the transaction log, but once the transaction completes the space immediately becomes available for reuse. So, if you have a large transaction for things like collating your state reporting data for Medicaid consumers or something similar, let's say for the sake of argument that it is really oddly written and is done as one giant transaction which ends up needing 50GB of storage. If you started with a 1GB Transaction log file, you'll end with a 50GB transaction log file. SQL Server won't shrink it back down for you, and frankly neither should you. Transaction log growth, especially in simple mode, is telling you that the largest transactions in your system need a certain amount of space to work inside of. I'm less inclined to think your problem is the TX log, though. Without knowing your data model I can't say for certain, but I have seen cases where development stores XML data in a table and then creates an XML index on that data for XML search performance. We had that come up with a 270/271 process for eligibility verification that converted the EDI data into XML and stored it instead of splitting it out into tables, and the XML indexes were actually 6 to 10 times the size of the actual table data.
Do you just need to replace the last 0 with a 1? Currently if the sum is 0, it'll return 0. If you return 1 and the cell is formatted as a `%`, then it'll be 100% =Iif(Sum(Fields!LYSUMAMOUNT.Value, "Group3") &lt;&gt; 0, ((Sum(Fields!SUMAMOUNT.Value, "Group3") - Sum(Fields!LYSUMAMOUNT.Value, "Group3")) / Sum(Fields!LYSUMAMOUNT.Value, "Group3")) * 100, 1)
Thanks for the reply. I'm still getting #error for that field.
The application will be down during the update, so blocking shouldn't be an issue. We want all rows updated, so the lack of a predicate is intentional. 
A foreign key means "make sure values in this column exist in another column" You can query related tables without foreign keys
Not an answer to your question, but this caugt my eye: &gt; Id != null I don't know which DBMS you are using but in all systems I know this should be: &gt; Id IS NOT NULL Comparing NULL values with any operators other than "IS NULL" and "IS NOT NULL" will always return UNKNOWN.
You may want to use TOP to only update chunks of rows at a time. If it fails the rollback could take a looong time.
And customers, transactions, line items, HR info, basically all of the things
Do remember that the database size *on disk* can, and often will, be larger than the actual data itself. For example, our main DW is only about 20GB at the start of each fiscal month, but by the end it will be close to 300GB. So we pre-inflate the datafiles to 300GB. Why? So that there isn't a delay writing data to disk due to the data file size needing growth. Also, pre-inflating prevents datafile fragmentation from occurring.
Replace Sum(Fields!LYSUMAMOUNT.Value, "Group3") with CASE WHEN Sum(Fields!LYSUMAMOUNT.Value, "Group3") = 0 THEN 1 ELSE Sum(Fields!LYSUMAMOUNT.Value, "Group3") END
I don't know if this is actually what you're looking for, but I believe Amazon uses Hadoop or some other type of unstructured distributed file system. The way they have it setup allows for very high performance, but less accuracy. So, searches can return in milliseconds but might be slightly different each time. This was told to me at a conference about a year ago, so apologies if this is outdated or I'm remembering wrong.
Or replace with ISNULL(NULLIF(Sum(Fields!LYSUMAMOUNT.Value, "Group3"), 0), 1)
Where Clause can take multiple conditions connected by and and or logical operations
first of all, it's WHERE, not WHEN secondly, your `item type` and `item quantity` column names need to be delimited because of the space in them finally, you forgot the GROUP BY clause try this -- SELECT `item type` , SUM(`item quantity`) AS total_quantity FROM order_table WHERE item type IN ('item 1','item 2') OR ( item type IN ('item 3','item 4') AND their_date &gt; '2017-03-02' ) GROUP BY `item type` 
&gt; a university course about SQL and database the Mere Mortals book is excellent, because it covers standard SQL unless you want specific in-depth analysis of T-SQL (Microsoft's superset of standard SQL, supported only by SQL Server), i would steer clear of Ben-Gan for now
&gt; Table for each item Heh. This could be read the wrong way.
I would do it with CASE. For the item field you could substitute Case item When in ('item 1','item 2') then item When in ('item 3','item 4') and date &gt; cast('3/2/2017' as date) then item Else NULL End as item Then just filter out the nulls in Excel, Tableau, etc. or make that a subquery/CTE and apply filter to item field. If it's Teradata you could probably add where item is not null. Assuming you're on MS SQL though. Lots of possibilities. 
Where num=(select max(num) from table)
this is just a secondary database that tracks certain changes to the main DB
Thank you so much! Instead of doing max(num) I just did max(recorded) I'll post the final result in an edit on the top post. 
doh.... I can't believe I didn't catch that earlier. Good catch.
Then, in that case, there's not much left to do but to execute the update. But, be prepared to run more frequent backups of your transaction logs and monitor their usage because you definitely don't want to run out of log space and have this query take four or five times longer to rollback. Edit: it looks like some others in this thread have a few different ideas that myself. The only thing that I can say for sure is that you won't know how long its going to take to execute it until you actually do it. If you have the ability to have the DB restored to another MSSQL instance, you might want to test it there a few different ways.
 I also enjoyed the Mere Mortals book. Tons of examples to comprehensively cover fundamental topics. 
thanks, I might take you up on that!
I think you get the error when trying to attach a database. This error is not related with SQL Server Management Studio but with SQL Server engine. You both need to have the same version of sql server to be able to attach databases. 
Ok, I'll bite. Why would a social worked need SQL for their job?
&gt; Case item When in this is invalid syntax 
Thank you for that! I've never had any sql classes or training so I'm just learning off MySQL examples!
It's not a 'sister' script - it's domain specific to Calq. There is one and only one SQL as defined by the ANSI standards. Various vendors choose to implement some of that standard, some more than others, and nearly all of them add they're own extensions- Oracle, Microsoft, IBM, Teradata, SAP, etc.
Good point. I guess it builds up to "it depends".
Right, yea I get that. This situation that isn't whats goingon though. There's no doubt in my mind that this particular database was thrown together by someone that didn't really understand any kind of proper database mojo. I'm no DBA and never claim to be, but what these guys do sometimes just makes me shake my head..
If I were to do that, then it would return all IDs that weren't null. I may not have worded it the correct way. But, what I'm trying to do is group the IDs together, and select the IDs did not have any instance where it was null. Like 105 and 107
This should do the trick. (I didn't test it.) SELECT DISTINCT ID FROM table WHERE ID NOT IN (SELECT ID FROM table WHERE Value IS NULL) ; You could do `SELECT DISTINCT ID` in the inner `SELECT`.
You got it! Thanks a bunch
I use names, easier to read
Curious... how so? I find that difficult to read or change later, personally, but Im far from a professional.
I actually hate using numbers unless it's a one off query or there's only 1 column.... it's shorter to type, but takes so much more time and effort to figure out the order by statement. Not to mention if you add another column into the query / report you might have just broke your statement. Something like this is OK: SELECT InvoiceDate FROM Invoices ORDER BY 1 DESC
I think its a preference thing. Pros : Quicker to type Cons : Harder to read / debug
It depends. If it's a throw away query I will use numbers as its faster. If it's some process that's being automated then the name.
I use names, always. Using the column numbers just means you can screw up your query if you add or change your select list. I don't even understand why the "order by 4" syntax exists. Using it just seems to be asking for trouble/annoyance.
I don't think I've ever actually used numbers. 
I'll turn it around in you OP. Why would someone who's proficient in SQL take a job as a social worker? On average is pay is at least double doing database work compared to being a social worker.
Alias the derived table and it should work just fine. Example: select status,count(*) from ( select * from myView ) as X group by status
Depends on how lazy I am being. Anything past 4 I do not not, ever. 
I suppose I should've said for querying, with scripts and the like, they definitely use column names, my bad, that's on me. 
So if I wanted to show the `InvoiceNumber` before the `InvoiceDate`, I would have to do this: SELECT InvoiceNumber, InvoiceDate FROM Invoices ORDER BY 1 DESC But now it's going to `ORDER BY` the `InvoiceNumber`, which I don't want. So I'd have to update it to `ORDER BY 2 DESC`.
I don't think ordinal numbers matter much in day to day queries like this and wouldn't worry about it too much - maybe if you were writing triggers or production level logic in TSQL it may differ may be useful: http://stackoverflow.com/questions/2253040/benefits-of-using-sql-ordinal-position-notation
Is that how they get away with not having to pay you money?
Thanks very much. 
BI Developer here... Column names always, stuff happens, apps change...stuff gets depreciated; All these can affect column orders. You should also think about the next person to maintain your code, make their lives easier... use columns names.
&gt; You should also think about the next person to maintain your code, make their lives easier... use columns names. *Total* agreement on this point! Heck, two months later I need stuff like that to work on my *own* code, because I've forgotten half of it by then.
You're doing God's work, truly.
I've never seen the number method before, but I would not use it. For most of my scripts it would be far too unwieldy. The only textual shortcut I use are aliases instead of table names, but that's because we use multiple databases so I usually need the full name (e.g. database_name.dbo.table_name), but you also declare your own aliases and they don't change unless you intentionally change them.
Can you do a cross join on all possible parts and all possible components? You can left join against that table and populate where values are empty.
Yes a lot of the data might be stored in SQL, but most agencies would either have an analyst pull the information the social worker needs, or have a self service tool so the social worker wouldn't really be working in SQL. At most, you'll likely be analyzing pre-aggregated data in the form of excel documents.
Names. Always. If names are long get a query tool to assist
Why not just dump the data from each iteration into the same temp table inside of your while loop / cursor / etc?
Interesting. How would you do a cross join to keep where only one component exists for the part? In the above example, a Part (TS1) may or may not have all the components. I realize that part numbers should be unique to the components but in this data set, it is not true. The SKU is what makes it unique.
there doesn't seem to be anything here
I found a solution after figuring out the right way to use the FOR XML. SELECT DISTINCT SKU, Part, Component STUFF((SELECT '; ' + a.Component FROM table1 a WHERE a.SKU = b.sku FOR XML PATH('')),1,1,'') as [Combined] FROM table1 b ORDER BY 1 That gives me a field with all of the components, ordered. SKU Part Component Combined 112 TS1 1901 1901; 1903 113 TS1 1901 1901 114 TS1 1901 1901; 1903 When I'm doing my SELECT, I'll check to see if the new record's Combined field exists.
I tried that, but I'm getting a problem with my into statement, as sql tells me that "There is already an object named '#Louisiana1' in the database."
Oracle lets you optionally use AS in the SELECT clause for column aliases, but not in the FROM for table aliases. 
Oracle lets you name the column name with AS, but does not allow you to name the table name with AS :-) 
I use order by 3 regularly. I do not save that code so it never makes it to production. It's just so I can see a min or max of that column and get an understanding of the data.
[removed]
you need a subquery to find the latest sale for each item then join this back to the ??? table to get the rest of the data i'd write it for you but i can't figure out why you have a sales table and a transaction table please identify all your primary and foreign keys, thanks
Yeah you need define your table outside of the loop and use INSERT INTO before your select rather than an INTO statement after it. Currently it's attempting to create the table with each iteration of your loop. 
Names always. I won't even `SELECT *` in a query when I write code that calls SQL queries, I'll list out the columns I want. But that's a bit more picky than usual.
The transaction table contains the location the item was sold from. It's not the most important table at the moment, but I was hoping to be able to use that 'transaction' number to pull in additional information in the future. Ignoring the Transaction.Location, how would I create a subquery? 
Always names. Easier to read and manage and easier for the next person who comes and reads it.
I am about 50/50. The benefit of using numbers is when columns that are returned as expressions. For example if your second column is defined as **CASE WHEN SomeValue &lt; 0 THEN ' Negative' WHEN SomeValue &lt; 100 THEN '&lt;100' ELSE '&gt;=100' END** Then you can order by 2, instead of repeating the expression in your ORDER BY clause.
Names pretty much always. For a simple order by, definitely. 
thanks ;o) 
Thank you for the input, I believe I understand it a bit better. I was still unsuccessful when I attempted it though...This is a new scenario, but I can hopefully spell it out more clearly. There are several items in stock. Each of these items has a location, batch, and quantity associated with it. There can be multiple of the same item with differing locations, quantities, and batches. I want to keep each of these items in the same row, but add a column containing the last sell price, date, and receipt number. These columns should be the same for all identical items, regardless of differing locations, quantities, and batches. Right now the items are being duplicated based on the number of receipts. Below is my current SQL script: SELECT Item.Item, Item_Location.Location_Code, Item_Location.Batch, Item_Location.Quantity, Receipt.Price, Receipt.Date, Receipt.Receipt_Number FROM (DatabaseName.dbo.Item Item INNER JOIN DatabaseName.dbo.Item_Location Item_Location ON Item.Item=Item_Location.Item) INNER JOIN DatabaseName.dbo.Receipt Receipt ON Item_Location.Item=Receipt.Item ORDER BY Item.Item, Receipt.Date DESC 
If you do it with a subquery, you can take /u/PrezRosslin's query and nest it. Something like Select &lt;a bunch of fields&gt; from ITEM inner join LOCATION etc. inner join (select sales_table.all_the_things from (select item_id, max(sold_date) from sales_table) as anchor inner join sales_table on sales_table.item_id = anchor.item_id and sales_table.sold_date = anchor.sold_date) last_sale on item.item_id = last_sale.item_id You can also use an analytic function to get the last sale info without a subquery: select sales_table.all_the_things from sales_table qualify sold_date = max(sold_date) over (partition by item_id) If your DBMS doesn't use qualify, you can wrap it: select a_bunch_of_fields from ( select max(sold_date) over (partition by item_id) max_dt, s.all_the_things from sales_table) x where max_dt = sold_date Then use either of those as the last_sale subquery in the first example. 
Thank you!
My take: If you are peeking at something as a one time thing, numbers is quick. If you are writing something that may be useful ever, then named columns is better. Do yourself a favor and build a good habit of using names.
By name with one exception: When concatenating strings using XML FOR PATH: OUTER APPLY ( SELECT COALESCE(STUFF(SELECT DISTINCT ', ' + T.Field FROM Table T ORDER BY 1 FOR XML PATH, TYPE).value('.','nvarchar(max)'),1,2,''),'') ) F(Field) There's never going to be more than one column and you can't give the column a name. You could do ORDER BY ', ' + T.Field But come on, that looks ugly and introduces the possibility of an error, however small. (You can't ORDER BY T.Field because of the SELECT DISTINCT)
Numbers for quick, ad-hoc queries. Names for anything beyond basic or anything official 
Preach! 
If it's going into production, I'm definitely using names. If I'm just querying data, I'll use numbers all the time. It's a whole lot easier to write "ORDER BY 4 DESC" then whatever mess of a column 4 represents.