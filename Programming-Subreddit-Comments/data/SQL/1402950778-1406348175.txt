Why not SELECT MAX(CASE WHEN Field_Name = 'NumbAmt' THEN Field_Value END) Amt, MAX(CASE WHEN Field_Name = 'LocAdd' THEN Field_Value END) Address, MAX(CASE WHEN Field_Name = 'LocDesc' THEN Field_Value END) Description FROM Table1 WHERE RefID = 1 AND Type IN ('INT','String') or using PIVOT SELECT * FROM ( SELECT FIELD_NAME, FIELD_VALUE FROM TABLE1 WHERE RefID = 1 AND Type IN ('INT','String') ) psource PIVOT ( MAX(FIELD_VALUE) FOR FIELD_NAME IN ('AMT','LocAddr','LocDESC') ) p Your answer is.... WITH Record(Field_Value, Field_Name) AS (SELECT Field_Value, Field_Name FROM Table1 WHERE RefID = 1 ) SELECT (SELECT Field_Value FROM Record WHERE Field_Name = 'NumbAmt') Amt, (SELECT Field_Value FROM Record WHERE Field_Name = 'LocAdd') Address, (SELECT Field_Value FROM Record WHERE Field_Name = 'LocDesc') Description FROM Record It will work but why correlate your subqueries? If you aren't specifying "WHERE RefID = 1" and lets say they want to create this as a view over the table (turning row data into columnar data). You are pre-pulling the data into a CTE and then for each data field you will cause the optimizer to execute a query against your CTE. Ultimately, what I'm saying is your code wont scale with the database size. Many systems that have entity attribute value tables, these tables are usually enormous. The one we have for our claims system has +260 million rows as sits at 28gb of storage with an additional 10gb for indexes. Your answer may as well have been written in a cursor because it got the desired result but if someone reads your code and thinks this is a totally acceptable use to pivot data, will implement in an organization and your solution is far from optimal. Selecting the data in a single select and aggregating the data (through max or pivot max) is far more optimal for IO and parallelism. Pregs second answer is correct to use Pivot. His first answer is terribly wrong, you never rejoin the table back on to itself.
Not sure how SQL Server performs with these things, but in Oracle, chances are high that the CTE will produce an in-memory temp table that can be queried very efficiently in the nested SELECTs. This can be seen in [example 6, "not using common table expressions" of this blog post](http://blog.jooq.org/2014/05/26/yet-another-10-common-mistakes-java-developer-make-when-writing-sql-you-wont-believe-the-last-one/). So in other words... &gt; Selecting the data in a single select and aggregating the data (through max or pivot max) is far more optimal for IO and parallelism. ... at least in Oracle, my CTE solution would probably do precisely this.
Your answer is in your description, I think. You have one row in an entity that maps to (potentially) many dates (for whatever reason). Make another table, 1:Many relationship to capture the dates.
Before I go further I will apologize in advance if I come off Condescending or mean, its not my intention. I don't think you took away the right lesson from example 6. The code the developer had rewritten was code that used non-ansi compliant joins. Non-ansi compliant joins are the old way of joining tables in the where statement (still supported in oracle, no longer supported in MSSQL with database compatibility &gt; 2000). For each table in the where statement the original developer was limiting the results of the joined tables using a correlated subquery that resolved the c.id, cur_id and org_id. Honestly the original code was just terrible code. His resolution honestly wasn't the most optimal. WITH payment AS ( SELECT cur_id, org_id, amount FROM payments WHERE id = :p_id ) SELECT round(p.amount * e.bid / c.factor, 0) FROM payment p JOIN currencies c ON p.cur_id = c.id JOIN exchange_rates e ON e.cur_id = p.cur_id AND e.org_id = p.org_id This is essential the same thing as SELECT round(p.amount * e.bid / c.factor, 0) FROM payment p JOIN currencies c ON p.cur_id = c.id JOIN exchange_rates e ON e.cur_id = p.cur_id AND e.org_id = p.org_id WHERE id = :p_id CTEs are great but there is no reason to create a CTE to replace a table. CTEs allow you to do precalculation and preaggregretion prior to being referenced by your main query. Pre-pulling the same data for CUR_ID, ORG_ID and AMOUNT are already in the payments table is redundant. Referencing a CTE as opposed to the table is not going to make it go faster. Going back to your original CTE answer on stack overflow. [I would read the following article on Correlated Subqueries.](http://en.wikipedia.org/wiki/Correlated_subquery) &gt;The subquery is evaluated once for each row processed by the outer query. If you have 10k rows with 3 correlated subqueries in your select or where, you will have 30k executions for a query that should have less than a handful. Select (Select a.value from table2 where t1.key2) as A (Select b.value from table3 where t1.key3) as B (Select c.value from table4 where t1.key4) as C from table1 As rows are being pulled from table1. Each rows key2,key3,key4 are being individually executed from table2,table3 and table4. Oracles optimizer is probably the best optimizer out there but correlated queries work this way intended and it the optimizer will not optimize this much further. [Per oracles documentation](http://docs.oracle.com/cd/E11882_01/server.112/e26088/queries007.htm#SQLRF52357) &gt; A correlated subquery conceptually is evaluated once for each row processed by the parent statement. However, the optimizer may choose to rewrite the query as a join or use some other technique to formulate a query that is semantically equivalent. It *may* choose to rewrite your code and attempt to try to do it better. Even if it does rewrite your code and join hashed tables, you are talking about a read to a hash table for each correlated sub query. Depending on the size of data, current workload of the server many times large hashes are written to temporary storage. So you are adding more read/writes for the execution. This is where I get up on my soap box and try to encourage everyone to write their code to be scalable and to avoid as many reads and executions as possible. System crippling code is rarely the code written today but the code written 4 years ago. Just because its Oracle doesn't mean you don't need to follow ANSI 98 compliant requirements. Just because its Oracle doesn't mean sub par code should ever be considered efficient. What happens when the optimizer has a bad day and decides not to hash the tables? What if the optimizer never does and the data set keeps growing and the query becomes slower and slower each day.
Ok, that makes sense. I was thinking to avoid repeating dates, because any given date will probably be listed 50 times, but I guess they don't take up much space.
If you really want to do that, the you will have a many-to many relationship...one table (the original) with your rows, one with unique dates, and then an interstitial table to join them. But yeah, if the dates would be ok to repeat, then your looking at one less join.
I too would just use max. You can use it in a group by, and get the results for more than just the one ID as well. Not sure how well it performs against CTE (my instinct says CTE done right might win), but it gets the job done without using advanced concepts.
Kerberos is my #1 interview question. I was asked the same question when I first started my IT career. 
I... I would have bombed that interview.
Here you go, my comment below was the right answer, this one has the the correct answer and all the steps needed to get it working.
Mostly agree. One of the primary areas NoSQL development has proven to be superior is it's large file/stream handling; which is usually low in sensitive data. &gt; I think what happens is that so many people just love the idea of a schema being defined entirely by the code, instead of defining a schema along with the code, or before the code. It makes some things easier. This is a nightmare for defining data owners and audit compliance with legislated rules regarding personal information storage; more-so outside the U.S. My biggest issue with it is that it needlessly sacrifices some of the core design principles of RDBMS at the cost of the end-users for the benefit of the creators, which is what most high level Developers/DBAs strive against. Ease of development should *always* come second to the trust of the 'consumer' in your product's security when their personal data is concerned(everything else considered equal). Edit - Technology is a strange mistress for the vast majority of people; having a security breach is the leaked sex tape revealing all the dirty things you did with her. 
I just posted the first of a four part series on normalization. Please let me know what you think. I really want to try and make this understandable for everyone that is interested in learning. Thanks!
Nice tutorial so far. I have it bookmarked so that I can maybe use it as a reference for a DB class I teach. One suggestion is to clarify your statement that there are 3 normal forms. The first 3 are commonly used, but you can still go past 3NF.
Question for you: in your instructions, you say to use setspn -s **host**/ssrsservername. I'm assuming you actually mean the word "host" rather than the hostname. [This article here](http://blogs.technet.com/b/rob/archive/2011/11/23/enabling-kerberos-authentication-for-reporting-services.aspx) -- which was also very helpful -- replaces "host" with "http". Any idea why there's a discrepancy? Thanks.
Yes you are correct. Host is usually a dummy term. Http/ssrsservername for SSRS. For something like SQL server it's MSSQLsvc/sqlservername:1433 With SSRS you don't need to supply the port number. 
 It aint pretty but it'll work: &gt; CREATE function fn_extractupper(@var varchar(50)) &gt; returns varchar(50) &gt; as &gt; begin &gt; &gt; declare @aux varchar(50) = '' &gt; declare @size int = len(@var) &gt; declare @position int = 0 &gt; declare @lastposition int = 0 &gt; while @position &lt; @size &gt; begin &gt; if ASCII(SUBSTRING(@var,@position,1)) = ASCII(UPPER(SUBSTRING(@var,@position,1))) and SUBSTRING(@var,@position,1) &lt;&gt; ' ' &gt; begin &gt; set @aux = @aux + SUBSTRING(@var,@position,1) &gt; set @lastposition = @position &gt; end &gt; else if @lastposition = @position -1 &gt; BEGIN &gt; set @aux = RIGHT(@aux, len(@aux)-1) &gt; set @lastposition = 0 &gt; END &gt; set @position = @position + 1 &gt; end &gt; &gt; return @aux &gt; END Use it like this: update T set [LastName] = DBA.dbo.fn_extractupper(name), [FirstName] = ltrim(rtrim(left(name, charindex(DBA.dbo.fn_extractupper(name), name, 0)-1))) from #T T 
Normally during the install of SQL a SPN is created using the service account set for sql. You can attempt you add an SPN using the -S operator and the SQL service account. If it comes back and says one is already found for that host than its already been created. I would also check by using SETSPN -L DOMAIN\SQLSERVICEACCOUNTNAME. See if that service account has a record for both the SQL box and it's fully qualified name. Add the fully qualified one if it's missing.. While you are doing SPN work on your network you may want to use the duplicate SPN command list in the help found when you run setspn -?. At the bottom of the help is a list of examples. One shows you how to run a command to show all duplicates. Dup spns for user machines are fine but any production boxes should be analyzed and the duplicate needs to be removed.
I ran the setspn -l for the SSRS account and there was nothing there. I didn't do it for the SQL account. Not yet. I did add the SPNs for the SSRS account and we're still receiving the prompt, but I didn't get around to adding the site to local intranet yet. Thanks again for all your help.
Hey mindtehgap! Thanks for the suggestion. I'll make that change. I have post for each normal form. I'll make a comment here one each one is done, rather than crating a new reddit.
The trick is to say you don't know but you can find out. Then ask questions about it. Allow them to teach you. They will then feel like your teacher and as their student they will want to help you. This is also try in life. Being humble pays big dividends.
I'd go with something like: UPDATE table set firstname = SUBSTRING(name,1,(LEN(name) - PATINDEX(REVERSE(name),'% %')), lastname = SUBSTRING(name,(LEN(name) - PATINDEX(REVERSE(name),'% %')) + 1, LEN(name)); I don't have SQL on this machine so I'm doing it from memory but this should be mostly right, though you may have to +/- 1 on the substrings. It essentially finds the location of the last space and does a substring to grab the needed areas.
I recently changed companies for the first time in thirteen years. I was still employed when I started interviewing, so I went into my first (and only) interview with a lot of confidence. And a bit of "ring rust", I suppose. I was as humble and honest as possible because I knew that if I bullshitted (BULLSHAT?) my way into the job, it was only a matter of time that they realized that I was a fraud. And I had just dealt with a fraud at my previous job. It's the pits.
Ok, I'm quite busy at the moment but I have downloaded PGSQL and will take a look when I get the spare time, probably the weekend. You make a compelling case ;) The thing is though, I should really be sticking to SQL Server as much as possible since I use it at work. I also use MySQL on linux VMs and VPSs for small web projects because it's lightweight, simple and speedy. It really is a joy to use and linux allows for very stripped down web servers. I really don't know if I need to start using another RDBMS just yet to be honest, but I will check Postgre out. Back to the OP... Did you see the link I edited into the OP? I simply downloaded the pre-built worldcup.db file and opened it up in SQLite Browser. I then exported it as a .sql file. It is ~7500 lines long... Here is the beginning of the file with a few tables and a little data. At the end are some indexes and the end of file... BEGIN TRANSACTION; CREATE TABLE "badges" ("id" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, "team_id" integer NOT NULL, "league_id" integer NOT NULL, "season_id" integer NOT NULL, "title" varchar(255) NOT NULL, "created_at" datetime, "updated_at" datetime); CREATE TABLE "leagues" ("id" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, "key" varchar(255) NOT NULL, "title" varchar(255) NOT NULL, "country_id" integer, "club" boolean DEFAULT 'f' NOT NULL, "created_at" datetime, "updated_at" datetime); INSERT INTO `leagues` VALUES(1,'world','World Cup','','t','2014-05-18 09:30:47.743534','2014-05-18 09:30:47.743534'); CREATE TABLE "groups_teams" ("id" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, "group_id" integer NOT NULL, "team_id" integer NOT NULL, "created_at" datetime, "updated_at" datetime); INSERT INTO `groups_teams` VALUES(1,1,210,'2014-05-18 09:31:19.862094','2014-05-18 09:31:19.862094'); CREATE INDEX "index_groups_teams_on_group_id" ON "groups_teams" ("group_id"); CREATE UNIQUE INDEX "index_groups_teams_on_group_id_and_team_id" ON "groups_teams" ("group_id", "team_id"); CREATE INDEX "index_events_grounds_on_event_id" ON "events_grounds" ("event_id"); CREATE UNIQUE INDEX "index_tags_on_key" ON "tags" ("key"); COMMIT; Drop me a PM and I'll dropbox the whole thing or take a look on the site yourself. Thanks again, I have no idea how to regex :/
Database student checking in. Thanks for the tutorial. 
Hi - I've been putting together posts to help people learn SQL. Check out this page: http://www.essentialsql.com/getting-started/ There are two things I think you'll be interested in seeing: 1. There is a link on that page talking about installing SQLite3 on Windows. I'm on Win 8.1, so I know it works... You can contact me through my site if you need help. 2. I have simple SQL exercises that you may like. I haven't gotten to joins quite yet, but group by's and other stuff are there. I just put out the first of four posts on normalization. There is a redit on that in the /r/sql section. Once I'm done with normalization, I'll start on joins. Let me know how I can help.
I got it working! 8 left outer joins :P
Thanks! 1. How do you paste from notepad into the terminal?
something like [current_table.date] &gt;= (select max(current_date.date from [current_table]) might have to use aliases.
Thanks for letting me know that. I've been having my wife read them for clarity.
Once the command window is open (terminal), right click on the title bar. A menu should pop-up. From there just select Edit and then Paste. I don't know of a short cut key. Also, check out the Properties menu item. You'll definitely want to change the windows size to something like: width: 130 or greater! height: 50 or greater! Hope this helps. 
This 1983 article from William Kent of IBM called A Simple GUIDE TO FIVE NORMAL FORMS IN RELATIONAL DATABASE THEORY http://www.iai.uni-bonn.de/III//lehre/vorlesungen/TDWA/WS07/1.pdf is the paper I've been dragging with me my entire IT career.
This sounds like a rather elaborate home grown solution - have you looked into hierarchies ? Ideally there would be a relational way to represent your data, but without knowing much I can't say.
I can recommend this book. Takes you from NOTHING to basics. http://www.amazon.com/Head-First-SQL-Brain-Learners/dp/0596526849/ref=sr_1_1?ie=UTF8&amp;qid=1403097661&amp;sr=8-1&amp;keywords=head+first+sql
Something like this should work for you: *** SELECT DISTINCT a.b.value('VersionID[1]','nvarchar(32)') AS Exists FROM TABLE AS yourtable CROSS APPLY youtable.xmlcolumn.nodes('//BEGIN_FORM/') a(b) WHERE UNIQUE_IDENTIFIER = '12345' AND a.b.value('VersionID[1]','nvarchar(32)') = '3.1ABC' *** 
Assuming that both columns are in the same table: SELECT X FROM item_table WHERE X = desired_value AND Y = desired_value;
Thank you for your reply. I'm getting a syntax error on the WHERE statement. Also, the "a(b)" kind of confuses me. What does this stand for? Thank you again.
X and Y in his example are literals not column names.
Hierarchies were not a good fit, unless there is something amazing about them I don't know. This method maintains full referential integrity, and MSSQL seems to cope with the recursion well through the CTE. There is a caching tier at the application layer. The caching tier services the read requests and dispatches the write requests to SQL. The database is under a super light load from reads, and really just needs to keep up with writes. This operation (which is the most complicated of my collection operations) takes on average 20ms-25ms to execute in my tests, which is livable. I'm sure it can be optimized if that ends up being too long. 
Well, if it works for what you need then that's what counts.
What database are you using? it can be done with a case statement: select case when col1 &lt; col2 then col1 else col2 end min from &lt;table&gt;
oops that was stupi of me. Access
If you are using a Sum you need to group on the fields that are not being aggregated Select CAW.CompanyName, SOH.SubTotal, Sum(PAW.Weight) from ProductAW as PAW join SalesOrderDetail as SOD on PAW.ProductID=SOD.ProductID join SalesOrderHeader as SOH on SOD.SalesOrderID=SOH.SalesOrderID join CustomerAW as CAW on SOH.CustomerID=CAW.CustomerID group by caw.companyname,soh.subtotal
Use LIKE with no % sign at the beginning but one at the end. This will make the word you're searching for match the beginning of the data. SELECT * FROM database.promohistory WHERE mailername LIKE 'personal%' 
Thank you very much for the quick response and solid tip. Extremely helpful. 
That's different. To show each order you would remove both the Group By and the Sum
The problem calls for the weight to be summated. I added a column and grouped by salesorderID in hopes that it would break it up for me that way, but it looks like it only gave me the distinct instances, possibly. Not really fully aware if it works that way. Thanks for your help!
Looked at the data with a different query. It wouldn't even be possible to differentiate between the distinct orders and the customer name, because they are always the same. 
Here is the syntax for a case statement in access: http://www.techonthenet.com/access/functions/advanced/case.php Something like this might work but I am not at all familiar with Access. Also I am not sure how to properly handle null values in Access. select Case Date1 -Date2 Case Is &lt; Date3 - Date4 then Date1 -Date2 Case Else Date3 - Date4 end min from &lt;table&gt;
This should do it (note: untested anywhere): SELECT SUM(CASE WHEN SALUTARY = 'MR.' THEN 1 ELSE 0 END) AS MALE_COUNT ,SUM(CASE WHEN SALUTARY IN ('MS.','MRS.','MISS') THEN 1 ELSE 0 END) AS FEMALE_COUNT FROM PERSON ; The first CASE statement returns 1 for each SALUTARY = 'MR.'. The surrounding SUM simply adds up all the 1s it encounters and returns the sum in derived column MALE_COUNT. Same logic applies to FEMALE_COUNT, allowing for the obvious change in SALUTARY values. There's no need for a GROUP BY as there are no columns in play other than those being SUMmed. HTH
Thanks for the help. It doesn't work in access, because you can't use a CASE statement. I think it is replaced with IIF. I'll try to rework it and see how that goes.
Ok, so I ended up getting it to work with this: SELECT SUM(IIF (SALUTARY = 'Mr.', 1, 0)) AS [Number of Men], SUM(IIF (SALUTARY IN ('Ms.','Mrs.','Miss'),1,0)) AS [Number of Women] FROM PERSON; Thanks again!
In PostgreSQL : LEAST( (date1 - date2), (date3 - date4) )
thank you sir!
thank you sir
Access does not have CASE. It does have IIF. and ISNULL. And SWITCH. Dealing with your nulls is what made this difficult. If a date pair has a null value, it will use the other date pair for the subtraction. If there is a null date for a row in **both** column pairs, the record gets a null value. MinDate: Switch(Not IsNull([date1]-[date2]-[date3]-[date4]),IIf(([date1]-[date2])&lt;([date3]-[date4]),[date1]-[date2],[date3]-[date4]),Not IsNull([date1]-[date2]),[date1]-[date2],Not IsNull([date3]-[date4]),[date3]-[date4])
The a(b) is just the nodes where the cross match occurs, it could be named anything you'd like. I quickly tested this on an MSSQL 2012 server and it looked to work correctly. There might be a bit of tinkering with the value/etc. What is the error?
I don't think I explained myself very well... I am selecting item_name. There may or may not be 2x item_name. If there is 'X' and not 'Y', return 'X'... If there is 'Y' and not 'X', return 'Y'... If there is 'X' and 'Y', return 'X'...
***ANSWER*** SELECT * FROM ( SELECT item_name FROM item_table WHERE item_code IN ('X','Y') ORDER BY DECODE(item_name, 'X', 1, 'Y', 2, 3) ) WHERE rownum &lt; 2; Thanks for your help, chaps!
This is completely different to what you originally said! 
Add a gender field to PERSON table. Then count persons grouped by gender.
Glad you got it working. Perhaps move away from Access if you can (and you want to use a more 'standard' SQL form).
nice one! 
I totally agree. I just havent had the opportunity to do much with the CASE statement. 
Yes... well... not reeeeeally. Original query: Select returns 2 rows, X &amp; Y. When both exist, return X. Clarification: If X exists and not Y, return X. If Y exists and not X, return Y. If X &amp; Y exist, return X. Answer: Worked.
 SELECT it.* FROM item_table it WHERE (it.item_name = 'X' OR (it.item_name = 'Y' AND NOT EXISTS ( SELECT 1 FROM item_table itt WHERE itt.item_name = 'X') ))
A database is like a set of shelves, and SQL is like a cat, running a query is like the cat running into those shelves to reorganize the content.
&gt;At first I got an error saying [Sybase][ODBC Driver][SQL Anywhere]connection was terminated Error. I know you can't modify your post title, but this error is referencing [Sybase SQL Anywhere](http://www.sybase.com/products/databasemanagement/sqlanywhere) Hopefully this helps your search for a solution.
* Verify that the Firewall on any of the systems aren't getting hits from the other servers, if they are investigate what port/etc * Disable any power saving features on the servers that could be putting the NIC to sleep * Make sure your IPs are static and there isn't a lease refresh for the client/servers in a DHCP scope when the error occurs * Check to make sure you aren't hitting the databases client connection limit; check maximum connections and use the query 'EXEC sp_who' to see how many connections there are. 
when 'SQL-is-explained' THEN 'not-understood'
Haha - that was pretty good!
Looks like you're using the server name, and it looks like the operation doesn't recognize the server name. This could be due to a problem with your DNS server, a problem with your network prohibiting your data server from obtaining DNS information, a hosts file on the data server overwriting the DNS information, or any other of a variety of reasons. To test that your data solution works, and bypass the DNS issue, simply edit the connection string so that it points to an IP address rather than server name. This should only be a temporary solution though, just to test that your SQL code/process works. You should get the DNS issue sorted, and quickly. 
 Declare @nope datetime Set @nope = select max(date) from source Insert into destination Select * from source where date &lt; @nope
two questions: could you give examples of the data in this varchar column? what dbms is this?
I am using SQL Server Management Studio(2012) The data is typically a string of 5 - 15 words.
Thank you, I will review this link. 
Use 'varchar(max)' instead of 'text' and you'll be fine.
the reason i asked for samples is to understand how you had "broken the field down to separate words"
I have a query that takes the field and removes common words like 'A' or 'the' and the breaks the field down based on spaces into new fields in temp tables with a primary key and then brings them all together in a master temp table 
Yea if you are having issues with your software, you need to contact your vendor, Patterson/Eaglesoft. I've seen Eaglesoft back when it was just a MSAccess database application in 1998 or so. If you are coming here for support, you've come to the wrong place. 
 IF OBJECT_ID('tempdb..#tmp') IS NOT NULL DROP TABLE #tmp; WITH cteA AS (SELECT ROW_NUMBER() OVER (ORDER BY v.name) AS ID, v.name, '&lt;r&gt;&lt;w&gt;' + REPLACE(RTRIM(LTRIM(v.name)),' ','&lt;/w&gt;&lt;w&gt;') + '&lt;/w&gt;&lt;/r&gt;' AS x FROM [master].dbo.spt_values v WHERE v.name IS NOT NULL) , cteB AS (SELECT ID, CONVERT(xml,x) AS x FROM cteA) SELECT b.ID, a.a.value('.','varchar(50)') AS Word, ROW_NUMBER() OVER (PARTITION BY ID ORDER BY (SELECT NULL)) AS Sequence INTO #tmp FROM cteB AS b CROSS APPLY x.nodes('/r/w') a (a) OPTION (MAXDOP 1); CREATE UNIQUE CLUSTERED INDEX CIX ON #tmp (ID,Sequence); SELECT t.Word AS Word1, t2.Word AS Word2, COUNT(*) AS [Count] FROM #tmp t INNER JOIN #tmp t2 ON t.ID = t2.ID AND t2.Sequence = t.Sequence + 1 GROUP BY t.Word, t2.Word ORDER BY [Count] DESC; SELECT t.Word AS Word1, t2.Word AS Word2, t3.Word AS Word3, COUNT(*) AS [Count] FROM #tmp t INNER JOIN #tmp t2 ON t.ID = t2.ID AND t2.Sequence = t.Sequence + 1 INNER JOIN #tmp t3 ON t.ID = t3.ID AND t3.Sequence = t.Sequence + 2 GROUP BY t.Word, t2.Word, t3.Word ORDER BY [Count] DESC;
Something like this (where GETDATE() is your date column): SELECT CASE WHEN DATEPART(HOUR,GETDATE()) &gt;= 17 THEN DATEADD(HOUR,32,CONVERT(datetime,CONVERT(date,GETDATE()))) WHEN DATEPART(HOUR,GETDATE()) &lt; 8 THEN DATEADD(HOUR,8,CONVERT(datetime,CONVERT(date,GETDATE()))) ELSE GETDATE() END edit: can only add hours to datetime, not date edit2: morning hours
I'm going to assume he also wants hours 0-7 bumped up to 8. OP, just add another CASE WHEN.
Good point. ^^*editted*
+1
Syntax error. I've got it working now though. I was using brackets around the version id which caused the issue. Picked that up by looking at another forum result. By using isnull I was able to return results for all which is what I was needing. See below. isnull(alias.column.value('(/BEGIN_FORM//EMBEDDED_FILE/@VersionID)[1]', 'varchar(10)'),'') Thanks for your help though.
There is no memo field type in MSSQL. What are you referring to?
OMG. Tears. Too funny. This is how I feel when I listen to the web development team. 
NVARCHAR (MAX) is the right conversion. See [this]( http://blogs.msdn.com/b/ssma/archive/2011/03/06/access-to-sql-server-migration-understanding-data-type-conversions.aspx) for details.
No. NVARCHAR (MAX) is the conversion. See [this]( http://blogs.msdn.com/b/ssma/archive/2011/03/06/access-to-sql-server-migration-understanding-data-type-conversions.aspx) for details. 
You can add 15 hours to the timestamp (7 hours in the evening, 8 hours the next morning) and compare it to today's date (midnight) + 32 hours. If it's more, add 32 hours to today's midnight, otherwise return the current timestamp. This will work in PostgreSQL: SELECT CASE WHEN now() + interval '15 hours' &gt; current_date + interval '32 hours' THEN current_date + interval '32 hours' ELSE now() END AS adjustedtime 
Cheers LittleRedDot... I always see you around here and, I've probably said this before but my cat fucking loves you. Big up yourselves. Also... I fudged the answer and the below is working for me: SELECT * FROM ( SELECT item_name FROM item_table WHERE item_code IN ('X','Y') ORDER BY DECODE(item_name, 'X', 1, 'Y', 2, 3) ) WHERE rownum &lt; 2;
This probably won't be a terribly popular comment, but I've got to say it anyway: in my experience, SQL is not very good at handling a lot of text. It's pretty slow, and there are definitely better ways to handle data mining like this. Unfortunately, I don't know what they are, because at my company we just passed that sort of thing along to our Data Science guy who would do some sort of Natural Language Processing (NLP) magic on it and voila - results! But I don't even know what program he used for that. Sorry I don't have anything more helpful to say than, "Try something that's not SQL."
A while loop? Boner killer
 Declare @TempTable table (DateValue datetime) Insert @TempTable(DateValue) Values ('1/1/2014 16:59:59') , ('1/1/2014 17:00:00') , ('1/1/2014 17:05:00') Select t.DateValue , [Hour] = DatePart(hh,t.DateValue) , DateValue_New = Case When DatePart(hh,t.DateValue) &gt;=17 Then DateAdd(hh, 32, Convert(DateTime, Convert(Date,t.DateValue))) Else t.DateValue End From @TempTable t DateValue|Hour|DateValue_New :---|:--|:--- 2014-01-01 16:59:59.000|16|2014-01-01 16:59:59.000 2014-01-01 17:00:00.000|17|2014-01-02 08:00:00.000 2014-01-01 17:05:00.000|17|2014-01-02 08:00:00.000 
Thank you... tell your cat I love her too 
Heh, the usual INSERT INTO #dilemma VALUES ('(ノಠ益ಠ)ノ彡┻━┻', N'(ノಠ益ಠ)ノ彡┻━┻') Anyway, varchar(max) and nvarchar(max) string operations support is identical - they work the same as varchar(n)/nvarchar(n). Searching for values and GROUP BY which are usually the first two problems you encounter with text fields work fine.
┬─┬ノ(ಠ益ಠノ)
I haven't gotten into more advanced sub queries like this yet. I appreciate the help. I see what you are doing here. You are creating temporary tables with the subqueries, then naming them. and running the main querie off of them. On another note. I formatted the code, and it shows as being formatted when I click on edit, but not when I post?? Edit: Found the formatting rules.
Thanks! My issue was that I did not create a line space between the text before writing out the code.
I absolutely agree that SQL is poor at parsing text. If I were writing this for work, I would use a CLR function to split the text into words. As it is, I avoided SQL string manipulation and used XML functionality instead. SQL is good at set operations and aggregation. Once text has been parsed into sets of words, SQL is a reasonable solution to compile the results.
Yes, this I understand. Again, there is no MEMO field type in MSSQL, only in MSAccess.
I'd suggest you get the Express version of SQL 2014 and use that. It's free anyway, and I believe you could connect it to msaccess.
I would Document some highly detailed specifications. It sounds like the original design was more "Organic" and not planned out up front before development began. This would do a few things for you 1)Give you experience in writing technical Specs 2)Allow you to work out any design issues with minimal effort Then if you have capacity you can develop your own spec or hand it over to the new person so you don't have to do it on the fly 2 weeks before you leave. 
What are you joining to? I don't see an "sh" table.
no idea what happened there. I fixed it.
The table structures are a little confusing.. I don't understand why your SalesOrderHeader table would have a SubTotal and TaxAmount column. That doesn't seem all that normalized, since you have Qty and unit price columns in your SalesOrderDetails table. Wouldn't it make more sense to just have SalesOrderID and TaxRate in your SalesOrderHeader table? Or were these tables already defined for you? 
This is the idea: SELECT CASE WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 100 THEN '0- 99' WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 1000 THEN '100- 999' WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 10000 THEN '1000-9999' ELSE '10000-' END AS [Range], COUNT(sd.OrderQty) AS Num_Order, SUM(sd.OrderQty*(sh.SubTotal+sh.TaxAmt)) AS Total_Value FROM SalesOrderHeader AS sh JOIN SalesOrderDetail AS sd ON sh.SalesOrderID=sd.SalesOrderID GROUP BY CASE WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 100 THEN '0- 99' WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 1000 THEN '100- 999' WHEN sd.OrderQty*(sh.SubTotal+sh.TaxAmt) &lt; 10000 THEN '1000-9999' ELSE '10000-' END *edit:* There are other problems with the query as written, but using a case statement to define buckets for aggregation is the concept the question is after.
&gt; Should I go back and make it "normalized"? Datamarts are not supposed to be normalized in the classical sense. &gt; I think it would take me roughly three months to go back in and redo everything. What do you think? It's good to have pride in your work, but if it meets specs you did your job. Reworking large projects is a luxury more than good practice. More productive is to apply lessons learned to something new.
Thanks very much!
According to w3schools, &gt;The size parameter specifies the maximum length of the column of the table EDIT: Eliminating the (255) seemed to fix the issue. Thanks!
This one is actually quite easy. Just cheat a bit and add a group by clause that counts the number of chars in the unit price (convert to char first through). Pad the unit price by a lead in 0 for 0-9 so you have correct results. It keeps your code readable a bit more if you create a function that returns a price class text based on the unit price as well.
Do you have MySQL workbench? It generates the SQL for you (which you can then inspect).
Off the top of my head char, varchar, and decimal all require a length specification. Most others dont. Date definitely dosent. 
subtotal is the sum of SalesOrderDetail LineTotal for a given SalesOrderId
Ohhh thanks a lot! I wanted Charindex(' ', REVERSE(r.[ Athlete]))+1 Have a nice day sir!
Thanks for all your help. A few questions. Is it necessary to use '/* create a CTE with details*/'? And can you explain the importance of the way you used WITH? 
combine the timeslots tables, and use a "type" code to distinguish between pool and gym
you're welcome!
 Select F.facility_name, Case When b.facility_id = 1 Then s.start_time When b.facility_id = 2 Then g.start_time Else NULL End as start_time, Case When b.facility_id = 1 Then s.end_time When b.facility_id = 2 Then g.end_time Else NULL End as end_time From facilities f Inner Join bookings b on f.id = b.facility_id Inner Join swimming s on s.timeslot_id = b.timeslot_id Inner Join gym g on g.timeslot_id = b.timeslot_id 
I agree, but rather than a type code, it makes sense just to use add facility_id to the timeslots PK And by inference, also drop facility_id form bookings table, since now timeslot id is unique even across facilities 
&gt; add facility_id to the timeslots PK oh no, you will frighten all the guys in this subreddit who insist upon an autonumber for the PK (in any dbms) in mysql, you can at least make the auto_increment subordinate to another column, but only for myisam tables 
Facility_id would be the FK of timeslots table. IF each time slot and facility combination is unique, then each row is, essentially, a composite primary key for the timeslots table.
so how do you form the timeslot_id? as an autonumber, or ... ?
Yea agreed. I'm assuming that the table architecture is set in stone. Best case scenario is to have a simple Timeslots table and a bit field distinguishing gym or pool. Or, better, and int field (in case they add some other offering like a basketball or tennis court). I wrote my query on the assumption that the dB structure can't be changed. 
the query is still wrong, though i think if they were LEFT OUTER JOINs it might work better, but you will still sometimes pull up both a gym and a swimming for the same timeslot_id, when the timeslot_id really belongs to only one of them... ...unless you're also assuming (and it's a huge assumption) that the swimming.timeslot_id and the gym.timeslot_id *will never have duplicated values* (which even the sample data does have)
Ah you're right. Ok convert to left Join for a working query. The dB design invites bad results. Best bet is to just have a Timeslots table and identity facility_id in the table, which others have recommended. But I'm assuming that the structure can't be changed. 
Else Begin Select Self Into #Kink.com If state = excited Repeat (forgive me, i'm shiftaced)
&gt; should i be combining temp tables to get my results? no SELECT TOP 1000 * FROM ( SELECT t1.file_name , t1.record_id , 'sent' AS type , t2.sent AS t2t3_data FROM table1 t1 INNER JOIN table2 t2 ON t2.RECORD_ID = t1.record_id UNION ALL SELECT t1.file_name , t1.record_id , 'recd' , t3.received FROM table1 t1 INNER JOIN table3 t3 ON t3.RECORD_ID = t1.record_id ) AS u note TOP without ORDER is pretty suspicious
ah i see what you did here. i believe i can leverage that. i appreciate it! the top 1000 was just so i could toy with a smaller sample, its not going to be used in final code or i would be missing a lot of records.
You have to alias even if you don't Union. At least, you do in MS SQL. SELECT AVG(price) AS AVG_PRICE FROM ( SELECT price from product a JOIN pc b ON a.model=b.model AND maker='A') SELECT AVG(price) AS AVG_PRICE FROM ( SELECT price from product a JOIN pc b ON a.model=b.model AND maker='A') hello Try it out. Top one will say "Incorrect syntax near ')'." Bottom one will work. You have to "Select from" an object. That's the way SQL works. If you have a sub query, you need to give your sub query an alias to "objectify" it. Otherwise there's no object to select from. 
Use Union instead of Union All to prevent intermittent dupes. This is a great solution though. 
Thanks for the reply. So if I am reading correctly, the correct setup allows this: Each employee can have multiple PositionIDs and for each PositionID that an employee has, they have a separate skill level. (Employee A can do position 2 with a skill level of 5; position 3 with a skill level of 2.)
I've updated my original post. For PositionA you would have several entries in the position table for it, each with a different level in the skilllevel column.
I wouldn't need the ID in the EmployeePosition table, would I? PositionID in [EmployeePosition] has a relationship with ID in [PositionTable]? Thanks again for the help!
Yeah probably not, I don't use access tbh, but in MSSQL I'd put an identity column on any tables that need replication so it might be overkill in access
Okay, last question... How would I set up my SQL to give someone a few positions and a skill level for each position? INSERT INTO EmployeePosition VALUES (1, 5); That gives an employee a position, but how do I give that specific employee a skill level for that specific position? Example: Employee A can do position 1 at skill level 2. Employee B can do position 1 at skill level 5. Should I move SkillLevel into the EmployeePosition table, not as a key? edit: Okay, I've got it now and things are coming back to me. Thanks a lot!!
The specific position would be in the positiontable several times, but each entry would have a different skill level in the SkillLevel column. So * [ID] [Position] [SkillLevel] * 1 PositionA 1 * 2 PositionA 2 * 3 PositionA 3 * 4 PositionB 1 * 5 PositionB 2 * 6 PositionB 3 Then you add the ID from this table and the employeeID to the mapping table. * [EmployeeID] [ID] * 1234 1 * 1234 5 * 2468 5 * 2468 6 * 0369 4 Edit: I can't help on writing it for access, but if it helps this would the kind MS SQL query you could run: Select FirstName, LastName, Position, SkillLevel From EmployeeTable a Join EmployeePosition b on a.EmployeeID = b.EmployeeID Join PositionTable c on b.ID = c.ID
You should also look into correlated subqueries: https://dev.mysql.com/doc/refman/5.0/en/correlated-subqueries.html For more information on the subtleties of subqueries and aliases.
Reddit Gold for anyone who can help me figure this out in the next few hours. I've been racking my brain on this for a while now.
I have tried both Red Gate SQL Compare and Apex SQL Diff. We use them for to keep production DBs sync. Of the two I prefer Red Gate, the comparisons are pretty quick and reliable. The tool requires acknowledgement when generating potentially dangerous scripts (for example if a drop column is included) and you can setup warnings for DB versions as well. Red Gate also has a command line diff which I don't have much experience with, but could be useful in the dev/qa scenario you described.
+1 for Red Gate SQL Compare, I use it regularly. Very helpful. Can't speak to Apex SQL Diff, however.
Here is a brief post on the first normal form. I'll have another article ready sometime tomorrow.
Do you mean you want to convert a list of Insert statements into a CSV file? A regex should work.
That's a very interesting question, which has been confusing SQL developers time and again, so [I've wrapped up an answer in this blog post](http://blog.jooq.org/2014/06/24/should-i-put-that-table-alias-or-not/). In summary, the [SQL standard](http://www.andrew.cmu.edu/user/shadow/sql/sql1992.txt) specifies table references as such: &lt;table reference&gt; ::= &lt;table name&gt; [ [ AS ] &lt;correlation name&gt; [ &lt;left paren&gt; &lt;derived column list&gt; &lt;right paren&gt; ] ] | &lt;derived table&gt; [ AS ] &lt;correlation name&gt; [ &lt;left paren&gt; &lt;derived column list&gt; &lt;right paren&gt; ] | &lt;joined table&gt; &lt;derived table&gt; ::= &lt;table subquery&gt; &lt;table subquery&gt; ::= &lt;subquery&gt; &lt;subquery&gt; ::= &lt;left paren&gt; &lt;query expression&gt; &lt;right paren&gt; In essence, this means that: - A "derived table" is another name for a subquery in the `FROM` clause (as opposed to other subqueries) - A derived table MUST always be aliased - The AS keyword is optional, for improved readability - The parentheses MUST always be put around subqueries As others have mentioned, this does not have to do anything with your subquery using `UNION ALL`, which is perfectly valid in any `&lt;query expression&gt;`, including `&lt;subquery&gt;`. It's just that MySQL's SQL parser (much like most SQL parsers) is a bit bad in reporting the true cause of a syntax error... In the case of MySQL, [the syntax is "specified" by an even simpler example](https://dev.mysql.com/doc/refman/5.7/en/from-clause-subqueries.html) SELECT ... FROM (subquery) [AS] name ... As you can see, the only optional thing is the `AS` keyword. The alias (`name`) is mandatory **Takeaway:** - ALWAYS wrap derived tables in parentheses - ALWAYS provide an alias for such derived tables
irrelephant to OP's problem
I'm tearing up right now. That was beautiful. Thank you.
You're welcome :)
I recommend you read and keep this article(PDF): http://www.iai.uni-bonn.de/III//lehre/vorlesungen/TDWA/WS07/1.pdf
A CTE (WITH clause) allows you to query for data then reuse it multiple times like it were a table. If you just want to know how many meetings were attended in a month, [this query](http://sqlfiddle.com/#!2/a3eb2/1) works: select A.Empid, A.mon, count(*) attended from ( select empid, month(mdate) mon from meetings ) A group by A.Empid, A.mon If you actually want the Pivoted data, then I'd first build a query to get the MIN date each month. Then I'd use that to get the second MIN date for each month. I'd UNION those two together to have a final set. THEN I'd use a PIVOT statement to transform the rows into columns.
Nice Article!
the second paragraph is more what I am looking for. I'm having difficulties capturing the 2nd min date. 
Not really a SQL question, but a /r/programming one. MS SQL is a Microsoft product, so would be best supported with their development tools, Visual Studio .NET in particular.
My bad. Though reading up on it couldn't possibly hurt.
wait wait WAIT Normalization just means making sure your tables are semantic? I thought it was some black art optimization voodoo Really?
because the order of the data within the partition is different. The first query is returning each sum of the top six identifiers partitioned by sex the second is returning the sum of the payment amount for the top 6 payment amounts partitioned by sex 
In a delete trigger there is no `INSERTED` available. The `DELETED` table will have the same schema as your table with only the rows that were deleted to fire the trigger. On an insert trigger, the situation is reversed with only the `INSERTED` table available. On an update trigger, `DELETED` contains the updated rows before the `UPDATE` was applied, and `INSERTED` is the same rows after the update. Your trigger is malformed. The line: IF (SELECT quantity FROM order_details) &gt; 0 Will be an error if there is more than one row in order_details: &gt; Subquery returned more than 1 value. This is not permitted when the subquery follows =, !=, &lt;, &lt;= , &gt;, &gt;= or when the subquery is used as an expression. You probably want: IF (SELECT COUNT(1) FROM order_details) &gt; 0 But even then, that's probably not right because if you delete everything from the table, the result of that test will be false. If what you're trying to do is just stop people from deleting from that table, it's probably best to deal with that using permissions. 
The best way I've found to do that is to query sys.dm_db_index_usage_stats to look for index usage. That dm view is cleared with each server reboot so it only shows you usage since the last reboot of the server.
Yeah, you could make a view of a view (we use it for joining multiple company database reports into one report). A view is sort of like making another table, but it doesn't hold data. It's a stored query you can call on with a quick select all statement rather than however table joins and sub queries and whatever else you might have in an insanely complex query. 
You can make views out of arbitrary queries. These queries can involve whatever other views. There isn't much of a limit. So view A can make use of view B, which can in turn make use of view C and D, having D making use of view E, and E making use of view F. It could go on and on. There is a Stanford course on databases which is available on-line. It goes into some of this explaining. It even goes into explaining an idea of how your DBMS could implementing views. The Stanford course =&gt; https://class.stanford.edu/courses/Home/Databases/Engineering/about
You want a DELETE trigger to do this. Here is code that will use context_info in case you ever need to delete as an admin you can set your context to allow delete. The reason for the context_info is otherwise you need to disable the trigger if you ever do need to do a delete. This will schema lock the table which reduces concurrency. Having context info allows you to always leave the trigger on and as an admin still be able to delete when necessary. The 'ALLOWDELETE=1' is arbitrary...you can make that whatever magic you want to have set in the context as required to allow someone to actually delete. CREATE TRIGGER [dbo].[CancelDelete_ContextInfo] ON [dbo].[Test] FOR DELETE AS DECLARE @AllowDelete VARBINARY(128) DECLARE @ErrorMsg VARCHAR(250) SET @ErrorMsg = 'dTest CAN NOT be deleted. Please contact the System Administrator' SET @AllowDelete = CONVERT(VARBINARY(128), 'ALLOWDELETE=1') IF @AllowDelete != Context_Info () BEGIN RAISERROR(@ErrorMsg,16,1) ROLLBACK TRANSACTION END RETURN To do the delete: DECLARE @ContextInfo VARBINARY(128) SET @ContextInfo = CONVERT(VARBINARY(128), 'ALLOWDELETE=1') SET CONTEXT_INFO @ContextInfo DELETE FROM dTest WHERE ...
I was just working on triggers earlier. This video helped me understand the inserted and deleted tables. https://www.youtube.com/watch?v=dAIWugrlL9E&amp;index=58&amp;list=PL31B586F8DE1ACCEB he's got a very causal style.
I normally do a straight import, cleanse / consolidate, and then move to the final location. 
With a traditional ETL (extract, transform, load) the intention is to do most of the cleaning during the transform. For most data warehouse type jobs i try to keep most of the cleansing during the transform step before saving to the destination. Sometimes a significantly more complex cleanse, will happen in a temporary staging location or as a post script to the load. For me the idea is, to take care of the problems while you have the data in memory the first time, and not have to go back through it again... it will be a lot faster. 
I almost always do an import to a staging location first then cleanse then merge into. My reasoning is that I'm way more paranoid about files reliably being imported and then just recording changes I made than trying to do weird acrobatics during the initial import.
+Another1 for RedGate.
I have worked for a client whose database employed this "view upon view upon view..." technique, layering views to filter base data at various incremental levels, using views on lookup tables to effect the filtering. In theory, it's a good solution. It works. The multi-level lookups started to affect performance eventually, though... Problems started when the practice got out of hand. There was no metadata retained that helped track data lineage within the related objects (tables/views/more views). That client is now rationalizing their view hierarchy to make it more manageable. My guidance would be to use the technique with a close eye on maintainability and data traceability. It is as easy to build a view against base tables as it is to build one against other views.
it's not easy to spot with your formatting SELECT SO.[02_TERRITORY_NAME] , SO.[11_SALES_CHANNEL] , Material_Tbl.[Brand (CS) (N)] , SO.[12_CUSTOMER_ID] , Sum(SO.[33_QUANTITY]) AS SumOf33_QUANTITY , Sum(SO.[34_AMOUNT]) AS SumOf34_AMOUNT , Left([SO]![27_SALES_DATE],4) AS 35_Year , Mid([SO]![27_SALES_DATE],5,2) AS 36_Month FROM SO LEFT JOIN Material_Tbl ON Material_Tbl.[Distributor Material] = SO.[28_ID_PRODUCT_DIST] GROUP BY SO.[02_TERRITORY_NAME] , SO.[11_SALES_CHANNEL] , Material_Tbl.[Brand (CS) (N)] , SO.[12_CUSTOMER_ID] , SO.[27_SALES_DATE] , Left([SO]![27_SALES_DATE],4) , Mid([SO]![27_SALES_DATE],5,2) now it's easier to compare SELECT and GROUP BY lists and spot the column that's throwing it off by the way, this is MS Access, not MS SQL Server, am i right?
Yep, sales date. Once you format it properly it's like a slap in the face. Thanks a lot!
Thank you for answering. I'm currently studing IT and I do have courses about relational databases. So I have some idea about it (some, I realise that the course itself cannot teach me all the things) For now, I just want to learn SQL.
Yeah, that is pretty much how SQL works, with one addition, it can usually be done 5 different ways.
Right. Don't get me wrong. I had a relational databases course at university too. It sucks, hard, in comparison to that textbook. I never took the Stanford course, but I've read the textbook. I can tell you for sure that the textbook is almost certain more helpful than a bunch of the materials you can find out there (including university courses). I imagine the Stanford course is pretty good too. There are plenty of reasons for this. A major one is that the book doesn't have time limitation. The author doesn't have to write a book so that it can be read/understood in 2-4 hours/week for 5 months (not that it doesn't try, since the authors are university professors too). The other is revision of content. Also, the fact that Ullman is one of the authors should give you a big clue that the book might be good. But.... For just SQL, you can start with the w3schools guide. It's pretty useful. I could be wrong, but as far as I remember, it was the first material I've used to learn SQL. 
The basics are easy and if you understand how databases work, then you'll be good. Learning whatever programs you work with and how their database works takes time. Especially ones that want to follow a pattern but don't. Then you just memorize around where the data you want lies over time. I'm looking at you, Dynamics suite... If you're talking versions, do you mean like sql vs mysql? Personally, I prefer the former but that's probably because it's where I started and mysql tends to deal with web type stuff (I lost interest in designing sites years ago). 
Since I don't really know much about SQL versions I'm not sure here. From what I heard MySQL is more like webservices oriented? I know that there are Oracle db, Microsoft db, SQL, MySQL, NoSQL, SQLite. And from what I understand they all differ in some ways.
You could always write a cursor that goes through each date-timestamp column on each table and then create a results table of max and min for each table. For me... to see the max and min would be best because you can get a better picture of the situation, since it seems like you're trying to do a clean up from previous dbas or something similar. and/or do the same thing but with date restraints. 
Check out the kimball groups website. It will put you on the right track but destroy your current perception of a database. 
This is based on price. Oracle is by FAR the best but licensing fees may make you shit yourself. MySql is basiclly free and microsofts sql server is a good happy medium. I recomend sticking with microsoft, everything fits together rather nice
Might as well add this here. Exact same question. http://www.reddit.com/r/Database/comments/293jtu/z/cih401k
I understand the concern but the only time I do this is when reading from a flat file where column order/structure can change. If you can manage to get rid of the paranoia your development process will speed up. If your doing data warehousing you dont even need a relational database in 3rd normal form. This is very hard to get used to but also speeds the project up. 
Kind of need a bit more information like what the tables look like, what's in them, and what is the expected result. 
In order to help we need to know what fields connect those tables. Your JOIN statements (one for each of the four tables being connected) need ON clauses that define that connection.
I have to use a SELECT query to provide all data necessary for the Student Details and Academic sections of the form which I linked. So I need to use data from five tables (some provided, I made the "STUDENT" one with the info given) but the other tables I need are MAJOR, MINOR, GENDER and FACULTY which give more detailed information like in the picture of the form I linked too. Hopefully that was somewhat coherent. http://imgur.com/uX4PyZ9
So like in the STUDENT table I have Major_Code which is linked to the MAJOR table to show the Major as Information Technology not just ITEC as it appears in the STUDENT table, is that what you mean? 
Yep, so a two table join between those would be something like SELECT STUDENT.*, MAJOR.* FROM STUDENT JOIN MAJOR ON STUDENT.MAJOR_CODE = MAJOR.CODE Only three tables to add to the mix.
oh cool thanks! so can i do multiple joins like that in one query? what would that look like? Also is it SELECT STUDENT.MAJOR or what? I'm just unclear with how you phrased that with the comma and italics.
Definitely need more information. But from what you wrote your going to need something along the lines of select s.student, m.Name, g.Type from student s left join major m on s.major = m.majorid left join gender g on s.gender = g.genderid What this does is allows you to name the student table s and the major table m. This allows you to specify what you want to be displayed What im assuming you have is a Major ID. Something along the lines of 1,2,3,4,5 etc. and each of these numbers mean something 1-Computer Science, 2-Art etc. So when you join them on the ID you can pull out other information like the name. I hope that helped 
ok so I have this: SELECT* FROM STUDENT INNER JOIN MAJOR ON STUDENT.MAJOR_CODE=MAJOR.MAJORCODE; and i think it works. how do i do that with MINOR_CODE and two others. Can i do them all in one query (join)?
You're joining on common columns between the tables. You can be very complex with your joins. If you use SQL fiddler and make up a few tables with data like you have for the project, I can steer you. 
That's what comes from my posting from my phone. SELECT Student.*, Major.*, Organization.* -- Gets all fields from student, then all fields from Major, then all fields from Organization. FROM Student JOIN Major ON Student.Major_Code = Major.Code JOIN Organization ON Student.Organization_code = Organization.Code I added a fictitious third table organization, that like Major has a code field in student. You'd just add each additional table to the select, and add another Join clause The things get messy when you have a student with two majors, or two organizations, or more than one of any other connection. If everything's limited to one row per student this will be perfect.
formatting in this post is going to help a lot here. first, in another post you made, doing a select * with a join won't bring data from all the tables together. you'll only get all the data from the first table and the join has no purpose. what you're looking for is something like this: select a.Name, b.major_description, c.semester from student a left join major b on a.major_code=b.major_code left join schedule c on a.ID=c.ID like others said, seeing the data is going to help with us helping you because i know that semester one isn't right, but not knowing if they're using an index value for the semester like they did with the major, i can't be positive. for me, i'm always looking at all the tables i need to compare, finding where the tables line up and i can bring data together, and then go by trial and error and hope what i'm looking for ends up right. (ha) right now i'm trying to bring about eight tables together where i have to link them all separately going from a to b, b to c, c to d, etc to end up with f displaying a result with some a data. don't be afraid to write this stuff down on paper, too. if i get more than three hops across tables to link data together, i have to write it down or i get lost. the fun part comes in when you start doing subqueries in your joins. i'm finally comfortable with those. 
I would do a left join. Also do the same. Left join minorTable mT on s.minorid = mT.minorid
'User_id' is a string, not a column. You are telling it to return a column containing just the string value you give in your query. Note your other columns don't have single quotes around them. 
You read it.
Whats wrong with using triggers?
Use a subquery mayhaps? SELECT * FROM recursivebom WHERE ItemNumber NOT IN ( SELECT DISTINCT ParentItem FROM recursivebom )
Here is a picture of the schema made from mysql workbench. http://imgur.com/KjBhoFl
Gotta watch out for the NULLs, this should do it: SELECT itemNumber FROM item WHERE itemNumber NOT IN (SELECT DISTINCT parentItem FROM item WHERE parentItem IS NOT NULL)
An outer apply can help here. select * , A.ChildItems from recursivebom.Item I OUTER APPLY (select count(2) as ChildItems from recursivebom.Item CI where I.itemNumber=Ci.parentItem ) A where A.ChildItems=0
Thousands of things. I'll settle with: You designed something wrong.
This should work: *** SELECT itemNumber FROM your_table a LEFT JOIN your_table b ON a.itemNumber = b.parentItem WHERE b.parentItem IS NULL ***
Thanks, this worked great. I also found an alternative that worked which was select t1.itemnumber from item as t1 left join item as t2 on t1.itemnumber = t2.parentitem where t2.parentitem is null; Now I have another question. What would I have to add into the query in order to add up the product of cost and quantity of all the nonparent items.
You must be really good at your job
Is this a query or do you want it to generate that on a insert into a table? What SQL platform is this for?
So now I want to roll up all these costs but I ran into quite a complex problem that I am not sure how to approach. The following query doesn't quite give me what I want because sometimes the parent item will have a quantity other than 1 that has to be multiplied. SELECT itemNumber, quantity, cost, parentItem, sum(item.cost * item.quantity) AS final_rollup FROM item WHERE itemNumber NOT IN (SELECT DISTINCT parentItem FROM item WHERE parentItem IS NOT NULL) group by itemnumber with rollup; It is giving me the final cost of 22 when it should actually be 31. The current query I have is not taking into account the quantity of level 2 items. Any ideas would be much appreciated.
If you learn ANSI SQL, it is portable to most databases. I'm not so sure how well MySQL supports ANSI but Microsoft, Oracle, IBM DB2, PostgreSQL all support ANSI syntax. Then they bolt on their own extensions and special sauce but ANSI code should be pretty portable.
If I understand what you want, this would give it to you in a select: select ftr_pak, ot_sum, (dense_rank() over (partition by ftr_pak order by ot_sum) - 1) *0.1 as udf_offset from table_name order by ftr_pak, ot_sum; Converting that to an update statement is left to the reader. ;)
Ahh yes, that pesky MYSQL tag at the top, subqueries can do the job. select * , (select count(2) as ChildItems from Item CI where I.itemNumber=Ci.parentItem) ChildItems from Item I where (select count(2) as ChildItems from Item CI where I.itemNumber=Ci.parentItem)=0 [SQL FIddle](http://sqlfiddle.com/#!2/160cb/2)
SQL Server 2012
hrm, maybe ill just select everything from the table, generate this, drop the table and replace it with this.. id much rather just update the udf_offset.. ill keep fiddling! 
Not sure which version of SQL you've got, but try some of the options from [this link](http://stackoverflow.com/questions/2334712/update-from-select-using-sql-server).
I think this might be acceptable tho. Thank you very much for your help. I will keep messing with trying to update, but i might have to end up just droping the table and loading this, its a yearly routine, so it might not be that big of a deal
If I recall, you can open FoxPro dbf files with both Excell and Access. It's been a long time since I've done it though. I believe they will be read only. I think the Microsoft Jet driver can load them as an ODBC source as well if you want to do it programmatically.
A join often generates a better execution plan than a subquery, so that might be the one to use. I kept away from it in this case because I try to avoid joining a table against itself unless I can gauge performance, and I don't have a mysqld to test against. You might try doing an EXPLAIN against both queries and see if one comes out looking more efficient than the other. As for the totals... You ought to be able to accomplish that by also selecting SUM(t1.quantity) and SUM(t1.cost) in the query.
I ran in to a situation to where I had to export a 1.6GB FoxPro dbf and excel just wouldn't handle it (Excel 2003 row limit). I used [DBF Viewer](http://www.alexnolan.net/software/dbf.htm) to export to CSV and then SSIS to import them in to MSSQL.
As lukeatron mentioned, you can open DBF files with Excel and Access. Or anything that supports dBase files. But there are a couple limitations: - FoxPro DBF files can be stand-alone, and this way are compatible with dBase files. But usually they're accompanied by a DBC (Database Container) file. The DBC file stores things like long field names, stored procedures, referential data integrity rules etc. that weren't part of the dBase standard. In most cases your conversion could ignore it, but you'll only get the first ten characters of each field name. - The last couple versions of FoxPro added auto-incrementing fields and a few more field types common on SQL servers. This breaks compatibility with things that read DBF files **IF** those field types are used. There's a Windows ODBC driver for FoxPro. But it only works with 32-bit Windows. (Since it was produced Microsoft spent years telling everyone to drop ODBC in favor of OLE DB. Before telling everyone to drop OLE DB and go back to ODBC.) (I did my first FoxPro programming, actually with FoxBase+, under Microsoft Xenix. Back when Microsoft was calling Unix the OS of the future. Before they changed their mind and told everyone to switch to OS/2. Er, wait, Windows NT.) There's a FoxPro OLE DB driver for Visual Studio projects, and it works fine under 64-bit Windows. You could wrap a SOAP web service around your FoxPro tables and call them from another program. Of course if you have a copy of the FoxPro development system itself, you can copy data to a wide variety of other formats - XLS, CSV etc - with the COPY TO command. Or use the SQL upsizing wizard to shove the data into the nearest SQL server. Or the best option: The SQL upsizing wizard was one of the bits that was made open source. There's a much improved version on CodePlex. 
Check out the WITH RECURSIVE query structures: http://guilhembichot.blogspot.com/2013/11/with-recursive-and-mysql.html and here are some videos from the stanford database course i took last year: https://www.youtube.com/watch?v=Yceqbp_DKbA https://www.youtube.com/watch?v=uU7JBM3cAWU https://www.youtube.com/watch?v=nsE4umE8r_8 
Caveat: No warrantly implied - use entirely at your own risk! Tested with my own data and table, under slightly different conditions (i.e. with only a single partition). Test thoroughly first before using on any live data. UPDATE TABLE FROM ( SELECT FTR_PK, OT_SUM, ((CAST(ROW_NUMBER() OVER ( PARTITION BY FTR_PK ORDER BY OT_SUM) AS DECIMAL (11,1)) * 0.1) - 0.1) FROM TABLE ) NEW_VAL (FTR_PK, OT_SUM, INCR) SET UDF_OFFSET = NEW_VAL.INCR WHERE TABLE.FTR_PK = NEW_VAL.FTR_PK AND TABLE.OT_SUM = NEW_VAL.OT_SUM ; HTH 
Thank you very much for your reply!
If SQL Express is installed and running, the server is the computer you installed it on. You should be able to connect to it in SSMS by opening a connection to "." (a single period) or `localhost`.
~~sql server management studio being *just* the tool, I guess that would be my problem.~~ EDIT: Just checked and the install path I took should have installed ssms and the accompanying server. I haven't tried SQLSERVEREXPRESS as a server name yet though. edit: Just checked that server name as well and I still get an error.
This sounds unnecessarily complicated. Can you describe the actual problem rather than your intended solution? Immediately it is strange to me that you are storing the same information twice--once in FLFF and a second time in Num36. Instead of storing the information, add it when you retrieve the row.
That just seems wrong. In fact, I immediately think of http://dbareactions.com/ when I think of your solution. You've basically created your own indexing table. SQL engines don't just "select random rows". The whole purpose of the query is to tell the SQL engine which rows to return. You should really step back and evaluate if your data schema meets your needs, what queries you are running, and if you have the proper indices on your tables. Again, creating your own index table just seems wrong and unnecessary.
I dunno. It might be. I definitely do want the data pulled randomly. Maybe there's a better way to do that without the indexes?
I can't really say without more context. Can you share your schema with us along with some common queries?
Did you get this fixed? If not, open sql server configuration manager and show us a screenshot of the services tab. Make sure the service for sql server is started, and then to connect to it should be localhost\instance where instance is what is in parenthesis, unless it's mssqlserver which means you leave instance blank, but you already said that didn't work.
SQL Server Services tab is empty. There's nothing in there at all.
How many lines we talking? Anything over a couple hundred thousand will often crash on me and I split it up in to smaller chunks, say 200k each
Check if the service is running
Rdpclip acting up?
Most databases including the Open Source ones have decent documentation along with a sample database for demos. The easiest way to learn SQL is by implement something. Do you have an assignment? Working through the assignment/problem gives rise to questions which you can read up on or pose here. 
If you're copying the code, why not open the .sql in notepad. If it's a huge script ( over a million lines) get ultraedit (I think it's free). Else the default thing is to uninstall and re install SSMS. Which is very time consuming.
 Declare @StartDate Datetime Set @Startdate = (Select Min(Date) from #Temp1) Declare @EndDate Datetime Set @EndDate = (Select Max(Date) from #Temp1) If you want the date range from the first insert then use it like this. You can set it as a static date too and then reference it throughout your transaction.
Will that function if I do multiple SELECT * INTO #TEMP queries beneath it?
I've edited my original response as not entirely sure how you want to populate them. If you want the date range used in your insert used in other queries please see my edit. You can set those declarations to specific dates and them reference them in your between 
Just as a simple example: SELECT * INTO #temp1 WHERE date between @Startdate and @Enddate SELECT * INTO #temp2 WHERE date between @Startdate and @Enddate SELECT * FROM Tabl1 LEFT JOIN #temp1 LEFT JOIN #temp2 WHERE date between @Startdate and @Enddate
Set the declarations as static dates: Declare @StartDate Datetime Set @Startdate = 'YYYY-MM-DD' Declare @EndDate Datetime Set @EndDate = 'YYYY-MM-DD'
Thanks, I'll try it out tomorrow. It isn't pretty but I have a query that necessitates an approach like this and I hate having to use replace, or run the risk of having one of the date ranges out of sync with the rest of the query.
No, no assignment. It's practically "free-roam". I appreciate your feedback though! I guess I could try organizing something generic like my music or contacts and practice from there.
w3schools was an awesome source! Thank you, much appreciated!
Here is my latest post on normalization; the second normal form. Enjoy!
Normalization isn't complicated! :) The definitions are hidden in complex terms, but once you "get-it" you "just know" how to adhere to the rules. I think it is like riding a bike. Once you know how to ride one, it is almost done subconsciously. One the other hand, if you try to explain how to ride a bike, it is hard to do so.
Hi, Newer version of SQLite allow you to insert multiple rows of values in one call. DELETE FROM tableName WHERE A = 'userName'; INSERT INTO tableName (A,B) VALUES ('userName','ABC'), ('userName','DEF'), ('userName','XYZ'); And as you suggest I would "wrap" these two commands in transaction.
&gt; Make sense? (I hope). not quite it appears that P2P and P22DZ are found in separate rows no CASE expressions is gonna handle that you want a LEFT OUTER self-join, with a COALESCE 
This is great! Too often I've seen database tables look like Excel Spreadsheets (and it's usually from people who use Access).
Yes, both exist. COALESCE in the FROM clause or SELECT statement?
COALESCE in the SELECT clause, to pick P22DZ when P2P does not exist
What is the syntax for the COALESCE to have 'P2P' return first? 
I tried: COALESCE((CASE WHEN (BA.LaneType = 'P2P') THEN BA.LaneType else NULL END),(CASE WHEN (BA.LaneType = 'P22DZ') THEN BA.LaneType else NULL END)) Both results returned 
 SELECT LM.Num , COALESCE(BA1.LaneType,BA2.LaneType) AS LaneType FROM dbo.Match as LM (nolock) INNER JOIN dbo.Stops as DS (nolock) ON LM.Num = DS.Num AND LM.DestinationCode = DS.WarehouseCode INNER JOIN [Central].[dbo].[Award] as BA1 (nolock) ON BA1.[Lookup] IN ( LM.OriginCity + LM.OriginState + LM.DestinationCity + LM.DestinationState , LM.OriginCity + LM.OriginState + LEFT(DS.ZIP,2) ) AND BA1.LaneType = 'P2P' LEFT OUTER JOIN [Central].[dbo].[Award] as BA2 (nolock) ON BA2.[Lookup] IN ( LM.OriginCity + LM.OriginState + LM.DestinationCity + LM.DestinationState , LM.OriginCity + LM.OriginState + LEFT(DS.ZIP,2) ) AND BA2.LaneType = 'P22DZ' WHERE LM.Num = 12346 
You most likely need to use the to_date function to get an actual date. My guess is you're doing a string comparison, the SQL engine is implicitly converting your date to a string then comparing it to the string you gave it '1997-01-01' the problem is that the default date format most likely isn't yyyy-mm-dd. for example, if the format is 'MM-DD-YYYY HH24:MI:SS' you'll be comparing '1997-01-01' to '07-30-2014 12:00:00' you want something like: where OrderDate &gt;= to_date('1997-01-01','YYYY-MM-DD') EDIT: I want to stress this really hard: You cannot count on the implicit conversion of a string to a date. It's a terrible practice and a terrible idea. You need to treat dates as dates and not as strings. My server may have a different defaults than yours, even if it works on yours, it will not necessarily work on mine. For example if NLS_LANG is set to American_America.WE8ISO8859P1, the date will print as DD-MON-YY, so the date February twentieths, nineteen eighty will print as '20-FEB-80' that will sort greater than the string '1997-01-01' and be in your query results. conversely if it's set to French_France.WE8ISO8859P1, it formats it as DD/MM/YY. EDIT(again): Here is the oracle documentation for data conversion. http://docs.oracle.com/cd/B28359_01/server.111/b28286/sql_elements002.htm#SQLRF00214 &gt;Implicit conversion depends on the context in which it occurs and may not work the same way in every case. For example, implicit conversion from a datetime value to a VARCHAR2 value may return an unexpected year depending on the value of the NLS_DATE_FORMAT parameter.
Thank you! 
Are you using Wildapricot? 
This site? http://www.wildapricot.com/
in TSQL: where datediff(day, orderdate, '1997-01-01' ) &gt;= 0
thank *you!* :)
the actual problem is that you forgot the quotes around your date change this -- and OrderDate&gt; 1997-01-01 to this -- and OrderDate&gt;= '1997-01-01' also, next time, please indicate which platform you're using (see sidebar)
Absolutely not. You still have the same issue. OrderDAte, is already a date, casting it again will have no appreciable impact. The SQL engine will still convert it to it's string representation before comparing it to '1997-01-01' It's really easy to see this by adding OrderDate and '1997-01-01' to the select clause. look at how each is displayed, OrderDate will display as a string as described by the default date format, you will see that it is not comparable to '1997-01-01'
No this one http://www.communitytech.net/solutions/apricot
Do you know any SQL at all? Sqlzoo.net is a great resource for getting your feet wet. I have all my new hires go through this site's&amp;dd exercises start to finish unless I am completely sure they have the chops I need.
Access is great, but can be evil... :) Especially when summer interns get their hands on it, leave to back to school, and the users come running to IT to support it... :)
No problem - I really like databases and coding. This is fun for me. -- geek -- :)
You can try [SQL/PLSQL Tutorial](http://www.techhoney.com) You will understand sql and plsql better and will get best results if you exercise the examples presented in above website.
Just starting. Thanks for the help.
Excellent post and blog. 
How many columns?
What exactly is "it"? Knowing both the database server and client you're using would be helpful.
If SQL Assistant won't let you do this, you can always use BTEQ (sample script below). .LOGON user,pass; .SET WIDTH 1000 .SET RETRY OFF .EXPORT DATA FILE=c:\Users\{yourWindowsusername}\Desktop\mydata.csv,CLOSE SELECT TRIM(COLUMN1)||','|| TRIM(COLUMN2)||','|| TRIM(COLUMN3)||','|| TRIM(COL.... ...etc... FROM YOURTABLE; .IF ERRORCODE &lt;&gt; 0 THEN .QUIT ERRORCODE .EXPORT RESET .LOGOFF You would save this to a file on your PC (maybe MYBTEQ.SQL), changing table name and column names as needed. Then run it using BTEQ (hopefully this is installed as one of the Teradata Tools on your machine). From a command line: cd to the location of BTEQ.EXE on your PC. Then: BTEQ &lt;{path}\MYBTEQ.SQL &gt;{path}\MYBTEQ.LOG The exported data will be in "c:\Users\{yourWindowsusername}\Desktop\mydata.csv" and the log of the BTEQ execution will be in {path}\{path}\MYBTEQ.LOG. This gives you an option. It is not as tidy as SQL Assistant, but if you cannot use that, the BTEQ route will work. You might have some pain tailoring the above script - it is only intended as a guide. HTH.
You should be able to achieve this pretty easily using [Pentaho's free ETL tool](http://community.pentaho.com/projects/data-integration/)
If you're using SSMS, you can right click the top corner of your results grid to select all, then click Save Results As... Then save as CSV and open in Excel. 
I would use a function to strip the ugly word strings from your company name. I'm on mobile atm, can't browse to an example but I think you can find one pretty easily. 
Here is a quick and dirty Proof of Concepts (it works as written). This is *NOT* the most efficient way to do this. But it shows you the logic. You should be able to use this as a starting point and: 1. Create a function to sanitize the data 2. Use CTE's and Cross Apply to do this as a Set-based approach which would be infinitely faster. This just what I could bang out in 5 minutes: -- Create example data IF OBJECT_ID('tempdb..#Company') IS NOT NULL BEGIN DROP TABLE #Company END IF OBJECT_ID('tempdb..#BadWord') IS NOT NULL BEGIN DROP TABLE #BadWord END CREATE TABLE #Company ( id INT IDENTITY(1, 1), CoName VARCHAR(100)) CREATE TABLE #BadWord ( id INT IDENTITY(1, 1), Word VARCHAR(100)) INSERT #Company SELECT 'McDonalds Co.' UNION SELECT 'The Coca Cola Company Ltd.' INSERT #BadWord SELECT 'Co.' UNION SELECT 'The' UNION SELECT 'Ltd.' /************************************************* Perform Data Cleansing *************************************************/ DECLARE @CurrComp INT = 1, @CurrWord INT = 1, @MaxComp INT, @MaxWord INT, @Word VARCHAR(50) -- Determine number of records to process SELECT @MaxComp = MAX(id) FROM #Company SELECT @MaxWord = MAX(id) FROM #BadWord -- Iterate through each Company WHILE @CurrComp &lt;= @MaxComp BEGIN -- Reset Word Counter SELECT @CurrWord = 1 -- Iterate through each Word WHILE @CurrWord &lt;= @MaxWord BEGIN SELECT @Word = Word FROM #BadWord WHERE id = @CurrWord UPDATE #Companys SET CoName = REPLACE(CoName, @Word, '') WHERE id = @CurrComp SELECT @CurrWord = @CurrWord + 1 END -- End Word SELECT @CurrComp = @CurrComp + 1 END -- End Company SELECT * FROM #Company
It actually has nothing to do with jdbc at all, just that oracle cannot directly compare a date to a timestamp. The easy fix is to create a functional index on the column converting it to a timestamp. 
Thank you. Did you ever get the certification? How did you go about the other ones?
I'm doing this one in August and it will be my first. I'll look at the others depending on how this goes. 
Your query runs without error? Is this sql server? If so use isnull(shipping address 1,' ') as shipping_address. If it's Oracle use nvl(). Also to dedupe maybe do this instead of a group by. Row_number over(partition by customer_Id order by date_column desc) as rank. And then put everything in a sub query and pull back columns where rank =1. If you don't have a date column to use in rank maybe use address_Id desc, assuming a higher numbered Id is newer and would be good for ranking the row. Using my phone but hopefully this helps. Obviously change my columns in pseudo code to the actual ones from tables.
Well, it does have to do with JDBC in the way that JDBC (partially) mimicks the SQL standard's notion of what `DATE` and `TIMESTAMP` really are - as opposed to Oracle's idea of `DATE` and `TIMESTAMP`. Yet, there is no simple way to tell JDBC that the `TIMESTAMP` bind value is really meant to be an "Oracle `DATE`", i.e. a `TIMESTAMP` without fractional seconds. If that could be hinted to ojdbc, then Oracle wouldn't make the "wrong" decision of widening the `DATE` column precision, it would truncate the `TIMESTAMP` parameter to seconds precision. This is explicitly done in the accepted answer through a `CAST(? AS DATE)`. But thanks for the hint with the functional index. That could be quite a helpful workaround in one or two occasions.
The books are great and if you consistently pass the practice tests (on the dvd) you'll pass the exam. 
Pluralsight. It's great for all kinds of technical training, but especially SQL Server. Some of the most respected people in the SQL Server community have classes up there. I wouldn't have passed Querying SQL Server (70-461) if it wasn't for the class up there for the exam. 
Windowing functions can be your friend here. 
Ditto, the guys over at [SQLSkills.com](SQLSkills.com) are some of the most respected in the industry and they have all their training over at PluralSight, I have an annual membership and highly recommend it. 
You don't have an ELSE clause in your case when, could that be why it's not working?
GROUP BY products.id ORDER BY CASE WHEN price = 0 THEN oldPrice ELSE price END ASC LIMIT 0, 300 not working :(. It sorts it. But i can't figure out how and why. This is how it comes out :/ http://imgur.com/Rw5Tciw
http://pastebin.com/Pvccvahk
okay, i suggest you assign column aliases that ~aren't~ the same as a column MIN(models.price * (1+vatRates.rate)) AS sort_price then the next column, shouldn't it have MIN or MAX? (models.oldPrice * (1+vatRates.rate)) AS sort_oldPrice anyhow, once that's cleared up, then you can use this -- ORDER BY COALESCE(NULLIF(sort_price,0),sort_oldPrice) ASC LIMIT 0, 300 
&gt; Reference 'sort_price' not supported (reference to group function) that doesn't sound like a MySQL error message could you please show the modified query, and the entire error message 
Could you just: SELECT idProduct, prodPrice FROM ( SELECT idProduct, price prodPrice WHERE price != 0 UNION SELECT idProduct, old_price WHERE price = 0 ) priceAbstract ORDER BY prodPrice 
Start with [SQLZOO](http://sqlzoo.net/wiki/Main_Page) for some interactive basics. Though there are different SQL variations, they're very close to one another with some syntax differences that you can generally handle case by case with some Googling. Highly recommend the Intro to Databases class from Stanford too (free, online, offered occasionally). Goes into a lot more than SQL but it's fantastic. 
Before you settle on using SQL, you need to understand what kind of data you're storing. Key/value pairs? A hierarchy? Relational data? Documents? Don't make the mistake of deciding upon a method of storage before settling on *what* you're storing and *how* it should be represented. If you still need relational storage, and just need something small, lightweight &amp; embedded in your application, [SQL Server Compact Edition](http://msdn.microsoft.com/en-us/data/ff687142.aspx) may be the ticket. It integrates well with the rest of your development in Visual Studio and there's no ugly licensing to deal with.
okay, when you're back from eating, please try this -- SELECT * FROM ( SELECT products.id , DATE_FORMAT(onHold,'%d-%m-%Y') AS onHold , productTexts.name , brands.name AS brandName , metadescription , MIN(models.price * (1+vatRates.rate)) AS minValue , MAX(models.price * (1+vatRates.rate)) AS maxValue , MIN(models.price * (1+vatRates.rate)) AS sort_price , MIN(models.oldPrice * (1+vatRates.rate)) AS sort_oldPrice FROM categories INNER JOIN products ON products.categoryId = categories.id AND products.inactive = 0 LEFT JOIN productTexts ON productTexts.productId = products.id AND productTexts.langCode = 'nl' LEFT JOIN brands ON brands.id = products.brandId LEFT JOIN models ON models.productId = products.id AND models.price BETWEEN 0 AND 3324 LEFT JOIN vatRates ON vatRates.id = models.vatRateId LEFT JOIN products_tags ON products_tags.productId = products.id AND products_tags.tagId = 2 WHERE categories.parentId = '118' AND categories.id = 188 GROUP BY products.id ) AS thequery ORDER BY COALESCE(NULLIF(sort_price,0),sort_oldPrice) ASC LIMIT 0, 300 
Have you managed to get postgres working with entity framework? I tried for ages (relatively new to all this so take that with a pinch of salt) but still failed. 
I don't work with MS frameworks, so I don't know.
sqlzoo.net
Thanks a lot!
Thanks a lot!
Sorry ahead of time that I can't provide links to any resources (as I'm on my phone), but here are some things to look up: - views - triggers - cursors - nested queries - group by &amp;amp; aggregate - order by - joins (inner, outer, natural, left, right)
Thank you! It works! by putting my query into a SELECT * FROM ( query ) ORDER BY... 
Thank you very much for you help and time! but /u/r3pr0b8 was able to provide me with a solution that needed very little editing. 
Understand your joins
thanks man. i wish these companies would just switch the syntax over!!
Because you mentioned statistical programs in assuming that you're job uses sql to extract and you'll analyze in another program. Learn how to make a table, possibly a temp table or perm with a population of interest and use it to re pull people's records. Inner join vs left join is the big things to learn. Basic summary functions should be something that you know too. 
I love me some WITH blocks.
I think "||" is the ANSI standard concatenation operator in SQL, so I don't think it will ever change...
Cursors load rows into memory and lock them
In my experience, cursors can be eliminated 8 in 10 cases, maybe more, with better performing alternatives. That said, I'd focus on just the worst performers first, leaving those fast cursors scripts alone - they're likely not hurting anything, they aren't the end of the world. With all of the sql alternatives out there like recursive cte's, window functions, lead, lag, lead, apply, while, etc., cursors are ever-easier to replace, and gain performance increases.
I prefer common table expressions but pulling 2 sets of data and joining them together will definitely be a topic either way you look at it. 
A buddy of mine got caught out not understanding what a union join is. Learning your joins is the best way to show you truly understand. Everything else is fine...
Both can be bad if they're just a lazy way of dealing with something that could have otherwise been done in a set-based manner. I find they're both useful for more administrative tasks though. I work on a db that was designed by an Oracle DBA, with most logic written by .NET developers (contractors to boot), and the cursors and while loops are thick, with triggers out the wazoo - and performance is completely horrible, go figure! If you need to step through rows on the DB often, then you're probably doing something wrong.
Not sure why do you state that cursors are no longer supported by MS. They definitely seem to be supported in 2014: http://msdn.microsoft.com/en-us/library/ms191179.aspx Usual reasons to use cursors are 1. VERY specific and STATIC access pattern that no one cares to figure out a way to hint SQL server optimizer into. 2. Variable chunking (e.g. based on the load) - now that we have offset/fetch shouln't really be needed anymore. 3. Every record processing needs to be a transaction on its own (so it's not a set operation to begin with). 4. Complex logic (e.g. stored procs) and/or CLR calls for each record. 5. Complex ordering (e.g. the 'rank' of the remaining records is re-evaluated on the existing set and some external metric) 
Step 1: Install SQL
[This covers most of the common questions that I've encountered.](http://careerbaba.in/2014/04/sql-server-interview-questions-answers/) I stole it from some other post here a few weeks back.
Why not like this? :- WHERE ID = pID AND Date = pDate AND Name = pName AND City = pCity EDIT, what I'm saying is I don't understand why you had the OR in there at all.
The 70-461 book is pretty good, as are most SQL books by Itzik Ben-Gan. I wrote that exam a few months ago and felt well-prepared. I just finished the 70-462 book in the same series (different authors) and found it to be much worse in comparison. I'm looking through MSDN, books online, and regular (not cert-prep) admin books to prepare for the exam now.
WHERE ID IN (your list of IDS) AND Name = pName AND City = pCity
okay great, thanks
thanks!
&gt; Is there anything you changed other than adding the min() around the week? no :) 
Off the top of my head.... Select wrid, max(case when WRRSLT like '%TDS%' then substr(WRRSLT, -3,3) end) as TDS_RESULT, max(case when WRRSLT like '%COND%' then substr(WRRSLT, -3,3) end) as COND_RESULT from WTRESULT group by WRID That will pivot your row oriented info into columns for you, anyway. That's not going to get you all the way there, but that should get you started.
SSRS has the ability to do all three of the bullets you listed. Though it does take quite a bit more maintenance to provide real-time commentary to each report that is put out. That idea may require constantly updating the ssrs files...which could get tedious. Good luck with your work!
without seeing your query, it sounds like you're aliasing a column with AS, but you need to terminate a function. edit: I know you've checked parentheses, but I've told myself that many times before, and found issues in stupid places. The only thing I can recommend is comment out columns you're selecting until the query runs. Maybe, remove all "AS" from your aliases and see if the error changes to something more debuggable?
Pretty useless without the query or do you want us to guess?
Its fed by a variety of programs. Ifigured I do not need to change the table structure as I am writing a query with the goal of creating a new table with the information I need from two previous tables. Pretty much the only obstacle I have is that I still need to know what data came from each table initially after the query runs.
I can't find any resource to show how to automatically paginate through the report pages. Would you happen to able to provide a resource on how this is done? Thanks 
 UPDATE table2 SET table2.discontinued = 1 FROM table2 INNER JOIN table1 ON table2.item_number = table1.item_number AND table2.item_description = table1.item_description WHERE table1.discontinued = 2;
Just quickly looking through youtube, I found this video that may be of some help. https://www.youtube.com/watch?v=nBYn1DU3VMQ Other than using the report properties, expressions, and groups for pagination, there are still more subtle ways of going about it. But this is a good place to start.
Silly question but have you tried localhost as the server name? Both of my Win 8.1 installs work that way.
Thank you. This shows grouping and rownumber, but still does not show a way to automatically navigate to the next page and loop back to the first. 
Thank you!. I'll look into some of these options if they're within budget. I don't really want an interactive dashboard. The only interaction that is required after is to add comments on certain performance indications. This will be done by the report creator edit: Would you happen to know if the software you provided can satisfy my criteria? Thanks
When you say "reference" do you mean you just want a select query to return all the results? Select * from table1 t1 inner join table2 t2 on t1.item_number = t2.item_number where t1.discontinued = 2 and t2.isdiscontinued = 1 This will return rows that meat your criteria 
He said he wanted to "reference" the data, not sure if he was ok with changing the data in table2. Then again, I'm not sure what he meant by "reference" either.
I've done this in the past by having individual reports open in chrome tabs and using a chrome extension to tab between them and refresh every time it displays. 
Would something like this work for you? SELECT t2.clean_string FROM t1 LEFT JOIN t2 on t1.dirty_string LIKE '%' + t2.clean_string + '%'
Check your mysql log My guess is innodb plugin is failing to load, or the innodb log is corrupt and mysql is taking a long time to rebuild it
You could potentially do this with the use of triggers (before-insert). For maintenance operations I usually use scheduled monthly/quarterly jobs that archive old data. What would be the use though, why store only a specific number of rows inside of a table?
Basically to implement RRD without RRDTool. Get rid of the RRD files and have data in SQL. One solution is indeed not to have exactly a round-robin table but just delete old records on regular basis.
**UPDATE:** oh I got it, yeah, you just MOD it with the number of rows. I'll go experiment with this, thanks! Yes, pre-populated is OK, initially just having zero values but all the rows with IDs should be there and never change. ~~Now, I'm not DB admin so the "(next(sequence) MOD N)" part escapes me. The way I see it, I would not know which row was updated in the "previous run", so I don't know which one to insert to in "this run". Or does the sequence thing do some magic there?~~ 
Here's an idea: have a separate table that holds the key-value pair. For instance (this is Oracle by the way, but you can adapt if you want): CREATE TABLE my_param_table ( seq_name varchar2(50), seq_value number(5)); INSERT INTO my_param_table (seq_name,seq_value) values ('MYSEQ', 1); COMMIT; And now for the trigger part: CREATE OR REPLACE TRIGGER my_trigger BEFORE INSERT on my_table FOR EACH ROW DECLARE v_seq number(5); v_max_seq number(5) := 1000; /* the maximum number of rows allowed */ BEGIN SELECT (CASE WHEN t.seq_value = v_max_seq then 1 ELSE t.seq_value + 1 END) /* obtain the next sequence number */ INTO v_seq FROM my_param_table t WHERE t.seq_name = 'MYSEQ'; DELETE FROM my_table m WHERE m.id = v_seq; /* delete the line that holds the current sequence number and replace it with the new one */ :new.id := v_seq; UPDATE my_param_table t SET t.seq_value = v_seq WHERE t.seq_name = 'MYSEQ'; EXCEPTION WHEN others THEN --log error-- NULL; END; / 
I think it would be far more elegant to run a scheduled job on a nightly or weekly basis to cull old records. 
That looks like that was it... is the innodb even something I would have access to when working with a hosting provider? Or is that a setting they would control?
We've used all three for non-interactive dashboards that are displayed on televisions around the office. I don't use or have access to Ducksboard or Cyfe anymore, but I know with Klipfolio you can add annotations - they're not generally visible on the charts or anything like that though. It's also the more expensive of the three, but also more flexible we found. Edit: And they all generally support a type of TV/Carousel mode that will flip through pages as well.
I'm not sure, I only use dedicated servers so Ive no idea what sort of access managed hosting provides. Do you have access to my.cnf, and ability to restart mysql? If not then its a job for your hosting provider 
Thank you sir for the information. It turns out that I wont be able to open my database for access. I'd rather have a native app hosted on the dashboard machine rather than open up a SSH tunnel to read the SQL Server DB data. I registered for Klipfolio trial, but wont be able to use it due to this constraint. I haven't looked at the other two suggestions yet. The hunt for software continues...
Thank you. I'd much rather pay for a software that fits all my needs without doing workarounds. I would definitely give this a try to see how well it works. Cheers,
I've not had any of these dashboards connect directly to my db - instead we've used several different push/pull models. Currently for Klipfolio we're using an FTP on our LAN, and we dump a CSV file to it regularly (produced by an SSRS sub) that it checks for and pulls into the dashboard. They have a myriad of different ways to get the data into the dashboard, besides connecting directly that is - you can even have it sent via email.
Impossible to say for sure, it really depends on whats in the mysql logs which are crucial. If you cant access the logs yourself, I'd be asking the hosting provider to zip them up and email them to you. Its possible the innodb log became corrupt, and the log had to be rebuilt. In this case the -1 from storage engine is because innodb has not fully initialised yet Its also possible mysql crashed or was not shut down properly, and innodb tried to run an integrity check when it next started up, but failed for some reason. Make sure you are keeping off-site backups of your database, as until you get to the bottom of it, treat the server or storage as somewhat suspect
No need to feel embarrassed at all, we all have to learn! 
I barely know C and I never programmed databases with C but I suspect that there are better alternative languages to takedown the problem mostly if you are a bit rusty. I could suggest - groovy look for the groovy.sql package/documentation. It's a jvm language interoperable with java with a script syntax. You just need the right jdbc driver (mysql, sqlserver, ...) - python, ruby, nodejs and others scripting languages. Every language should have a library to interact with databases. - in a Microsoft environment you should try with visual studio and c# (vb.net and others .net languages). If you are using sql server (and maybe others db) you can use all the drag and drop and wizard feature that vs has. It could help to connect to db, create in language rappresentation of db tables and query them using linq. 
Thanks for the help. C isn't really a requirement, it's just what I'm most famliar with. I might give a look into the visual studio and see how they work with SQL.
If C isn't a requirement, you should absolutely look at .NET. C# is the same basic syntax, so you'll be able to get started pretty quickly. With .NET, you can use Entity Framework which will take a massive load of work off of you when it comes to connecting to your database and managing CRUD operations. You can do everything in C as well, but if you don't already know exactly *how*, then you are going to be doing a lot more work. When it comes to an MS environment, using .NET for simple applications just makes sense. 
My main background is C but I taught myself C# because if you're working strictly in Windows, there's really nothing better. Wherever you're trying to do, "There's a class for that." Worst case you can just write a console application to run tsql queries.
Check on line 15 character 18. If it is not there then check all the other places in your code.
Based on everyone's suggestions, I have started looking into C# and .NET &gt; Are you post-processing or analyzing the data? Or are you displaying the data? If I can get this working, I have number of ideas I would like to try. So hopefully, all of the above.
This is what Microsoft Access is born to do - Connect to disparate data sources while running on a desktop PC. It has the GUI capability, a programming language built-in (VBA), Query capability using either the built-in GUI, a pass-through query, or built via the VBA language in code. A lot of (server side) types bash Access. But that's because they are server-side types to begin with. Not bread-and-butter PC power users. What you are trying to do is what I do professionally. Access has what you need here.
Can't believe I didn't think of using Access. I think I'll give that go first, before I delve into any new languages. Thank you.
Let me know if you need any help.
Your path of least resistance is probably a high level language like C# or personally I'd just use a scripting language like PHP. As much as I love C, there's really no need to use it, and be worrying about drivers, low level string manipulation and null pointers. A scripting language would mean its then a fairly natural evolution to provide some web features on your intranet, maybe report pages your users can use etc. I wouldnt recommend Access personally. You will have some success with it, but essentially you will reach a limit to what you can do with it (single user, windows desktop), and then you've painted yourself into a corner 
I remember Dr. Dobb's when it was a magazine!
Here is a visual of SQL Joins I recently made. I don't have a post yet, for joins, but hopefully you can appreciate the pic "as is." 
Honestly, you can get pretty far by just having them in a table and not worry about rolling over. What I would probably do is have one table that stores the raw data, and then a trigger which automatically rolls up to an aggregated table for a defined interval, like 60 seconds. Then write a job that deletes from the raw table weekly or so. 
In SQL Server, WHERE NOT EXISTS is faster than LEFT JOIN...WHERE tblB.fld IS NULL.
 SELECT SUBSTRING(.....) AS FIRSTCHAR, COUNT(*) AS COUNT_OF_FIRSTCHAR GROUP BY FIRSTCHAR ORDER BY FIRSTCHAR /* OPTIONAL */ ; HTH
Speaking of joins, I recently had a problem that was solved by using a Cartesian result, but not before I tried a FULL OUTER JOIN. I'm still trying to wrap my head around why the full outer didn't work. Can anyone give a quick rundown on why that might be?
Is there an equivalent of full outer in access?
The WHERE NOT EXISTS uses a semi-join.
The Cartesian returns every combination of rows and isn't dependent on any key matching. If you are joining two tables A and B with 100 and 200 rows respectively, then the total number of row returned is (100 x 200) = 20,000. The full outer join incorporates a key match. So at its center are the rows in common with the key, and on "each side" are the rows from the respective table that didn't match. Using the same tables from A and B above, assume that the key matches 50 rows in A with 100 in B. All other rows' key don't match. Then the total rows returned should be: 1) Key Match: 50 x 100 2) A not matching B: 50 3) B not matching A: 100 For a total of (50 X 100) + 50 + 100 = 5000 + 50 + 100 = 5150 Bottom line... The Cartesian product returns EVERY combination of rows from the two tables; whereas, the full join returns the combination of rows where the key is in common, and then one copy of each non-matching row from the respective tables... A Full Join equals a Cartesian product if the match key is a the SAME value in each row of both tables. 
Gotcha. Thanks so much for the explanation. 
It appears that some of the most basic things in SQL are overly complicated in Access. I really need to convince my boss that we should be using SQL.....
Select char_lenght(NAME) - replace(upper(NAME), 'A') from table Counts A's. Create another table for your sargs a-z and join in to your customer table.
I turns out its not running in there, how do I install it? Thanks for your advice EDIT: Never mind, i gave up on trying to make SQL server 2008 r2 try and work on windows 8.1, I just went with SQL 2014 and it works perfectly.
Noob here. Is there any difference between a JOIN and an INNER JOIN? How about a RIGHT JOIN and a RIGHT OUTER JOIN?
None at all.
[SQL Server 2008](http://i.imgur.com/pdvWnhC.jpg)
oh I forgot to mention the UNION approach as well. I tried to UNION ALL all of my tables and then delete the records with duplicate IDs based on the hierarchy but its the same performance problem.
So, question. Do you have duplicates of any records *within* a given table? If not, have you tried using NOT IN rather than EXCEPT? EXCEPT also includes a DISTINCT which may be slowing you down some. Overall though, I'd say there's no reason for this to take hours. Do you have indexes? 
Yes I did have duplicates but I used a DISTINCT before inserting them into temp tables. No, these temp tables are not indexed. 
Any particular reason you're not using a LEFT JOIN or NOT IN to exclude? I generally prefer using the LEFT JOIN method. So in your example that would look something like this: SELECT pm.ID FROM @projectmanagers pm LEFT OUTER JOIN @teamleads tl ON pm.ID = tl.ID WHERE tl.ID IS NULL 
I tried NOT IN but had the same problem, didn't try LEFT JOIN though. However, I just added indexes to my tables and everything is running fast now. 
As per [this documentation](http://dev.mysql.com/doc/refman/5.1/en/control-flow-functions.html#function_if), I think it should go more like this: IF (B.Jobtype="Repair","Repair","Install") AS JobTypeGroup2,
Use the Pythagorean theorem. This is a test of your [math](http://www.purplemath.com/modules/distform.htm) skills and how to apply them.
I just tried replacing that line with yours, but still saying that line is the issue. 
I did something like this previously in access and had to use the arctan function. Something about the diameter of the Earth (watch your rounding). 
Would: CASE WHEN B.jobtype='Repair' THEN 'Repair' ELSE 'Install' END AS JobTypeGroup2, do the trick? I don't use MySQL at all and am fairly new to SQL myself; so I'm not confident in that answer.
What is the point of having {(IF B.jobtype="Repair" THEN "Repair" ELSE "Install") = "Repair"} in the where clause? Do you only want Repair entries? If that is the case then why not just {WHERE B.jobtype="Repair" AND B.jobtype IS NOT NULL} and put B.jobtype in the select by itself. At that point you know it will only say Repair. You could also try using CASE instead. SELECT *, A.WeekEnding, A.AssignmentStartDate, A.Status, A.CircuitNumber, A.TaskCallID, A.Product, A.District, A.JobType, A.JobTypeGroup, A.ServiceType, A.AssignedEngineer, B.JobType, B.ServiceType, B.Status, B.AssignmentStartDate, B.AssignedEngineer, B.TaskCallID, B.jobtype AS JobTypeGroup2, CASE WHEN B.jobtype = 'Repair' THEN 'Repair' ELSE 'Install' END JobTypeGroup2_case --Will always return repair because of where clause. FROM [SCORECARD WEEKLY Step 1] AS A INNER JOIN [WEEKLY Data] AS B ON A.CircuitNumber = B.CircuitNumber WHERE B.jobtype = 'Repair' AND B.jobtype IS NOT NULL AND B.Status = 'Complete' AND B.AssignmentStartDate &gt;= A.AssignmentStartDate + 1 AND B.AssignmentStartDate &lt;= A.AssignmentStartDate + 15; AND A.AssignmentStartDate &gt;= '2014-01-15';
Teachers were right. Math is useful. :)
lol
at least i dont need to deal with the earth, this is a flat land.
A few things. It's confusing to say timestamp when talking in terms of datetimes. In MSSQL, timestamp is a special datatype which has to do with record sequence and not dates or times. You can't add a string and date. ~~`CONVERT(DATE,stuff) + ' 7:59:59.999'`~~ datetime (not datetime2) rounds off to 3 milliseconds. `'7:59:59.997'` is the last measurable datetime before 8 am. AND BCDateLocal &lt; CONVERT(DATETIME,CONVERT(VARCHAR(10), DATEADD(DAY, -1, GETDATE()),121) + ' 07:59:59.997')
&gt;AS JobTypeGroup2, FROM [SCORECARD WEEKLY Step 1] as A IDK if this will help but, in MSSQL that comma between the SELECT and the FROM clause would cause an error.
it seems what you're looking for is "yesterday at 8 am" try this -- AND BCDateLocal &lt; DATEADD(HH,-16,DATEADD(D,DATEDIFF(D,0,GETDATE()),0)) 
&gt; I'm so used to IF THEN ELSE in SQL, IF is a statement, not a function use CASE
he pointed you to MySQL documentation, because you said you were using "MySQL 2008" there is no such animal you are using microsoft SQL Server (not oracle MySQL) this is obvious from the square brackets around your table names in any case, use CASE
I am checking w my boss about posting the query. In the meantime, if I am using a union for three sub queries, can I give the matching fields in each sub query the same name? That's where all the AS notations come in. Thank you! 
I guess I left a bit out of this.. What I am looking to do is create a a query which can be used daily to report stats over the course of the day (hourly), because our BCDateLocal actually records a stat at ~.35 seconds, this gives us way too much data over the course of a day. We have a TimeProfile that is called sliding hour which basically gives us a total at the selected time, for the window of an hour from that point. The query in full right now looks like this: /*** 7:00AM - 7:59AM **/ SELECT SUM(cast(StatValue as int)) FROM [SD].[dbo].[StatServer] WHERE StatisticType = 'Q_N_Ent' and BCDateLocal &gt;= '2014-06-27 07:59:34.999' and BCDateLocal &lt; '2014-06-27 07:59:59.999' AND TimeProfile = 'SlidingHour'; /*** 8:00AM - 8:59AM **/ SELECT SUM(cast(StatValue as int)) FROM [SD].[dbo].[StatServer] WHERE StatisticType = 'Q_N_Ent' and BCDateLocal &gt;= '2014-06-27 08:59:34.999' and BCDateLocal &lt; '2014-06-27 08:59:59.999' AND TimeProfile = 'SlidingHour'; ...etc My goal is to use the Converted year month day, so that I don't need a unique query for this every day I want to run it. The struggle is that I am unable to simply convert todays date into a string, and add the last part for the time interval. When I use your suggestion, it also gives me an error saying that my last And statement is invalid. /*** 7:00AM - 7:59AM **/ SELECT SUM(cast(StatValue as int)) FROM [SD].[dbo].[StatServer] WHERE StatisticType = 'Q_N_Ent' and BCDateLocal &gt;= CONVERT(DATETIME,CONVERT(VARCHAR(10), DATEADD(DAY, -1, GETDATE()),121) + ' 07:59:34.999') and BCDateLocal &lt; CONVERT(DATETIME,CONVERT(VARCHAR(10), DATEADD(DAY, -1, GETDATE()),121) + ' 07:59:59.999' AND TimeProfile = 'SlidingHour'; &gt;Msg 156, Level 15, State 1, Line 6 Incorrect syntax near the keyword 'AND'. 
The way our data is stored in the db is that this particular table grabs a statvalue roughly every 30 seconds, which stores the data as a running total from the previous hour (TimeProfile = SlidingHour), what I am trying to do is hit the last thirty seconds of that hour which gives me an approx total for the hour. If you see the reply I had to the other commentor, perhaps it will make more sense. Thank you for trying to help though! I really appreciate it, and if you end up finding another suggestion for me, that too would be greatly welcome. 
Maybe something like this: SELECT CONVERT(varchar(14),BCDateLocal,121) + '00' AS HourInterval, SUM(CAST(StatValue AS int)) FROM [SD].[dbo].[StatServer] WHERE Anything GROUP BY CONVERT(varchar(14),BCDateLocal,121) + '00'
Ok... so lets put it this way -- I have set time intervals I want to query on - but I want to make my query flexible so that the DATE for Year month day can roll forward, without me needing to make any changes. So I am hoping to Concatenate my static time, with my dynamic date. Does this help?
Ok, I dont think that I am getting an understanding leveled between us here. My original Query is set up so that I am performing a query repeated over a 15 hour interval. My goal is to be able to use the actual Date from the system variables, and concatenate my static intervals so that the query is automated to some degree when I run it. I run the query today, to give me yesterdays data, for the intervals specified. I just want to use the Date function to grab year-month-day, and concatenate it to the hourly intervals I have predefined. 
anti-semi-join to be precise In exceptional cases it can still be useful to do `OUTER JOIN WHERE OneSide IS NULL` when using join hints or force order, but in general `NOT EXISTS` or `NOT IN` are absolutely better.
helps a little what is the static time you want to concatenate? and what is the date you want to concatenate it to, yesterday or today?
Sorry, which comma? The one at the end of the IF statement?
I added a ")" /*** 7:00AM - 7:59AM **/ SELECT SUM(cast(StatValue as int)) FROM [SD].[dbo].[StatServer] WHERE StatisticType = 'Q_N_Ent' and BCDateLocal &gt;= CONVERT(DATETIME,CONVERT(VARCHAR(10), DATEADD(DAY, -1, GETDATE()),121) + ' 07:59:34.999') and BCDateLocal &lt; CONVERT(DATETIME,CONVERT(VARCHAR(10), DATEADD(DAY, -1, GETDATE()),121) + ' 07:59:59.999') AND TimeProfile = 'SlidingHour';
Thank you so much for your input – I have been working with your responses, but I am self-taught in SQL so I am also including the query, as someone out there will no doubt have a better understanding than I do. I’m probably making this harder than it ought to be, but I’m working with limited resources. Thank you!! Note: This query is super-complicated because it’s all I could come up with given the near-impossible needs from staff – This is the command for a Crystal Report census sheet. The end product needs to display every name of every person on the unit during the selected shift (NOC, DAY, or PM) on the selected date. Everyone has data in system.episode_history, but not everyone will have a bh.bh_phf_census_sheet entry for the selected shift. They also need it in order of room number (which only comes from the bh_phf_census_sheet, IF they have an entry), hence numerous subqueries and unions…. If they have no bh_phf_census_sheet entry for that shift, their name should still display. If they have several bh_phf_census_sheet entries for the selected shift, only the last record for each person should display. ‘2014-05-27’ is a random date selection used for testing. Shift times: NOC, 00:00:00 to 06:59:59; DAY, 07:00:00 to 15:59:59; PM, 16:00:00 to 23:59:59 ---- select distinct present.ehfacility, present.ehpatid, present.ehepnum, present.ehadmit, present.ehdischarge, present.ehptname, present.ehuac, present.ATime, present.DTime, present.admitshift, present.dschgshift, present.shift, cen.pmxcendate, cen.pmxcentime, cen.proomnum, cen.phours_slept, cen.psuicide_level_checks_value, cen.psuicide_level_other, cen.pguarantor_value, cen.pdoctor_for_the_day, cen.pdoctor_for_the_day_value, cen.pdoctor_on_call, cen.pdoctor_on_call_value, cen.phomeless_v, cen.shift as cshift, cen.mlegalcode, cen.mlegalvalue, cen.mlegal_effective_date, cen.mlegal_effective_time, cen.mlegal_expiration_date, cen.mpatient_city, cen.msex_code, cen.mdate_of_birth from (((select distinct eh.facility as ehfacility, eh.patid as ehpatid, eh.episode_number as ehepnum, eh.preadmit_admission_date as ehadmit, eh.date_of_discharge as ehdischarge, eh.v_patient_name as ehptname, eh.user_row_access_code as ehuac, ad.ATime, ad.DTime, ad.admitshift, ad.dschgshift, 'NOC' as shift from system.episode_history eh left outer join (select distinct a.patid, a.episode_number, a.facility, convert(time, a.admission_time, 108) as ATime, convert(time, d.discharge_time, 108) as DTime, '0' as admitshift, (Case when (convert(time, d.discharge_time, 108)) between '00:00:00' and '06:59:59' then '0' else '1' end) as dschgshift from system.admission_data as a left outer join system.discharge_data as d on (a.facility=d.facility and a.patid=d.patid and a.episode_number=d.episode_number)) as ad on (eh.facility=ad.facility and eh.patid=ad.patid and eh.episode_number=ad.episode_number) where eh.facility='1' and eh.program_code='44281' and eh.user_row_access_code='1' and eh.preadmit_admission_date &lt;= '2014-05-27' and (eh.date_of_discharge is null or eh.date_of_discharge &gt;= '2014-05-27')) union (select distinct eh2.facility as ehfacility, eh2.patid as ehpatid, eh2.episode_number as ehepnum, eh2.preadmit_admission_date as ehadmit, eh2.date_of_discharge as ehdischarge, eh2.v_patient_name as ehptname, eh2.user_row_access_code as ehuac, ad2.ATime, ad2.DTime, ad2.admitshift, ad2.dschgshift, 'DAY' as shift from system.episode_history as eh2 left outer join (select distinct a2.patid, a2.episode_number, a2.facility, convert(time, a2.admission_time, 108) as ATime, convert(time, d2.discharge_time, 108) as DTime, (Case when (convert(time, a2.admission_time, 108)) between '00:00:00' and '15:59:59' then '0' else '1' end) as admitshift, (Case when (convert(time, d2.discharge_time, 108)) between '00:00:00' and '15:59:59' then '0' else '1' end) as dschgshift from system.admission_data as a2 left outer join system.discharge_data as d2 on (a2.facility=d2.facility and a2.patid=d2.patid and a2.episode_number=d2.episode_number)) as ad2 on (eh2.facility=ad2.facility and eh2.patid=ad2.patid and eh2.episode_number=ad2.episode_number) where eh2.facility='1' and eh2.program_code='44281' and eh2.user_row_access_code='1' and eh2.preadmit_admission_date &lt;= '2014-05-27' and (eh2.date_of_discharge is null or eh2.date_of_discharge &gt;= '2014-05-27')) union (select distinct eh3.facility as ehfacility, eh3.patid as ehpatid, eh3.episode_number as ehepnum, eh3.preadmit_admission_date as ehadmit, eh3.date_of_discharge as ehdischarge, eh3.v_patient_name as ehptname, eh3.user_row_access_code as ehuac, ad3.ATime, ad3.DTime, ad3.admitshift, ad3.dschgshift, 'PM' as shift from system.episode_history as eh3 left outer join (select distinct a3.patid, a3.episode_number, a3.facility, convert(time, a3.admission_time, 108) as ATime, convert(time, d3.discharge_time, 108) as DTime, (Case when (convert(time, a3.admission_time, 108)) between '16:00:00' and '23:59:59' then '0' else '1' end) as admitshift, '0' as dschgshift from system.admission_data as a3 left outer join system.discharge_data as d3 on (a3.facility=d3.facility and a3.patid=d3.patid and a3.episode_number=d3.episode_number)) as ad3 on (eh3.facility=ad3.facility and eh3.patid=ad3.patid and eh3.episode_number=ad3.episode_number) where eh3.facility='1' and eh3.program_code='44281' and eh3.user_row_access_code='1' and eh3.preadmit_admission_date &lt;= '2014-05-27' and (eh3.date_of_discharge is null or eh3.date_of_discharge &gt;= '2014-05-27'))) as present left outer join 
And you are now my freaking hero. Thanks alot jc4hokies! That is a tremendous help. I greatly appreciate you took the time to help 
Yeah, that is sort of lost in translation, I have been providing slightly differing queries. The time reported from ServerStats is UTC, and I am looking for EST which is the reason for the difference. In any event, another poster was able to help me resolve this. I really appreciate you taking the time though to chat a bit with me and offer a solution. Im still new to this stuff, and the community here with guys like you is such a blessing. Thanks
DML operations and reporting should never use cursors. If you think you must use them, you either don't understand advanced SQL queries or you have a procedural programming mindset and don't understand advanced SQL queries. Currently, my DBA team will reject all new code and updated code containing cursors. We have quite a few cursors implemented by the development team that was fired over the past year. They had other strikes against them, poor dml code and cursors being the majority. 
This makes much more sense, as the Between shouldn't exist as well. Thank you Jerk. You're not as Jerky as your name suggests.
It still feels wrong that you are running 24? selects when you could be running 1, but I'm glad your problem is solved.
The format of a recursive cte is this: WITH cte AS ( SELECT fields FROM table WHERE seeds --(top level parents) UNION ALL SELECT fields FROM cte INNER JOIN table ON hierarchy ) SELECT * FROM cte; The way it works is first the initial query runs: `SELECT fields FROM table WHERE seeds` Then the first recursion runs using the result from the initial query as the dataset for `cte`: `SELECT fields FROM cte INNER JOIN table ON hierarchy` Then the second recursion runs using the results from the first recursion as the dataset for `cte`: `SELECT fields FROM cte INNER JOIN table ON hierarchy` This continues until a recursion returns zero records. The "hierarchy" condition is typically something like `ON cte.RecordID = table.ParentID`.
Yes, in MSSQL it separates columns and it throws an error if you have one between the last column and the FROM statement.
Yes - I agree! Do you know how I could go about looping the query so that I can avoid 24 runs? The problem with the time interval being every 35 seconds has caused me some struggling.
Thank you. That does certainly make a lot of sense. I think I may be over complicating what I need to do though.
I don't fully understand the data, but one of the columns should work for you (I'm guessing LastStatValue). SELECT CONVERT(datetime,CONVERT(varchar(14),BCDateLocal,121) + '00') AS HourInterval, SUM(CONVERT(int,StatValue) AS SumStatValue, MAX(CONVERT(int,StatValue) AS MaxStatValue, CONVERT(int,STUFF(MAX(CONVERT(char(23),BCDateLocal,121)+CONVERT(varchar,StatValue)),1,23,'')) AS LastStatValue, SUM(CASE WHEN DATEDIFF(second,BCDateLocal,DATEADD(hour,1,CONVERT(datetime,CONVERT(varchar(14),BCDateLocal,121) + '00'))) BETWEEN 1 AND 25 THEN CONVERT(int,StatValue) ELSE 0 END) AS SumOfLast25Seconds FROM SD.dbo.StatServer WHERE StatisticType = 'Q_N_Ent' AND TimeProfile = 'SlidingHour' AND BCDateLocal &gt;= DATEADD(day,-1,CONVERT(date,GETDATE())) AND BCDateLocal &lt; CONVERT(date,GETDATE()) GROUP BY CONVERT(datetime,CONVERT(varchar(14),BCDateLocal,121) + '00'); 
&gt;[I'm hungry]( http://www.seriouseats.com/recipes/images/20081009-jerk-chicken.jpg)
hmm. thank you, I will have a look and try to figure this out. It's odd because after plugging this in I can't even get it to execute hah. It throws up an error right at line 2: &gt;&gt;'SUM' is not a recognized built-in function name. To contextualize what I am doing, we have a column that stores the StatValue as a number Total for the past hour (TimeProfile = SlidingHour), and this value is written to the DB approx every 25-35 seconds. So 25 seconds after the hour, StatValue has the past hour from that 25 second period. I am trying to report an hourly total, which is why I am looking at that very last record being written to our database. Maybe it makes more sense to take the first 25 seconds of the hour. Im not sure, that can be debated later.. but since the data is written every 25/35 seconds into our database, its not as nice and clean as it would be if we had a historical stats table where this data was written to hourly. (at least that is my opinion, but I am still quite new to this stuff)
SELECT a.Field, b.Field FROM ( Query A ) AS a LEFT JOIN ( Query B ) AS b ON a.PrimaryKey = b.PrimaryKey *** Hope this helps 
SELECT * FROM selectpanic.table WHERE EntryDate BETWEEN GETDATE()-1 AND GETDATE() This filters down to records with EntryDate within 24 hours of current system time *** SELECT COUNT(subquery.EntryID) AS RecordCount FROM ( SELECT * FROM selectpanic.table WHERE EntryDate BETWEEN GETDATE()-1 AND GETDATE() ) AS subquery WHERE COUNT(subquery.EntryID) &gt;= 3 GROUP BY subquery.EntryID *** This will select all EntryIDs for which 3 have been entered. Let me know if you have any questions or if I missed the point of the question altogether :P This method is called a "nested query"
 UPDATE TABLEB SET Active = 1 WHERE TABLEB.ID NOT IN (SELECT TABLEA.ID FROM TABLEA)
To set ACTIVE = 0 where ID in TABLEB not in TABLEA: UPDATE TABLEB SET ACTIVE=0 WHERE TABLEB.ID NOT IN (SELECT ID FROM TABLEA ) ; HTH
For which SQL server?
UNION also does an implicit DISTINCT which supports your assumption. You have to use UNION ALL to avoid the implicit distinct. 
With any query the first thing you always check is the Execution plan so you can see if Indexes are being used.
blog spam
True, didn't catch that bit.
 WITH Jobs AS ( SELECT ID, null parentID, agent, 'false' inheritAgent FROM table WHERE parentID IS NULL UNION ALL SELECT t.ID, j.ID parentID, CASE WHEN t.inheritAgent = 'true' THEN j.agent ELSE t.agent END agent, t.inheritAgent FROM table t JOIN Jobs j ON t.parentID = j.ID ) SELECT * FROM Jobs If I properly understood your request, this will give you a list of all the jobs and their potentially inherited agent. The agent/agentList didn't quite make sense. If agentList is just another column, then you simply add another CASE statement for it. 
Sorry. I realize I wasn't entirely clear. Agents can be part of an agent list. There's a third table that maps agents to lists. Thank you for this. I'll see if I can make something out of it tomorrow morning. 
&gt; Either way, outer joins are usually faster than subqueries/not ins Not true. `IN`, `NOT IN`, `EXISTS`, and `NOT EXISTS` are a kind of join called a semi-join or anti-semi-join. They are typically more efficient than `LEFT OUTER JOIN WHERE b.ID IS NULL` because they anti-semi-join does not contribute to duplication nor requires an additional filter step.
I'd start with setting up in instance of a database engine for him to play with (for a no-cost solution, set up a Linux virtual machine with Postgres or mySQL installed), complete with a web-based query tool such as phpPgAdmin or phpMyAdmin. There are free databases available for learning - eg see http://www.postgresqltutorial.com/postgresql-sample-database/ and https://dev.mysql.com/doc/employee/en/
Part 1: select distinct present.ehfacility, present.ehpatid, present.ehepnum, present.ehadmit, present.ehdischarge, present.ehptname, present.ehuac, present.ATime, present.DTime, present.admitshift, present.dschgshift, present.shift, cen.pmxcendate, cen.pmxcentime, cen.proomnum, cen.phours_slept, cen.psuicide_level_checks_value, cen.psuicide_level_other, cen.pguarantor_value, cen.pdoctor_for_the_day, cen.pdoctor_for_the_day_value, cen.pdoctor_on_call, cen.pdoctor_on_call_value, cen.phomeless_v, cen.shift as cshift, cen.mlegalcode, cen.mlegalvalue, cen.mlegal_effective_date, cen.mlegal_effective_time, cen.mlegal_expiration_date, cen.mpatient_city, cen.msex_code, cen.mdate_of_birth from ( (select distinct eh.facility as ehfacility, eh.patid as ehpatid, eh.episode_number as ehepnum, eh.preadmit_admission_date as ehadmit, eh.date_of_discharge as ehdischarge, eh.v_patient_name as ehptname, eh.user_row_access_code as ehuac, ad.ATime, ad.DTime, ad.admitshift, ad.dschgshift, 'NOC' as shift from system.episode_history eh left outer join ( select distinct a.patid, a.episode_number, a.facility, convert(time, a.admission_time, 108) as ATime, convert(time, d.discharge_time, 108) as DTime, '0' as admitshift, Case when (convert(time, d.discharge_time, 108)) between '00:00:00' and '06:59:59' then '0' else '1' end as dschgshift from system.admission_data as a left outer join system.discharge_data as d on a.facility=d.facility and a.patid=d.patid and a.episode_number=d.episode_number) as ad on eh.facility=ad.facility and eh.patid=ad.patid and eh.episode_number=ad.episode_number where eh.facility='1' and eh.program_code='44281' and eh.user_row_access_code='1' and eh.preadmit_admission_date &lt;= '2014-05-27' and (eh.date_of_discharge is null or eh.date_of_discharge &gt;= '2014-05-27') ) union (select distinct eh2.facility as ehfacility, eh2.patid as ehpatid, eh2.episode_number as ehepnum, eh2.preadmit_admission_date as ehadmit, eh2.date_of_discharge as ehdischarge, eh2.v_patient_name as ehptname, eh2.user_row_access_code as ehuac, ad2.ATime, ad2.DTime, ad2.admitshift, ad2.dschgshift, 'DAY' as shift from system.episode_history as eh2 left outer join (select distinct a2.patid, a2.episode_number, a2.facility, convert(time, a2.admission_time, 108) as ATime, convert(time, d2.discharge_time, 108) as DTime, case when (convert(time, a2.admission_time, 108)) between '00:00:00' and '15:59:59' then '0' else '1' end as admitshift, Case when (convert(time, d2.discharge_time, 108)) between '00:00:00' and '15:59:59' then '0' else '1' end as dschgshift from system.admission_data as a2 left outer join system.discharge_data as d2 on a2.facility=d2.facility and a2.patid=d2.patid and a2.episode_number=d2.episode_number ) as ad2 on eh2.facility=ad2.facility and eh2.patid=ad2.patid and eh2.episode_number=ad2.episode_number where eh2.facility='1' and eh2.program_code='44281' and eh2.user_row_access_code='1' and eh2.preadmit_admission_date &lt;= '2014-05-27' and (eh2.date_of_discharge is null or eh2.date_of_discharge &gt;= '2014-05-27')) union (select distinct eh3.facility as ehfacility, eh3.patid as ehpatid, eh3.episode_number as ehepnum, eh3.preadmit_admission_date as ehadmit, eh3.date_of_discharge as ehdischarge, eh3.v_patient_name as ehptname, eh3.user_row_access_code as ehuac, ad3.ATime, ad3.DTime, ad3.admitshift, ad3.dschgshift, 'PM' as shift from system.episode_history as eh3 left outer join (select distinct a3.patid, a3.episode_number, a3.facility, convert(time, a3.admission_time, 108) as ATime, convert(time, d3.discharge_time, 108) as DTime, Case when (convert(time, a3.admission_time, 108)) between '16:00:00' and '23:59:59' then '0' else '1' end as admitshift, '0' as dschgshift from system.admission_data as a3 left outer join system.discharge_data as d3 on a3.facility=d3.facility and a3.patid=d3.patid and a3.episode_number=d3.episode_number ) as ad3 on eh3.facility=ad3.facility and eh3.patid=ad3.patid and eh3.episode_number=ad3.episode_number where eh3.facility='1' and eh3.program_code='44281' and eh3.user_row_access_code='1' and eh3.preadmit_admission_date &lt;= '2014-05-27' and (eh3.date_of_discharge is null or eh3.date_of_discharge &gt;= '2014-05-27') ) ) as present left outer join 
Part 2: --continued-- (Select distinct cbyshift.pfacility, cbyshift.ppatid, cbyshift.pep, cbyshift.pmxcendate, cbyshift.pmxcentime, cbyshift.proomnum, cbyshift.phours_slept, cbyshift.psuicide_level_checks_value, cbyshift.psuicide_level_other, cbyshift.pguarantor_value, cbyshift.pdoctor_for_the_day, cbyshift.pdoctor_for_the_day_value, cbyshift.pdoctor_on_call, cbyshift.pdoctor_on_call_value, cbyshift.phomeless_v, cbyshift.shift, cbyshift.mlegalcode, cbyshift.mlegalvalue, cbyshift.mlegal_effective_date, cbyshift.mlegal_effective_time, cbyshift.mlegal_expiration_date, cbyshift.mpatient_city, cbyshift.msex_code, cbyshift.mdate_of_birth from( --Shift make table via union stmt-- --max individual census record for NOC-- (select distinct mxnoc.pmxcentime, mxnoc.pfacility, mxnoc.ppatid, mxnoc.pep, mxnoc.pmxcendate, mxnoc.proomnum, mxnoc.phours_slept, mxnoc.psuicide_level_checks_value, mxnoc.psuicide_level_other, mxnoc.pguarantor_value, mxnoc.pdoctor_for_the_day, mxnoc.pdoctor_for_the_day_value, mxnoc.pdoctor_on_call, mxnoc.pdoctor_on_call_value, mxnoc.phomeless_v, mxnoc.shift, mxnoc.mlegalcode, mxnoc.mlegalvalue, mxnoc.mlegal_effective_date, mxnoc.mlegal_effective_time, mxnoc.mlegal_expiration_date, mxnoc.mpatient_city, mxnoc.msex_code, mxnoc.mdate_of_birth from ( select max(convert(time, pcs4.census_time, 108)) as pmxcentime, max(pcs4.facility) as pfacility, max(pcs4.patid) as ppatid, max(pcs4.episode_number) as pep, max(pcs4.phf_census_date) as pmxcendate, max(pcs4.room_number_value) as proomnum, max(pcs4.hours_slept) as phours_slept, max(pcs4.suicide_level_checks_value) as psuicide_level_checks_value, max(pcs4.suicide_level_other) as psuicide_level_other, max(pcs4.guarantor_value) as pguarantor_value, max(pcs4.doctor_for_the_day) as pdoctor_for_the_day, max(pcs4.doctor_for_the_day_value) as pdoctor_for_the_day_value, max(pcs4.doctor_on_call) as pdoctor_on_call, max(pcs4.doctor_on_call_value) as pdoctor_on_call_value, max(pcs4.homeless_value) as phomeless_v, 'NOC' as Shift, max(l.legal_status_code) as mlegalcode, max(l.legal_status_value) as mlegalvalue, max(l.legal_status_effective_date) as mlegal_effective_date, max(l.legal_status_effective_time) as mlegal_effective_time, max(l.legal_status_expiration_date) as mlegal_expiration_date, max(p.patient_city) as mpatient_city, max(p.sex_code) as msex_code, max(p.date_of_birth) as mdate_of_birth from bh.bh_phf_census_sheet as pcs4 left outer join system.history_legal_status as l on pcs4.patid=l.patid and pcs4.episode_number=l.episode_number left outer join system.patient_demographic_history as p on l.facility=p.facility and l.patid=p.patid where pcs4.phf_census_date= '2014-05-27' and convert(time, pcs4.census_time, 108) between '00:00:00' and '06:59:59' and l.legal_status_effective_date &lt;= '2014-05-27' and p.data_entry_date &lt;= '2014-05-27' ) as mxnoc) --max individual census record for DAY-- union (select distinct mxnoc1.pmxcentime, mxnoc1.pfacility, mxnoc1.ppatid, mxnoc1.pep, mxnoc1.pmxcendate, mxnoc1.proomnum, mxnoc1.phours_slept, mxnoc1.psuicide_level_checks_value, mxnoc1.psuicide_level_other, mxnoc1.pguarantor_value, mxnoc1.pdoctor_for_the_day, mxnoc1.pdoctor_for_the_day_value, mxnoc1.pdoctor_on_call, mxnoc1.pdoctor_on_call_value, mxnoc1.phomeless_v, mxnoc1.shift, mxnoc1.mlegalcode, mxnoc1.mlegalvalue, mxnoc1.mlegal_effective_date, mxnoc1.mlegal_effective_time, mxnoc1.mlegal_expiration_date, mxnoc1.mpatient_city, mxnoc1.msex_code, mxnoc1.mdate_of_birth from (select max(convert(time, pcs5.census_time, 108)) as pmxcentime, max(pcs5.facility) as pfacility, max(pcs5.patid) as ppatid, max(pcs5.episode_number) as pep, max(pcs5.phf_census_date) as pmxcendate, max(pcs5.room_number_value) as proomnum, max(pcs5.hours_slept) as phours_slept, max(pcs5.suicide_level_checks_value) as psuicide_level_checks_value, max(pcs5.suicide_level_other) as psuicide_level_other, max(pcs5.guarantor_value) as pguarantor_value, max(pcs5.doctor_for_the_day) as pdoctor_for_the_day, max(pcs5.doctor_for_the_day_value) as pdoctor_for_the_day_value, max(pcs5.doctor_on_call) as pdoctor_on_call, max(pcs5.doctor_on_call_value) as pdoctor_on_call_value, max(pcs5.homeless_value) as phomeless_v, 'DAY' as Shift, max(l2.legal_status_code) as mlegalcode, max(l2.legal_status_value) as mlegalvalue, max(l2.legal_status_effective_date) as mlegal_effective_date, max(l2.legal_status_effective_time) as mlegal_effective_time, max(l2.legal_status_expiration_date) as mlegal_expiration_date, max(p2.patient_city) as mpatient_city, max(p2.sex_code) as msex_code, max(p2.date_of_birth) as mdate_of_birth from bh.bh_phf_census_sheet as pcs5 left outer join system.history_legal_status as l2 on pcs5.patid=l2.patid and pcs5.episode_number=l2.episode_number left outer join system.patient_demographic_history as p2 on l2.facility=p2.facility and l2.patid=p2.patid where pcs5.phf_census_date= '2014-05-27' and convert(time, pcs5.census_time, 108) between '00:07:00' and '15:59:59' and l2.legal_status_effective_date &lt;= '2014-05-27' and p2.data_entry_date &lt;= '2014-05-27') as mxnoc1 ) 
Part 3: --max individual census record for PM-- union (select distinct mxnoc2.pmxcentime, mxnoc2.pfacility, mxnoc2.ppatid, mxnoc2.pep, mxnoc2.pmxcendate, mxnoc2.proomnum, mxnoc2.phours_slept, mxnoc2.psuicide_level_checks_value, mxnoc2.psuicide_level_other, mxnoc2.pguarantor_value, mxnoc2.pdoctor_for_the_day, mxnoc2.pdoctor_for_the_day_value, mxnoc2.pdoctor_on_call, mxnoc2.pdoctor_on_call_value, mxnoc2.phomeless_v, mxnoc2.shift, mxnoc2.mlegalcode, mxnoc2.mlegalvalue, mxnoc2.mlegal_effective_date, mxnoc2.mlegal_effective_time, mxnoc2.mlegal_expiration_date, mxnoc2.mpatient_city, mxnoc2.msex_code, mxnoc2.mdate_of_birth from (select max(convert(time, pcs6.census_time, 108)) as pmxcentime, max(pcs6.facility) as pfacility, max(pcs6.patid) as ppatid, max(pcs6.episode_number) as pep, max(pcs6.phf_census_date) as pmxcendate, max(pcs6.room_number_value) as proomnum, max(pcs6.hours_slept) as phours_slept, max(pcs6.suicide_level_checks_value) as psuicide_level_checks_value, max(pcs6.suicide_level_other) as psuicide_level_other, max(pcs6.guarantor_value) as pguarantor_value, max(pcs6.doctor_for_the_day) as pdoctor_for_the_day, max(pcs6.doctor_for_the_day_value) as pdoctor_for_the_day_value, max(pcs6.doctor_on_call) as pdoctor_on_call, max(pcs6.doctor_on_call_value) as pdoctor_on_call_value, max(pcs6.homeless_value) as phomeless_v, 'PM' as Shift, max(l3.legal_status_code) as mlegalcode, max(l3.legal_status_value) as mlegalvalue, max(l3.legal_status_effective_date) as mlegal_effective_date, max(l3.legal_status_effective_time) as mlegal_effective_time, max(l3.legal_status_expiration_date) as mlegal_expiration_date, max(p3.patient_city) as mpatient_city, max(p3.sex_code) as msex_code, max(p3.date_of_birth) as mdate_of_birth from bh.bh_phf_census_sheet as pcs6 left outer join system.history_legal_status as l3 on pcs6.patid=l3.patid and pcs6.episode_number=l3.episode_number left outer join system.patient_demographic_history as p3 on l3.facility=p3.facility and l3.patid=p3.patid where pcs6.phf_census_date= '2014-05-27' and convert(time, pcs6.census_time, 108) between '16:00:00' and '23:59:59' and l3.legal_status_effective_date &lt;= '2014-05-27' and p3.data_entry_date &lt;= '2014-05-27' ) as mxnoc2 )) as cbyshift ) as cen on present.ehpatid=cen.ppatid and present.shift=cen.shift group by present.ehpatid order by cen.pmxcentime, cen.shift desc
Open that query editor and write this; use YourDatabaseName go create procedure dbo.DoSomething as begin select SomeField from SomeTable; end Not so hard. Now start learning about variable and parameter declarations, and you'll be a champ in no time.
If you download SQL Server Express (free) it comes with the Adventure Works database. Almost all online walk-through guides, training materials, blog articles, etc., make reference to the adventure works database as their "sample data" to play with. Install that thing, and start with sqlservercentral.com using their walk-through's. The site is loaded with a wealth of learning, and a very enthusiastically responsive community.
I came here to say just that. Heck, some of these (What is ACID? Define candidate key, alternate key, composite key, Auth modes, scheduling jobs, etc.) I would expect from any developer who works with the database outside of just writing simple queries. And then there's nothing about everyday things like optimizing query plans, which is just puzzling.
Enjoy! I spent a lot of time trying to get across what "Transitive Dependence" means. I hope everyone learns something from this post.
That is spot on... At its simplest, a stored procedure is just a wrapper around a query. You can add logic, such as IF-THEN statements to them, but as IncredibleMouse suggests, learning how to add variable and parameters is huge. I bet 90% of my SP are just simple calls. Usually just simple Update, Insert, and Delete statements. 
Check out www.essentialsql.com. The article assume you know nothing about SQL. I have a guide to get you going in installing MS-SQL Server, or you can stay simple with SQLite. Eitherway, if your buddy sticks with it, he will know the ins and outs of the Select statement in no time.
I'm disappointed in the "answer" for #2 (Profiler). Profiler is deprecated (Extended Events is the new hotness) and can very easily cause major performance issues. Not something you want to point at your production instance in the middle of the workday on a whim.
so, if you get this sorted and running, here's a suggestion to refactor both for performance and readability/scalability. First, get all of your sub-queries into temp tables. For example, make mxnoc, mxnoc1 and mxnoc2 all temp tables, and run them at the top of the query. Then, make cybshift a temp table as well. declare @cybshift table (field int, field varchar(1), field bla) insert into @cybshift select * from mxnoc union select * from mxnoc1 union select * from mxnoc2 select distinct field1, field2, field3 from cybshift Doing this is going to make it run a bit faster, and make it much easier to read and/or modify. Another hint is to put filters on your joins, if possible, rather than WHERE statements. This is perfectly acceptable and again, helps increase performance. So this: select field1, field2, field3 from bh.bh_phf_census_sheet as pcs4 left outer join system.history_legal_status as l on pcs4.patid=l.patid and pcs4.episode_number=l.episode_number left outer join system.patient_demographic_history as p on l.facility=p.facility and l.patid=p.patid where pcs4.phf_census_date= '2014-05-27' and convert(time, pcs4.census_time, 108) between '00:00:00' and '06:59:59' and l.legal_status_effective_date &lt;= '2014-05-27' and p.data_entry_date &lt;= '2014-05-27' Becomes this: select field1, field2, field3 from bh.bh_phf_census_sheet as pcs4 left outer join system.history_legal_status as l on pcs4.patid=l.patid and pcs4.episode_number=l.episode_number and l.legal_status_effective_date &lt;= '2014-05-27' left outer join system.patient_demographic_history as p on l.facility=p.facility and l.patid=p.patid and p.data_entry_date &lt;= '2014-05-27' where pcs4.phf_census_date= '2014-05-27' and convert(time, pcs4.census_time, 108) between '00:00:00' and '06:59:59' Good luck
&gt; learning SQL and Stored Procedures would save me and the company I work for a lot of time I'm all about learning SQL, but I've seen several otherwise solid systems get scrapped due to excessive unprofessional user-generated code. While writing your own stored procedures may save time now, it's a slippery slope and can potentially cost the company serious money (100,000+) later. In full disclosure, I've seen a lot more systems get scrapped due to professionally written code.
There is no typo, as you indicate it can be rolled back in a transaction, what other type of ROLLBACK is there? [Rolling Back Truncate Table](http://sqlblog.com/blogs/kalen_delaney/archive/2010/10/12/tsql-tuesday-11-rolling-back-truncate-table.aspx)
Seconded, writing stored procedures may save you money if done correctly, but just writing stored procedures could lose your company a lot of money if not executed well. Rather than just learning how to sproc up queries, learn SQL from the ground up, understand relationships, normalisation, etc. *This* will save your company money. If you only understand how to write basic queries into stored procedures you might think everything's great, but a little knowledge can be dangerous. The more you know, the more you know you don't know. Learn from the ground up and don't take shortcuts.
if your SELECT queries that you've already created and use regularly include WHERE statements at the bottom, and you find that you're constantly changing those for various needs .. but only changing one specific value .. then you'll want to add a parameter to the beginning. use db go create procedure dbo.coolproc @val1 as begin select * from table where val = @val1 end This way, when it comes time to exec your procedure, you can exec dbo.coolproc 'Derp1' And you'll get back select * from table where val = 'Derp1' Same goes for UPDATE/SET, INSERT and DELETE procedures. Once you get familiar with writing these, you'll find they compliment SSRS or Crystal Reports, or web services, and you'll be able to start collaborating with other users and teams.
Here's a nice little reference guide with a bunch of best practices. http://sqlblog.com/blogs/aaron_bertrand/archive/2008/10/30/my-stored-procedure-best-practices-checklist.aspx It's never too early to make sure your stored procedures are easy to read and conform to good coding practices. Good luck.
[SQLZoo](http://sqlzoo.net/wiki/Main_Page) is pretty cool learning material
Well done.
thanks guys i figured it out already: the code is SELECT SUBSTRING(name,1,1) AS 'Name', COUNT(name) AS 'Total' FROM customers GROUP BY SUBSTRING(name,1,1) HAVING COUNT(name)&gt;2;
Each column(s) declared unique will limit the number of rows. The number of rows your table will hold is the lowest of those. Currently, you seem to have two columns declared unique: ID and uniqueID. Your autoincrement ID has the smaller range; it will put an upper bound of 4,294,967,295 rows on your table. If you remove that column, the number of rows will *probably* be [limited by the OS](http://dev.mysql.com/doc/refman/5.7/en/table-size-limit.html). If you remove that column, and if it's not limited by the OS, and if you use only the English upper- and lowercase letters and 10 digits, your table can hold something on the order of 4.7x10E18 rows. (Very rough estimate. Actual number of combinations is higher.) 
That's a lot more than I'll ever need I think ha, thanks so much for the rough estimate :)
Well, now he's gone and changed it. TRUNCATE TABLE can be rolled back. Try it.
I'll bet that if I walked over to the Dev area and asked the java people here, not a single one of them would be able to tell me what ACID means without resorting to Google. And even if they could recite the words, they have no idea what it actually *means*.
you seem to have forgotten OP's 1 GB limit
Thanks for doing this. 3rd normal form has been the hardest one for me to grasp. It's like I get it, but it's hard to pick out examples. I look forward to reading this one, enjoyed your first 2.
That makes me sad.
It makes it difficult to explain why encoding everything into a giant comma-delimited string in a single VARCHAR() field is a bad idea, for example.
Like, a MUCK table on steroids?
You need to fix the title. These are not SQL server DBA questions. They are questions if you want to hire a tech to manage Microsoft's attempt at an SQL server.
tilt your head, maybe? seriously though, the view is just another source of data (that is, you will use it in the "from" clause), so use whatever your SQL platform of choice conversion function or go with concatenation of date parts of the shipped_date.
&gt; any ideas on how to proceed? pretend the view is a table :)
HiveQL is not much different from SQL conceptually. HiveQL is something that any experienced SQL developer could pick up relatively easy. Just like it would be fairly easy for an object orientated programmer in object C to pick up another object orientated such as java or C#. [Looking at their big picture](http://docs.treasuredata.com/articles/data-processing-overview), it looks like HiveQL isn't really intended for data analytics. They show JDBC connectivity and push to MySQL/PostgreSQL for more complex data warehousing, dimensional modeling and reporting. HiveQL could probably do some live reporting but unless you are doing a report for the sales generated in the past hour or outstanding orders to be filled. However, based on the business of your company, you'll want to extract that data into your BI project on another SQL server. I would suspect if you are to be put in a position with a potential client your interactivity with HiveQL would be stage 1 "extract" such as: Select invoicenumber, somefield, somefield, somefield from www_invoices Once the data is pushed off the treasure system through HiveQL, Your experienced SQL developers can move to stage2 which is "Transform &amp; Load" into the dimensional model that they designed. It sounds really easy but for someone new this may seem challenging. As the "Guy who has been brushing up on HiveQL", you would also be in charge of developing a ["Scheduled Job"](http://docs.treasuredata.com/categories/scheduled-job) in which the data is extracted through HiveQL to your SQL server. I'm writing a lot of this based on what you say your company does and the business they are probably trying to expand to. Your boss/supervisor could be well versed in HiveQL or at least how to integrate it, or he could be hearing ,"We need to look into it". As a Business Intelligence Developer/Admin, I look at HiveQL and shrug my shoulders. It doesn't really matter what the source system is imo if you are just extracting from it. If its a treasure database with home grown database and terrible developers who set up the rest API, I may be worried for the "transform and load" stage because its probably filled with crap data. If you are doing stage1 extract, that's not your problem :). MOST likely, its an out of box e-commerce app written by an actual software company. Which means the tables are fairly clean, well designed and the data should be well maintained and easy to extract. If your company is not offering BI analysis tools on another engine such as mysql or postgreSQL with your own front end, and you are intending to offer LIVE BI tools through the JDBC connectivity tool kit... you are way out of your league.
http://sqlschool.modeanalytics.com/ is a new tutorial for learning SQL. It's written in plain English and provides access to a database to query. Best of luck to your friend!
Views are basically just stored queries, so any time the view is accessed, it will query the tables. 
Thank you for the resources. I'll try to give you a bit more information on what exactly I'll be doing. In retrospect, I should have specified I'm working with clickstream data. My company provides a large amount of relational data (NOTE, on second thought, I'm not certain its relational; I think it is, but it's somewhat unlikely, I'll find out soon enough) automatically compiled from consumers. The services my company is going to be providing is twofold: a SaaS that provides demographic and other information from clickstream data, and custom contracts from the same data. The custom contracts are usually quite simple queries that cannot be obtained from our SaaS. However, the nature of our queries as a whole is fairly uncertain, it all depends on what the customer wants. For instance, one company might demand, "Tell me the top 500 sites for white males 30-50 in the month of April" or "What is the peak time for my site for users who manually access our site by typing in a URL (i.e., is not reached through a clickstream)" from which we would compile a report and sell for a fee. So I believe you are right, I will be working with stage 1 "extract". These kind of queries I can write in SQL in my head and I don't have any problems. I want to have this level of understanding for Hive. It's my understanding that their cluster is using Hadoop and Hive is the "language" they use to query their data for these custom contracts. I would rather not disclose too much more information for fear of giving wrong information or violating my NDA, but I hope this is enough to give a greater idea of what I'm working with. Again, thank you so much!
Thanks, will do. I feel like triggers are more of an ongoing thing than this one-time procedure I'm looking for, am I misinterpreting things?
i'm not sure if i'm explaining the problem correctly so my prof is asking for me to create a view that shows the customer id, the customer name, the customer name, the customer city and the customer country from the customer table then he wants the order id and the shipped date from the orders table, so clearly with the view i must do a inner join. so that how I created my original view. Which is correct, now he wants me to select everything in this view, but he wants me to convert the shipping date from default which is dd/mm/yy to MON/DD/YYYY but as far as I can tell I'm not supposed to include a second shipping date column, is what he asking possible or am I misunderstanding his instructions?
i've just decided to paste the question into the forum, and see what others can come up with, and btw our professor encouraged us to use stack overflow or whatever we wanted to figure out some of these problems so your not cheating by giving advice Create a view called vw_all_orders to list all the orders. Display the order id and shipped date from the orders table, and the customer id, name, city, and country from the customers table. Run the view for orders shipped from January 1, 2002 and December 31, 2002, formatting the shipped date as MON DD YYYY. Order the result set by customer name and country. The view should produce the result set listed below. Here is the script I wrote to create the view: CREATE VIEW vw_all_orders AS SELECT o.order_id AS 'Order ID', c.customer_id AS 'Customer ID', c.name AS 'Customer Name', c.city AS 'City', c.country AS 'Country', o.shipped_date AS 'Shipped Date' FROM customers c INNER JOIN orders o ON c.customer_id=o.customer_id; 
I read this and my first thought is why didn't I insist on an onsite interview. Cubes, old technology and drama could have been seen before I took my current position. Great read. 
Yeah, it's kind of a weird article. I joined a company specifically because they had no processes, procedures, documentation (they did, sort of, kind of, have source control). I've really been enjoying it because the team is all very eager to learn how to do things the right way and improve the products. Just a couple of weeks ago I taught a half day class on the subject of TDD - already had more than one person ask followup questions over time because we're actually using it now. But then, maybe I'm just an exception.
Another benefit to views is they have the unique ability to be indexed like a regular table as well.
This sounds fab. Commenting to find my way back here later when I'm at home.
Thanks! Good to ingrain best practice in any skill.
or maybe something like this: select owner, table_name, ( select count(*) from owner.table_name where serial_num = '12345' ) as "num of rows" from all_tab_columns where column_name = 'serial_num' 
The policy would more than likely just be the DISA STIG for databases. Management is considering doing manual validation instead of an automated scanner. I'm all for this when it comes to source code audits (static and dynamic scan tools just don't do the job well) but it seems like it'd be a waste of time for database checks. 
You can't determine that information for the subnets as you don't know what their mask is. Basically you'd have to find all the networks; use the IP Address Bits AND Machine Subnet Mask Bits to determine all the list of network address segments. Edit - Subsequently you can use the same method to pull at least one host from every network.
Yeah, for a database I would definitely recommend the scanner over manual review, at least to get a baseline. Manual review should focus on business rule stuff. In particular, for high-risk applications like databases that serve Internet-facing web applications, your manual efforts should be focused on confirming that the web app has least privileges in the database. But when you're checking basic permissions on 10,000 database objects, the human reviewer is going to screw up way more than the scanning tool, and take 1,000 times longer.
Easiest way I would see to do (update existing records) is to just join the table to itself and pad the zeros for the update. *** UPDATE yourTable SET yourNewField = REPLICATE('0',8-LEN(CAST(rnid.rnum as VARCHAR(8)))) + CAST(rnid.rnum as VARCHAR(8)) FROM (SELECT tblId, row_number() OVER (ORDER BY tblId) as rnum FROM yourTable) AS rnid INNER JOINyourTable AS orig ON rntbl.tblId = orig.tblId ***
You can create a union-all view that has the owner, table name and the serial_num and that would fit the purpose of your last SQL statement, it seems. 
&gt; your manual efforts should be focused on confirming that the web app has least privileges in the database. Yup, that's about the only case I can come up with where you would need a human over a scanner. But even then, you could probably write a script that parses every SQL statement and stored proc used and compares targeted resources against the permissions for that user. I'm not a DB guy, though, so I have no idea how granular permissions get beyond read/write to tables or schemas.
http://img.pandawhale.com/post-25179-you-can-do-it-gif-Waterboy-Img-JC2V.gif
trying my best!
Do your tables have actual dates, or do they literally have the strings 'June 1-5' and 'June 4'?
Understanding your role has a huge impact on what you should be researching. Over the past 15 years I've been sent on witchhunts to learn new software, only to come to find that its a single product of a certain type. Also, I'm sent on witch hunts and my employer doesn't even know what they expect to get out of the product. The latter is more annoying as a enterprise program is not something you just pick up as a whole. MySQL for example. I'd come to realize later that the windows shop had heard MySQL can do something that MSSQL could have and would have integrated more easily. Plus we had MSDN at the time and could of saved countless hours trying to use MySQL in its infancy. In the end, they still think its some magical product that just "does". 14 years later, with so much more information on the internet, you can really find specific information on subjects about a software and you have to manage your scope of learning. **Manage the scope of what you are learning** 
Post your answers and we'll tell you if you are on the right track.
actual dates 
It be very helpful if you presented the data as it actually appears in the database in a tabular format rather than this sort of plain english rendition of what the data means. Generally speaking, you're going to want join the tables where the date from table 1 is between the two dates in table 2. If you tell me what the tables look like, I can tell you what the query looks like.
If you have old_passwords enabled in my.cnf, then PASSWORD() is still returning an old style hash. SET SESSION old_passwords=0; set password FOR 'exactuser'@'exacthostmask' = PASSWORD('new password here'); flush privileges; Only when ALL old style accounts have been upgraded to new hashes can you remove old_passwords from my.cnf, and then restart mysql Also check you are logging into mysql with the exact same user as your query is using. If they are connecting from different hosts its possible you have seperate b3065237 user records for each hostmask 
select coalesce(whatever you want to see) from table1 join table2 on table1.date=table2.date
something like this: SELECT t1.Name + t2.Name as Result FROM Table1 t1 INNER JOIN Table2 t2 on t1.Date &gt;= t2.StartDate AND t1.Date &lt;= t2.EndDate
Clearly
Thank you! So, unfortunately I'm working w cache and the syntax, apparently, is different. But, I'm trying this from another perspective and researching temp tables. Crossing my fingers - I really appreciate you taking the time. I knew/know there has to be a more efficient way to build this :)
What if you just try the syntax you were using previously? I only added the FOR to demonstrate you can do other users too. You probably need SUPER privs to set other users passwords, and b3065237 doesnt have it
Ok, try logging in with your new style password anyway flush privileges isn't always necessary, it just ensures mysql writes the permission changes to disk immediately 
Yes that might have been a typo.
Even with --skip-secure-auth ? Check the mysql log, if you have successfully upgraded the account to secure passwords, but mysql is trying to authenticate with pre 4.1 ones, mysql will log a warning
You can't access local tables inside an `OPENQUERY`. Link servers are not the ideal solution, but something like this may work: SELECT * FROM OPENQUERY(Temp,'SELECT * FROM CustMore') CustMore WHERE CustMore.CustomerID IN (SELECT CustomerID FROM Customer);
If you have a linked server already added you should be able to query them directly. [You should also be able to right click on the table in object explorer under the linked server to generate scripts to start with.](http://i.imgur.com/BdNPfm3.png) &gt;Scipt Table as &gt; SELECT To &gt; New Query Window SELECT * FROM CUSTOMERTEST4...Custmore INSERT INTO CUSTOMERTEST4...Custmore (CustomerID, CustomerNote) VALUES (2, 'My Note') Joining should work too. --Customers that exist in Access but not in SQL SELECT A.ID, A.CustomerName FROM CUSTOMERTEST4...Customer A LEFT OUTER JOIN LocalDev.dbo.Customer S ON A.ID = S.ID WHERE S.ID IS NULL --Custmore for only customers that exist in the SQL database SELECT A.ID, A.CustomerNote -- or whatever FROM CUSTOMERTEST4...Custmore A INNER JOIN LocalDev.dbo.Customer S ON A.ID = S.ID [CUSTOMERTEST4](http://i.imgur.com/0ZBkutn.png) being my linked server name. LocalDev is a SQL database.
I definitely need to learn PL/SQL. In the meantime, this will work for me: first run this query: select 'select count(*) as "' || a.table_name || '" from ' || a.owner || '.' || a.table_name || ' where ' || a.column_name || ' = ''12345'' ; ' from all_tab_columns a where a.column_name like '%serial_num%' Then I take that result and paste it into my script runner. Gets the job done. 
all sql languages are pretty similar except for some syntax differences..
You might want to put which SQL server the post applies to in the subject so those of us that don't use Microsoft's attempt at an SQL server know not to waste our time or waste bandwidth by clicking on something that doesn't apply to the vast majority of us.
Java isn't one of my options where I'm at right now. It's either C programming(and maybe some .net) or Oracle PL/SQL 
def learn everything you can..
Thank you very much for your support, I really appreciate it! Mind if I shoot you a PM sometime?
Gotcha, thanks.
Yeah sure anytime
... It hadn't even dawned on me what site it was on. Now it makes sense.
You're missing some parenthesis and most likely some more. Can you reformat that and clean it up?
You actually have 2 separate queries here. The first one is Select * From (Select * From Orders_Table ) You can really just write this as Select * From Orders_Table The second query is missing a ")" at the end but I think I know what you mean. It can be written like this. Select date_Field from Inventory Inv, Customer_Table ct where Inv.Key=Ct.Key However you should be using the ANSI-92 standard for joins that look like this. Select date_Field from Inventory Inv join Customer_Table ct on Inv.Key=Ct.Key I suspect there is something you're trying to do with the first query in the second query but without knowing what your data looks like I can't really tell. 
The first paragraph does not make any sense - neither as plain English nor as SQL. So 'this SQL' is an oxymoron in this case. Furthermore it seems silly to join a Customer to an Inventory item, unless you have some kind of pawn-shop/consignment system. What is your schema and what is that you're trying to do? 
PL/SQL is pretty different than t-SQL (for example). I'm excellent at t-SQL, but I can't really hold a candle in PL/SQL. The bottom line is that SQL is arguably the de facto database language, and has been for some time. I'm just going to pull something out of my ass, but it will take a couple decades for PL/SQL to be close to considered legacy. There are definitely some business cases for NoSQL or other sorts of dynamically managed databases, but those are the exception, and an Oracle/MSSQL/etc installation will be sufficient for most databases. The question (to me) is almost like, "is it valuable to learn Excel, because a coworker said Google Docs is catching on?" I would say, though, that you're kind of comparing apples to oranges. C &amp; .net are application languages, and PL/SQL is a database language? Fair warning: I have never taken a comp course. Edit: another thing to consider is the trend in systems moving from batch to realtime. Batch processing is heavy on SQL, whereas realtime requires far more application-level programming (along with a little database). I think that would be the case that database programming may be waning.
Oracle is not on the way out. SQL Server and MySQL are definitely taking market share frome oracle but it isn't going anywhere. The thing about databases is that once they are entrenched it is very hard to get rid of them. I still work with clients who are on DB2. If you want to be a database developer take the Oracle class. If you don't have any interest in databases you should still take the class because it never hurts to learn more and it may come in handy. Don't worry about Oracle, at this stage of your training SQL is SQL. 
I think it's definitely worth your time to take the classes, as PL/SQL is a marketable skill. If it's legacy and kind of niche, that makes it all the more marketable. That said, learn .Net, too. It seems odd that you would have to choose. If you only have so many credits, and it's between the two, take the C or .Net course, as that sounds newer to you. SQL will still be there, but a basic knowledge of a functional or OO language is pretty essential, even for a database admin. Plus, the skills you learn with a C-based language will actually help you write better SQL.
Because I have no control over this database. I can't add tables, edit tables, and the access database is being used in tandem with the sql database. My project is building a stored procedure that updates the SQL database with stuff from access. 
Your colleagues are talking absolute bullshit. Hundreds of thousands of large Oracle based systems use PL/SQL to implement their business logic with new ones being written all the time. Oracle 12c the latest recent release has new PL/SQL enhancements, their technical evangelists still promote and teach it. There is no way Oracle will drop a feature so rich, it would cause a mass exodus. PL/SQL is still a great route to make a development career, you can earn lots of money with skills in it. Tell your friends they are talking absolute shit and they have no idea what they are talking about.
A very simple way to build a table full of dates with days of the week for every date is to open SSAS and create a fake cube.. Make one dimension time and SQL server will generate the table for you. 
Can be done easily with [**window functions**](http://www.postgresql.org/docs/9.1/static/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS), but I'll leave you to put a little effort in...
something like that ? update et set col1 = it.col1 ,col2 = it.col2 ,col3 = it.col3 from [existing_table] et inner join [import_table] it on it.[LINK] = et.[LINK]
While a second tab is necessary for my first Excel solution that comes to mind, it doesn't necessarily need a VLOOKUP. There is an Excel function "WEEKDAY" that returns an integer day of the week. This can be nested in the "TEXT" function that will do the conversion. So if you have your dates stored in column "C", the formula would be =TEXT(WEEKDAY(C2), "dddd") Create a column that calculates this based on your date column, then select the entire thing and create a pivot table to perform all of your aggregations.
You need to be able to create a table that you can load your raw data into, and then develop your queries from there. Also, I hope you have a development environment for you to work on.
* Normally sanitation of the data should be done **before** an INSERT is ever committed(AKA on the application server). Edit - There is just too much potential for change in the rules, the application layer is generally much easier to change than the data layer. Unless you are running proprietary software that can't be changed; in which case you may need to consider why you are using software that requires you to sanitize inputs on the database. * [DB2 Documentation on Triggers](http://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.1.0/com.ibm.db2.udb.admin.doc/doc/r0000931.htm?lang=en) * [DB2 Trigger Examples](http://www.toadworld.com/platforms/ibmdb2/w/wiki/6880.create-trigger-examples.aspx)
Are those joins covered with indexes? That's probably the first step. 
Full plan?
Remove the `ORDER BY` edit: inline functions (dbo.RsNextWeekEndDate) also limit optimization potential
Thanks. What's the definition of dbo.RsNextWeekEndDate? ~~Is there a reason you're grouping by both `YEAR(dbo.RsNextWeekEndDate(i.off_code, i.care_date))` and ` MONTH(dbo.RsNextWeekEndDate(i.off_code, i.care_date))` instead of just `EOMONTH(dbo.RsNextWeekEndDate(i.off_code, i.care_date))` (not that it will probably matter all that much)?~~ Duh, because they're in the select list (reading comprehension, how does that work?). My guess is that you have a poorly chosen clustered index though (assuming it's very important that this query run fast).
`OPTION (HASH GROUP)` at the end may help (still leave off the `ORDER BY`).
Table Scan of `office` at the bottom end of a Nested Loop is bad news (the full table is read 14,000 times). `office` desperately needs a good index on off_code.
Removing the `ORDER BY` opens the possibility for a hash aggregate instead of a sort + stream aggregate.
In general, but his grouping keys aren't sargeable.
It really appears that way from the inside of a MSSQL shop I work with, but my MIL manages an Oracle system and they're going strong. Inside her company, they say the same about MSSQL. So I would say its really just a personal exposure thing. Change is hard and slow, so you'd probably best benefit from being as versatile as you can be. 
One partially open-ended question we usually start with is: &gt;We have 100 CSV files with approximately a million records in each. How would you load these into a database? &gt;Now tell us a way to improve your first answer. &gt;The records are updated randomly every day/hour. How would you handle synchronizing the data between the files and the database? We also ask about how and when they have used the Script Task.
Interesting. You (or at least I) learn new things every day. What key did you group on?
What happens if you try the code below this? SELECT [startdate] ,[today] ,YEAR(fvalue) as [year] ,MONTH(fvalue) as [month] ,MIN(fvalue) as [week] ,[off_name] ,[payclass] ,count(DISTINCT i.client_no) AS [Active Patients] FROM ( SELECT DATEADD(year, - 1, GETDATE()) AS startdate, GETDATE() AS today, o.off_name, ISNULL(g.descr, 'Self-Pay') AS payclass, i.client_no, g.descr FROM visit i LEFT JOIN office o ON (o.off_code = i.off_code) LEFT JOIN invoice inv ON (inv.off_code = i.off_code AND inv.client_no = i.client_no) LEFT JOIN payer p ON p.off_code = i.off_code AND p.payer_no = inv.payer_no LEFT JOIN gencode g ON g.field_id = 'PAYER_CLASS' AND g.value = p.payclass WHERE i.off_code IN ('0001') AND (visit_stat = 'C' OR visit_stat = 'T' OR visit_stat = 'V') AND i.care_date between DATEADD(year, - 1, GETDATE()) and GETDATE() ) a GROUP BY [startdate], [today], YEAR(fvalue), MONTH(fvalue), [off_name], [descr], [payclass] ORDER BY [year], [month], [descr] Basically what I did was do a sub select inline query and you'll pull the data and do your user functions (dbo.rsnextweekenddate) just once. After it builds that dataset, it no longer has to perform them again in your group by or orderby. I have no idea whats in dbo.RsNextWeekEndDate but I would try avoid running user functions more than the necessary amount of times. Keep in mind that sort will usually be a large part of execution plans, especially when your data is not physical sorted in that manner. Sort is very IO dependant. 
I have a numbers table and grouped on a made up complex function: CREATE FUNCTION dbo.Test (@ID int) RETURNS int AS BEGIN RETURN (SELECT Mod10 FROM dbo.Number WHERE ID = FLOOR(SQRT(@ID))); END; GO SELECT dbo.Test(n.ID), COUNT_BIG(*) FROM dbo.Number n WHERE n.ID BETWEEN 1 AND 100 GROUP BY dbo.Test(n.ID); GO SELECT dbo.Test(n.ID), COUNT_BIG(*) FROM dbo.Number n WHERE n.ID BETWEEN 1 AND 10000 GROUP BY dbo.Test(n.ID); edit: The break even point between sort + stream aggregate and hash aggregate is approx 1000 records in this scenario. 
Maybe pre-compute all RsNextWeekEndDate values in a CTE instead of using that directly in the query? Something like this: (edited: actually probably the # of patient visits is significantly greater than the # of combinations, so apply distinct early) with RsNextWeekEndDate as ( select i.off_code, i.care_date, min(YEAR(dbo.RsNextWeekEndDate(i.off_code, i.care_date))) AS year, min( MONTH(dbo.RsNextWeekEndDate(i.off_code, i.care_date))) AS month, min(dbo.RsNextWeekEndDate(i.off_code, i.care_date)) AS week FROM ( select distinct off_code, care_date from visit WHERE off_code IN ('0001') AND (visit_stat = 'C' OR visit_stat = 'T' OR visit_stat = 'V') AND care_date between DATEADD(year, - 1, GETDATE()) and GETDATE() )i group by i.off_code, i.care_date ) select DATEADD(year, - 1, GETDATE()) AS startdate, GETDATE() AS today, RsNextWeekEndDate.year, RsNextWeekEndDate.month, RsNextWeekEndDate.week, o.off_name, ISNULL(g.descr, 'Self-Pay') AS payclass, count(DISTINCT i.client_no) AS [Active Patients] from visits i join RsNextWeekEndDate on RsNextWeekEndDate.off_code = i.off_code and RsNextWeekEndDate.care_date = i.care_date ....
I dislike triggers on principle, mainly as a result of them being unpredictable and their tendency for cascading problems when they are used by people to lazy to create proper solutions; essentially have to be flawless out of the box for something that will *never* change on that system. The only place I believe I have legitimately used them is for determining last_modified columns for nightly jobs/etc with products we don't have control over. (It is also a custom table for this purpose, made by me)
You're asking a database oriented group so the answer you're going to get is "use normalized tables". CREATE TABLE Team ( TeamID int NOT NULL PRIMARY KEY, TeamName varchar(50) NOT NULL ); CREATE TABLE User ( UserID int NOT NULL PRIMARY KEY, TeamID int NOT NULL FOREIGN KEY, UserName varchar(50) NOT NULL ); The requirement that one team cannot see other teams' data is an application problem, not a database problem. ^(P.S. Forgive my syntax. I know it's not exact.)
Don't use triggers, ignore their existence, carry on.
In addition to what Thriven has done, you can also look at trying to replace whatever RsNextWeekEndDate is doing with some inline code within the query itself. 
I have an automated system to test the performance of my application. I want this dashboard to display the data collected which is stored in SQL server. 
Hi Guys, I'm still figuring out how i should go about this. If anyone has any more suggestions, please feel free. I tried klipfolio, but there's no way to add commentary... 
If you enable schemabinding on your view. It's also recommended that the underlying tables do not have large load/truncates.
Very good example. I like that question because you can see how the candidate thinks.
Fix that process. Throw rocks.
What you are trying to do is what is called dynamic sql. It's generally considered not a best practice but in your case it would work like this: DECLARE @select varchar(max), @table_name varchar(50); SELECT @table_name = table_name FROM languages WHERE name = 'French'; SELECT @select = 'SELECT letter, value FROM ' + @table_name + ';'; EXEC (@select); Dynamic sql usually arises from poor table design. In this case a normalized structure with a single `LanguageLetters` table would have avoided this problem.
I think you need to sit down and take ten minutes to make your post more readable. A little grammar formatting goes a long way. I assume you meant that you have a MS Access and a T-SQL script to create a SQL Database. You will need a SQL database server installed to do the compare if you plan to do the compare there. Start with that.
Imagine you had a library of books. They are all stored in alphabetical order of title. Someone asks you to find all the books with titles starting with 'T'. That's easy. However then they ask you to find all the books with author's surname starting with 'T'. Now you have to go through every book from first to last to look at the author's last name. You make the list, but only one copy, and give it to them. The next day someone asks you the same thing but you already gave away your list so you have to do it again, from first book to last. Making an index would be making and keeping a list of books by author surname and ensuring it remains updated whenever someone adds a new book to the library or discards one. It means you can always go directly to the right books whenever asked in future for books by author surname even though they are physically sorted by title.
Possibly: SELECT a.ID, MAX(c1Date)AS MaxC1Date, MAX(c3Date)AS MaxC3Date FROM table1 a WHERE c2Date IS NOT NULL GROUP BY a.ID Could you provide some sample input values for a test case? That along with the expected result would help. I'm not sure if I understand what you are asking for. 
This is probably better (only reads the table once): SELECT a.ID , a.c1Date , a.c3Date FROM (SELECT a.ID , a.c1Date , a.c3Date , ROW_NUMBER() OVER (PARTITION BY a.ID ORDER BY a.c1Date DESC, a.c3Date DESC) AS RowNum WHERE a.c2Date IS NOT NULL FROM table1 a) a WHERE a.RowNum = 1; 
Thanks for your reply! For example, let's say his ID for c1 is 0001. The top entry might be C2=1/1/2014 (the start date of a product), C3 might be null (the end date of the product) and C4 might be 1/1/2014 (start date of the product's rate). The second entry - the product the customer had before renewing to the top entry that has a null end date - might be 0001, 1/1/2013, 1/1/2014, 1/1/2013. I need his old product, the entry that actually has an end date. My problem is that I can't just take the 2nd highest entry as some of the customer ID's don't have the current product listed in the table (ugh) so the entry I want *might* be the top entry, but it might not. Again, thanks
It appears to be working, thank you for your help.
Thank you! Out of curiosity, why is not considered a best practice?
I'm doing it with a small app I wrote, not SSIS. I also try it with DTS just to test and it does the same thing. If everything fails I guess I'll try it with SSIS. 
show your vb.net code
To name a few: * It can be vulnerable to sql injection attacks. * It tends to be difficult to maintain. * It cannot be used in many situations, such as views. * It can usually be avoided by proper table design. * It can only be compiled at run time, and thus doesn't take full advantage of cached execution plans. * It is more prone to errors, even without any code or schema changes.
Posted as an edit.
Excel has a tendency to make datatype assumptions based on the first row of data. Then if it encounters a different datatype in a subsequent record, it returns null. For example if you have this data: |PhoneNumber| |:--| |1234567890| |(123)555-6547| |132-4567| Excel will assign the datatype of integer based on the first row and null out any string values it encounters, losing the 2nd and 3rd phone numbers. I'm not sure if this is what's happening in your case, but it's one of several notorious issues Excel has with importing data.
It's difficult to maintain, and as jc4hokies mentioned it generally arises from poor design in the first place. Here's what they suggested in SQL (this is for SQL Server, since I don't know what specific RDBMS you're using): CREATE TABLE Languages ( ID INT IDENTITY(1, 1) NOT NULL PRIMARY KEY , Name VARCHAR(250) ); CREATE TABLE Letters ( ID INT IDENTITY(1, 1) NOT NULL PRIMARY KEY , LanguageID INT FOREIGN KEY REFERENCES Languages ( ID ) , Letter CHAR(1) , Value INT ); GO And an example of usage: INSERT INTO Languages ( Name ) VALUES ( 'English' ), ( 'French' ), ( 'German' ); GO INSERT INTO Letters ( LanguageID, Letter, Value ) VALUES ( 1, 'A', 1 ), -- English, A, 1 ( 2, 'A', 1 ), -- French, A, 1 ( 3, 'A', 1 ); -- German, A, 1 GO SELECT le.Letter , le.Value , la.Name AS LanguageName FROM Letters le INNER JOIN Languages la ON le.LanguageID = la.ID; GO And if you still really wanted to have a French_Letters table, you could create a view: CREATE VIEW French_Letters AS SELECT le.Letter , le.Value FROM Letters le INNER JOIN Languages la ON le.LanguageID = la.ID WHERE la.Name = 'French'; GO SELECT Letter, value FROM French_Letters
What's bizarre is that it does it on every subsequent column after that point, the columns afterwards are all formatted very consistently. I'll keep messing with it thank you.
&gt;Well one of them that told me that is a developer, mainly with C#, but he does use SQL everyday. I don't think he has ever been in a business that used Oracle PL/SQL, /shrug. Then how would he know anything about the future of Oracle PL/SQL if he's never used it? C# w/ SQL is pretty much the exclusive domain of Microsoft SQL Server, a direct competitor to Oracle. I won't get into the differences, but given a choice I always develop in PL/SQL.
SSIS is DTS (since 2005), which is why I asked what version. Could be that the older DTS engine has a bug (or they'd tell you it works that way by design most likely).
Tried that no luck :(
I'm using the tool where you right-click on tables-&gt;Tasks-&gt;Import Data. I call that DTS I may be wrong there, I thought SSIS was what you do in the Business Intelligence Design Studio... Anyhow I'm using the one that comes with 2008 R2. 
One the sql server can you select * FROM [GPS Location$] Do you have data in the columns? I'm curious if the issue is in the read portion of your vb.net application or in the loading of data.
Yes I have data there it looks the same as the 2nd image (the import preview). All the data is great except from the location onward it's all empty though (not null).
Check Trim(drSheet(6).ToString) It should be Trim(drSheet(5).ToString()) The way I found it was by reformatting the code to be a bit more readable. sqlcmd.CommandText = "INSERT INTO dbo.GPSImport (Vehicle, Name, DateTime, Speed, Heading, Ignition, GPSQuality, Odometer, Location, ECMOdometer, ECMFuel, ECMSpeed, ECMIdleTime ) &amp; _ "VALUES" &amp; _ "(" &amp; Convert.ToInt16(Trim(drSheet(0))) &amp; ",'" &amp; _ Trim(drSheet(1).ToString()) &amp; "','" &amp; _ Trim(drSheet(2).ToString()) &amp; "','" &amp; _ Trim(drSheet(3).ToString()) &amp; "','" &amp; _ Trim(drSheet(4).ToString()) &amp; "','" &amp; _ Trim(drSheet(5).ToString()) &amp; "','" &amp; _ Trim(drSheet(6).ToString) &amp; "','" &amp; _ Trim(drSheet(7).ToString()) &amp; "','" &amp; _ Trim(drSheet(11).ToString()) &amp; "','" &amp; _ Trim(drSheet(12).ToString()) &amp; "','" &amp; _ Trim(drSheet(13).ToString()) &amp; "','" &amp; _ Trim(drSheet(14).ToString()) &amp; "','" &amp; _ Trim(drSheet(15).ToString()) &amp; "')" 'MsgBox(sqlcmd.CommandText) sqlcmd.ExecuteNonQuery() 
I didn't catch that thanks. I changed it but still goes limp at location. 
So somehow these pictures of myself are supposed to be as embarrassing as this guy who has a very loose grasp of the English language?
Sah-dah-tay: what were you seeking to achieve by posting that?
Try this. select b.name from table2 b where 1=1 and exists (select 1 from table1 a where a.id = b.id and a.status = 'U' and (b.year - 1) = a.year);
to be fair to all readers of this subreddit, your title should really say "MS SQL" instead of "SQL"
&gt; The newly added window functions in SQL-2003 standard provides some computation capabilities relating to sequence, which makes it possible to solve some problems in a relatively simple method and alleviate the problem of SQL to a certain extent. But the use of window functions is often accompanied by sub-query, and it cannot enable user to directly use the sequence number to access set member, so there are still many ordered computations that are difficult to solve. i call bullshit on this claim
I don't want to be a Debbie Downer, but the first thing I read after creating an account (which didn't force me to link with FB or supply an email, so that's a plus) was "Did you ever heard about analyst without Excel or another Spreadsheet editor knowledge?" Since the website is about teaching me a language, it doesn't build any confidence when I see so many syntax errors in just the very first sentence. Next I decided to take a lesson... if I didn't know SQL already, I don't think I would have been able to learn anything. There's no indication that the phrases you are doing are SQL, and nothing showing you how your phrases interact with the database. Then when you finish the first lesson it shoots you straight into the second one. To get back to the lessons screen you have to click a whole bunch of times, and then when you finally find the page there's no indication that you finished a lesson. I'm still relatively new at SQL so I thought maybe I could use this site to brush up on the basics and learn more advanced techniques, but I don't think I will be going back.
It's obvious English isn't your first language. If you are going to go with a website in English, I'd bring in a native speaker to help clean up all the text. It is difficult to read and distracts from the goal. In the first SQL lesson I played with (I did SQL Lesson 2) the overall look and feel was nice and simple , but the way it's coded allows me to more or less guess randomly. I don't have to select the pieces of the query in the proper order which may not help someone truly understand how a query goes together. "Check My Work" is also misleading. I can't select a wrong choice, as it turns red and does not go into the query. So unless I don't fill out the entire query, my work is always going to be right. When I don't fill the query, the button does nothing. No error message, no nothing. There's also no message to say it's correct, it just loads the next question. I would also be pretty interested in your table structure to see why you would ever be selecting a column called "Cats" from a table called "Pets" where color is either black or orange. Obviously it's a new effort, so don't get discouraged. You have start. Just really focus in trying to eliminate distractions and teach not only the syntax but proper database fundamentals
You should be able to answer your question by writing a query against the DB's data dictionary, but each DB product has a different data dictionary schema. WE need to know whether you're working with MS SQL, Oracle, or for instance, MySQL.
By the 'latest' do you mean in the prior year? If so, just a simple join will do: select from table2 join table1 on table1.id = table2.id and table1.year = table2.year - 1 and table1.status = 'U' where table2.attendance = 'Y'
Lots of examples of joins would be nice. I got left inner joins down pretty well, but i can't find a reason to use any other join. I understand what the other joins do, but i am missing the 'why?'
You didn't specify a platform, which would help a bit. That being said, it sounds like you are trying to replicate something that already exists. MS SQL Server, for example, has much of what you're looking for built in. you would use functions like "suser_sname()" to determine the current user, and filter their results appropriately. Actual object-user relationships are handled as with the grant statement.
Not just the previous year because there can be gaps between status and attendance so somebody could have a U in 2009 but no attendance until 2012 for example.
On my phone and in the hospital so forgive me if it is not quite what you need. Assuming your fk is squared away in the table, use the Merge keyword to populate the fk and other data from the servers table. Be sure to LTRIM AND RTRIM your values. http://msdn.microsoft.com/en-us/library/bb510625.aspx
SELECT * FROM survey_questions as sq LEFT JOIN survey_options as so ON so.survey_question_id=sq.id That should be all the questions with all possible answers. Then you could left join to this set as a subquery, on the question id and option id to the response data. If you want rows as columns, there are a few ways. The tablefunc module has some crosstab functions; or you can write a crosstab manually, with many columns like 'CASE WHEN question_id=1 THEN max(question_id_value) END' and group by subject, response_id; or you can write some procedure to generate these CASE statements; or possibly use the pivot features in Excel or Access. 
 select b.name from table2 b where 1=1 and exists (select 1 from table1 a where a.id = b.id and a.status = 'U' and b.year &gt; a.year);
Got it! Thanks for letting me know. I'll make sure to have many examples and reasons why you'll want to do outer joins.
&gt;When I say that I don't know anything about SQL, I really mean it--I'm not even entirely sure what it's used for exactly... I stopped reading after '--'... Seriously though, here's a handy introduction to the basics : http://www.w3schools.com/sql/ 
&gt; I'm not even entirely sure what it's used for exactly. SQL (Structured Query Language) is a programming language used to work with Relational Database Management Systems (RDBMS). SQL's syntax can be divided into several categories: * Data Definition Language (DDL): The commands used to create and modify the structure of a database. (CREATE, ALTER, DROP, etc.) * Data Manipulation Language (DML): The commands used to get data in and out of tables in the database. (SELECT, INSERT, UPDATE, DELETE, etc.) * Data Control Language: The commands used to control who can access data. (GRANT, REVOKE, etc.) * Transaction Control Language (TCL): The commands used to work with transactions. (COMMIT, ROLLBACK, etc.) You will probably be focused mainly on the DDL and DML portions of the language. DCL is usually only a focus for Database Administrators. TCL won't really be a concern until you're functional with the other areas. Many commercial RDBMS products use extended dialects of SQL. These are usually a superset of the language with the addition of procedural constructs (variables, loops, conditional statements, etc.). For example, Oracle uses PL/SQL, and Microsoft SQL Server uses T-SQL. As for finding good tutorials, check out [the wiki page for this subreddit](http://www.reddit.com/r/SQL/wiki/index). I've looked through much of [SQL Zoo](http://sqlzoo.net/) site and it seems to be pretty good. I also like the [SQL tutorials on BlackWasp](http://www.blackwasp.co.uk/SQLProgrammingFundamentals.aspx), but I should point out that they are specific to Microsoft SQL Server. Keep in mind that learning SQL isn't just about learning the language; you also need to understand the Relational Model. The short version is that databases are comprised of tables, and that these tables have relationships with one another (declared via Foreign Key constraints). These relationships determine how tables can be JOINed together in SELECT statements. Fortunately, most SQL tutorials cover this as well. &gt; How hard is it and how long would it take to learn SQL at a level high enough to pass the MTA exam? I don't know anything about that exam; hopefully others can shed some light on that. However, a few weeks should be enough time to become functional with the language.
Thanks for the link.
Thanks for the information. I'm definitely going to begin browsing the links you provided. Some of this sounds similar to what I've done using SAS, but I guess I'll find out more once I start reading.
something like this then, maybe: select ... from table2 join ( select t2t2.id, t2t2.year, max( t1t1).year latest_year from table2 t2t2 join table1 t1t1 on t1t1.id = t2t2.id and t1t1.year &lt; t2t2.year )t2t1 on t2t1.id = table2.id and t2t1.year = table2.year join table1 on table1.id = table2.id and table1.year = t2t1.latest_year where table1.attendance = 'Y' and table2.status = 'U' You might be able to rewrite this better with analytic functions.
Just FYI... [/r/sqlserver](http://www.reddit.com/r/sqlserver) exists, and is dedicated to MS SQL Server. 
Learn SQL the Hardway is another good free resource to learn SQL http://sql.learncodethehardway.org/ 
An SSIS package would be best form. You can put it on an automated SQL Server Agent job to auto run on a schedule. You can also set all the connection strings on the job or use a package configuration to change them.
How would I go about using an SSIS package? Would it have to get run on each server? I don't think he wants it to run automatically, he just wants to run the tool on whatever machine needs it whenever that machine needs it. Is SSIS easily deployable onto multiple machines easily?
It is usually best practice to run it on the SQL Server, but you should be able to [run a *.dtsx package anywhere](http://stackoverflow.com/questions/7431124/how-to-run-ssis-package-without-sql-server). It sounds like you have a few Access databases spread across a few machines and he wants a centralizing process. You could build the package with this in mind and deploy the SSIS package to each machine and all they have to do is double click an icon on their desktop to run the sync package. It's cumbersome and bloated, but it should work.
With a one-one relationship (which this is), you insert into the primary table `A.Servers` then insert into the auxiliary table `A.Network`. It doesn't make sense to update the `ID` of `A.Network`. Proper normalization would have the FK the other way around. `A.Servers.NetworkID` would reference `A.Network.ID`, but there may be other reasons to have the design described.
Pluralisight and Lynda.com have the best tutorials on the Internet. Check them out. 
Hmm.. Can't seem to find a way to do this wit SQL 2012. Is it not available in SQL 2012? Also, I am not sure if they are uniform among all systems, whether they have 2008 or 2012. I'll have to ask him the specifics and see if I can figure out the best way to go about this. Probably after lunch. He usually gives me minimal specifics and just tells me exactly how he wants it done. 
Thanks, all of these links will help.
The MTA is Microssoft's entry level certification exam involving SQL, I believe. Given all of this information, I may try to just learn it myself and take the exam later down the road once I feel confident enough.
Thanks.
I actually do a lot of work for Microsoft, and in my experience they haven't cared about certifications. Definitely can't hurt you to have it, but probably not necessary.
Im in the BI world and lots of times you will have disparate data sets where you have partial data your are joining to partial data. The classic example is a sales table joining an inventory table. I dont always have sales where I have inventory and vice versa, so I need to do an outer join (really a full outer join). The real fun comes when you have to join the results of that to another table and are coalescing key fields.
Certificates are mostly useless. At best they might keep you out of the reject pile. Most positions with an actual need for SQL will have some sort of technical interview process. I interviewed with Amazon one time and was asked how I would query a sales table to get the top 10 items for a period (hint: you need a subquery). In my experience that kind of thing is hard to do without practice. Look for ways you can practice the mental task of how to build a query.
You should just be able to go to w3school.org to learn plenty. SQL is easy to learn. My last position required me to learn on the job, and it never became an issue. I recommend just diving in, set up a test environment and get your feet wet.
I'll definitely check out the site. 
At this point, staying out of the initial reject pile would be better than nothing. Hopefully it will give me a little more attention when applying to jobs not entirely focused on the use of this program.
Consider it a blessing. Moving from Access to SQL is a great career move. Having a good knowledge of Access queries will help you when learning SQL for SQLserver. Most people nowadays supporting Access applications are still using a SQL backend to hold the data and Access for the front end. Be honest and say ,"I'm pretty well versed in SQL for Access. I know the syntax is a little different for SQLserver but I'm really happy to be moving towards enterprise applications." If you get the job. Embrace SQLserver and SSRS. Don't fight it and try to use access. I've seen too many analysts forgo learning the most simplest of SQL query language and drop the pulled data into Excel and Access just because they are comfortable with it. 
I feel like I see too many posts here from people that are saying: I'm 100% NOT qualifies, but I have an interview. Help! But this does not seem to be the case here. You already met them, they've presumably seen your current resume, and they loved you enough to call you back in. Personality and getting along well with coworkers is hugely important. Continue to wow them but be clear about your reservations, but not to the degree that you're self deprecating. You have the upper hand because it sounds like you don't NEED the job. Consider it a fact-finding, info gathering discussion and make sure you're a good fit. If not, OK. If they are willing to work with you, excellent.
I think this should help prepare for the official test should I give it a try, thanks.
Tech positions are hard to fill these days. They see promise in you, don't worry about if you are or are not qualified. Just brush up on the product you need to work with for the interview and do your best. If they give you a hands on test and you need to use google during the interview they will probably be OK with it. That's what everyone else does too. Being able to find the solution to your problem via the internet is really a job skill these days.
Please cover Hash, Merge, and Nested Loops. Way too many people are not aware that these terms exists, much less what they mean. If you feel ambitious, cover semi-joins and anti-semi-joins. If you feel super ambitious, there is not enough `RIGHT OUTER JOIN` (and [RIGHT] `INNER JOIN`) in the world. More often than not, database engines are switching the join order from how the query is written. But when the DB engine doesn't do the switch, writing a query the `RIGHT` way can save your server a bunch of resources. The `RIGHT` way: SELECT stuff FROM tabled d RIGHT OUTER JOIN (tablec c RIGHT OUTER JOIN (tableb b RIGHT OUTER JOIN tablea a ON a.BravoID = b.BravoID) ON a.CharlieID = c.CharlieID) ON a.DeltaID = d.DeltaID Writing `INNER JOIN`s this way can be equally important.
just go for it dude. try your hardest!
Unrelated - but really a fortune 100 company uses access? Maybe I'm just sheltered but this seems wrong on so many levels.
It's 20-25 commands you need to worry about. Take a weekend class.
conditional joins!! argg. also REAL WORLD examples of all types of joins..yes..you can include the venn diagram that every blog/website has but how about using some real world examples like say..inventory from a warehouse and some sales data from the marketing side?? lets get THRIFFy
Word!
The first &amp; last name. The data isnt neccesarily duplicated out of error, its just that everytime the person calls in it gets logged so it is unique...unfortunately this ist the only way to join these tables togehter
yeah, but since the duplicates are in the data, you actually have to do something about it in the query in order to return only some of the rows instead of all the rows that actually do match
the duplicate first and last name from table A...
Select distinct * will get you unique results but isn't the most efficient way of doing things. If you still have dups after that, then you're getting multiple matches by first and last name. Especially if you have two people named Bob Ross - you will get duplicate rows because they're two different rows. One way to solve that is use a middle name, but still not 100% dup proof. Is there a primary key in each table that you could join on? Could you list the column names? 
yea i tried it..doesnt work at all. there are too many column names as its BI related data. There is no primary key...there are too many duplicates by just joining on names so i conjured up a way to join on the telephone numbers and there are still duplicates. 
Exactly this. If a company is that big, someone, somewhere likely uses it for something. 
You need to decide what records you want to keep from table A. If there is a timestamp, for example, you can choose the earliest or latest records. This will return the latest record for each first/last/phone tuple: SELECT * FROM tableA a JOIN tableB AS b ON a.first = b.first AND a.last = b.last AND a.phone = b.phone WHERE (first, last, phone, timefield) IN (SELECT first, last, phone, max(timefield) FROM tableA GROUP BY first, last, phone); EDIT: forgot some stuff
There are untold thousands of Access databases being used to support Fortune 10 companies. And even more super-advanced Excel spreadsheets. Connected to real-world data in real time. Spreadsheets that move trading markets world-wide. The world's economy runs on the PC desktops. Not on servers.
that's the thing..i need all of the columns
the dates wont match up exactly though..can i still use this? 
Where I work we manage over 200$ Billion (with a B) and our Asset Management database is MS Access. We have more databases (MSSQL, Oracle, MySQL) than we do employees in our company and yet someone felt the urge to develop this application with an MS Access db... Seriously WTF.
I will concede excel. Excel is for frontline staff doing analytics on raw data that is extremely real-time like feeds from bloomberg/etc. that they access through plugins. Access on the other hand... shudder.
And the best thing is you just need 1 SQL course and it will cover 90% of the context for all environments. You learn how to SELECT, UPDATE, CREATE, INSERT, etc. in SQL Server you're going to know how to do it in Oracle, MySQL, Postgres, etc.
IT usually goes something like this, savy user has read-access to some database to pull data. Their excel spreadsheet starts growing and passing it around to everyone gets things out of sync, so they come up with Access as the answer. Next thing you know you have a department of 80 folks working off this non-scalable Access database, which is actually just a proxy for the real database. In my last DBA position I was at a Fortune 700 or so finance company, and the big job at the time reeling in all the rogue IT departments around the company, including these Access gurus. 
You may want to think about some refactoring, as that seems like a fairly poor design. Otherwise, CREATE TABLE tableA ( First VARCHAR(50), Last VARCHAR(50), Phone VARCHAR(50) ); CREATE TABLE tableB ( First VARCHAR(50), Last VARCHAR(50), Phone VARCHAR(50), Timestamp DATE ); INSERT INTO tableA ( First, Last, Phone ) VALUES ( 'Buzz', 'Lightyear', '555-5555' ), ( 'Woody', '???', '555-5555' ); INSERT INTO tableB ( First, Last, Phone, Timestamp ) VALUES ( 'Buzz', 'Lightyear', '555-5555', '2010-01-01' ), ( 'Buzz', 'Lightyear', '555-5555', '2010-02-01' ), ( 'Buzz', 'Lightyear', '555-5555', '2010-03-01' ), ( 'Woody', '???', '555-5555', '2010-01-01' ); SELECT a.*, b.* FROM tableA a INNER JOIN ( SELECT First, Last, Phone, COUNT(*) AS Calls FROM tableB GROUP BY First, Last, Phone ) b ON a.First = b.First AND a.Last = b.Last AND a.Phone = b.Phone And, [fiddle](http://sqlfiddle.com/#!15/4c2d9/2/2).
I concur. I've faked it for four months now, and i was just promoted to developer. I indeed, am making it! Good luck! 
Learn sql the hard way is looking like a great read thanks for pointing out the wiki
so how do you plan to distinguish between rows that have the same fname, lname, phone? like i said, you actually have to do something in the query to pick which rows to ignore so that you don't get duplicates either that, or live with the duplicates
nice, way to go!
You'd have to re-query the table since you want the row with the max time, but you don't want the actual max time returned, you want the value field on that row. So, something like this: select sensor_id, event_type, value from ( select sensor_id, event_type, (select value from events where sensor_id = e1.sensor_id and event_type = e1.event_type order by time desc limit 1 ) as value from events e1 ) e2 group by e2.sensor_id, e2.event_type, e2.value There's probably a better way to do it, as I'm not very familiar with MySQL, but you'll get the correct result.
thanks, it seems to work good...not sure what e1 &amp; e2 are though.
Honestly this is bad form to begin with; in my experience there is always a much better solution. It makes complicated queries hard to read and very difficult to modify in the future.
I'll play. I have 3 solutions written (for TSQL). Here is the join solution. SELECT e.sensor_id, e.event_type, e.value FROM dbo.events e INNER JOIN (SELECT e2.sensor_id, e2.event_type, MAX(e2.time) AS time FROM dbo.events e2 GROUP BY e2.sensor_id, e2.event_type) e2 ON e.sensor_id = e2.sensor_id AND e.event_type = e2.event_type AND e.time = e2.time; 
Aliases
worth repeating for those unfamiliar order of execution: - FROM - WHERE - GROUP BY - HAVING - SELECT - UNION - ORDER BY 
be careful using same aliases both inside and outside subqueries, e.g. e2
"*Order by*" happens on the result set (the last thing to happen), it allows ordinal referencing because columns in the result set can be calculated or aggregated, so ordinal ordering prevents the re-execution of the result set. It makes no sense whatsoever to allow the *where* clause predicates to be ordinal based.
Yes. Whenever "LOL SQL SUX THIS SHOULD BE DONE" comes up, it's almost always because the user does not know what they want.
tablefunc looks really good to know. Thanks for your help!
No worries. I'm careful.
The best way is to have a new table that stores the relationship between the attribute and the entry. Example: *** TABLE item(idItem, etc, etc, etc) TABLE tags(idTag, tagDescription) TABLE itemTags(idItem, idTag) *** Data would look something like this: *** item(1, 'Shoes', etc) item(2, 'Sandles', etc) item(3, 'Boots', etc) tags(1, 'Waterproof') tags(2, 'Leather') tags(3, 'Plastic') tags(4, 'Running Apparel') itemTags(1, 2) itemTags(1, 4) itemTags(3, 1) *** The above would be Shoes with the tags 'Leather' and 'Running Apparel' and Boots that are 'Waterproof'. You'd get this information by a query like: *** SELECT i.Description, i.etc, i.etc2 FROM items AS i INNER JOIN itemTags AS g ON i.idItem = g.idItem INNER JOIN tags AS t ON g.idTag = t.idTag WHERE t.tagDescription in ('Leather', 'Waterproof') *** 
As a side note this will be very fast it will only have to find the idTags to filter out which items meet that requirement. 
Thank you.
That looks like exactly what I need. Thanks!
If you already have flat files, but need to import them [this](http://www.convertcsv.com/csv-to-sql.htm) has worked pretty well for me.
[SQL Workbench\J](www.sql-workbench.net/) also has commandline capability for importing files.
Great website. Might work well when you have a one-off file needing to be imported!
I worked for a hospital and doctors who think they can program decide that they need an MSAccess database, which then becomes mission critical. That's how even fortune 10 companies get loaded with MSAccess databases galore. People who think they are smart, in positions of power, create these 'database applications' and now IT is stuck supporting them.
Yes, if you work in IT, I think it's your duty to make sure as little people as possible have MSAccess installed.
Outside the scope of what I'm trying to do, but I'll look into it anyway. Thanks. SQL is a great tool and I feel like I could solve a lot of problems with it. I'm just having the darndest time actually learning it.
Sure, just that's usually what's next...
We tried, but HR says we can't have a "hobbits only" policy on Access deployments.
Netezza (at least the Aginity client) allows column references in the group by, and maybe in the where. So, it's possible, but custom. As others said, it's due to the order SQL parses the various clauses.
I'm sorry to say, but I consider referencing the column by column ID of the resultset in the order by clause to be one of the 7 deadly sins when it comes to sql development. It's so easy to miss that adding a column to the select will completly screw the ordering of the resultset. If you ask why it is not possible to do that on the FILTERING of the result set, the only thing I can say is : I thank the god almighty, that i dont even believe existing, that it is not possible.
I can't believe the people suggesting you fake it....Idiots. I had an interview once where they had me write a SQL statement by hand. Can't fake that. My suggestion is to go to your favorite book store and buy a book on SQL. Then download SQL Express...install &amp; practice. There is nothing wrong with not knowing the exact syntax of substring, but you should learn the concept behind such things. Good luck!
&gt;Most x-Access experts I know hate Access with a passion normally reserved for Hitler.... I have the same feelings for Access, but was never an expert. Similar to other people, I work for a very large financial company. Every one has Access, and is used heavily by accounting teams. This is acceptable, and when they get tired of Access's limitations, we introduce them to a grown up RDBMS and help them migrate their data.
Why not make the FROM clause a derived table, then you can use the custom name outside of the clause in a where statement? SELECT Custom FROM (SELECT Custom = Field FROM Table1) T1 WHERE Custom = X
In Oracle use external tables. You define the format of the CSV and it manifests it as a table that you can write SQL against and it's fast. It also handles incorrect data, bad rows etc.
It might be a little overkill with features, but I've been using pentaho's etl tool, spoon, for this exact purpose and love it. It will integrate with all major databases and can generate/execute table creation and insert scripts for you.
Never used external tables before but this might actually be something I can use straight away for a project I'm working on. I have not really used SQL Loader a lot before this and the existing code base only uses some simple functionality. 
I would love to get my hands on something like Pentaho. I looked at it a long time ago but dismissed it since it seemed to have a steep learning curve (at least if you are not looking for full-fledged ETL). How's pricing? I've read it's open source but can't seem to find a free download? 
OK, I just read an introduction to it in some Oracle documentation. They presented it as a compliment to SQL Loader, or, as an outright replacement. I'll have to read a bit more but it does seem similar and/or solve similar issues. 
so how do you propose to decide which row from the duplicates to include in the results and discard the others? this has to be based on the content of some of those other columns, right? and if you say "just pick whichever one comes first" then you have to do this with window functions
I've never seen this. Are you sure you're not experiencing the [cache](http://en.wikipedia.org/wiki/Cache_%28computing%29) speed up from running the query a second time?
By far the best answer. 
can we have a look at both queries, please
Just for informational purposes the four steps [SQL Server uses when passed a query](http://thomaslarock.com/2014/07/sql-2014-cardinality-estimator-care/) are: * **Parsing** – This is a syntax check, looking for things like reserved keywords. * **Binding** – Also known as “normalization” in SQL 2000, this is now called the algebrizer. It does name resolution and handles aggregates and grouping to form a “query tree”. * **Optimization** – This step takes the query tree and sets about to find a “good enough” plan (i.e., one with the lowest cost). Optimization is cost based and this is the place where statistics and indexes matter most. * **Execution** – Once the plan is found, this is physical retrieval of data from disk and memory. 
Have you confirmed that the execution plans are exactly the same?
What do you plan to do with the data after you do your comparison?? Triggers can add a lot of overhead to any dml statement they act on. I'd avoid them in the vast majority of situations.
It will be strickly used just to see the data before and after the changes. And to add, this will not be implemented in any business, it is simply for personal use. I'm thinking I may be able to use a UNION or something to that extent, but still am not sure how to get the data before and after the update. I realize that triggers are not always the best solution, but like I said, I am fairly new to SQL, only really familiar with the basics. I would consider making a stored procedure to run, but I have multiple tables I would like to do this for, and if I'm not mistaken, I would have a separate procedure for each table, and would have to run the correct one. With triggers, I figured it would be easier in the sense that they would automatically run when updated. I am open to suggestions on what I possibly could do, as there could be something I don't know about that could help me with this issue.
thank you for this answer.
This. Any aggregate functions in the SELECT portion of the query itself could just as easily resulting in this. Edit2 - Something has to of changed. SQL as a language is Turing Complete, which means that given the same input it will produce the same output. The [grammar rules](http://savage.net.au/SQL/sql-92.bnf.html#xref-NO) + syntax rules for SQL show that it only cares about the first white space, others are ignored; AKA the generators that produce an execution plan for optimization will be the same. Just to clarify. Everything else is equal correct? * Same station querying? * Same credentials running the query? * etc Also try the following (if it isn't an in-use production server): * Run **DBCC FreeProcCache** and **DBCC DROPCLEANBUFFERS** * Execute first query * Run **DBCC FreeProcCache** and **DBCC DROPCLEANBUFFERS** again * Execute second query If you are feeling adventurous, have it pipe out the actual execution plan for each, get your DBA to run them if you must. *Edit Note: Commands are SQL Server only*
To expand on an already excellent answer, you can also have the users hit views which take into account the current user and their permissions. Though in all fairness, I don't like the idea granting a non-dev/DBA query access to the DB, too many ways a little knowledge can cause a whole lot of trouble. The business needing info from the DB is why we have reports.
"Yeah... I once built and compiled 50 procs in one night. It was nothin'." *pushes up glasses with a smirk*
This is totally me at work, with the exception that the women are about 40 years older.
Alright - consider adding the following fields to your table: Version INT IsDeleted BIT CreatedDateTime DATETIME LastUpdatedDateTime DATETIME And, create a table that mirrors your table with all of the fields, but has it's own primary key - call this &lt;table&gt;History. In an insert trigger on your table you initialize version number to 1, and set CreatedDateTime to current system UTC datetime. In an update trigger on your table you copy all fields to the History table, increment the version on the table to +1, and set the LastUpdatedDateTime to current system UTC datetime. In a delete trigger on your table you run an update that sets IsDeleted=1 (and make sure this update triggers the update trigger) instead of doing the actual delete. Then when you want to compare, you join your original table to your history table based on the ID of the original records, and version number on the original table = version number -1 on the history table. You can then also write a query to identify which fields have changed between versions as well, or if need be update the original table based on a historical version. Make sense? 
You jest, but this is an exact image of our current SQL intern helping 3 finance interns get some data.
What kind of data ;)
Kindle edition shows its $5 more. Also O'Reilly books are all DRM free so its not locked into Amazon 
Mine showed it as $12 more, but I'm in a different country. *shrug*
I bet he inner joined all their tables if you know what i mean ;)
He's probably grouping DimDate by astrology sign or something, cause business is never that interesting. We call him "smooth operator".
Yes, makes sense lol. A lot more in depth than what I was looking for (in the sense I do not need to catalogue the date/time when it was updated, just the information before and after). Thank you for your advice, I will play around and see what happens.
I'd check out 'Querying Microsoft SQL Server 2012', it's said to be one of the best - and it's also an exam prep book for 70-461. Unfortunately OReilly no longer carries it, as MS apparently took back all of the MS Press rights from them - but if you happen to be in Canada it's ridiculously cheap on [Amazon.ca](http://www.amazon.ca/Training-Kit-Exam-70-461-Microsoft/dp/0735666059) right now, has an exam prep on the CD, and a 10% off coupon for the exam as well.
I've been learning the wrong kind of SQL apparently
dear OP, please identify your dbms platform
Wasn't very busy at work today, worked out that -1/2+1/2\*sqrt(1+8\*sum(rownum)) will return the exact same result as count(*). Was so stoked at my discovery, I had to tell my SO about it when I got home, she couldn't keep her hands off me. ^^^not ^^^really
BIG DATA
Oracle
OP here. I haven't found a general, scalable solution to this, but for my purposes, I don't need to concatenate beyond 20 or so values, so I'm using code similar to this: drop table if exists temp.agg_test; create table temp.agg_test (num int, num2 int); insert into temp.agg_test (select 1 as num1, 1 as num2 union select 1,2 union select 1,3 union select 2,1 union select 2,2 union select 2,3 union select 2,4 union select 3,1 union select 3,1 union select 4,cast(null as int) union select 4,cast(null as int) union select 4,5 ); commit; --------------------------- select num, '(' || case when max(r)&lt;=3 then case when max(decode(r,1,num2)) is null then '' else max(decode(r,1,num2)) end || case when max(decode(r,2,num2)) is null then '' else case when max(decode(r,1,num2)) is null then '' else ' | ' end || max(decode(r,2,num2)) end || case when max(decode(r,3,num2)) is null then '' else case when max(decode(r,2,num2)) is null then '' else ' | ' end || max(decode(r,3,num2)) end else 'Multiple values; no bins' end || ')' as num2_values from (select num, cast(num2 as varchar) as num2, row_number() over (partition by num) as r from temp.agg_test order by 1,2) foo group by 1; This does what I need when there are 20 or fewer associated values. Ugly, but effective.
*Puts on his slightly larger glasses...*
He's teaching them... *sensual query language*.
As a girl, I can tell you sql doesn't get the guys. The stored proc must be failing again somewhere. 
That big D
Uh, no, SQL *SELECTs* the girls. Keep your get methods out of my declarative language.
Sexy Query Language.
Yeah, but some of us are wicked...
I actually dated a girl who worked a bit with SQL, one of our dates was a SQL Saturday together, possibly the nerdiest thing I've ever done in my life, we made a weekend out of it, had a hotel room, and went to the aquarium in the morning.
In America, first you learn the SQL, then you get the power, then you get the women..
nope, ambiguous statement
To be honest, that looks like it could be very distracting if you're actually trying to do anything work-related and I don't think I'd be able to focus properly on the SQL either. Also I freaking hate it when people are looking over my shoulder when I work. Just...send an e-mail or something. 
Here's the link to the actual site: http://www.tutorialspoint.com/sql/
I will look into that. I passed on the O'Reilly book if only because this books feels like it would be better as a physical book I can quick reference 
The very bottom is our candidates response to the request, will this get the job done? Thank you very much for any help on this.
Where to start? How about, your tables are named wrong.
Do you mean in the response or in the original question?
The tables are not named 'Customers' or 'Accounts'. They are named 'CUSTOMER_TABLE' AND 'ACCOUNT_TABLE'. Columns in the select are aliased (c.) but the tables in the from are not. They don't use modern join notation. Neither of the columns in the join condition exists. They missed the whole point of the exercise of going through `ACCT_CUST_MAPPING`. They do any grouping or aggregate or any attempt to get the first and last of anything. I could go on, but that probably satisfies your inquiry. This is one of the worst queries I've seen in a while.
ha, fair enough! I appreciate you taking some time to look at that and providing feedback. If you had a moment, would it be possible to provide what a correct response would look like? If that is asking a lot, don't worry about it.. Thanks again!
There are several solutions of course. This would be mine. SELECT c.CUSTOMER_ID, c.CUSTOMER_NAME, MIN(a.PERIOD) AS FIRST_PERIOD, MAX(a.PERIOD) AS LAST_PERIOD, a.ACCOUNT_NUMBER, SUBSTR(MIN(TO_CHAR(a.PERIOD,'YYYY-MM-DD')+a.PRETAX_PREPROVISION),11) AS FIRST_PERIOD_PRETAX_PREPROVISION, SUBSTR(MIN(TO_CHAR(a.PERIOD,'YYYY-MM-DD')+a.PRETAX_PREPROVISION),11) AS LAST_PERIOD_PRETAX_PREPROVISION FROM dbo.CUSTOMER_TABLE c INNER JOIN dbo.ACCT_CUST_MAPPING m ON c.CUSTOMER_ID = m.CUSTOMER_ID INNER JOIN dbo.ACCOUNT_TABLE a ON m.ACCOUNT_NUMBER = a.ACCOUNT_NUMBER GROUP BY c.CUSTOMER_ID, c.CUSTOMER_NAME, a.ACCOUNT_NUMBER; 
Amazing. Want a job here in Charlotte?? Haha, thank you again. I really appreciate it.
I'm not sure a correct answer can even be provided given the request. How does one determine the first and last period, for instance? What is the data type of that field? 
Thank you!
the first and last period will be datetime, so you would do an aggregate min() for the lowest possible date in that column, and max() for the largest
I've worked on systems that didn't use a datetime for columns named 'period' before.
You missed the last part: * The Pre-Tax Pre-Provision Number tied to the Account for the First and Last Periods
Yeah I guess it really could differ, data type might be helpful if provided.
I don't think that would work on MS SQL. No TO_CHAR function. It'd be CONVERT.
I can see why you didn't want to use the word (execution) plan.
not that it matters but in ms sql you don't have to specify inner join
Here's my modification of jc4hokies query for MS SQL: SELECT D.CUSTOMER_ID, D.CUSTOMER_NAME, D.FIRST_PERIOD, D.LAST_PERIOD, f.PRETAX_PREPROVISION AS FIRST_PRETAX_PREPROVISION, l.PRETAX_PREPROVISION AS LAST_PRETAX_PREPROVISION FROM ( SELECT c.CUSTOMER_ID, c.CUSTOMER_NAME, MIN(a.PERIOD) AS FIRST_PERIOD, MAX(a.PERIOD) AS LAST_PERIOD, a.ACCOUNT_NUMBER FROM dbo.CUSTOMER_TABLE c INNER JOIN dbo.ACCT_CUST_MAPPING m ON c.CUSTOMER_ID = m.CUSTOMER_ID INNER JOIN dbo.ACCOUNT_TABLE a ON m.ACCOUNT_NUMBER = a.ACCOUNT_NUMBER GROUP BY c.CUSTOMER_ID, c.CUSTOMER_NAME, a.ACCOUNT_NUMBER) d OUTER APPLY (SELECT PRETAX_PREPROVISION FROM ACCOUNT_TABLE a2 WHERE d.ACCOUNT_NUMBER = a2.ACCOUNT_NUMBER AND D.FIRST_PERIOD = a2.PERIOD) f OUTER APPLY (SELECT PRETAX_PREPROVISION FROM ACCOUNT_TABLE a2 WHERE d.ACCOUNT_NUMBER = a2.ACCOUNT_NUMBER AND D.LAST_PERIOD = a2.PERIOD) l I skipped using the convert/concat method as I'm assuming the PRETAX_PREPROVISION fields could be decimal and I don't know the effects of converting and concatenating monetary values to a string would be. The First/Last Pretax Preprovision fields could have also been grabbed without the outer apply and the subquery I'm sure - this is just how I generally approach such queries as a first pass. You could also probably do it reading the table just once with a windowed function, but those vary in availability depending on the version of SQL Server.
not sure i've understood the question. Why wouldn't you just order the results by the length of the sample string, something like this: select tableD.sample, tableB.original from tableD join tableB on tableB.original like concat('%',tableD.sample,'%') order by tableB.original, len( tableD.sample) desc
Duh! You just need this: SELECT * FROM girls
They seem to have found the magic method to get us to stop talking.
I've never laughed so hard in this sub. Come to think of it, I've never laughed in this sub.
Lol. I didn't even pay attention to the db engine tag. When I see all caps my brain turns to Oracle.
The design with the a PersonPet table is a many to many relationship. It has the advantage of a family having several pets; each person can be linked to each pet.
Select * from TBL_OtherPeople Where Gender='M' and BMI&gt;=40
In addition to what jc4hokies mentioned, If your person table contains PersonID, Name, Age, PetID, then you could have multiple lines for people with multiple pets, but the Name and Age would be duplicated where it isn't necessary. If you then needed to update someones name or age, you would have to update multiple rows. By having a Person table and a Pet table, you only list each person and pet once. The person to pet table would serve as a linking table. In this case, if you have to update a Name or Age on a person, you are only updating one row on the person table. [Second Normal Form](http://en.wikipedia.org/wiki/Second_normal_form) relates to the problems with updates, but looking into the other normal forms would probably benefit you at this point.
It's not really that big.
Here's what bugs me about this question - The ACCT_CUST_MAPPING table suggests that there is a many-to-many relation between ACCOUNT and CUSTOMER tables, and that the primary keys on each of the related tables should be those referenced in the many to many relation (CUSTOMER_ID, and ACCOUNT_NUMBER). However, if ACCOUNT_NUMBER is a primary key then it has to be unique within the ACCOUNT table, and thus there could only ever be a single PERIOD per ACCOUNT_NUMBER in that table. There's also no field that could be a primary key in the ACCT_CUST_MAPPING table, if they're both foreign keys. Maybe I'm missing something, or maybe there are no primary/foreign keys in this schema. If there are - then the request does not make sense (which DBAs run into more commonly than you'd think).
With this model you can represent a Shelter, Foster situation or any other multiple pet situations. Or you can represent giving away a dog, or a dog dying.
Whoever submitted this answer should not only not get the job you're hiring for, but you should also try to get them fired from wherever they're working now. 
With proper hardware and ~~datamart~~ fact/dimension table design, queries can usually aggregate and report on a couple million records in a second. I don't have a strong opinion on the use of views vs stored procedures. In terms of performance, table design and hardware are more important.
Also they would be indoors.
So what I'm using is technically a datamart. They want me to query the data warehouse.
I don't subscribe to differences between the terms "datamart" and "datawarehouse", so I'll rephrase. &gt; With proper fact and dimension table design.
Pretend that there is a `UNIQUE_ACCOUNT_TABLE` with `ACCOUNT_NUMBER` as a primary key, and both `ACCOUNT_TABLE` and `ACCT_CUST_MAPPING` have foreign keys to `UNIQUE_ACCOUNT_TABLE`.
Me interviewing: Do you work with any open source software? Person: Loon-icks *silence* The interview was quick. I didn't even get to ask how they had previously used Loon-icks. :(
Assuming you have a Primary key in your table that survives the update: SELECT * FROM inserted AS i JOIN deleted AS d ON i.id = d.id WHERE 1 = 1 AND (i.Field1 &lt;&gt; i.Field2 OR i.Field2 &lt;&gt; i.Field2 OR i.Field3 &lt;&gt; i.Field3) This will show all records that were updated where any one of your OR i.Fieldx has changed. From that basic check you can build out whatever you want to do with those records.
This is akin to people who alias tables with A, B, C, D. I want to slam my nuts in a car door every-time I see this. Also, if you are writing code behind a data driven application, it only takes one DB developer to not pay attention to reshuffle the result set order to break the application.
Plot-twist: OP is the submitter.
Agreed. If stored procedures are holding things down, can you pre-calc and store the results somewhere and then have a job to re-calc on a schedule?
"Do you work with any open source software?" Me: "Oh yes. All of the porn I work with is open source. Wait, what was the question?"
I have to ask, why are you tasked with coming up with the answer? If the hiring manager is asking this question, why couldn't he answer it himself? And even if the answers provided are correct, would you know why and what to look out for?
Have you looked into caching the report for the values they need in SSRS? If all you are worried about is user wait time, then why not cache the report every hour or however often your data warehouse is updated? Then the user should have minimal wait time.
That query shows very little understanding of SQL to be honest, but it's fine if this job is extremely junior... edit: by junior i mean this candidate would require training, there's no way you could leave him alone with your database.
1. De-normalisation of data normally occurs for performance reasons, so performance dependent systems (e.g. real time) are the type of systems which benefit. 1. Standard notation of LDMs for Datasets and their relationships would probably be one of [Crow's Feet, Chen, Bachman or Martin Notation.](http://www.smartdraw.com/resources/tutorials/cardinality-notations/)
thank you! :)
Normalization was 'invented' to prevent 'update anomalies' in database systems - i.e. to assure that multi-user transactional systems (OLTP) do not corrupt the data during multi-user read/write operations. For systems that are not 'transactional' - i.e. write-once (Hadoop or streaming real-time), or read-only (i.e. Data Warehousing/OLAP) - data can be represented/stored in a de-normalized form to provide performance benefits. 
Call the Patterson Technology Center. They handle these issues for you. That's what they're for. 
All the tables have indexes including a primary key and foreign keys. I'm not sure what else I could do on top of that. I'm new to warehousing to please elaborate if needed.
I do something similar for GIS data that we need to access to build our maps. Sometimes we have a view that copies the data into a table and creates very large indices so symbology is quick to draw. It makes it easier to work with and the cache refresh is adjustable. I have timers that run python scripts to drop records and re-insert the data compiled from the views that take time to build from the intense queries we use. The problem with views and indexing is only inner joins can be indexed on views and as you may not that isn't always the reality. Hope this helps. Good luck. 
All BI environments are generally demoralize to a degree. It makes it much simpler to write queries and build data cubes.
&gt; the people who wrote this didn’t know what they were doing that's often the case with SQL too
SPs are prevalent on most RDBMS implementations, but the capabilities and limitations of each SP language implementation vary by vendor. A huge SP may be in place because there is no ability to call other SPs from within an SP - or indeed any 'call' mechanism at all - leading to monoliths. This can also lead to the copy/paste scenario you mentioned - but my guess would be that laziness, poor initial design or an unwillingness to refactor are the most realistic set of causes for this. Perhaps it is fair to say that when implementing an SP, the author often has a single DB-related task in mind, which only rarely need complex nested processing.
How big would the DB be? You could setup a script that could be run each time they're on site to restore a copy of the latest backup of the master to the machine you're taking out to clients. If it's only a few GB it'd take less than a minute to restore. You would need to have a server running on the machine you were restoring to, if it's a small DB you could use SQL Express. You could do it in SSIS, bringing across only newer records if timestamped. 
perhaps you will get better, more specific insight at [/r/SQLServer](http://www.reddit.com/r/SQLServer)...
SQL is the ANSI standard, and only really includes select/update/insert/delete statements. T-SQL is Microsofts extension language for writing stored procedures. It allows if, while, variables, try/catch and most of the features you could expect in a declarative language. Oracle's is pl/SQL, Postgres is plpgsql, etc. SQL Server seems to engender a style of an incredible amount of really long stored procedures and over usage of temp tables. Refactoring them can lead to even weirder stuff -- we have several at work that create an empty temp table and then call another sproc to insert data into it. You can certainly write functions and more stored procedures and use exec to call them, but that can also lead to a rat hole and the dependency tracking is not great. I don't know why, but my instinct is that there's no good equivalent of perl or python in the Microsoft ecosystem, and DBEs needed to get work done without going through a full development cycle so they started writing stored procedures and the monster was born. 
Most likely because with SQL, you have to reference subqueries a lot to access the data you're trying to get at. I am not a programmer but I use SQL a lot. The queries I write can easily be a hundred lines, which does not seem long to me. Spacing and formatting are mandatory; if the writers weren't using a standard protocol, GOOD LUCK figuring out what they were doing... [Try here for a basic run-through of some of the standard commands.](http://www.w3schools.com/sql/sql_intro.asp)
You mentioned that this is MS SQL. Assuming its relatively current, you could always just punt on the T-SQL portion of it, and move stuff into SQL CLR ^[1](http://en.wikipedia.org/wiki/SQL_CLR) ^[2](http://msdn.microsoft.com/en-us/library/ms254498%28v=vs.110%29.aspx). It's basically .Net (C#, VB.net) run from the SQL server.
TSQL can be a fairly verbose language, a 1200 line stored proc is not necessarily a bad thing. In TSQL (or any SQL) you're also generally trying to work on data in a set based manner - scalar functions are BAD in this scenario, so encapsulating functionality in separate functions isn't necessarily a good idea. A scalar function in a set based operation can throw the whole query optimization process for a loop, lead to bad cardinality estimates, and otherwise result in poor performance. I think this is one of the most difficult things for traditional programmers to get past with SQL. That said - considering that the company hired you to write SQL when you admittedly have no experience, your instinct with regards to your predecessors may well be on the mark! :)
&gt;Oracle - A device whose mystery is only exceeded by its power! Sweet
T-SQL or PL-SQL which is the variant of SQL that you're dealing with is relatively easy to learn as it focuses on one thing; moving data in and out of the database and processing that data. No graphics, no UI, very simple console messaging if any. You still have the aspect of breaking a task down into steps and coding for them but it's mostly about reading and writing data or managing a collection of data and making required changes across many rows. Your obstacle might be that you're trying to compare it to the other languages. Try just approaching it on its own. Having said that, the scripts can get very ugly, especially in an organization that lacks good coding standards or management. I would do everything to avoid a 1200 line stored proc simply because it's so hard to maintain and transfer knowledge on and therefore is more prone to error. If possible, I would break it into separate procs or a server job. Without seeing the code, I can't really comment on the copy / paste issue in the other procs. They might have been developed over time in response to changing needs of the process and it might go back to a lack of coding and maintenance standards. As far as learning T-SQL / PL-SQL, I would follow this general process: * Start with SQL itself, which is very easy to learn. At it's heart, it's about the verbs (SELECT, UPDATE, INSERT, DELETE), the joins and the various clauses (WHERE, ORDER BY, etc.). Become proficient in making single-statement changes to some test data. * Then learn how to turn those individual SQL statements into stored procs that can accept and return variables. * Then learn how to combine a series of SQL statements in one proc using conditional statements, temporary tables, etc.. * From there, just start adding functions available through T-SQL / PL-SQL and more levels of complexity. There's a ton of free resources through Google. W3Schools has a [SQL Tutorial](http://www.w3schools.com/sql/) and there are plenty of [T-SQL tutorials](http://www.databasejournal.com/features/mssql/article.php/3087431/T-SQL-Programming-Part-1---Defining-Variables-and-IFELSE-logic.htm) available around the web as well. Best of luck. 
Ok. For you we can take the 1200 line stored proc, remove all formatting, and jam it all into 50 lines. Problem solved?
Thanks for the reply! &gt; In TSQL (or any SQL) you're also generally trying to work on data in a set based manner - scalar functions are BAD in this scenario, so encapsulating functionality in separate functions isn't necessarily a good idea. A scalar function in a set based operation can throw the whole query optimization process for a loop, lead to bad cardinality estimates, and otherwise result in poor performance. This is exactly the kind of thing I was looking for: can you elaborate some more, or point me in the direction of something which expands on this? I should perhaps explain that this code is part of an internal tool which is used by data analysts do produce various kinds of reports. It's written entirely in SQL, so all of the data cleaning and program logic (which is admittedly rather simple), is all done within TSQL. In this case, aren't scalar functions (e.g. for string manipulation) inevitable, and do they actually impact performance by that much? I have very little knowledge about how SQL gets compiled into execution plans and then to assembly, so any places I could learn about that would also be appreciated :) 
&gt; You're likely experiencing code smell . Great link! I think this sums up exactly my position. I'm trying to distinguish between my unease at the superficial things (seemingly strange SQL formatting conventions), and the potentially important structural problems. &gt; You've likely got features to implement or bugs to fix. I probably should have said that I am not a developer in the traditional sense of the word, but actually a data analyst. The code in question is an internal tool, and my role is to use it to generate various reports, rather than to develop it in any particular way. Unfortunately this seems to be how the tool was made from scratch, in a rather piecemeal fashion, so each job seems to need moderately large changes to the code. Hence why I seem to end up writing a lot of SQL, rather than simply using it.
Thanks for the links! This is one option I have been considering, but I'm afraid that there will be resistance to such a change due to my team being relatively non-technical, and therefore might struggle to adjust to a new paradigm. If the rest of the team mainly has to use the code rather than develop/maintain it, would there be much difference from their perspective in terms of setting up new DBs, or interacting with the code if a CLR environment was used?
Thanks, this is exactly what seems to have happened. Unfortunately, this is an internal tool which exists ENTIRELY in SQL, so I can't just keep the logic in the codebase (where I personally think it belongs).
&gt; Having said that, the scripts can get very ugly, especially in an organization that lacks good coding standards or management. I would do everything to avoid a 1200 line stored proc simply because it's so hard to maintain and transfer knowledge on and therefore is more prone to error. If possible, I would break it into separate procs or a server job. I think you have hit the nail on the head here. The code itself has not been written by developers, but by data analysts, who have little to no formal programming background. To give you an example on the copy/pasting of code, lets say we have a table full of cats* and all the details like addresses, phone numbers, etc. For good reasons, all of the same details about dogs* are kept in a separate table. One of the tasks which the program is supposed to do is find duplicate addresses, even if they aren't exactly identical. So currently, we have one (long!) procedure to find duplicate addresses within the cat table, one to find duplicate addresses within the dog table, and one to find duplicate addresses between cats and dogs, even though both tables store addresses in exactly the same format. *Table contents changed for privacy reasons.
Here's a [good breakdown](http://sqlblog.com/blogs/adam_machanic/archive/2006/08/04/scalar-functions-inlining-and-performance-an-entertaining-title-for-a-boring-post.aspx) from Adam Machanic. You can use inline table valued functions, which perform much more reasonably (though they don't allow flow control statements). A lot of scalar or control flow operations that may be used for formatting can also be done more effectively in the presentation layer. As for the performance impact, it varies depending on how they're used. If you're using them regularly in set based operations, specifically as predicates, then yeah it'll get ugly pretty quick. Here's some more links: [Ten Common Threats to Execution Plan Quality](http://sqlperformance.com/2012/11/t-sql-queries/ten-common-threats-to-execution-plan-quality) [Inline Scalar Functions](http://sqlmag.com/sql-server/inline-scalar-functions) [T-SQL Best Practices](http://www.databasejournal.com/features/mssql/article.php/3845381/T-SQL-Best-Practices-150-Don146t-Use-Scalar-Value-Functions-in-Column-List-or-WHERE-Clauses.htm) 
I have a similar problem at my job - so much original code is just 'there' and 'working most of the time, with a few peculiarities'. We're currently trying to build a sustainable reporting framework, but it feels like an uphill battle with the technology department.
I'm thinking the issue your predecessors came across is that with exec-ing a stored procedure within another is that you don't know if that stored proc will change. So if you do "Insert into # temp exec somesp" then what if someone adds columns to some sp? What will break? There are built in stored procs in MS SQL to determine the columns out of a sp but they don't work under certain conditions. This isn't to excuse their bad practices, but I can understand why people do certain things. 
That sounds familiar. I'm guessing there are some serious data normalization issues with the tables, too. That can happen even with experienced programmers and others who understand normalization when management starts demanding quick solutions to that week's emergency project.
Don't separate the app across multiple databases and don't separate users into their own tables. 
Add a n:m relationship between user and DVD ex. User-userDVD-User. 10,000 people may all have the same edition of Hackers, so you don't need to duplicate all of the DVD info. Any user specific notes, ratings, etc would key off the UseDVD table. Also I wouldn't be overly concerned with supporting different databases in the beginning. Most popular ORMs will already support this. Instead make sure that all custom SQL or database engine specific features are encapsulated so that you can easily update them if you ever change database engine. For example, web pages or UIs that have custom SQL scattered about in strings will be a bear to update. 
This isn't always right. Sharding the database, not tables, on user can be a great approach, you will just need to build an auto balancer later to keep the databases around the same size by moving records. It is a more complex setup but if you get a lot of data later it is easy and cheap to scale out instead of up.
Hello, jazzman! Thanks for your feedback! We made a several updates based on your opinion: 1. After every lesson (when you need to click, trying to guess query) now shows query with result table. Thus, you can see result of your work after pressing button. 2. Now the lessons which you are finished marked as complete 3. And of course syntax errors. We tried to fix them It's very sad that you don't want to come back. But we will be happy if you can give us one more chance ;)
In this case it clearly isn't warranted, he only has hundreds of users with hundreds of DVDs. If that's the case the DVDComment table would have less than a million rows. Further, the table would be highly selective for either DVD or User ids, so an index or two would make queries perform very well. 
Hi! Thanks for your feedback! Couple words about what we changed: 1. We tried to made text more correct (with help of native speaker) 2. Well, as I can guess you tried "guess sql" lesson. And the real goal for this lesson is to show how similar could be real language and SQL :) For more detail work with tables and queries you can try other lessons (on the right side) Thank you! =)
Thank you! That's very good to see. :)
 INSERT INTO db3.validation ( test_result ) SELECT CASE WHEN (SELECT COUNT(1) FROM db1.transaction) = (SELECT COUNT(1) FROM db2.transaction) THEN 'pass' ELSE 'fail' END 
This works. Thanks!
What is your background? Are you coming from IT support, SysAdmin, networking, development?
That's a huge topic, and it largely depends on what one (or one's employer) defines as "DBA". For some places, it means "Does 'Bout Anything". For larger companies (particularly those subject to the various "separation of duties" regulations like mine) it means you're narrowly focused on the "operation and maintenance" of databases. The ramifications thereof include (for me, at my ginormous company): * No development. We are not allowed to be involved at all in the development of "end-user" databases. That's for the developers. * No physical server responsibility. We are responsible solely for the SQL Server services running on the various servers. If a server needs rebooting or more memory added or whatever, that responsibility lies with the sysadmins. * We're heavily involved with planning new databases from a resource perspective (how much data, how often, retention requirements, recovery requirements, etc...) * We're heavily involved with planning database MIGRATIONS (moving from one server to another, usually to a higher level of SQL at the same time). That might sound like a rare-ish thing, but we have hundreds of servers and with the constant evolution of SQL Server and the associated sunsetting of older versions, we seemingly always have several databases in some stage of the migration process. * It really pays to have, or develop, a deep-dive understanding of SQL Server's under-the-hood operations. Database file layouts, index fragmentation, query plans, etc. * IMHO, it also pays to have been SQL developer at some point, so you can have some developer-side perspective when needed. (Some co-workers call this "know thine enemy" LOL ) Some of the above may sound negative, but I enjoy the "change of pace" being a DBA has afforded me from what was becoming a repetitive development career. ("Let's see... LNAME VARCHAR(50), FNAME (VARCHAR(40), ADDDRESS1 VARCHAR(100).....yadda yadda yadda"). That's a bit of a rambling, wandering response, so feel free to ask followups or (probably more productively) wait for other more eloquent DBAs to respond Another thing: Be prepared to ALWAYS be the first, main target of blame when there's ANY problem. Application spits an error claiming it can't connect to the database? Must be a database problem. Send an incident to the DBAs! Edit: punctuation. Nothing to see here. Move along. Edit 2: Added the "blame" comment.
Good post. 
LOL. Good point. I find that "good" developers "get it". The worst (IMO) are the quasi-developers, as typified by report developers (those who use some GUI to develop reports and end up with... dreck. I especially love reports that generate queries which the optimizer can't use, so all 50 skamillion rows get retrieved from the DB to the report server and THEN filtered down to the 20 or 30 the user wanted. 
See, this is funny because while I'm one of two DBAs and I'm also partly responsible for writing the reports in Cognos. :D Don't worry, I take no offense. Cognos isn't bad, to be fair, assuming you have your model properly defined. It does often turn into putting square pegs into round holes. Like I've never used a GUI report writer that lets you turn a one-to-many column into a comma delimited list, so you're forced to make the server do it and SQL Server lacks `GROUP_CONCAT()` or `LISTAGG()`. I still think SSRS might be better in the end, but I haven't written enough in that to tell. 
&gt; or why they can't assume that their application will be the only program with access to the application DB This mind-set correlates strongly with the use of ORMs, in my opinion, as ORMs usually treat an RDBMS as a simple storage for objects. Once you design your applications this way (domain-model centric), it's hard to escape this mind set and understand that in fact, the database is at the center of your (and others') applications.
I wouldn't worry about separation until you get to the point of millions of rows. And even then, databases have features to intelligently handle much larger datasets. Spend some time learning about optimising the database itself rather than writing overcomplicated application code. Assuming the database is set up right, it will most likely do a better job than anything you come up with and do in the application code. The things you learn will increase your knowledge generally, and translate over to other projects. I assume you're not using SQLite for the web app? Better not to if performance is a concern. 
I'd recommend starting here: [Oracle Two Day DBA](http://docs.oracle.com/cd/E11882_01/server.112/e10897/intro.htm#ADMQS001)
As /u/SQLDave mentioned, DBA means a lot of different things to different people,. Knowing your skillset, as well as what you'd like to be doing would go a long way towards getting a good recommendation.
Sounds like you need a date part function. Select datepart(month, read date) as readdate_month, sum(whatever the summed data is) as summed From table Group by date part(month, read date)
Thing is, that would return 100 for August. I need the true value for August - 5 days worth of august from a value of 100 (which would be 5/30 * 100) plus 26 days worth of August from the 150 value (or, 26/30 * 150). You see?
You first must become the access nazi. Developers are your archenemy. Protect the data and backups. If you say the above the job is yours.
Would you mind explaining this a bit further? Like, what each chunk is doing exactly? And what you mean in the closing part? Thanks!!
No - think of it like this: the 150 reading taken on 8/4/14 is the reading for all 29 days between 7/5/14 and 8/4/14
It's basically creating a table with the twelve months in it and generating a row number for the twelve segments of your table and joining them to attach months to row numbers. My second part was about how to get what you where asking for from the data. Assuming we name the above results as '**rd**' you could join it like: SELECT DATEDIFF(dd, a.ReadDate, b.ReadDate), am.mName FROM rd AS a INNER JOIN rd AS b ON (a.rnum = ((b.rnum+1) % 12)) The *((b.rnum+1) % 12))* is basically to add a month and wrap the number if it goes over 12. Without knowing how everything works and the edge cases it's hard to come up with something foolproof. Honestly it seems like there might be some legacy stuff going on.
Sorry, I'm having a hard time following what is going on... I think I'm out of my depth on this problem, but I'd like to see what the result would look like anyway, so would you be able to pull it all together into one query using table t1 that has the columns ID, ReadDate, and Amount so I can mentally try to piece together the process better? Again, thank you so much for your help :)
I guess it also depends on the industry background. In healthcare, where I work, for example, every application has it's own schema/database, and in some cases, like for our primary operational system, multiple database/schemas. Then, there is user and group level access permissions on each database/schema. This is all necessitated by the HIPAA principles of "minimum necessary information" - allowing multiple applications access to the same datastore would be a privacy risk, so data is extremely compartmentalized. With some applications there is also an additional layer of security within the application itself of course - for instance, all base users have access to the system for our primary operational systems - however at the application level, only certain items are accessible by the end user. And the database itself is generally restricted only to the application, batch accounts, and DBA's/Analysts.
The best I can do is a quick excel sheet showing what they would be roughly doing. Basically it's trying to link the 'month' to a 'period' despite the dates being different. [http://i.imgur.com/MCDS7f9.png](http://i.imgur.com/MCDS7f9.png) The % (modulus) probably isn't required and may just require an end of year date, but I have no clue how the system works it. 
If you have the time, could you please make the query still? I'm sorry for asking so much, but it would really help me out.
Alright - thanks for your help!!
Select ID, ReadDate, Month(ReadDate) as Month, Year(ReadDate) as Year, Amount From T1 OR ----- Select ID, ReadDate, Month(ReadDate) as Month, Year(ReadDate) as Year, SUM(Amount) From T1
Use a subquery http://technet.microsoft.com/en-us/library/ms189575%28v=sql.105%29.aspx 
Look, if you are not going to specify the database, then I can't even begin to give you advice on what approach may be best. It all depends on the database, not on the ORM. I'd suggest you try out what you consider 'slow' before you go about optimizing. I think you'll find that it's not really that slow after all. Use science and investigate, insert fake data, run some queries, gather results. Once you have your benchmarks, then you can go about optimizing.
&gt; Instead, it seems to be taking the count of all the rows in my first JOIN No, its simply counting the IncidentIds within the group. If you use a GROUP BY, then all aggregate functions such as SUM()/COUNT() etc, will count PER group (thats the whole point of a GROUP BY, right?) For something like SQL Server, it may be as simple as adding WITH ROLLUP after your GROUP BY clause. Then you will get summary rows for each group and subgroup, and a final 'ALL' row at the end 
You should be fine if you are using 5.5 you may have a harder time but start here http://robsphp.blogspot.com/2012/09/how-to-install-microsofts-sql-server.html and here http://robsphp.blogspot.com/2012/06/unofficial-microsoft-sql-server-driver.html . I would regime d stored procedures but there should be lots of examples online and syntax isn't that different. This is kind of a general question so if you have specific questions reply in this thread and I'm sure someone can help you. 
I think it's because Max() and nulls don't jive. Try replacing your nulls if possible, or write a case statement to ignore nulls.
You might want to cover the fact that you can do sub-select type joins by using parentheses (mssql); especially good for aggregate function comparisons (in my limited experience). And for the love of god good cross join examples!
There's no way I could convince him to download a third party driver. Is that the only option? &gt; This is kind of a general question I really don't know how else to phrase it since my experience with mssql is limited. I have an html form whose attributes I need to store in a database on a server that runs mssql. I'm using php because its what I've always used to validate forms. Is there an easier or better way?
I made a similar system that required data to be taken out from a mssql server that was being used for another legacy program and ended using dblib with pdo on linux, on windows im not sure what you can use, to get dblib i just installed php_mssql from yum in centos and got it working
You mean with php? Can I use another language to store the form data that's native to ms sql? Why is this so counter intuitive, it seems like such a standard use of an sql server. 
solid advice.
&gt; Does anyone know why? your update statement looks like this -- UPDATE Table_2 set SSI = foo where you see *foo* should evaluate to a single value your subquery results in multiple values because of the GROUP BY your query should produce an error message 
In SQL Server, you have ROLLUP, CUBE, and GROUPING SETS for things just like this: http://technet.microsoft.com/en-us/library/bb522495(v=sql.105).aspx
I just posted my first article in a series on database joins. This one is an introductions, INNER JOINS, OUTER JOINS, and more to follow!
This makes a lot of sense, thank you. I was visualizing the JOIN wrong, and thought that the GROUP BY would still have the same number of rows. 
Thanks for the suggestion. This ended up being the same as my query, since there's duplicate rows as a result of the JOIN's, it over counts the Incidents. 
Yes, with php, and with any language. Some sort of driver is going to be required. Yes, it's certainly possible to use another language (say, python) to access the mssql db, but you're still in the same boat, because that will require its own driver as well. There's no way around it. Are the website and db hosted on the same machine, or on separate machines (and possibly separate platforms)? I'm not sure I understand the fear with "third-party" drivers. Microsoft provides necessary drivers, and the standard trusted linux repos provide them as well. Anyway, connecting is impossible without one.
In fact, the sql may not even change at all, but how screwed you are really depends on how big your code is. So, not only will you have to possibly change/modify your sql, you'll also need to change your php code, e.g. changing mysql_connect to, say, odbc_connect, or mssql_connect, and mysql_query to odbc_exec, etc. Depending on how long and how much code you have, it could be tedious.
It turns out this was the solution after all. The problem was with my JOINs. Since they were an INNER JOIN, any rows without any flags didn't appear, and so doing DISTINCT was giving the wrong result. Changing it to DISTINCT with a LEFT OUTER JOIN gave the correct results. Thanks a lot for the help. 
Look at [this](http://ruiromanoblog.wordpress.com/2010/05/08/configure-reporting-services-ssl-binding-with-wmi-powershell/)
Learning SQL. I may be a bit past this, but I'll look it over anyway. Thanks for sharing.
`datepart(hh,datefield) &gt;= 9 and datepart(hh,datefield)&lt;12` will get you the records between 9:00:00 and 11:59:59. So in your `select`, you could do this: case when datepart(hh,datefield) &gt;= 9 and datepart(hh,datefield)&lt;12 then 'latemorning' when datepart(hh,datefield) &gt;= 12 and datepart(hh,datefield)&lt;15 then 'earlyafternoon' when datepart(hh,datefield) &gt;= 15 and datepart(hh,datefield)&lt;18 then 'lateafternoon' end case as timebucket and so on. Then you can use that `timebucket` field to group your report. For example: SELECT CASE WHEN datepart(hh, getdate()) &gt;= 9 AND datepart(hh, getdate()) &lt; 12 THEN 'latemorning' WHEN datepart(hh, getdate()) &gt;= 12 AND datepart(hh, getdate()) &lt; 15 THEN 'earlyafternoon' WHEN datepart(hh, getdate()) &gt;= 15 AND datepart(hh, getdate()) &lt; 18 THEN 'lateafternoon' END AS timebucket Produces this result: (my local time is 13:57): timebucket ----------- earlyafternoon There's probably a way to do it with a windowing function as well, but if you just want a field in your resultset that you can then use within your report for grouping, this should do (and will work on any version of SQL Server, instead of depending on windowing features that may only be available in the latest releases).
Its the Canadian branch of a billion dollar corporation. I don't know how they have their servers set up which is why I'm cautious about every little thing. I just recently noticed that sqlsrv is native though and not third party so it'll be fine. I'm validating it with mysql functions first and then just trying to find the appropriate sqlsrv function and switching them out; is that a recipe for disaster? 
All you really need to know about is the datepart function. Check the syntax and you'll see you can get just about every possible bucket a date could be put in (year, quarter, week of year, day of week, hour, etc). /u/alinroc also gave you a bunch of extra info and a sample implementation. 
Its not too extensive, its a single form but there are a decent amount of inputs. &gt;In fact, the sql may not even change at all, That's what I'm banking on, is it changing even a possibility? I assumed I would just need to switch out the functions from mysql to sqlsrv? Also, do you know of any resources that compare the functions or better describe them? I'm having trouble finding which mysql function correlates with its sqlsrv counterpart. 
http://stackoverflow.com/questions/1378593/get-a-list-of-dates-between-two-dates-using-a-function
&gt;is it changing even a possibility It depends. The more general your query, the less likely it is you'll have to change it. Something like select * from table; will work anywhere. Dates/date/time/timestamp functions can be different. Auto-incrementing or auto-updating fields could be different. I assume you have access to the db itself (because otherwise how would you know what sql to write), so you could run queries directly and see if you get the results you expect, and/or test your forms to see if they produce what's expected. &gt;Also, do you know of any resources that compare the functions or better describe them? Other than google, no.
learn PDO, mysql functions are deprecated and insecure, PDO should be installed on any decent hosting service, if it's not then ask for it and they should enable it.
If the data needs to be queried anyway and it resides on the same server resources. There doesn't really sound like a good reason to be duplicating data. 
I think it's resolved for the time being. Thanks for your help, everybody.
you could use a table variable like so... declare @List table (s varchar(50)) insert @List values ('1234'),('5678'),('9101'),('1121'),('3141') select * from dbo.sql join @List on createddate = s 
You can GROUP BY a field that has nulls. Can you paste your query here, or maybe via a [SQL Fiddle](http://sqlfiddle.com)?
dear OP, please identify your platform... MS SQL presumably?
&gt; Does that make sense? nope
Yes, sorry, 2008 R2.
Here's a table function you can use to handle creating the table: USE [Transportation] GO /****** Object: UserDefinedFunction [dbo].[fnSplit] Script Date: 07/22/2014 14:41:40 ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO create Function [dbo].[fnSplit] ( @List varchar(8000) , @Delimiter char(1) = ',' ) Returns @Temp1 Table ( ItemId int Identity(1, 1) NOT NULL PRIMARY KEY , Item varchar(8000) NULL ) As Begin Declare @item varchar(4000) , @iPos int Set @Delimiter = ISNULL( @Delimiter, ',' ) Set @List = RTrim( LTrim( @List ) ) If (@List &lt;&gt; '' ) Begin -- check for final delimiter If Right( @List, 1 ) &lt;&gt; @Delimiter -- append final delimiter Select @List = @List + @Delimiter -- get position of first element Select @iPos = Charindex( @Delimiter, @List, 1 ) While @iPos &gt; 0 Begin -- get item Select @item = LTrim( RTrim( Substring( @List, 1, @iPos -1 ) ) ) If @@ERROR &lt;&gt; 0 Break -- remove item form list Select @List = Substring( @List, @iPos + 1, Len(@List) - @iPos + 1 ) If @@ERROR &lt;&gt; 0 Break -- insert item Insert @Temp1 Values( @item ) If @@ERROR &lt;&gt; 0 Break -- get position pf next item Select @iPos = Charindex( @Delimiter, @List, 1 ) If @@ERROR &lt;&gt; 0 Break End End Return End GO 
Unless the list of values is very short, an indexed temp table is going to perform better. OP, think tables, not arrays! This is a set-based language. Also, WHERE EXISTS performs significantly better than WHERE IN or an INNER JOIN for this type of operation. 
It is very short. I have a very long and ugly query with lots of selects.
The syntax is a little simpler for a table variable and you don't have to worry about dropping them if you're going to run the script a few times in the same session. If it's only a few values, you don't need to worry about performance. 
Don't do that.
 Select * From dbo.sql S Inner Join fnSplit('1234,5678,9101,1121,3141',',') V on V.Item = s.value
An indexed table variable should perform identically because they function identically internally, AFAIK. You just have to create the index in the table definition. The only reason to use a temp table, IMO, is if you need persistence between batches.
Thanks for this! I look forward to more articles. 
 SELECT * FROM dbo.sql WHERE CONVERT(DATE, createddate) IN ('2014-01-01','2014-06-05','2013-12-31'); Does this not accomplish your task?
I'd be interested in the difference between an optimized query vs. non. Do you have some code examples? 
Are the issues always capable of being resolved remotely?
I'm a .Net developer that will become a DBA in 2 weeks. We are in the same boat brah~!
That's true, a table variable will be indexed if you declare a primary key or unique constraints when you create it. You still can't create arbitrary nonclustered indexes with INCLUDE columns which is going to limit your ability to optimize more complex queries. All of this is fairly academic in this case, because with a short list of values, SQL will likely scan the table every time regardless. 
Sounds like both of you are looking to become Database Developers, not Database Administrators. If I could go back and do IT all over again, this is the route I would of gone.
No problem. There are some many resources out there for experienced users, I figured I would try to help the beginner. IF you feel there are topics I should cover, please let me know.
You are welcome. 
Here is my take on what to know: http://www.essentialsql.com/what-are-ten-things-a-junior-dba-should-know/ 
I don't think that's entirely true. As one commenter said the only index you can create on a table variable is a clustered primary key, but also table variables don't have statistics so the query optimizer always assumes 1 row and doesn't always optimize joins the best way. For large sets of data you pretty much always have better success with a temp table. 
&gt; That's true, a table variable will be indexed if you declare a primary key or unique constraints when you create it. However, if you're using SQL Server, be aware that table variables can really screw with your execution plans because they don't have statistics.
&gt; The 3 updates take electronic addresses from one table and apply them to a row in an export table. *A* row in an export table, or *different* rows in an export table?
Correct.
The source table. Type| Location| Locator ---|---|---- 2| 1| not@example.com 1| 1| (555)121-1212 5| 1| (555)121-1213 The export table Location| contactFax| contactPhone| contactEmail ---|---|----|---- 1 | (555)121-1213| (555)121-1212| not@example.com This is not to say that there would be only one row for a given location in the export table. There may be more than one. EDIT: corrected export column names to match the sql. 
Isn't the where clause the only difference? You could just combine those together with brackets and or: Update dbo.mydest set contactfax = locator From dbo.mydest mt inner join dbo.partylocation d1 on d1.PARTY = mt.party inner join dbo.ElectronicAddress ea on ea.LOCATION = d1.LOCATION where (ea.TYPE = 5 and mt.uniqueid = @user) OR ( where ea.TYPE = 1 and mt.uniqueid = @user) OR (where ea.TYPE = 2 and mt.uniqueid = @user ) 
No, the set is different in each of the 3 as well. 
I'd be tempted to create a view, and get rid of the export table entirely.
You might be able to use a case statement here, there's a similar question on : [StackOverflow](http://stackoverflow.com/questions/4830191/t-sql-using-a-case-in-an-update-statement-to-update-certain-columns-depending-o) basically just set all the columns to either, a value based on a condition, else set to the current column value. 
 UPDATE dbo.mydest SET contactfax = CASE WHEN ea.TYPE = 5 THEN locator ELSE contactfax END , contactphone = CASE WHEN ea.TYPE = 1 THEN locator ELSE contactphone END , contactemail = CASE WHEN ea.TYPE = 2 THEN locator ELSE contactemail END FROM dbo.mydest mt INNER JOIN dbo.partylocation d1 ON d1.PARTY = mt.party INNER JOIN dbo.ElectronicAddress ea ON ea.LOCATION = d1.LOCATION WHERE mt.uniqueid = @user AND ea.TYPE IN ( 5 , 1 , 2 )
I'm not sure if this is right but this might work for you... Update dbo.mydest set contactfax = case when ea.type = 5 then locator else contactfax end ,contactphone = case when ea.type = 1 then locator else contactfax end ,contactemail = case when ea.type = 2 then locator else contactfax end -- select locator,contactfax,case when ea.type = 5 then locator else contactfax end as Fax_Locator,contactphone,case when ea.type = 1 then locator else contactphone end as Phone_Locator,contactemail,case when ea.type = 2 then locator else contactemail end as Email_Locator From dbo.mydest mt join dbo.partylocation d1 on d1.PARTY = mt.party join dbo.ElectronicAddress ea on ea.LOCATION = d1.LOCATION and ea.TYPE in(5,2,1) and mt.uniqueid = @user See if that works. You can use the commented select (after the SET commands) to run the query and see what results. 
As long as you have a backup, you can make a new db anywhere. 
What is hour? Do you need to find the difference based on a start time vs end time? Based on your sample query the following should be fine. select t1.field1, t1.field2, t1.field3, t1.hour, t2.totalcost, t1.hour/t2.totalcost as TotalCostPerHour from table1 t1 LEFT JOIN table2 t2 ON t1.invnum = t2.invnum where t1.date &gt;= '4-01-2014' and t1.cust = 'custname' group by t1.field1, t1.field2, t1.field3, t1.hour, t2.totalcost Can you show your table definition?
First, indexes? Second you have quite a few non-sargeable predicates, such as `b.product_class_code like '%prepay%'` and `YEAR(a.read_date)=2014`. The year one would (probably) be faster as `a.read_date BETWEEN '2014-01-01' AND '2014-12-31'`. How many rows are in the tables? When you view the query plan is it suggesting any indices? Do the estimated and actual numbers of rows match fairly well? Edit: Just FYI, most people would write that query using a `WHERE` clause as SELECT DATEPART(MM, a.read_date) as Month , a.cust_id , (c.efl_1000_kWh * sum(a.usage_units)) as Usage FROM t1 a JOIN t2 b ON a.cust_id=b.cust_id JOIN t3 c ON c.cust_account_id=b.cust_account_id WHERE b.product_class_code like '%prepay%' AND YEAR(a.read_date)=2014 AND a.read_sub_type_desc='ams daily read' GROUP BY a.cust_id, DATEPART(MM, read_date), c.efl_1000_kwh The semantic matters when you use outer joins.
Have you considered using a PIVOT? For example, SELECT Location , [1] AS contactPhone , [2] AS contactEmail , [5] AS contactFax FROM sourceTbl PIVOT (MAX(Locator) FOR [Type] IN ([1], [2], [5])) AS pvt and then updating with that?
 select t1.field1 ,t1.field2 ,t1.field3 ,MAX(t1.hour)-MIN(t1.hour) as TotalHours ,t2.TotalCost ,MAX(t1.hour)-MIN(t1.hour)*1.0/t2.totalcost as TotalCostPerHour from table1 t1 LEFT JOIN table2 t2 ON t1.invnum = t2.invnum where t1.date &gt;= '4-01-2014' and t1.cust = 'custname' group by t1.field1, t1.field2, t1.field3, t2.totalcost If hours is an INT then you'll need the *1.0 which will change the data type to a decimal. If hours is a numeric datatype with a decimal (float for example) then you don't need the *1.0. I think that should do it. Edit: Had min/max reversed
 SELECT TOP 1 CUSTOMERS.customer_ID, CUSTOMERS.customer_fname, POINTOFSALE.item_inventory FROM POINTOFSALE INNER JOIN CUSTOMERS ON POINTOFSALE.Customer_ID=CUSTOMERS.Customer_ID; WHERE CUSTOMERS.customer_fname = 'JIM' AND CUSTOMERS.customer_lname = 'SMITH' CUSTOMER_ID CUSTOMER_FNAME ITEM_INVENTORY
Are you just wanting that result set for only Jim Smith? ("12346 jim asparagus") If so, Thriven's answer is mostly correct. Unless I missed something where more than one item is associated with him. SELECT CUSTOMERS.customer_ID, CUSTOMERS.customer_fname, POINTOFSALE.item_inventory FROM POINTOFSALE INNER JOIN CUSTOMERS ON POINTOFSALE.Customer_ID=CUSTOMERS.Customer_ID WHERE CUSTOMERS.customer_fname = 'jim' AND CUSTOMERS.customer_lname = 'smith'
Yes, I only need information returned for Jim Smith.
The PostgreSQL documentation for ["The SQL Language"](http://www.postgresql.org/docs/9.3/static/sql.html) is very well done. It's written for novices and it's not too inwardly-focused on PostgreSQL. They are usually pretty good about indicating when a feature deviates from the SQL standard, so it should be fairly universal. I am frustrated with MySQL documentation. It's poorly organized. Some sections are simply bulleted lists of assorted notes about the feature (see the page for [SELECT Syntax](http://dev.mysql.com/doc/refman/5.0/en/select.html), compare with [PostgreSQL's SELECT Syntax](http://www.postgresql.org/docs/9.0/static/sql-select.html) page). I first learned the basics (SELECT, INSERT, UPDATE, DELETE), functions/type casting, then DDL (CREATE TABLE, ALTER TABLE, etc), then aggregate functions/GROUP BY. That worked out pretty well for me. But... I wish I'd been shown views a lot earlier. It took me a long time to realize how useful they were at simplifying (both visibly and conceptually) complex queries. I wish I'd had someone drill me to tears on JOIN semantics, as I still struggle to grasp where I would want to use a JOIN other than a LEFT JOIN or FULL OUTER JOIN ... WHERE. Other things like proper normalization, indexes and stored procedures can come later. I think I'm alone on this, but I was frustrated by tutorials that lead with ERD diagrams. I just didn't find the abstractions, relationship spaghetti, or symbolic short-hand very helpful. I was confused by the abstraction without the understanding of what was being abstracted. I was frustrated by math-oriented tutorials. Set theory and complexity are important to properly modelling relationships, normalization, optimization, etc. But keep the college-level math notation AWAY from the noobs. At least get through the DDL and JOINs with concrete examples showing row-column table output before switching to venn diagrams, set notation, complexity notation, etc.
It sounds like you just need a where clause? SELECT CUSTOMERS.customer_ID, CUSTOMERS.customer_lname, CUSTOMERS.customer_fname, CUSTOMERS.customer_email, CUSTOMERS.customer_address, CUSTOMERS.customer_zipcode, CUSTOMERS.customer_areacode, CUSTOMERS.customer_phonenumber, POINTOFSALE.item_inventory, POINTOFSALE.item_barcode, POINTOFSALE.item_numsold, POINTOFSALE.item_price, POINTOFSALE.dateof_sale, POINTOFSALE.timeof_sale FROM POINTOFSALE INNER JOIN CUSTOMERS ON POINTOFSALE.Customer_ID=CUSTOMERS.Customer_ID WHERE CUSTOMERS.CUSTOMER_ID in (select CUSTOMERS.CUSTOMER_ID from customers where CUSTOMERS.CUSTOMER_FNAME = 'jim' and CUSTOMERS.CUSTOMER_LNAME = 'smith'); Side note: In SQL Developer, if you select all of your code, then hit shift + f7 it will do a decent job of formating your code. To preserve formating when posting here, add 4 spaces to the start of each line. In SQL Developer, select you code, then hit the tab key twice, each should append on 2 spaces.
Did you try just putting on a where clause? Like so: &lt;your select piece&gt; where customer.customer_id = 12346
Hi, I'm frustrated by these three things hardly any SQL tutorial gets right: 1. Improper explanations about indexes (if at all). This is why I wrote my own tutorial on indexing (http://use-the-index-luke.com/). However, assumes SQL knowledge. Indexing is a development task and must be properly explained in every SQL documentation that is aimed at developers. I'm explaining this in my presentation "[Indexes: The neglected performance all-rounder](http://use-the-index-luke.com/blog/2013-07/indexes-the-neglected-performance-all-rounder)". 2. No mention of bind parameters. String concatenation is just not they way to pass data to a database. SQL injection is one of the most dangerous security vulnerabilities of the past decades! Just because the tutorials show the inline values all the time. 3. Limited to a SQL-92 feature set. SQL did evolve quite a lot in the past 20 years. The two most important additions were window-functions (the OVER-clause) and Common Table Expressions (CTEs—the WITH clause). The first being the way more important one. If you do an SQL tutorial today without covering window functions, then you are doing it wrong. EDIT: ps.: I'm not actually learning SQL, but teaching it. I'm mostly engaged by companies to [tune their developers for high SQL performance](http://winand.at/). There I get an idea how they learned SQL and what they are missing. Hence the three points above. 
I'm with jcampbelly on the subject of JOINs. Inner JOIN, outer JOIN, LEFT JOIN, and etc.
Put the nextval inside the select from sale_vehicles eg. INSERT INTO vehicles SELECT vehicle_code_seq.NEXTVAL, CONCAT(make, CONCAT( ' ', model)) FROM sale_vehicles;
insert into vehicles select vehicle_code_seq.nextval, CONCAT(make, CONCAT( ' ', model)) from sale_vehicles;
It would be awesome if somebody made training videos to go along with the Schemaverse (http://schemaverse.com). The game is a great resource for learning SQL (I may be a bit bias admittedly), but it can be a bit intimidating - videos would help that.
The joes2pros YouTube videos on SQL taught me about everything I needed to know. Perfect explanations of "how" and "why". If they only named their lessons better, I'm sure they'd get a lot more hits, but they base their lesson names off of their own instructional books, so it is a little hard to find the ones that take you up the next step. If you are putting together instructional videos, please follow their format and you can't go wrong. 
 --Using Convert SELECT RIGHT('0' + REPLACE(RIGHT(CONVERT(VARCHAR(30), GETDATE()), 7), ' ', ''), 7) --Now SELECT RIGHT('0' + REPLACE(RIGHT(CONVERT(VARCHAR(30), DATEADD(HOUR, 12, GETDATE())), 7), ' ', ''), 7) --Opposite AM/PM --Using Datepart SELECT RIGHT('0' + CONVERT(VARCHAR(2), DATEPART(HOUR, GETDATE())), 2) + ':' + RIGHT('0' + CONVERT(VARCHAR(2), DATEPART(MINUTE, GETDATE())), 2) + CASE WHEN DATEPART(HOUR, GETDATE()) &gt; 12 THEN 'PM' ELSE 'AM' END 
I love how you mention how indexes are a development task. We were developing a Data Warehouse project and I was Admin (built environments, san, ect) on the the project most of the time. They couldn't get the project to run with full scale testing. They (developers) would send me a query that would be stuck. I would look at the query and they would be terrible. I would send them a revised version and they would do a small scale test and say it had better performance but would come back to me a few days later unable to do a full scale test. So I look at the execution plan of the query and "wtf". I quickly query the indexes of the tables on the ext, stage, fact and dims... They have primary keys but not a single clustered index or non-clustered indexes on any of the tables. These developers were contractors and I knew most were bad programmers but this was astounding. Stage tables with 456 million rows was one big heap. I tell them ,"why didn't you at least cluster on the primary key on this staging table?" "Thats the DBAs job!" I told them they were wrong. The developer should be creating clustered indexes based on how the data is being loaded. I told everyone they were wrong but the they said that they refused to do it. I ended up script a drop create skip to drop the primary keys and add them as clustered primary keys. Is that intelligent design? Certainly not, but hey, DBA shouldn't be creating clustered indexes.
Thank you!
Setup ODBC on the Ubuntu box. Setup link server via ODBC to MSSQL. Then just write an insert statement that pulls data from linked server.
Why didn't the insert statement work?
Correction...12c introduced a row limiting clause http://docs.oracle.com/database/121/SQLRF/statements_10002.htm#SQLRF55645 select * from customers fetch first 1 rows only; select * from customers fetch first 10 rows only; select * from customers fetch first 1 percent rows only; select * from customers fetch first 50 percent rows only;
How about this? SELECT c.customer_ID, c.customer_lname, c.customer_fname, c.customer_email, c.customer_address, c.customer_zipcode, c.customer_areacode, c.customer_phonenumber, p.item_inventory, p.item_barcode, p.item_numsold, p.item_price, p.dateof_sale, p.timeof_sale FROM pointofsale p, customers c WHERE c.customer_id = 12346; Gives you: CUSTOMER_ID CUSTOMER_LNAME CUSTOMER_FNAME CUSTOMER_EMAIL CUSTOMER_ADDRESS CUSTOMER_ZIPCODE CUSTOMER_AREACODE CUSTOMER_PHONENUMBER ITEM_INVENTORY ITEM_BARCODE ITEM_NUMSOLD ITEM_PRICE DATEOF_SALE TIMEOF_SALE ----------- -------------------- -------------------- ------------------------------ ---------------------------------------- ---------------- ----------------- -------------------- -------------- ------------------------------ ------------ ---------- ----------- ----------- 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 athletic 335A6A2 2 15 8/22/2014 3:00PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 asparagus 123GF6H 17 86 7/12/2014 1:00PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 cooking 321G47K 9 55 2/26/2014 12:40AM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 music 4476S7B 4 15 4/2/2014 5:20PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 film 996D34F 6 17 11/19/2014 7:00PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 clothing 8762ASJ 12 12 10/18/2014 4:00PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 fishing 4445ZZZ 8 15 1/15/2014 11:30AM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 buisness 678GG8H 45 29 8/9/2014 9:00AM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 furniture 334KJH3 22 62 3/3/2014 12:00PM 12346 smith jim blah@blah.com[2] 123 kentucky blvd 33458 215 11211145 medicene 334KJH9 33 33 1/1/2014 6:00PM 10 rows selected 
1900 records isn't many. Can you just do it programmatically?
Why didn't it work? What was the error message? Why not just use SSIS to import the data from a csv?
im not exactly well versed with sql to know how to do that, im being thrusted into it at work cause no one else is here to do it, and im so learning basically trial by fire. 
What I do in these cases is have Excel generate the insert statement. Though you don't want 1900 insert statements, you can union them all together and insert them once. So the excel formula for each row of your data would look something like: ="UNION SELECT '"&amp;A1&amp;"' col1, '"&amp;A2&amp;"' col2, '"&amp;A3&amp;"' col3..." Copying them all should result in something that looks like: UNION SELECT 'VAL1' col1, 'VAL2' col2, 'VAL3' col3... UNION SELECT 'VAL4' col1, 'VAL5' col2, 'VAL6' col3... UNION SELECT 'VAL7' col1, 'VAL8' col2, 'VAL9' col3... UNION SELECT 'VALA' col1, 'VALB' col2, 'VALC' col3... Then take out the first UNION, put an insert query around your big unioned query (using it as a subquery) and you should be left with something like: INSERT INTO TABLE SELECT * FROM ( SELECT 'VAL1' col1, 'VAL2' col2, 'VAL3' col3... UNION SELECT 'VAL4' col1, 'VAL5' col2, 'VAL6' col3... UNION SELECT 'VAL7' col1, 'VAL8' col2, 'VAL9' col3... UNION SELECT 'VALA' col1, 'VALB' col2, 'VALC' col3... ) sq
 INSERT INTO table_name (column1,column2,column3,...) VALUES (value1,value2,value3,...); If you use the (field name) before values you have to specify each column that you are inserting.
You can either use the Import/Export wizard that comes with SQL Server Management Studio, or use the an INSERT statement. Be aware that you're limited to 1,000 rows at a time when inserting values. The syntax for inserting multiple rows looks like this: INSERT INTO Table (Column1, Column2) VALUES (Value1, Value2) ,(Value1, Value2)
First I'd confirm that the report is actually calling the same stored procedure. Next I'd check the report to ensure that there aren't any filters or groupings on the report side. It sounds like the second step is probably your issue.
Dump My, install PG, and use an fdw.
Anytime! 
This x10000
Can I try to persuade you to not do this at all? Leave the formatting of your dates &amp; times to where they're being presented to the user - web page, reporting service, etc.
Dude. Seriously, seriously, seriously... Don't ball ache pointlessly hand creating insert statements or do even crazier shit with c#. Use the totally excellent import function in ssms. It generates a SSIS project and works very very well. 
i got it in there after some wrangling. Im a front end dev mostly who got thrown into a whirlwind of hell at my job. So Im learning on the fly and the crash course I got before this was nothing to brag about it. Im low on money otherwise Id pay for some high end videos to get a better idea. Otherwise the best options I have are here and good. 
&gt; Im low on money otherwise Id pay for some high end videos to get a better idea. Pluralsight, lynda.com and others all offer **free** trials (if not readily visible on their sites, you can pick up a coupon code from the podcasts they advertise on). But you shouldn't be paying out of pocket unless you're an independent consultant. If there's training you need, get the company to pay for it. A year subscription for Pluralsight is only $299 - if your boss won't pay $300 a year for you to get the skills needed to do your job, you need a new job.
&gt; if your boss won't pay $300 a year for you to get the skills needed to do your job, you need a new job. I would love to have this, but I dont work for a company who values their employees. Ive already started the job search, just running into issues cause what I like, PHP and dev work, arent used much at my company cause my boss likes DNN and C#. So as much as I have tried to pick up C# and SQL...more advanced SQL then since select commands and such, hasnt been easy. 
The SQL query is part of a larger powershell script that is gathering a lot of counts from different tables/databases. All I really need is the value though, and I'm instead finding myself having to parse through the unnecessary data with powershell to get just that value.
Man. I feel like I'm reading a post from myself 5 years ago. PM me anything you get stuck on - if I don't know the answer, I'll try to point you in the right direction. 
Wow excellent idea. I'll have to learn to play first though.
Oracle perspective: Select m.*, m1.count(1) From MainTable m left join (select Address, count(1) from maintable group by address) m1 on m1.address=m.address I think that should do it.
And to answer your question, you'd do this: *** UPDATE your_table SET AddressCount = c.cntr FROM your_table INNER JOIN (SELECT Address, COUNT(Address) cntr FROM your_table GROUP BY Address) c ON your_table.Address = c.Address ***
Use the weeezard Luke!
This throws a syntax error in MySQL at the FROM.
**2012 onwards** Select Left(Cast([datefield] as Time),5) If you don't mind keeping the seconds just cast it to time **Any Version** Select cast(Hour([datefield]) as varchar (2)) + ':' + Select cast(Hour([datefield]) as varchar (2))
Such a varying number based on company, location, project management responsibilities, etc. Anywhere in the $60-90k range would be my guess, but I'm not entirely what the Boise market is like.
Happy to help if you have any questions :)
There's a MySQL driver that works with SSIS. I think it's even included in newer versions.
This seems to give me a running count of addresses.
OK. Where I work, this is the standard set of settings we use. This should get you what you want. I can't say for sure which are necessary, but these should be sufficient. Set linesize 10000 set heading off set pagesize 0 set feedback off set verify off set echo off set timing off set trimspool on set colsep , set serveroutput off set termout off 
[roberthalf.com/salary-guides](http://www.roberthalf.com/salary-guides) DOE I'd guess like the other person said $60k-$90k, higher if you're in a expensive metro area (not sure about Boise perhaps a bit lower if there aren't a lot of companies that want BIAs).
Not sure if this is mentioned. Use bulk insert if it's from a csv. Let's u learn sql syntax and just as easy as the wizard. 
What industry or company type? If it is in the healthcare industry I would expect 40-60k starting wage for a BIA in the Boise area.
Apologies, using SQL Server syntax out of habit: *** UPDATE your_table yy JOIN ( SELECT Address, COUNT(Address) cntr FROM your_table GROUP BY Address) c ON your_table.Address = c.Address SET y.AddressCount = c.cntr ***
Having columns for all payment types doesn't scale well. One alternative is to store the extra columns in a separate key-value table. payment_types * id * name payments * id * payment_type_id * customer_id * order_id * (other columns common to all payment types) payment_extra * payment_id * key * value
Try it before you buy it. My approach is to implement each and figure out what it's like to write the queries and procedures you'll use to interact with these tables. Often, you'll find it requires a lot of extra effort and documentation to do one method over the other, or you may find one more intuitive when used in context. Also, go through the hypothetical where the boss asks you to add another payment type and consider what all is required to do that. 
Pro-tip: Wait for them to state a number. Assuming you are coming from a different career / background, your prior salary is not relevant. Tell them whatever the competitive market rate is will be fine. Tell them you'll want to consider the entire offer (i.e. pay, bonus, benefits, equity, whatever) before deciding. Ask for a day or two to consider their offer. Then, depending on how ecstatic you are, come back and tell them they are your first choice, you'd really love to come work for them, but really need 10% more to put all other options out of your mind.
what happens if you do select count(rows) from whatever group by table_name
I'm not sure how to make bcp handle those terminators. But I can say that I've successfully changed the terminators of pretty big textfiles with five rows of python. Not sure how well it performs beyond a million rows though. Do you think that could that solve your problem?
 SELECT u.unitid FROM Units AS u LEFT OUTER JOIN Data AS d ON d.unitid = u.unitid AND d.fetchtime &gt;= UNIX_TIMESTAMP(CURRENT_DATE) AND d.fetchtime &lt; UNIX_TIMESTAMP(CURRENT_DATE + INTERVAL 1 DAY) WHERE d.unitid IS NULL 
Is this just for BIAs? I thought overall healthcare paid better than average.
This is almost exactly what I do in Lancaster, Pennsylvania (not a major metro). I'm coming into it with eight years of experience, but I'm at 85K. What's your past experience and positions?
Perfect! This produces what I'm looking for when I spool it to a file (to get rid of the uncessesary "Connected to Oracle..." stuff). Thank you!
Boise area is going to be in a lower pay bracket than say, seattle.
I generally don't recommend this approach unless you have a highly variable schema that needs to be changed on the fly. Composing all the values into a single row becomes a chore, so if you're going to be doing that a lot, think twice. It is nice when you do need that flexibility and your handling all that of composing business in the front end. What I would suggest instead is either your first option, or if it makes sense, having a single `payment` table with only the columns that are common to all payment types then having a set of tables that contain the columns that are specific to the different types. This really helps when you want to do something like get a total of all payments. With this setup, all of that data is one table vs having to aggregate across several different tables. Data integrity needs to be carefully managed though so if you have a paypal payment, you have both `payment` record and a `paypalpayment` record. In practice I've never had much difficulty there but if you have a lot different ways for data to get into the database, it gets harder.
I had time, try this (works without any IIFs or CASEs): DECLARE @user int; WITH info AS ( SELECT mt.party, ea.*, d1.party FROM dbo.mydest AS mt JOIN dbo.partylocation AS d1 ON d1.PARTY = mt.party JOIN dbo.ElectronicAddress AS ea ON ea.LOCATION = d1.LOCATION WHERE mt.uniqueid = @user ), pvt AS ( SELECT party , Location , [1] AS contactPhone , [2] AS contactEmail , [5] AS contactFax FROM info PIVOT (MAX(Locator) FOR [Type] IN ([1], [2], [5])) AS pvt ) UPDATE dbo.mydest SET contactFax = p.contactFax , contactPhone = p.contactPhone , contactEmail = p.contactEmail FROM dbo.mydest AS mt JOIN pvt AS p ON mt.party = p.party WHERE mt.uniqueid = @user 
"Hilight five stars which have star id between 5000 and 15000, and have class 7. ***(Hint: don't try to do this with a single query at this point.)***"
It's because CONCAT implicitly converts strings-that-are-numbers into binary strings, and it sounds like your phone_num column is some character type containing only the numbers (hence the formatting query). MySQL Workbench doesn't display binary strings directly because they can contain zero-byte or null characters that can unexpectedly terminate the values being displayed. Instead, it shows the "BLOB" indicator, which you can click on to open the binary editor. The preferred solution is to explicitly cast the whole thing to a normal character type: SELECT CAST(CONCAT(CONCAT(CONCAT(SUBSTRING(phone_num,1,3),'-',SUBSTRING(phone_num,4,6)), '-', SUBSTRING(phone_num,7,10)), ' | ', phone_num) AS CHAR(32)) AS title FROM database.table LIMIT 100; There's also a setting in SQL Workbench that will change the way binary strings are displayed, showing you the actual result instead of the "BLOB" placeholder. Open up Workbench, go to Edit &gt; Preferences, and go to the SQL Queries tab. Look for a checkbox that says "Treat BINARY/VARBINARY as nonbinary character string" and check the box. This should cause your query results to show up how you want them in this case, but if you find yourself working with actual binary strings in the future, it could be dangerous to have that turned on.
try /r/SQLServer or Google. I've never done it. Sorry.
Thats the range I was thinking of as well. The smaller the city the less positions available for that job role. BIA's in Tucson make between 55-85k a year. It doesn't help that Boise has half the population that Tucson does (550k vs 214K). What does help is Idaho is along the internet backbone that runs from Seattle WA to the next big hub in Denver. This usually increases the amount of tech companies to put up shop in Boise. The hub that runs through AZ is through Pheonix which is home to a lot of tech companies. You don't see as many hosting and tech companies in Tucson. I would say 65K would be reasonable but I would always see what the company is pitching first.
I tried multiple queries of just INSERT INTO with each value but that doesn't work either,im a noob at sql so i know there must be something I'm missing?
Its not recommended to do this on the SQL server using CLR. That was my first answer until I started researching it. From the research I've done, its best to create an app (could be simple .net application) to pull the dataset that doesn't have spatial data and send the address to either [Bing maps](http://msdn.microsoft.com/en-us/library/ff701734.aspx) or [Google Maps](https://developers.google.com/maps/documentation/) which will then return the spatial data back to your application. You then update the corresponding row with the spatial data. For updates (an address changes but the spatial data isn't updated). You can use [CDC](http://msdn.microsoft.com/en-us/library/cc627369.aspx) in 2012 to track changes to the addresses and [query the cdc for address changes](http://sqlmag.com/sql-server-2012/tracking-changes-sql-server-2012). This will allow you to do partial updates to just the records that have changed within a given period (you can have it look for differences based on the CDC snapshots). I would also add a parameter to occasional allow for an entire update of the addresses tables and spacial data in the event that google or bing updates the spacial data on their end (as it may be erroneous) and recalculate all values on a quarterly basis.
How is category_paths.depth populated? Is that set to 1 (or 0) for all root categories? If so, that may be a way to get to them. Normally with something like this I'd have a single table, `categories`, which holds the id, name, parent_id (self referential to the same table), [...other columns...]. A root category would have a NULL parent_id, or there would be some master progenitor category whose id was 0 or 1, which would be set as parent_id on root categories.
depth is the distance between the ancestor and the descendant. Each category has a self-referencing record like so: INSERT INTO category_paths VALUES (1, 1, 0); You're describing the Adjacency List which has its own problems but might work. [Edit]Actually it wouldn't work because breadcrumbs and querying the entire tree is a pain with Adjacency Lists.
I've used this in the past, but it really requires a Bing spatial license if you want to do more than a couple hundred: [SSIS Batch Geocoder](http://ssisbatchgeocoder.codeplex.com/releases/view/66866) There's plenty of examples libraries out there in various languages to help you along as well: [geopy](https://code.google.com/p/geopy/) [Generic C# GeoCoding API](https://code.google.com/p/geocoding-net/) As /u/Thriven mentioned though, you're best off doing it outside of SQL Server.
Your indexes site isn't loading for me. Can you confirm it's running?
I'm actually working on creating a SSIS package to geocode address info we have and ensure the address is valid (for mailing purposes) using the Bing API. I'll probably be done by Monday but there are plenty of blogs online that describe how to do it. It will also be used for Reporting so all the data returned from Bing will be stored in table (ie. I won't be doing it on the fly). FYI there is a limit on the free key for the Bing API so you may need the paid one (the company I'm at pays so I'm in the clear). Edit: just remembered, if you already have Latitude and Longitude values, you can convert to spatial values for ssrs on the fly within your SQL code. 