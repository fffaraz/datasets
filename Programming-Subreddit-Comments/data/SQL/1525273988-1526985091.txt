I think that's a memory issue. Either expand the memory you have or figure out which is a greater resource hog. I'd imagine mysql uses more resources but I could be wrong.
 &gt; PRINT is only giving me the last two values which is by design? Are all the rows actually stored in those variables? Variables are not arrays in SQL, variables are 1:1. You need tables to store arrays of data essentially. There are other tricks and such, but for our purposes, variables are 1:1 and you can only store 1 value there. In SQL you should think in sets and batches, not in individual rows. We call this RBAR, row by agonizing row, because it is very inefficient and causes problems later. A lot of people run into this mental block when they switch to SQL because they are used to doing things row by row. SQL is also declaritive. You are telling it, I want the book from the room. (SELECT BOOK FROM ROOM) The database engine is going to figure out, am I going to scan the table? Should I use indexes? Do I need to calculate stats? So you tell it what you want and it figures out the best way to do it. (* Not always true, but for basics, we'll say that.) If you were to do: SELECT Column1, Column2 FROM TABLE ( I am assuming you will replace Column1... with the actual names of the columns as you depicted) What do you get back? It should be many rows of the columns you picked. SQL is grabbing all those rows of the columns you specified in a single transaction. If you were to loop through the table and print it out, you can achieve a similar outcome but you will execute a whole transaction for each row. You also need to order and come up with logic regarding the loop, typically cursors are good at this but I would stay away from that until you are more advanced. Tables have no REAL order to the data. (It does, but we are keeping things simple for basics. Understanding the page, leaf, and clustered index structure is a later learning point.) So if you said SELECT BOOK FROM TABLE, you can't guarantee it will come back alphabetical, it could come up totally random. You need to order your statement if the order matters. Does this help?
Figures. My current machine is a Lenovo Thinkpad 450s with 8GB of DDR3 RAM. The new Lenovo P510 Thinkstation has a whopping 64GB of memory in comparison, but it's still en route. I can also expand that to 256GB later on..so now I know what's bottlenecking me.
Now try doing this dynamically. It’s a fun project I had. 
It's a couple of dozen rows and growing. Trying to write a script that will detect ALL the clients and dump that data into another table along with other column values.
Thanks, nice and clear. Not near my rig; how does this compare in performance to SUM(CASE WHEN... ?
ETL Extract | Transform | Load I'm sure you've heard that term before. Extract - You are retrieving data from X. Transform - Manipulating the data you have. Maybe you need to trim the white space, change a varchar to an int, or something. Load - Moves data into Y from X. ETL is a common order. You will see every combination possible. So you could manipulate the data as you pull it. SELECT CAST(BOOK as INT) FROM TABLE Or perhaps you want to manipulate the data after it's been loaded. UPDATE TABLE SET BOOK = 'Lost in Space' WHERE BookID = 1 Or maybe you want to pull the data then perform operations on it and then load it into your destination. When you do the above I mentioned, you either use a physical table you drop / create or truncate out then load, or you use a temp table, or you use a table variable. They all have their place and time, the right tool for the right job. In your specific example, I would work on converting the data as you pull it like in my first example. The SELECT piece is changing the data you are pulling from the table. From that, the changed data can now go into the table you want to load into. Few use cases: Staging table - LOTS OF DATA. Let's say 1 million + rows. Temp Table - 1,000 - 1 million rows Table Variable - Less than 1,000 rows. Those are not hard guidelines, but just something basic to get you started. 
That is indeed a start! Now I can start googling.
&gt; Is COUNT only useful for one or all columns? it works on only a single expression &gt; If I can't use it for less than * and more than 1 column, is there any way around this? if you could give a specific example, please because i don't see why you can't just do this -- SELECT COUNT(col1) AS col1_count , COUNT(col2) AS col2_count FROM ... and the above makes sense only if you have NULLs in either column 
1. Promise your boss a detailed analysis of your business. 2. Panic for 2 days 3. Stay up for 24 hours cramming SQL into your head until you know it. Its the only way to learn.
What are you trying to end up with? 
To expand on my example above, I want to put in a username insert a row for each ClinetID and coresponding CompanyID: UserID RoleID CompanyID ClientID EmployeeID OverrideDefault AllowEETimeClock ClientAdmin ApproveAllPTO TimeSheetOverride SwipeClockLogin NewsItems NoSecurity AllowTimeSheetRates sampleadmin ClientAll 1 B14 False False False False False NULL NULL False True sampleadmin ClientAll 1 B15 False False False False False NULL NULL False True sampleadmin ClientAll 1 B10 False False False False False NULL NULL False True sampleadmin ClientAll 1 B08 False False False False False NULL NULL False True sampleadmin ClientAll 1 B05 False False False False False NULL NULL False True sampleadmin ClientAll 2 C04 False False False False False NULL NULL False True sampleadmin ClientAll 3 D12 False False False False False NULL NULL False True The sample above gives Mr Sampleadmin access to all of those clients. I can pull all this data but I'm not sure on the next step. I can pull all the usernames: SELECT DISTINCT UserID FROM dbo.UserRoleClientEmployeeAssignment WHERE UserID LIKE '%admin%' I can pull all the clients: SELECT ClientID,CompanyID FROM Clients I already know the other values I need to insert, they don't change. 
What connects userid, clientid and companyid? You could use a join, but it needs to join on something
Thanks! Could you give a simple example? I'm looking into this stuff and the words you used but it's a bit hard to wrap my head around.
So you'll need to join. You could join on 1=1, and that will join everything to everything. You can also put values after your select. Try this to see what I mean: Select 'Hey I'm just text', ClientId From Clients Combine this and join and select into, and that should be all the tools you need. 
I'm also teaching myself SQL, I'd love any help I can get :)
Kind of depends on the environment. If these are relatively small tables, then join the two tables together and use CASE statements. Avoid CTEs because you lose your indexes (assuming they exist in the first place). If these tables are in hadoop and saved as TEXTFILE, then do a diff on the files. Lastly, do a search for "&lt;database&gt; compare two large tables" and see what comes up.
The except clause is what your looking for.
Two options off the top of my head (I'm assuming MS SQL Server since you mentioned CDC): * If you have SQL Server 2016+, make it a temporal table. * If you don't have SQL Server 2016+, or don't want to go the temporal route, put an `AFTER UPDATE` trigger on the table which writes the changes to a second table. You may have to do some extra work in the trigger to detect which fields changed. In a high-volume system, this may cause a performance issue.
&gt; Avoid CTEs because you lose your indexes Can you explain this? I've not heard of this before.
How I've done is essentially to use except to find the rows that have differences and dumping those primary keys in a temp table (though if you have something like a modified date you can use that for the same purpose) Then basically iterate through that list of primary keys column by column to find differences and logging (i.e. literally inner join a to b where coalesce(TABLE_A.A,)&lt;&gt;coalesce(TABLE_B.A) and logging the results). But with dynamic SQL, so basically pulling the list of columns from sys.columns (excluding the primary key et al) and looping through the list subbing in a column at a time. It's not super elegant (to say the least), but it works.
Have a look at [`Test-DbaBackup`](https://dbatools.io/functions/test-dbalastbackup/) from the dbatools PowerShell module. Run that, and write the results to someplace for your dashboard to read. You may be able to pull that into [dbareports](https://dbareports.io) but I admit I haven't looked at that much. 
 WITH master AS ( SELECT a.pk AS pk FROM table_a AS table_a UNION SELECT b.pk AS pk FROM table_b AS table_b ) , my_results AS ( SELECT master.pk AS master_pk , CASE WHEN table_a.pk IS NOT NULL THEN 1 ELSE 0 END AS table_a_pk , CASE WHEN table_b.pk IS NOT NULL THEN 1 ELSE 0 END AS table_b_pk , CASE WHEN IFNULL(table_a.my_column,-1) != IFNULL(table_b.my_column,-1) THEN 1 ELSE 0 END AS my_column_mismatch LEFT JOIN table_a AS table_a ON table_a.pk = master.pk LEFT JOIN table_b AS table_b ON table_b.pk = master.pk ) SELECT ÇOUNT(my_results.master_pk) AS total_pk_count , SUM(my_results.table_a_pk) AS pk_count_in_table_a , SUM(my_results.table_b_pk) AS pk_count_in_table_b , SUM(my_results.my_column_mismatch) AS my_column_mismatch_count FROM my_results AS my_results Now we got our CTE. The derived tables in this CTE (master, my_result) will not have indexes. This means the second query in the CTE will run without an index. This is potentally very bad. CTE's are great for ad hoc queries. CTEs are bad for production code. A better way to do this would to be split this CTE up into atomic operations in an ETL script. This approach may or may not work depending on your environment, and size of the tables. Anywho, that's that. Use an ETL. After each CREATE TABLE statement in your ETL, create an index. 
Learn about joins first if you haven't already (and see the join with group by example) as that will be the crux of it. For example Revenue by product category, you would join OrderDetail, Product, and Category, and select CategoryName, sum(Price * Quantity) and do the group by on CategoryName. 
Start with the Orders table. Join the other tables that have the required information. Take Price * Quantity grouped by OrderId and ProductId in an order via subquery. Aggregate above this to get total sales. SELECT CategoryName, SUM(line_item_cost) AS total_sales FROM ( SELECT o.OrderID, od.ProductID, c.CategoryName, (p.Price * od.Quantity) AS line_item_cost FROM Orders o INNER JOIN OrderDetails od ON o.OrderID = od.OrderID INNER JOIN Products p ON od.ProductID = p.ProductID INNER JOIN Categories c ON c.CategoryID = p.CategoryID GROUP BY od.OrderID, od.ProductID, c.CategoryName ) GROUP BY CategoryName ORDER BY total_sales DESC
Hive is a different beast altogether. OP is asking about SQL Server. If I see a query using a CTE causing a problem, I'll look to either fix it till it's better or do a full rewrite. But I won't make a blanket "don't use CTEs" statement.
Your target table has a unique constraint. At least one of the new records violates that. First find out which column had the unique constraint, then figure out which value is duplicate.
SELECT table_a.* , CASE WHEN (code_number=10 AND product_available=0) THEN only_seven.value END as new_value FROM table_a JOIN ( SELECT * FROM table_a WHERE code_number=7 ) only_seven ON only_seven.id=table_a.id
Im not familiar with Oracle syntax but if all the results are the same then SELECT DISTINCT in the subquery... If not, can you rank the results by a criteria (like time stamp) and pull the 1st one in the ranking?
It's safe unless you go out of your way to use it to build strings of sql commands like this: exec 'insert into myTable(name) values (''' + @name + ''');'
Thanks, the error had to do with not having one of the selections as a primary key.
Not true. CTEs don't stop index use, they're just like a sub query or derived view. I advise not using too many in procs, and that they not be too deep - the reads are all parallel so performance can be negatively impacted if a couple of your CTEs are big, in that case I advice indexed temp tables. Recently took a proc that had 11 CTEs, 2 of which were &gt;200k rows, that was taking 45 minutes at night (used in ETL process), converted them all to temp tables - result, 3 minutes now. The temp tables are created serially, too may CTEs is asking SQL to act like a one-armed paperhanger. CTEs have a place, just like triggers and scalar functions, but I make my developers explain why they need them. Particularly inline scale functions (rbar ugh). 
The EXCEPT operator is your friend!
Sweet! That's what I thought but I'm inexperienced with SQL injection. Thank you for confirming.
Revenue by Product Category select CategoryName, sum(Price) as Revenue from Categories c join Products p on p.CategoryID = c.CategoryID join OrderDetails od on od.ProductID = p.ProductID group by CategoryName 
You may be safe from SQL injection but if your user puts more than a few words/strange characters into a parameter your passing in the url, you might have other issues...
I adore temporal tables.
I've seen a few usage patterns where CTEs lead to terrible performance, but generally they work great. The alternative to using CTEs is to materialize them into a temporary table. If your server is already short on disk resources this can be a bad idea. And materializing the table creates quite a bit more code and more possibility for an error. 
To each their own I suppose. I've done enough migrations in heterogeneous environments and optimized enough bad code to cringe when I see CTEs in production code.
If not too many rows change you can just save the whole row.
Don't get to use PIVOT (and UNPIVOT) often, but when I do, it saves time and the code is smaller.
I haven't run any tests but from my experience the performance is good. I use PIVOT when I can.
Do you establish the shortcut name within the INNER JOIN lines? This is what I got for employees. SELECT employeeID, LastName, FirstName, SUM(Line_item_Cost) AS total_Sales FROM ( select orders.orderID, orderdetails.productID, categories.CategoryName, products.productname, employees.EmployeeID, employees.LastName, employees.FirstName, (products.price * orderdetails.Quantity) as line_item_cost from Orders INNER JOIN OrderDetails on orders.orderID = orderdetails.orderID INNER JOIN Products On orderdetails.productID = products.productID INNER JOIN Categories on categories.CategoryID = products.CategoryID INNER JOIN Employees on employees.EmployeeID = orders.employeeID where CategoryName = "Beverages" Group by Orderdetails.orderID, Orderdetails.productID, Categories.categoryname ) GROUP BY employeeID ORDER BY total_Sales DESC
Almost always yes and i mostly use case when or use any BI.
Thanks. Ill see what it does back at work if its any better :)
Could you explain how you would do this in a temporal table? Trying to learn more of the usage.
Yeah anticipating this. My understanding is that the browser handles strange chars, so passing the variable to the URL SQL side isn't the place to handle them (just send spaces as spaces, etc). But yet to experiment with that side of it...
FOR XML PATH('') will not handle all characters correctly. Should use FOR XML PATH,TYPE).value('.','nvarchar(max)') instead, unless you know your data well.
Thanks! The arrays could have multiple times and multiple professors. I know I need to have multiple tables for those, but I'm not sure how I would go about doing that.
Try this select * from tableA A join tableB B on b.id=a.id where b.check_against !='True';
right; that's why you need to create one table with all the instructors in it ---- an instructor can teach more than one course or multiple instructors could teach a single course but you'll only want one line for each instructor some like instructorID, firstname, lastname, title, email, primary then you'll want an InstructorCourse table; this will have InstructorId and courseID to relate courses to their instructors. in this table you'll also want a course ScheduleID so you can relate individual courses to instructors and then to their assigned time slots ScheduleID, InstructorID, CourseID then in your schedule table you'll want scheduleID, courseID, dayweek, time wtc so you could relate instructor to a course and a specific time slot, as well as return the schedule for the course without having to join to the instructor table unless you wanted that data too you'll want unique identifiers between course, schedule, and instructor so you could have multiple instructors teaching the same class at the same time if you needed to --- building the tables like this allows you to have one line for each instructor, course time, and course but you can build the relationships from those tables so that you can have multiple combinations without impacting performance or space
Just insert it into the table like this, or a variation thereof: INSERT INTO Table VALUES (1,'Bill'), (2,'Joe'), etc
this is usually done in the application level, maybe you will need to know exactly when was the change, or who made the change, etc.
You're close but you can't have a primary key in courses on crn and also have insID, schID in the table --- what happens if you have a class that has more than one schedule or instructor? your insert statement will fail because you are violating the PK by creating a second row with the same CRN but different values in insID and schID i am actually headed into the office in about 15 minutes, once i get in i'll send you over the tables, i'm on mobile and it's a bitch to write sql haha
one way to avoid SQL injection would be to capture a second hidden variable from the built in fields (such as @UserID from globals User!UserID) this would prevent a 'billy') drop table dbo.students scenario as your insert would take the second variable as well ie insert into dbo.students values(@userInput, @UserID)
No mention this is only available for SQL Server Enterprise and above, not included in Standard. A workaround for Standard for easier data driven (send only when there is data) is to adjust the Job step to include a IF EXISTS() around the exec subscription. Its not really hard to adjust this to be more complicated but it will be gone if you make any adjustment in the subscription.
The first thing I would advise is to review the security permissions for the account that your app uses to connect to the database. Go with the lowest privileges possible, like read/write/execute. That would cause a DDL statement to fail. Second, I would sanitize your incoming parameters to be safe. Not sure what other platforms have, but in MS SQL you can use The QUOTENAME() function to prevent the apostrophe in the variable from breaking out of your dynamic sql.
What do you mean by shortcuts? Filtering? The inner join will only return results for both tables where the condition is true in the ON clause. 
now i'm sold even more than i was in my original comment
I am on 2012 standard. I have never used `AFTER UPDATE` but I will research that now. Thank you for the tips.
Thanks! I've seen both used, but didn't know there was a difference.
You could use UNION with a series of SELECT DISTINCT queries on each column.
If you want only combinations that appear in a given row: select distinct product_id, product_category, product_subcategory from table; If you want every possible combination of product_id, product_category and product_subcategory using the values for each column currently in the table: with x as (select distinct product_id from table where product_id is not null), y as (select distinct product_category from table where product_category is not null), z as (select distinct product_subcategory from table where product_subcategory is not null) select x.product_id, y.product_category, z.product_subcategory from x, y, z; Latter syntax may be Postgres specific.
Oh sorry, by shortcuts I meant being able to switch titles \(orderdetails.productID to od.productID\). I was just wondering when you establish this within the sequence. 
The `car_id`s `'3010'` and `'5004'` don't seem to exist in the `car` table.
Ye the guys on imgur helped me open my mind to figuring that out. Thats exactly what it was :D
well for one thing, you shouldn't post a camera picture of a computer screen and expect us to read the text post the actual text, luke!! use the copy/paste force!! 
:/ didnt sleep man I forgot I can screenshot ;-; was on a coffee high ...it was fading brother
That's a key as many places don't have Enterprise as I have found. At the place I'm currently at, their SQL developer has created a complex web or procedures to handle this along with more advanced features. I find it nearly impossible to debug issues since there is no documentation. 
Crap im back again im unsure how to make my stock table columns...like what does it need...
Oh sorry im making a car dealership and I have a stock table but im sure what it needs. Can I pm you? This is like my last table left :D
Probably not the _greatest_ way, but you can just select all rows where the user is NOT givenUser, then union the rows where the user IS givenUser... SELECT user, entry , is_save FROM Table1 WHERE 1 = 1 AND user != 'bill' UNION ALL Select user, entry, is_save FROM Table1 WHERE 1=1 AND user = 'bill' AND is_save != 1 
Use a case statement in the where clause. Something like : WHERE (CASE WHEN columna &gt;= 1 and columnb &lt;&gt; 2 then 1 else 0 END) = 1
The join probably works in most RDBMSes but I'd still do it the ANSI-92 way: FROM x CROSS JOIN y CROSS JOIN z
Constraint name is missed, should be smth like: ...ADD CONSTRAINT any_name LENGTH CHECK... You can find explanations and examples [here](https://docs.oracle.com/cd/B19306_01/server.102/b14200/clauses002.htm#sthref2914).
You could get online training much cheaper imo then take the exam. 
I don't really see how the EXCEPT clause would work here since I want to have an old value and new value column. If a value just gets modified, the except clause would give me a row for the old value and a row for the new value. Perhaps i'm missing something here?
Can you give me an example of what you were revoking from pg_catalog.pg_namespace et al.? Just loking through it right now and I'm unsure of what I'd be revoking. Trying to find help internally and stack overflow, but nothing has worked thus far. 
I can do you a special deal on a certificate m8. €50 - hard and digital copy with hologram Mail me - bigspicylove@askjeeves.net or alternatively thebrownalansugar@lycos.com
If I understand correctly your if condition, then you want all entries for all users, except for the one provided as parameter. And for this one user you only want entries with is_save = 1? Then this should work: SELECT user, entry FROM table WHERE (entry_time = date AND user = @givenUser AND NOT is_save = 1) OR (entry_time = date and user &lt;&gt; @givenUser) 
Nope, you're right, I missed the requirement to know the old and new values. You'd have to do a join of two separate sub queries (a except b) join (b except a). And that would be costly. If you can put triggers on tables, that might be the best. We've done that before when we had users claiming the data was phantom changing. Any update triggered an insert into an archive table. Then we could compare after the fact.
What about using Windows task scheduler?
I've always kept all SQL in files, which should be simpler than reading the desired queries from a table (using a query to fetch the queries). The mysql client allows you to read sql from a file: `mysql --option=xyz --other-option=abc &lt; command_01.sql` `mysql --option=xyz --other-option=abc &lt; command_02.sql` Or some variant of that.
MCSA: SQL Server 2012/2014 is a Microsoft certification. Anyone can get it by passing 3 online Microsoft certification exams (which cost about $150-200 each to take) - on on DBA Admin, on one SQL querying, and one basically on SSIS. In fact, I doubt your school gives you the cert - it's probably just prep for the certs that you then take online from Microsoft (and, if you do take the course, I'd make sure the certification exam cost is part of the class - it might not be). Anyway you can certain just study yourself via microsoft virtual academy and books and take the exams - it's what I did, though I already had work experience with SQL server. They also have MSCA:2016 SQL Server certs that are just 2 exams, but focused on a particular area (i.e. DB development, admin, or BI instead of one cert covering all three).
Pretty sure it was "revoke all" on all of those things.
I'm not understanding your question of what I was revoking. I said I did "revoke all".
Those subselects are killing it. It has to run all those queries for every row returned by the main query. I'm on mobile, but it should be easy to move those into multiple left joins to geoname, no?
I think you don't need pivot function but LISTAGG function. As I understand you want to list all persons per union_id, not to count them. Try this: SELECT UNION_ID, LISTAGG(name, ', ') WITHIN GROUP (ORDER BY UNION_ID DESC) "Name_Listing" FROM tab1 group by UNION_ID order by 1; 
You can do this on different ways, depending what is your logic behind it. Limit to 1 row select col_name from table where rownum=1; use where conditions if you need to select specific row select col_name from table where col_name2='some_string'; As I don't know what is in the table and what is expected can't help you further.
I assume you mean: CASE WHEN t1."country"&lt;&gt;'' THEN (SELECT... In which case I was looking at what it was doing to see if I could figure it out. I'm only about 4 days into learning SQL so I'm not too hopeful of getting this sussed out on my own.
Maybe try to bring up what I mentioned to the person who wrote the query. I'm at work but if youre still stuck and no one's chimed in by the time I get off, I can help when I get home.
That person has moved on, 5 years ago. I'll thank you for any help you can provide.
Ah, bad luck. No worries, though. I've set a reminder on my phone to check this thread when I get off work. Happy to help.
SSIS package maybe 
Not a great article, full of bad practice. Don't use SQL reserved key words as column names. Don't store months as strings. We have the date type, stick the dates to the 1st of the month and just ignore the day if you have to. In my experience pivots can often introduce performance issues, its got to rotate the entire dataset and when that's a lot of data it gets expensive. Personally I believe pivot operations should be avoided in SQL and done primarily in the app code if at all possible. The author specifically mentions excel as the destination for the data. Excel is perfectly capable of pivoting, chuck the data into a hidden tab and have some formula on another sheet to pivot and display. The users machines have a processor, use it. The authors other articles are similarly bad - http://jackworthen.com/2016/02/29/creating-a-function-to-determine-if-a-specified-year-is-a-leap-year/ Why pass around years as ints into functions when you can do a nice quick case statement on the year datepart? 
I will try test this now. Fwiw; I'm using MySQL. Thanks!
Yes.
SQL Server Express Edition is free to use anywhere, but has limitations. For your own development/testing, Developer Edition is free to use. You'll need Express Edition or another production-licensed edition if you go live with your project.
If you're looking to learn SQL, any SQL, I would recommend SQLite. Python has SQLite support right out of the box. "import sqlite3" You can download something like DB Browser for SQLite.
Which is better? Sqlite or sql express?
Depending on what you're doing and how efficiently you're storing it (good data type selection, normalization, etc.), 10GB can be quite a bit of information. By the time you outgrow it, you're ready to pay to license a full version or host on Azure.
Sorry hit post too soon. Sqlite is very easy to setup and requires no additional configuration. It's not a database per se, but a database contianer. In other words, you have a file, full stop. It works great for anything where there is minimal concurrency. There are no licensing restrictions as well. Because of that, it's perhaps the most widely used database in the world. However, if your ultimate idea is to have 200+ people doing simulataneous read/writes to the database, it's going to have a bad time. Express is good for developing if you want to eventually purchase the Sql server for your service but are merely developing now. It limits your total database size and I believe limits commercial use so if you plan on just playing around with it to build a multi concurrent app, it'll be fine. You should also consider pgsql. It's a great, free database engine. So no licence limitations for using it. You get the full thing right odd the bat. I'd probably recommend starting with pgsql because it's strongly typed. Basically, you have data types that columns can be and you have to insert that data type in pgsql, sql server, etc. In sqlite, it's like a recommendation but you can put text in an integer and so on. 
Pure learning. SQLite3. If you're learning and want to work in the Microsoft ecosystem, SQL Server Express. 1) Installation. SQLite3 comes installed with Pyhton. SQL Server requires installation, but that is easy if you're a Windows or Ubuntu. 2) Administration. You're going to have to do some minor setup work to get a SQL Server user up and going. Not a big deal, but that's less time learning SQL. 3) Miscellaneous. SQLite has some quirks like foreign keys are disabled by default unless you submit "PRAGMA foreign_keys = ON;" and SQLite is all about surrogate keys. All implementations have quirks. tldr If you value the Microsoft Stack, SQL Server Express otherwise SQLite. 2)
That entirely depends on what you're doing, but SQL Express is a proper RDBMS (just with licensing/size limitations), while SQLite is basically very clever flat files. For local storage for a single app, for example, SQLite is excellent. For your main backend database, not so much. SQLite is more comparable to LocalDB
Hey quick off topic question. I had to create a database for my semester project in university. I’m done and everything works but he says I need to turn in a LOG text file of everything. Where do you go in MySQL to where that text is that tells you all that stuff so I can copy paste it onto a text file? 
&gt; Express is good for developing if you want to eventually purchase the Sql server for your service but are merely developing now I'd recommend SQL Server Developer, in that situation: It's fully featured, just not licensed for production, and is literally meant for this use case Express is more for small admin/hobby/personal apps etc than for development: where you're going to use Express once the project is live.
There was a good Talk Python To Me a few weeks ago about SQLite being used for more than a starter DB, for those who might be interested in it: https://pythonbytes.fm/episodes/show/60/don-t-dismiss-sqlite-as-just-a-starter-db Tl;dr If your needs aren't concurrency or strongly typed SQL, then it's probably sufficient. Web backend for static content? Can be surprisingly useful. 
!RemindMe 6 hours
Express has a 10gb limit and many more limitations, like lack of SQL Agent GUI and many lost features. It's still very beefy but I'd consider between PostgreSQL or MySQL for an enterprise size free DB client. (Probably lean to Post honestly.)
 I'll be that guy: move to Linux, haha. Cron is so good for this kind of thing. That said, there's probably a Windows equivalent (running jobs at various intervals). You could write a .bat file that just runs against mysql cmd tools. I'll have to do some research on this. 
Something like this should speed up your query. I've done the first two but I'm out of time. You should be able to see what I did and emulate it for the other 3 sub selects. CREATE TABLE geonamesAandP AS SELECT t1."name" ,t1."geonameid" ,t1."asciiname" ,t1."latitude" ,t1."longitude" ,t1."fclass" ,t1."country" ,t1."cc2" ,(CASE WHEN t1."country"&lt;&gt;'' THEN countrygeonameid."country" ELSE NULL END) AS "country geonameid" -- note that admin unit codes are only unique within the containing admin unit -- the feature code test is because sometimes admin1 is set to 00 when the record concerns a country ,(CASE WHEN t1."admin1"&lt;&gt;'' AND t1."fcode"&lt;&gt;'PCLI' THEN admin2geonameid.geonameid ELSE NULL END) AS "admin1 geonameid" ,(CASE WHEN t1."admin2"&lt;&gt;'' THEN (SELECT t2.geonameid FROM geoname AS t2 WHERE t2."country" = t1."country" AND t2."admin1" = t1."admin1" AND t2."fcode"='ADM2' AND t2."admin2" = t1."admin2") ELSE NULL END) AS "admin2 geonameid" ,(CASE WHEN t1."admin3"&lt;&gt;'' THEN (SELECT t2.geonameid FROM geoname AS t2 WHERE t2."country" = t1."country" AND t2."admin1" = t1."admin1" AND t2."admin2" = t1."admin2" AND t2."fcode"='ADM3' AND t2."admin3" = t1."admin3") ELSE NULL END) AS "admin3 geonameid" ,(CASE WHEN t1."admin4"&lt;&gt;'' THEN (SELECT t2.geonameid FROM geoname AS t2 WHERE t2."country" = t1."country" AND t2."admin1" = t1."admin1" AND t2."admin2" = t1."admin2" AND t2."admin3" = t1."admin3" AND t2."fcode"='ADM4' AND t2."admin4" = t1."admin4") ELSE NULL END) AS "admin4 geonameid" FROM geoname AS t1 LEFT JOIN ( SELECT geonameid , country FROM geoname WHERE fcode = 'PCLI' ) AS countrygeonameid ON countrygeonameid.geonameid = t1.geonameid AND countrygeonameid.country = t1.country LEFT JOIN ( SELECT geonameid , country , admin1 FROM geoname WHERE fcode ='ADM1' ) AS admin2geonameid ON admin2geonameid.geonameid = t1.geonameid AND admin2geonameid.admin1 = t1.admin1 WHERE t1."fclass"='A' OR t1."fclass"='P';
Microsoft SQL Server Developer Edition is free in development and testing. It is equivalent of Enterprise Edition (complete feature set). But i
Go with SQL Server Development Edition instead of both.
What RDBMS? Different platforms will have different methods/functions that can be used here. Also: this is where having a date table may come in handy.
[DESCRIBE](https://docs.oracle.com/javadb/10.8.3.0/tools/rtoolsijcomrefdescribe.html)
If I try a subset on the parts you where able to finish I get a bunch of errors I'm not knowledgeable enough to resolve. CREATE TABLE geonamesAandPTEST AS SELECT t1."name" ,t1."geonameid" ,t1."asciiname" ,t1."latitude" ,t1."longitude" ,t1."fclass" ,t1."country" ,t1."cc2" ,(CASE WHEN t1."country"&lt;&gt;'' THEN countrygeonameid."country" ELSE NULL END) AS "country geonameid" LEFT JOIN ( SELECT geonameid , country FROM geoname WHERE fcode = 'PCLI' ) AS countrygeonameid ON countrygeonameid.geonameid = t1.geonameid AND countrygeonameid.country = t1.country FROM geoname AS t1 WHERE t1."fclass"='A' OR t1."fclass"='P'; First I get a error if I leave out the left join: FROM geoname AS t1 WHERE t1."fclass"='A' OR t1."fclass"='P' &gt; ERROR: missing FROM-clause entry for table "countrygeonameid" LINE 3: ,(CASE WHEN t1."country"&lt;&gt;'' THEN countrygeonameid."country"... If I put in the left join: LEFT JOIN ( SELECT geonameid , country FROM geoname WHERE fcode = 'PCLI' ) AS countrygeonameid ON countrygeonameid.geonameid = t1.geonameid AND countrygeonameid.country = t1.country FROM geoname AS t1 WHERE t1."fclass"='A' OR t1."fclass"='P' &gt; ERROR: syntax error at or near "LEFT" LINE 4: LEFT JOIN I'm also not sure how this works: countrygeonameid."country" in CASE WHEN t1."country"&lt;&gt;'' THEN countrygeonameid."country" ELSE NULL END can anyone clarify? Thank you for your help! 
This isn't true anymore. SQL 2016 - every version has every feature. They now limit RAM and disk size based on edition. SQL 2016 was also when SQL Management studio was split out. It's completely free and backwards compatible. It's labeled SQL management studio 17. All that being said, you can use Developer edition which has no limits on RAM/disk usage for free. https://docs.microsoft.com/en-us/sql/sql-server/editions-and-components-of-sql-server-2016?view=sql-server-2017 - scroll down to Scale Limits to see version comparisons and resource limits 
I appreciate your input but I am unfamiliar with ij. Is there anything that can be done directly from Oracle SQL Developer or PL/SQL Developer?
Try this: SELECT t1.name , t1.geonameid , t1.asciiname , t1.latitude , t1.longitude , t1.fclass , t1.country , t1.cc2 , CASE WHEN t1.country!='' THEN t2.geonameid ELSE NULL END AS "country geonameid" , CASE WHEN t1.admin1!='' AND t1.fcode='PCLI' THEN a1.geonameid ELSE NULL END AS "admin1 geonameid" , CASE WHEN t1.admin2!='' THEN a2.geonameid ELSE NULL END AS "admin2 geonameid" , CASE WHEN t1.admin3!='' THEN a3.geonameid ELSE NULL END AS "admin3 geonameid" FROM geoname t1 LEFT JOIN geoname t2 ON t1.country=t2.country AND t2.fcode='PCLI' LEFT JOIN geoname a1 ON t1.country=a1.country AND a1.fcode='ADM1' t1.admin1=a1.admin1 LEFT JOIN geoname a2 ON t1.country=a2.country AND t1.admin1=a2.admin1 AND a2.fcode='ADM2' AND t1.admin2=a2.admin2 LEFT JOIN geoname a3 ON t1.country=a3.country AND t1.admin1=a3.admin1 AND t1.admin2=a3.admin2 AND a3.fcode='ADM3' AND t1.admin3=a3.admin3 LEFT JOIN geoname a4 ON t1.country=a4.country AND t1.admin1=a4.admin1 AND t1.admin2=a4.admin2 AND t1.admin3=a4.admin3 AND a4.fcode='ADM4' AND t1.admin4=a4.admin4 WHERE t1.fclass IN ('A','p') 
Unless you have a really compelling reason to use SQL Server (e.g. your company or companies in your industry use it), I'd suggest to instead use an open source database like MySQL and Postgres. I prefer Postgres, but you can't go wrong with either.
On a different subject, it's bad practice to use reserved words as column names. LENGTH is an Oracle function, so you should qualify the column name, such as slip_length or whatever.
Describe &lt;table-name&gt;;
I'm so sorry. I just noticed that this was PostgreSQL. What I wrote was just SQL. I don't know what the syntax would be to do this in postgre.
Good catch and good to know, thanks!
SQL 2016 changes that. Every version has every feature now. Developer edition just has no RAM/disk usage limits. Express has a 1GB RAM cap and 10GB database size limit. I do agree, use Developer edition to develop. But understand using express now doesn't mean you lose features.
Wouldn't just selecting the IDs that have code 'C' work then writing the query off of that work? http://sqlfiddle.com/#!18/0efe1/15 ;with ids as ( SELECT DISTINCT ID FROM IDs_codes_table WHERE Code = 'C' ) SELECT Distinct ID1.ID, CAST(STUFF((SELECT ',' + RIGHT('0' + CAST(IDC.code AS varchar(1)), 3) FROM ids AS ID2 INNER JOIN IDs_codes_table AS IDC ON ID2.ID = IDC.ID WHERE ID1.ID = ID2.ID FOR XML PATH, TYPE).value('.','nvarchar(max)'), 1, 1, '' ) AS varchar(max) ) AS [code_string] FROM ids ID1
yes there is a free version https://www.microsoft.com/en-us/sql-server/sql-server-editions-express but it doesn't make sense to ask "for C# or Python" any language that has an odbc driver can be connected to MS SQL Server or to Oracle or to PostgreSQL 
yes there is a free version https://www.microsoft.com/en-us/sql-server/sql-server-editions-express but it doesn't make sense to ask "for C# or Python" any language that has an odbc driver can be connected to MS SQL Server or to Oracle or to PostgreSQL 
That's not "normalizing". You're trying to encode one domain of values with another. This is optimization and it is only worth doing if you estimate that you can realize significant storage/processing efficiencies, relevant to your use case.
:) Glad it worked!
If using SQL Server Express or SQL Server Developer you can use SQL Server Management Studio for free. 
Look into recursive SQL. Complicated but should be possible
Write permissions are not needed to make temp tables see the link below. https://www.google.com/amp/s/jasonstrate.com/2013/05/21/security-questions-what-permissions-are-required-to-create-temporary-tables/amp/ Load a Excel file on a temp table is your best bet. Otherwise you could export the data out of Sql and use a vlookup in Excel
Maybe its an Oracle thing. I got a permission error when I last tried to make a temp table. 
Oh yeah. I assumed Microsoft. I don't have any experience with the oracle
I don't know about Oracle, SQL Server is easy - you can import it via DTS or add it as a 'linked server' - however both require permissions to do so (which it doesn't sound like you have). If your database look-up table is small enough (like under 35,000 rows I think), you could also go the other way and connect to the database table from Excel.
Does 2016 have the agent service?
I've only dealt with Oracle and Postgres. Played with MySQL for a home project once, several years ago. Of the three, I prefer Postgres. It also has the benefit of being free.
Yeah
Something like this could work, unless you run into some limit on query length: mydb=# select * from (values (1,2), (3,4)) as x (a, b); a | b ---+--- 1 | 2 3 | 4 (2 rows) mydb=# select * from (values (1,2), (3,4)) as x (a, b) where a &gt; 1; a | b ---+--- 3 | 4 (1 row) 
Oh excellent, then yeah, the above is less relevant - although I'd still think carefully about whether 1/10GB limits could ever be an issue, and at least build in some warning procedure for if you get close
&gt; Is SQL SERVER the best DBMS? betteridge's law
yes!! type **DESCRIBE mytable** into your screen and press enter, or whatever the UI wants you to do 
From my experience Oracle and SQL Server are the top enterprise databases. They have great performance and good standards out of the box. MySQL is alright but it has some issues. I imagine thats why AWS has developed Aurora to replace it. Ultimately use the database that fits your needs to scale. If it's a humongous scale and you have limited developer support then Oracle and SQL Server are perfect. Otherwise you'll have to do some more research based on your requirements. 
&gt;SQL SERVER IS the best DBMS. Prove me wrong.
Sql is a tool. Is it the best tool for the job? That depends on the job...
based on what... fanboys?
Thank you!
nice bot!
In SQL developer, Ctrl click on the table name works for me.
No, Db2 is.
Yep - I am a big fan of MS SQL but I gotta say I do like me some SAP HANA too. The out of the box speed just can’t be beat...although the case sensitivity sucks.
MS Sql Server is a very good database. Oracle has more features but is more expensive. Postgresql is missing a couple of features but is free. 
I feel much the same but with PG at the top, because I work in a shop that prefers FOSS and treats proprietary databases as the risky option. But honestly it really does come down to that personal context.
Say you want to store relational data in a relational database management system.
It's supposed to be faster than IBM's Mainframe? Lol. I'd be curious to see some stats. Sure, Mainframe may be an older technology, but it's backed by years of iteration. In contrast, NoSQL approaches seems to dominate if you need high transaction volume with minimal relational integrity. 
There are very few software categories with a single product that can be classified as the best, undisputed. RDBMS is not one of those categories. That said, SQL Server, along with Oracle, DB2 and Postgres are all enterprise RDBMS systems. 
Always wanted to work with HANA, but old bosses didn't want to invest in it...was kinda pissed about then.
I'm a Teradata fan, but i think that's on a completely different scale than MSSQL, heh.
My take: For the windows and *nix worlds, yes *Historically* Oracle used to be the better solution, but over the course of my career this its how it panned out: You unpack and install Oracle. You unpack and install MS-SQL. Same hardware, same OS. The DBA and sysadmin would beat on the Oracle install for days, trimming this tuning that. The DBA and sysadmin would beat on the MS-SQL install for days, trimming this tuning that. At the end of all that, Oracle would usually win, mostly. BUT things changed over a few years. Starting at the same place... You unpack and install Oracle. You unpack and install MS-SQL. Same hardware, same OS. The DBA and sysadmin would beat on the Oracle install for days, trimming this tuning that. The MS-SQL, meanwhile, works smooth right out of the box. That's **man-hours** and real days of difference. 
&gt; Supposed to be the fastest around [Given Oracle's attitude toward independent benchmarks](https://www.reddit.com/r/programming/comments/7imzth/that_time_larry_ellison_tried_to_have_a_professor/), I'd say that's a questionable claim at best. 
https://www.reddit.com/r/Windows10/comments/8h3b2a/i_cannot_install_sql_express_on_my_machine/?utm_source=reddit-android install error
https://www.reddit.com/r/Windows10/comments/8h3b2a/i_cannot_install_sql_express_on_my_machine/?utm_source=reddit-android got a bug while installing...
A project with a budget of $0 eliminates SQL Server as one of your tools. Point being choice is subjective. You pick the right tool for the right job.
NoSQL isn't SQL though \(it's in the name\). And I have no experience of IBM Mainframe, it's hardly a common choice.
Quite a few years in IT, MHO : DB2 on z/OS is a beast. Oracle is a beast. But these both cost serious money. When you take that money spent on licenses and put it into hardware, Postgresql will become your RDBMS of choice. And when you decide to create more clusters, with more hot standbys, and maybe a data warehouse, or replace Mongo with storing JSON in Postgresql, your finance people will thank you. And you'll wonder why more people don't go the same route. You can buy support for Postgresql from several companies, including a few that contribute to the code. 
I've written both MSSQL and Oracle quite a bit and personally I prefer Oracle because its scripting language allows for a more pleasant development experience and I really like their standard application Oracle SQL Developer. I definitely approach this as a developer though, I'm not looking at setup, performance, or anything else. My 2 cents.
This shares my thoughts as well. My average setup for a new MS\-SQL box from running the script to spin up a fresh VM in the cluster to SSMS connectable is usually 20\~30 minutes; with the custom installation manifest files remove 95&amp;#37; of the post install tasks. Also Microsoft's promotion of the community as a whole has completedly changed my perspective of them as a company. #SQLHelp has saved my bacon on some tasks things I likely would have never figured out.
Side note: For developers it is free; just can't be used in a production environment. Azure is also the easiest setup/hosting for an enterprise system I've seen, although it can and will get expensive if you need a large DTU box. If it's a minimal system it might only cost \&lt;$10 / month.
You, sir, are more informative and helpful than the entire internet has been for the past two days. I'll tell you if it worked.
Developer edition can’t be used in production, Express edition can
&gt;it’s hardly a common choice. please tell that to my employer. I swear, if I never see another AS400, I’ll be a significantly happier programmer.
https://www.microsoft.com/en-us/sql-server/sql-server-editions-express
It depends what you mean by 'best'. I have many years of experience in this field, and in my opinion for most applications under the vast majority of workloads, SQL Server's total cost of ownership is the lowest among the full-featured DBMSs. Arguments about performance are mostly silly (and religious), as design and code decisions are going to make orders of magnitude more difference to performance than the database platform will. At the end of the day, the 'best' DBMS is the one that lets you accomplish the task you need with the minimum of fuss, and for most applications, most of the time, that DBMS is SQL Server. In my opinion, of course. There are arguments for zero cost solutions like MySQL but many businesses are not comfortable with trusting their data to open source solutions. Also, SQL Server (intentionally or not) has fostered a very strong DIY attitude. If you don't know how to do something, or want to know something, odds are pretty good that someone out there has already figured it out and shared it with the community.
It is awesome once you get the hang of it. We have tables that are pretty poorly indexed (vendor delivered) that we apply security too. Really tough to get working in SQL Server without a lot of tuning and bringing the data into a custom DB we have. Hana solved all that and the queries run a lot faster without much tuning. I’m excited about the day that something like Hana becomes more affordable and easier to use.
Agreed. OP I think you’re confusing [normalization](https://www.essentialsql.com/get-ready-to-learn-sql-database-normalization-explained-in-simple-english/amp/) and standardization.
Oracle seems to be doing that a lot recently, their products seem to be going less and less accessible across the board. Java used to be almost the defacto language for self\-taught or new individuals; now it's slowly sliding into obscurity as they add more and more complexity/prerequisites to do anything useful with it as a small team or single person. 
It would be crazy to do this manually, so worst case, ask someone who does have write permission to blow this excel sheet into the database. 
**Midrange computer** Midrange computers, or midrange systems, are a class of computer systems which fall in between mainframe computers and microcomputers. The class called minicomputers emerged in the 1960s and machines were generally known at the time as minicomputers – especially models from Digital Equipment Corporation (PDP line), Data General, Hewlett-Packard (HP3000 line and successors), and Sun Microsystems (SPARC Enterprise). These were widely used in science and research as well as for business. IBM favored the term "midrange computer" for their comparable, but more business-oriented, systems. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
What happens when you run the bottom query by itself? Select id from a_cust where I'd not in (select id from b_cust)
the query seems fine. is a_customers duplicating IDs? maybe table b does in fact contain all the IDs in table a and they're all being excluded
I simplified the example to avoid over complicating the issue. a_customers and b_customers are both temp tables and I have tried using them as you described to no avail. Can you explain why that might be different? 
I work heavily with SQL Server and Oracle. SQL Server is much easier to deploy and maintain. That's a big plus. Howeve, I highly prefer Oracle's tool set (if you have the proper licensing). 
Thanks, I'll do a bit of googling to find those blog posts. 
You could also just coalesce the value. 
It's helps in the debugging process. Take a small part of a larger query that you know what should be returned and see if it returns the correct data. If it does, then the problem is with a different part of the query. So is the expected data being returned when you run the smaller query? (Select id from tempA where id not in (select id from tempB) If it is the expected data, then it's likely your issue is with another part of the query. I don't see anything wrong with your queries but I also don't know what your data/tables look like.
Thanks for the reply. I tested it all in stages as you described, which was why I was so baffled by the problem. Since I tested each component and created temp tables that essentially represent A and B, I’m pretty sure that the “fake” query that I posted is representative of the issue. Also I’m sure that not all of B are in A and vice versa. I haven’t had a chance to test it yet but I think what was commented elsewhere in the thread about using not exists instead of not in might be the issue. 
it looks like SmarmySnail is correct. i thought the query looked fine because i assumed your ID column was a primary key column thus not having nulls. if you have any nulls in your ID column then yes you will need to exist. for example: select 1 where 6 not in (8, 7, null) will return NULL in the where clause which is not true, which means the row won't be selected
I would use an unmatched join here, if I understand your example correctly. I think in redshift it is computationally more efficient than an IN/EXISTS clause. 
Maybe it is a reference issue. Try changing it to: WHERE id NOT IN (SELECT b_customers.id FROM b_customers)
I've advised on IRC that the best way to prevent UPDATE is to inline the condition in `WHERE`, e.g. async ( connection: DatabaseConnectionType, id: number, url: string, name: string ): Promise&lt;number&gt; =&gt; { await connection .query(sql` UPDATE venue SET url = ${url}, name = ${name} WHERE id = ${id} AND (url, name) IS DISTINCT FROM (${url}, ${name}) `); return id; };
Select for update
In mysql you have replace into or on duplicate key update,in other dbs there is similar functionality.
you're asking if running two sql statements in a row (but possibly only the select whenever the values are already set) is going to be faster than running the update by itself suppose you ran just the update... which would mean sometimes you're updating the values to what they already were how is that not the fastest *and simplest*? 
You sure the trigger still isn't firing? 
Oh, huh. Looks like you are right.
You can use a select to check for the existence of data to update with a select and then update, but you use two transactions and three statements that read the table three times. You can just run the update, you'll have one transaction and the table will be read twice. You can change the trigger to not fire for your statement. It will still trigger, but at minimal cost saving you one trip to that table. Altering the trigger to check for the existence of a temp table and then you creating a temp table in your update statement will have that effect. If mySQL has a version of context info, I'd use that. This begins to become very spaghetti like in architecture however and I would recommend to move away from triggers in this example or let the trigger fire for your statement. 
Pesemistic record locking only works if the information is displayed and updated in the same database session. If this is a web app it likely uses some sort of connection pooling and must use optimistic record locking to prevent lost updates. The workflow leading to this update is unclear and it might not be about defending against lost updates and just a timing issue of some sort. 
Sorry on phone or I would type it out You want to pivot your data. I would do it through an aggregate. Select ID, MAX (CASE WHEN SUBSTRING (no,1,1) = '1' THEN no ELSE null END) as Col_1, MAX (CASE WHEN SUBSTRING (no,2,1) = '2' THEN no ELSE null END) as Col_2, MAX (CASE WHEN SUBSTRING (no,3,1) = '3' THEN no ELSE null END) as Col_3 From table Group by ID I believe Oracle has substring or an equivalent. Basically each column you add to the report checks to see if a character index in the string contains a value and if it does it writes the whole value. If it doesnt, its null and will remain null unless another row in the aggregate contains a value with the same id. 
If you can create a materialised view, create one and use it to transform the numeric dates into proper ones. Then you can write a simple query to produce your report.
"If you set a column to the value it currently has, MySQL notices this and does not update it." -- da mysql manual seems like a useful idea...
this worked... somehow thanks!
no, just returned null.
Thanks for the response - I did try that to no avail. it was definitely the not in vs not exists issue that smarmysnail brought up. 
Thanks for the example/explanation! That solves the problem. 
This did work as expected. Thanks for the help.
It did work! thanks again for the help.
Sorry, I didn't even notice the input. So if you run the query replacing the input variable with some uporabnik id, there is a result, but if you run the function with that same id, there is no result? I'm afraid I have no clue what's wrong, then. I will try recreating it tomorrow though and see if I can figure it out. 
Yup, I ran the query by itself and it returned as it should have, but the function didn’t.
But then its not a job, is it ? :) Of course, you will not install ms sql server enterprise edition, but you still can use at least express edition sql server. So, ms sql server is the best, then goes postgresql with mariadb (mysql is a dead beet), there is also oracle db, but its oracle, so it is kind of a dead beet too, unless you want to get sued.
Whats the error? Sql version? 
Create View Working_Under(Fullname, Manager) as Select concat(employee.first_name, ',' ,employee.last_name) as 'Fullname', (Select concat(manager.first_name, ',' ,manager.last_name) as 'Manager' From employee JOIN manager On(employee.manager_id=manager.manager_id) Where employee_id = 201) I did a little adjustment now the error says...Error Code 1109. Uknown table 'employee' in field list 
 Create View Working_Under(Fullname, Manager) as Select concat(employee.first_name, ',' ,employee.last_name) as 'Fullname', (Select concat(manager.first_name, ',' ,manager.last_name)as 'Manager' From employee JOIN manager On(employee.manager_id=manager.manager_id) Where employee_id = 201) 
Oooo
Now I see what you mean...thanks! but nothing returned XD something I did is wrong...
Before creating a view make sure the selext statement can run. Two dashes to comment out the create view part should work. --Create View Working_Under(Fullname, Manager) as Select concat(employee.first_name, ',' ,employee.last_name) as 'Fullname', (Select concat(manager.first_name, ',' ,manager.last_name)as 'Manager' From employee JOIN manager On(employee.manager_id=manager.manager_id) Where employee_id = 201) You write select inside parentheses. Select is only written once. Usually because of database normalization you wont have an employee and a manager table because managers are employees and they have managers as well. However you can alias your table on its join. From employee Inner join employee as manager on employee.manager_id = manager.employee_id You also see that I joined the table named manager on its ID field not its manager field becausw that manager_id is reserved for the manager's manager's id field. --Create View Working_Under(Fullname, Manager) as Select concat(employee.first_name, ',' ,employee.last_name) as 'Fullname', concat(manager.first_name, ',' ,manager.last_name) as 'Manager' From employee JOIN employee as manager On(employee.manager_id=manager.employee_id) Where employee_id = 201) I also find it wierd you are concatting the name as "John, Doe". I also find it wierd someone would have a view with an employee_id field hardcoded in their view unless it was a view named vw_getDoeJohnsManager Once your select is done. Uncomment the top like and run to create your view.
Hey, Thriven, just a quick heads-up: **wierd** is actually spelled **weird**. You can remember it by **e before i**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Delete from bots where 'nazi' in (spelling,grammer)
Just to reiterate another comment, it is unusual (I'd say most likely incorrect) to have a separate employees and managers table. Usually it is one table called employees that would have a column called manager_id which is a foreign key referencing the employees table that you would self join onto to get the manager. This is because managers are employees and managers also have managers, so if you put managers into both a managers table and an employees table, you are doing redundant duplication with your data.
btw thanks for the thorough response. I did this CREATE VIEW Working_For ( Fullname , Manager ) AS SELECT CONCAT(employee.first_name, ',' ,employee.last_name) AS 'Fullname' , CONCAT(manager.first_name, ',' ,manager.last_name) AS 'Manager' FROM employee INNER JOIN manager ON manager.manager_id = employee.manager_id WHERE employee.first_name = 'Jerica' some reason it says 0 rows affected :/
Yup manager table exist :/ sorry ;-; and I just tried left join got 0 rows again
Did you check for NULLs. I don't use the concat function often, but I know when concatenating with '\+', NULLs can cause unexpected results.
Ill check that in a moment
Unfortunately I cant mess with that rn....i have to present my db soon :/
Well, at least now you can say how you would make improvements to the database if you're asked.
mhm 
If your left join returns no rows its your where statement that isn't right. Generally you don't put much in your where statement in a view unless you creating a view called ActiveEmployees and your where statement is "isActive = 1 AND terminationDate is null OR terminationDate &gt; getdate ()"
What a shitty bot "You can remember it by using exactly the opposite of the phrase everyone has memorized"
You can either pivot and join or case. Select case when priority = 1 then code else null end as priority 1, case when priority = 2....
This Comment has 0 content...
ok, let's see &gt;Anyone think that MySQL is now a worthy adversary of PostgreSQL with the advent of MySQL 8.0’s features? Yes, MySQL 8.0 is worthy adversary of PostgreSQL and personally I can't wait to use it in production. 
Thank's for this. I think i'm on the right track. I did setup multiple cases, and the logic does pull. However, it's creating a new row for each. looks sort of like this: https://i.imgur.com/9cj3yIz.png Basically, it's creating a new row for each duplication instead of keeping everything on one row. 
Sorry. I'm not a SQL expert. What exactly does this mean to do? I have never done that before and I usually just write a select code and copy and paste it into excel
Load the whole file into a staging table then select what columns you want in your query, or perhaps into your presentation table. Look up ETL. 
This works with a text file?
You would load all 35 columns into the database, then your query would select only the 13 you need. If you don't want do to that then you'll need to run a preprocessing step on the text file before loading it into the db. If your values don't contain the delimiter, you can probably do this fairly easily with one-liner using "awk" or "cut". I googled "select only certain columns from text delimited file" and this was the first result: https://stackoverflow.com/questions/7857090/awk-extract-specific-columns-from-delimited-file
I think they're now close enough that any dealbreakers are likely to be fairly niche and personal preference. It's not that neither have any potential dealbreakers, just that there are no hard and fast rules anymore, because things that are a problem to you may not be for me. Personally as long as I have CTEs either nested, or that run sequentially (eg the second CTE can use the first one) then I can live with basically any RDBMS. No CTEs = count me out, though. Ain't nobody got time for that
If you got budget then you can use 3rd party tool like CozyRoc tableDiff.
Also don't use keywords in field names. 
If your dates are actual *dates* you can write queries that take advantage of that. Selecting rows from the last `x` months, by financial year, doing date calculations accurately etc. If your source data isn't correctly *formatted*, you can use a view to make it nicer. Materialised views could perform better if you have lots of data, they 'store' data rather than query the source every time.
Ive just looked up HeidiSQL, and it appears to be a database management application for the mysql instance. I'd suggest you look for SQL commands to import from a delimited file and basic select statement Something like this? https://stackoverflow.com/questions/13579810/how-to-import-data-from-text-file-to-mysql-database 
I wish I could travel back 20 years and tell the guy that built it that :\
Can I do something like this, to select 13 columns out of 35? https://stackoverflow.com/questions/4202564/how-to-insert-selected-columns-from-a-csv-file-to-a-mysql-database-using-load-da
You can dump the data into a temp table using: https://shancarter.github.io/mr-data-converter/ You can use several WHEREs. As an example, I am going to type one up right now.. This was written using www.random.org to generate data and [UEStudio](https://www.ultraedit.com/) to format it into a query. Use Excel if your data is not as cleanly formatted. SELECT * FROM table1 WHERE value IN ('0bk7ykBh49', 'U13X6MB3au', 'aO4RCKUh08', 'W8SndxqmOH', 'ql9kOJaSBL', 'YsW01On6F4', 'nBvuWAyCeJ', '4vt7vZWDP2', 'GgbaNWAu4q', '5CZA2F07CC', 'iaXbIFEVJv', 'UwoKGmHkZ8', '7OLL4gVgOz', 'uw27z0yOGX', 'x8CC1RvOVp', 'HqprAviJ88', '1bytNPYHWN', 'SmtC3fP1O7', 'clPui44ZZY', 'hw7rvqx0MK', 'ZJK1pXoaYi', '603qh5YGCb', '8ZN4BXBg9H', 'lpqcgf82at', 'KW2J5sWgPE', 'URelx0WFTw', 'vcgdekEUhy', 'DjxILfQMZZ', 'KgGxGxjn6w', 'sxti6RDreS', 'eLNdkRf11f', 'ITHsDfvNAE', 'vGJbqGWQbp', 'YzxjCC2TSp', '95MhnOpobn', 'DlLlTYiJxd', 'xHXcPftlUC', 'Ev6J7wFdhU', 'XOOODWZaBe', 'iRlmzhnury', 'hzbCFZYx35', 'Iqp4gX3f4x', 'nqpV6jgsKW', '7XvjClwGzC', 'q8esqLc3Tr', 'EhtBlZSKn0', 'JSbCwdz1pf', 'wImCZILdan', '1f233IdeDs', 'fJNqhwDnO8', 'bi9qW3Prfu', 'nt6Lh8TJZg', 'AfDU0CTznE', 'UaYNfydLVk', 'U1yYtwS75b', 'hF0bjt2YWa', '5bf8TGMMXc', 'SQxuqeMgVR', 'ipM1r7Jioc', '0FZzTaVoQc', '5brsWUiB6z', '9h5luebJbs', 'xw28ugfE3B', 'kXkOWAxdTi', 'cxqbwmuTN0', 'MDqTgkmVTN', '6iI0Smj7ST', 'u6ioquWfCB', 'lYgmm5vIvW', 'JHGPkGwmxj', 'ONdVb9NINs', 'koxaQTksFc', 'hIPZyaX0kn', 'MaDXg9c81v', 'yL5ecTgQ9z', 'VxnPuOhCg2', 'B5QG3V42qI', 'EYxlE9k7Fv', 'gRuXfbABed', 'tOFYt8fUgK', 'Ia9nL0zkhH', 'nqY1kdRV19', 'ZgUSwko7db', 'L1EuQHqBGT', '6SwSGrVfL7', 'Hiymr7VZMj', 'XCjqac4m06', 'j4XxIlIA40', 'zK1kUhcgdH', '00dNfxd7Pz', '1E2Tf4h6k4', '7Jemd4u3ip', 'onjmEW5gsB', 'BkR5UEa8eU', 'c5ddS9zpSk', 'unz49FHnh9', '9rEX8TvkK2', 'VXQqrP8soq', 'CjvKAVliki', 'LzTvwY45wI', 'FOmmI0wB4a', 'S3e8dXmfjE', '99dqlyFzl9', 'tSG6cvlEIm', 'c8W5iCStL2', 'hBt2nb0h22', 'LcshEmfaAg', 'DtNb7oxZWx', 'DGRd1cy29f', 'eJGIsXqsG4', 'V5xna8iwU7', 'WHppXhysyx', 'PdwM8Rhhs3', 'laGIQvVF90', '6EWd9vbzjD', 'tWT8Gpy9G8', '0KvqlUfoef', 'Ylv9gbGMjl', 'WuuQ9PPYh0', '4esiWCkVUC', '9REkMZnfKK', 'alpWAPKxbN', '5a4E2tb6Y8', '021bHTYM9Z', '5qd73Ca960', 'bkl8idY4sj', 'sxkQzB8F7a', '8yHBDvFMHx', '0BBUXaU70i', 'oFusFJU5ya', 'rHJ18CHu5F', '2BP3TFSWM2', 'Hrcka8ciKN', 'YxFTBk1053', 'itbEBpbaYO', 'OfXAZnkwRH', 'PdpmAPkajp', 'ZcHcHRkmE4', 'bgYInsSbzt', 'VZuzjyWqyX', '2nlTndIs2b', '2ExxkZr63X', 'LPjrkvk5qx', 'hhnWoiX729', 'lNKNOeJHXu', 'tuTrn5DxlC', 'J9Nj5wcKR2', 'CvF7IVF7qp', 'YknDBtY1CH', 'fpprTMxQ9y', 'ScLa9uL18v', 'hpAdI8CjyF', '6JtfIEJhxb', 'kQOVVaVwqY', 'ZKVAIYKxiP', 'rKSFNyr7sn', 'RX9dQMRdvM', 'Jn1Iq3z8lj', '2P0M7V4Hsc', 'YLR6olqkFd', 'dJ27IG9484', 'SwqPUQ4NGl', 'MR0d79Cu3E', 'ZVyu7Vqs8x', 'vahxEorfWn', 'FxE1a0zInp', 'RanXXtbQoY', 'vfTCmOhATq', '1S940i2aQo', 'GIg9PWWCUe', 'q46yN9Zm5N', 'dCHYm0k1Oc', 'x0lL2VR0Ax', 'CPn7POb70v', 'jABroPlscK', 'GaNM4Q17WE', 'TAywIZUio2', 'rHvbnwwQz3', 'liVi9YeCA0', 'o0UIzQcPab', 'mxxq9OkLOx', 't5697GF5qr', 'JiAN1ZgViH', 'XjDhBhH3hf', 'wHqB83vL26', 'WNNGeheIo2', '2UkBxe4e07', 'ifEmsKFUad', '5hxWv8uXCB', 'viN7CAsQW1', 'cUFyNIGkTh', 'jzTKJHxkjz', 'PNBFQwlNI5', 'LqxRkgfToI', 'JgesWuJKi4', 'VBHXn56CAJ', 'GlmHq4ALoc', 'fCkqaJcxko', 'V1918t074X', 'vWrGuBcaSi', '4QpYfKkV48', 'zFxVDJmS0A', 'LigyepRKWW', 'pACwJhmHVr', '3f17DgJuRz', 'nkv1ATgc8n', 'DoLmMlNSzt', 'JDJdKbGEkp', 'h7LEMTMzcm', 'wICRj4gftr', 'sGYcll6q7r', 'fs3x4mnraL', 'uNiahSkRDk', 'mojHrMyZuJ', 'qUNxKsD2r4', 'sKs1J5qJKX', 'aa1nlyMToR', 'iHTmuAqQgs', 'HN1vmgPdh9', 'IpvSMglYXl', 'xv5f3psPc1', 'JCjWU24GtL', 'M32ifwFSJA', 'WZxyTSvHmF', 'u6VVJTtPih', 'SJaXyrgc1C', '0002cu1m3V', 'vLTY5w7rIK', 'ExFg72OIEj', 'hj1Fz9YSFI', 'Zl5N40XcCu', '8R9YuB7iPk', 'bgB4ojwip5', 'AxMApPYDxU', 'cN2W8G8ek1', 'J6Ej3Flmfa', 'tF8SASVeAk', '9QQEn1APKF', 'CKV5UCGhKn', 'sU9JkOj5av', 'Mb1WcwI9HF', 'yzNENkdUFI', 'CqRtBoEYMZ', 'eV9yt7pkG5', 'jA54LpWuus', 'Dfv6WcI4yj', 'doL3nNVGKz', 'C7tmik8GVX', 'mPJIgfe8ff', 'jleNJuDV6h', 'DPve0QXgj4', 'aTiRYpsEx9', 'hk1ddD2QIq', 'UJ1iVj9p66', 'g7Fn5ufk2U', '3bHB5IjBaV', 'mJbBVsWejq', 'mwMhtr1JSp', 'KoIZaiC1jQ', 'aFQWkwwWBe', 'Tvjr4YAA6K', 'oNrWziMvlS', 'lJF0P4gQLG', 'zNoqx5oa1k', 'rQWDVwl95S', 'iGwAQpFyPw', 'lcLWlXiin0', 'NZMTCPMYqF', 'RVYZ2ttfoj', '7amwEl1zrP', 'eBv27bTiwy', '9m2DwiAan9', 'vI3A01taMh', 'vCuDyt5BMs', 'Ui028oyRIZ', 'txktxqsEF8', 'CptpVzoiZo', 'n4A0B6yeQq', '166PXsP6Na', 'MEZpJwF6ib', '31in8ZwnMv', 'd6z3GiT4us', 'PnBC9N9Xz4', 'Nv0spOKKQf', 'UgZ8vAZMJw', 'Ls2ZQS3mHV', 'mzNEg166in', '8utU3ifjyO', 'fm8DMSn0sn', 'X9vZW8Ar67', 'ZLwLNiTzJL', 'kO7Di8VBjn', 'jUzOP1Hmp5', '2j4oCobasr', 'vhWANOv7rr', 'gHVngmxlKT', 'Y4lx2J58pS', '9FS2jomfYP', 'v2iEQ3P2Sz', 'Mf9vtNaxGY', 'WWmZGWAc3x', 'l8JljegUqH', 'Vjj6tIHErg', 'AF1MNxPqPG', 'pNkBeldAtF', 'eKVj93GmR1', 'LBLyPQeRGY', '2gWWKLpjRh', 'kcPnH5N0yz', '0YTyn4zFFD', 'x0ucX4vYwP', '1FTWFe0VdH', 'R03l9UmIHe', 'ZAx4QYRTw8', 'uiefSUlfRF', 'Gx7ASHC0gn', 'AU4XFc5qDC', 'AlyHCP7RDe', 'odY7k519Gr', 'NhSN4D3byZ', 'QlTI8hibTW', 'yneistmsyz', 'Z91OBfDLN3', 's38vutVTSn', 'C7rBjaFHiy', 'sIYCwZkKbl', 'spAdiXSvpi', 'l61wn2pByK', 'sbtWCsVizJ', 'gEhV3Uwnm8', 'HzRSO5stmA', 'ppClkflIx7', 'e6UnPKjghh', 'RNZTEYcdi2', 'YnSVs9pUg3', 'z1z9mKCWeV', 'klvtVMFVeT', 'Ssj2oW5SRP', 'WyLyE6B6n6', '4lD9Vw6qMe', '26CwrwLPZ2', 'yzoUJ9GFAX', 'YzGCvQ7s4l', 'hvJZU59bZZ', '8oqB6zajZq', 'xt6DDbfkJO', 'dgVq2eWJlw', '5HEYqdrlyJ', '0aDpLvCZAy', '0qMSPYWKqz', 'CN6vJsVEf6', 'UeInTpUroS', 'k82WXzAYN3', 'SAeYUfs1T4', 'vLDp8EB6aI', 'ksrH6xiyAk', 'ySiv2n6TOV', 'vuXQJCyVPB', 'd07YCdeALv', 'gTs5q3ipES', 'CUwrxjlPL7', 'NaqfVaqOfc', '2ZZPIgZnCn', 'Hy9YWcswTy', '0ObZEquAgr', 'TORSWbQOZ5', 'FthqeIQMOj', 'neJXmq72dB', 'N1ZsAbcgwk', 't5GHXHPKUZ', '5lmIOqtwl6', 'WcGHSt6gms', 'y4uVDxhN6C', 'ZdfNfcTgEi', '3PrQBAoN6Q', 'cObZsKjJ9A', 'jWRmgmP3RM', '8hHlKAEKVT', 'crCQy1CkzU', 'uW5SlMhuEx', 'uwXny61WCo', 'YurSjMtzzU', 'i2QJgpu5cx', '5iGAc5AZVM', 'vPg4s4ekXk', 'x1isysxMou', 'WIDlZhl3RR', 'rwQGFrLeSo', 'F6OOYVl9dc', 'EMYGfvE6eG', 'zMkkaoCWg1', 'fRSDNnfgNY', 'GyHdXLV5Qg', 'pS3iZVghVX', 'cIwGfGTjot', 'iuycfPt2LH', 'J8wru0F36T', 'WSsmTezPmi', 'DsQhyzJ2OS', '9swUg1JmW0', 'bON5iRrScU', 'Etxhk8Ssj5', '8QuQCOojlA', 'hb4w7C96XI', 'Iny4nnwOj3', 'Y91s9vd66U', 'EBZxo6mWGh', 'TBygCTSWzQ', 'Q8W8LoZFE0', '9mYYjtuxgo', 'MVgJIHTVpo', 'T5tK0aBTqK', 'icpjP1saDS', '2VuxiUB1Vh', 'RXfNXsyRDR', 'eMZi9VMsUL', 'WGWH4denLU', 'nwGNnOxMFw', '8LoIFEXcEG', 'xXTXlzPXys', '1On5ky3ztz', 'niZ0gdHlEV', 'QtFaCS57ni', 'Dy0gfwRd5Z', 'jrbMMel6Nx', '1ujhCovQat', 'jTs2EkBHM2', 'fI6jDE3ETp', 'PzUJjxvrCj', '4Pxbr9mPT3', 'gmKAAtPSps', 'HqkcXZ2uVj', 'vhW0ZoNESY', 'H5RAwgP3CG', 'Gy5xy1ZxAB', 'Mjwmc4zjX6', 'CUsVSQrnXH', 'TWbO7xIDTY', 'BDlZpQ5cUG', '4TYQVKABIj', 'hJhEuyfKx7', 'FG9ekAgKF7', 'VkInfO1Ucr', 'BK5eMMmzai', 'Y1lMzoMaD6', 'aH1Iqk7AnS', 'sXk5ErVtyd', '6suPU4l4pb', 'Viq3GNilj9', 'QBWzbpJ9Ql', 'prIK0rHEla', '19V1NASCc4', 'MdGPLS95ID', 'ivb3Z6bE79', 's3Xw1pqMm8', '1zcil0ix5u', 'SeXTuEEsVZ', 'PzbUCT0jvl', 'hztPUfmwbX', 'VdgqmRzIz0', 'f3DNQEdtHc', 'yPy5KkN1MT', '0JN3rXZ9vX', 'Ja1LbYRP14', 'deGDHhw7XZ', 'SMyANZvbVE', 'FL3I0H2sfb', 'uyc8f26SnL', 'hZjUC8u9f0', 'Jtd2woWQ9T', '6aIFobhF6u', 'puv3Y8NYvs', 'mmyEm4mqba', 'cN9Q0kDRVx', 'GhJqIacj4O', 'Oo7dINyIje', 'JFVu0dv584', '3PrTR67n21', 'klrnq6NZe3', 'rFfSSenqQv', 'FsqVf0dJQK', 'YoL26ezpbD', 'oOADzZrl3V', 'zv1Bp744Lo', 'Ew4u9JVSod', 'sdsmUbofc0', '7xCnX9f80U', 'wagH23rnId', 'ZPbQE6Ldvx', 't1Irk9zb6V', '5aNMVic2ZX', 'PBnzteQLXm', 'PW9DcjWQp9', 'kDcdu4FP8Y', 'UoQixpxdxo', 'ThGIMbRmjT', 'SMJOG1qSbC', 'G4kLDYQ5Mn', 'wVQsAFJ7Pp', 'bJArBRrNmS'); 
Kind of. I'm using SQL Server Management Studio. I tried throwing []'s at the problem keywords but now it tells me INVALID column name for all the fields 
Interesting. In Db2, we can specify which columns to load on the LOAD or IMPORT statement, no staging table or ETL tool required.
You can in TSQL but for me the safest method is to import the whole file into a staging table then perform whatever reductions filtering etc you want. No specific ETL tool required, just some SQL script.
Try getting the column name from the information scheme Also as a last measure try using a dynamic query. 
 try this SELECT * FROM ( SELECT T1.ID AS [ID], T1.FNAME, T1.LNAME,T2.CODE, T2.PRIORITY FROM TABLE1 T1 INNER JOIN TABLE2 T2 ON T1.ID = T2.ID ) AS T3 PIVOT ( MAX(CODE) FOR PRIORITY IN ([1],[2],[3]) ) AS PVT
It works now, dont even know how. Just changed the input from uporabnikid to uporabnikidd.
Got it! Thanks for the help everyone!! The working version of the code: INSERT dbo.UsedTanks (ID, PurchasedFrom, Cost, Manufacturer, Model, Serial, Capacity, Capacity_Unit, [Type], Orientation, Shape, [Top], Bottom, Exterior, Height, Height_Unit, Width, Width_Unit, [Length], Length_Unit, Diameter, Diameter_Unit, OAHeight, OAHeight_Unit, OAWidth, OAWidth_Unit, OALength, OALength_Unit, OADiameter, OADiameter_Unit, OutletSize, OutletType, Agitation, AgitationType, AgitatorCount, Baffled, HP, RPM, Voltage, Phase, Jacket, JacketType, JacketMedia, Sprayball, Manhole, Thermowell, SightGlass, Features, Gasket, ConsignmentInfo, Location, Tested, Date_Tested, create_user, create_stamp, mod_user, mod_stamp, Approved) VALUES (68163, NULL, NULL, NULL, 'T-1355', NULL, 6000, 'GALLON', 'STORAGE', 'HORIZONTAL', 'CYLINDRICAL', NULL, NULL, 'MILD STEEL PAINTED', '110', NULL, NULL, NULL, 218, '"', 102, '"', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, '3"', 'TC', NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL, 0, NULL, NULL, 1, 1, 1, 1, NULL, NULL, NULL, 22, 0, NULL, 'username', GETDATE(), 'username', NULL, NULL); 
Got it! Thanks for the help everyone!! The working version of the code: INSERT dbo.UsedTanks (ID, PurchasedFrom, Cost, Manufacturer, Model, Serial, Capacity, Capacity_Unit, [Type], Orientation, Shape, [Top], Bottom, Exterior, Height, Height_Unit, Width, Width_Unit, [Length], Length_Unit, Diameter, Diameter_Unit, OAHeight, OAHeight_Unit, OAWidth, OAWidth_Unit, OALength, OALength_Unit, OADiameter, OADiameter_Unit, OutletSize, OutletType, Agitation, AgitationType, AgitatorCount, Baffled, HP, RPM, Voltage, Phase, Jacket, JacketType, JacketMedia, Sprayball, Manhole, Thermowell, SightGlass, Features, Gasket, ConsignmentInfo, Location, Tested, Date_Tested, create_user, create_stamp, mod_user, mod_stamp, Approved) VALUES (68163, NULL, NULL, NULL, 'T-1355', NULL, 6000, 'GALLON', 'STORAGE', 'HORIZONTAL', 'CYLINDRICAL', NULL, NULL, 'MILD STEEL PAINTED', '110', NULL, NULL, NULL, 218, '"', 102, '"', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, '3"', 'TC', NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL, 0, NULL, NULL, 1, 1, 1, 1, NULL, NULL, NULL, 22, 0, NULL, 'username', GETDATE(), 'username', NULL, NULL); 
This is more or less out of the scope of control for Oracle/MySQL, but [AWS Relational Database Service currently only supports up to MySQL 5.7](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html). Now that 8.0 is in GA, it should be a couple of months before RDS adds support, but it's something to keep in mind if you're exploring some offsite use-cases for enterprise.
Yeah. I had some similar thoughts. CTE's are basically what makes standalone SQL Turing complete. Literally the difference between a SQL dialect being a full programming language or not. The other dealbreaker for me is when an RDBMs doesn't have multi-language support for stored procedures. As someone who knows a little bit of Python, Java, JS, and R, that is a must. Now that MySQL has this with GraalVM its a huge game changer. PG is still the leader here, but since MySQL has this now and has way more job openings its making me wonder if I should pay more attention. 
Totally not out of scope at all. I'm an enterprise db dev and enterprises are choosing open source databases more and more. The only advantage of MySQL over PostgreSQL previously was its ubiquity and the number of jobs available for it. If I could get a DBA job using MySQL with AWS and not have to sacrifice all the juicy goodness I love from more powerful RDBM's then I am definitely sold. Cloud DBA potential with AWS would be a very real area of career interest. 
Alright then. So other people are excited about it too (outside of just stakeholder organizations on Twitter lol). Good to know. Companies always play up the hype of a new product, but its validating to know others devs share my excitement. 
You almost had it: &gt; SELECT ID, Fname, Lname, &gt; &gt; MAX(case when priority = 1 then code else null end) as priority1, &gt; &gt; MAX(case when priority = 2 then code else null end) as priority2, &gt; &gt; MAX(case when priority = 3 then code else null end) as priority3 &gt; &gt; FROM table1 &gt; &gt; JOIN Table2 on table1.ID = table2.ID &gt; &gt; GROUP BY Fname, Lname 
Hello fellow ERA poster!
Use "TOP" or [TOP] Name shit smarter in the future eh
Tell that to the guy that wrote it. I'm sorting out spaghetti here a week into a new job. Thanks for the concise answer tho.
I would do this in two steps first creating a Staging table that will have all 35 columns. Secondly create your actual table with the 13 columns and you can create a select into statement. INSERT INTO tbl_temp2 (fld_id) SELECT tbl_temp1.fld_order_id FROM tbl_temp1;
The first line of the file has the column names. The rest of the rows have values for the 35 columns. How do I skip the first line?
Good luck then brother. Keep at er
You'll make a more readable question (and often understand your own problem better) if you reduce your code sample down to the smallest version that still shows the issue, such as: INSERT dbo.TableName (ID, Top) VALUES (68163, ) Bonus: you'll avoid revealing what might be proprietary information about your company's business logic or security holes.
Yeah. Hash doesn't work well with text columns of course but I might just have to map those columns and except them from change detection. 
Thanks!!
It isn't so much budget that is keeping me from using third party tools (employer pays) - it's more portability. Our recent move to Azure was mostly seamless since most all our logic is simple tsql. Had we been using third party tools we'd probably have had to spin up a vm in Azure to run them at many times the cost and more complexity as well.
I agree with this, from my understanding of the question. Expanding on it, the below uses a nested query. However, I've added a condition to exclude 'Combo' Categories, to avoid you getting multiple joins from Combos in the Combos table to Combos in the meniItems table SELECT m.foodName FROM Combos c INNER JOIN (SELECT combo_meal, dish from menuItems WHERE Category != 'Combo') m ON c.dish = m.mID Hope it helps...
&gt; `SELECT m.foodName` won't work because `m` doesn't have a column with that name 
 SELECT c.foodname AS combo_name , d.foodname AS dish_name FROM Combos INNER JOIN MenuItems AS c ON c.mID = Combos.combo_meal INNER JOIN MenuItems AS d ON d.mID = Combos.dish ORDER
Does that work when connected to AzureSQL db's as well?
You're right, mistakenly selected the combo columns when I meant menuItem columns
I have retrieved these for you _ _ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
1. Does the table "users" even have a column called "Image"? 2. What is the data type? You'll probably want to use a binary blob. If not, you'll need to encode the image as a string, which takes more space and requires that you decode it when you want to use it. 3. For ID columns backed by a sequence, i.e. auto-increment, you don't need to do anything special on an insert. Omit everything in your insert statement from "WHERE" until the end. No offense, but this is pretty basic stuff. You would probably benefit from getting a SQL book and reading through it, or doing an online tutorial.
1. why are you doing this in 2 steps in the first place? 2. how are the tables structured, what is indexed? 2. post the execution plan. https://www.brentozar.com/pastetheplan/ 
1. How would I do this in just one step? Isn't the only other way to have the token AND all the info in one table which make this table huge (we have around 20 tokens per package). Like so: token | package_identifier | name | substance | manufacturer | dosage_form | agent_strength | unit ---|--- | --- | --- | --- | --- | --- | --- token1 | 123456 | Vicodin | paracetamol | pfizer | pill | 100 | mg token2 | 123456 | Vicodin | paracetamol | pfizer | pill | 100 | mg ... | ... | ... | ... | ... | ... | ... | ... token22 | 123456 | Vicodin | paracetamol | pfizer | pill | 100 | mg 2. What exactly do you mean? I posted the table structure or no? Or are you talking about the column datatypes? 3. I'm already at home, will post the plan asap! 4. We're using prepared statements and parameters, is that what you mean?
By text do you mean lob text? not all text fields?
Actually, I got another problem before I can tackle the header problem. Whenever I load a text file only the first row is loaded. Additional rows are ignored. For example, in this text John|Doe|Male|Jane|Doe|Female| The table only recognizes the first three values FirstN LastN Gender John Doe Male
This is the one that works! Thank you! I tried a double join last night and couldn't get it to display properly, I don't even know what I did wrong, I had been working on the project for like 9 hours by that point. It returns 3 of the combo name in the left field, but I can tweak it to remove the duplicate values. I was hoping it would include a nested query, but I can find some other way to include one in a different query on the database somewhere else. Again, thank you, this will help so much towards the final part of the project when we actually have to build modular SQL in PHP to display the results on a website. 
You could've just done SELECT IDNumber , LastName , status = MAX(Specialty) FROM db.dbo.table WHERE activity = ('ACTIVE') GROUP BY IDNumber, LastName No? 
Why are you posting about proprietary garbage here?
You need to apply some form verification. This article may help: [https://www.acunetix.com/websitesecurity/sql\-injection/](https://www.acunetix.com/websitesecurity/sql-injection/)
I would say that the requirements need to be re-evaluated. What is the necessity for providing specialty? How is it intended to be used? What priority is placed on multiple specialties? The answers to these questions will help you evaluate the proper dataset response.
What I meant has been why are you doing the search in 2 queries instead of one. Structure: do you have keys (or unique constraints)? Are they unique? Since you've mentioned that, is "package_identifier" column datatype integer not varchar? How are you passing parameter into this statement, exactly: SELECT TOP 100 package_identifier FROM token_table WHERE token LIKE 'queryString%' 
A quick and dirty solution is to add `OR ? IS TRUE` to your where clause and just populate that with true when you want to ignore the other bit and false when you want to use it. The database will optimize appropriately.
You'd need to parse it first and create the proper end of line identifier for the db to ingest. If I was doing this in sql server I'd use a script task within my ssis package to reformat before ingesting. Or have a power shell script parse it before loading with bcp if doing a less formal upload process. It's all in the ETL. Extract, transform, load. You need to transform before loading. 
I tried that, but it only returned a single row
The employees in this particular dataset are all doctors, they're all Physicians that are required to be on an on-call rotation, regardless of specialty. However, the specialty still needs to be specified, and your guess is as good as mine as to why. I was just the DBA both parties knew and got stuck writing this query with not a whole lot of information. 
That's... odd... despite there being different IDNumber and LastName values? It should return one row per IDNumber and LastName, with the specialty just being whichever associated specialty comes last alphabetically (I assumed order didn't matter as your results would have returned an arbitrary specialty)
Took me awhile to figure out that the text file is serialized. So I'll need to do this in Java. How do I read in a text file? It has been awhile since I did Java programming.
What is the back end that generates your SQL statement? (PHP, VB, C)
ruby
please use text, not screenshots copy/paste, yes? what is your query, and what is the error message?
thanks!! I get 'invalid object name 'table' '. Even if I replace table with my table name
i just reread your request... i forgot to add one thing. You want to partition by batch # and part_id because you only want the max date if there are multiple part_ids per batch. So it should look like this.. if this doesn't work I'm out of ideas. &gt;select distinct [batch], [part_id], [date] &gt;from(select distinct [batch], [part_id], [date], &gt;row_number()over(partition by part_id, batch_id &gt;order by [date] desc)as rn &gt;from [table]) as temp &gt;where temp.rn=1
An example table with values and an according example query result is usually a better way to describe your problem than a textual description. However, if I understood you correctly this sounds like something you can do with a GROUP BY clause and the MAXIMUM aggregate function: SELECT batch, PartID, MAX(Date) FROM table GROUP BY batch, PartID;
since everything in the partition sub query I sent appears to also be in 'r' or [runs], just replace that table with what I posted as a subquery. should look something like this... did my best to translate it so you don't have to change your "as" lines in the select statement after pasting it: &gt;FROM [Measurements] m &gt;JOIN (select distinct [ProgramName], [Com5], [Com3],[Com1], [Com4], [Com5], [date] &gt;from(select distinct [ProgramName], [Com5], [Com3],[Com1], [Com4], [Com5], [date] &gt;row_number()over(partition by [Com1], [Com3] &gt;order by [date] desc)as rn &gt;from [RUNS]) as temp &gt;where temp.rn=1) r 
if it were me, I wouldn't put a comment_id in the thread table. instead, the comments table will have a column that references the id of the thread. so if you want all comments in a thread you'd say something like SELECT comment_body FROM comments WHERE comments.fk_thread_id = threads.thread_id on the application side, whenever you're looking at a thread, you'd use thread_id as the last parameter. so, your thread class could have a get comments method like this: function getComments { $sql = "SELECT comment_body FROM comments WHERE comments.fk_thread_id = " + this.id; } does this help?
Wow yeah that makes complete sense, thank you! 
First of all that is some ugly SQL. Naming is all over the place, you're trimming strings, some code is commented out etc. Anyway as for your question you need to join the two queries somehow. Are there any id columns or something like that in both your queries that match? 
Yeah, I realized that after my comment that the tuples are considered distinct and it wouldn't work.
https://stackoverflow.com/questions/17293991/how-to-write-and-read-java-serialized-objects-into-a-file 
Using mysql: LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 5.7/Uploads/filename.txt' INTO TABLE schemaname.temptbl1 **FIELDS TERMINATED BY '|'** /**Sets your delimiter **/ IGNORE 1 LINES;
Query tuning is not something you generally look into for **basic** queries. So the question becomes, how complex are your basic queries? Give a (properly formatted please) example query and we can see if tuning is actually something you need to worry about. 
&gt; FROM [Measurements] m &gt; &gt; JOIN (select distinct [ProgramName], [Com5], [Com3],[Com1], [Com4], [Com5],[RunID], [date] &gt; &gt; from(select distinct [ProgramName], [Com5], [Com3],[Com1], [Com4], [Com5],[RunID], [date] &gt; &gt; row_number()over(partition by [Com1], [Com3] &gt; &gt; order by [date] desc)as rn &gt; &gt; from [RUNS]) as temp &gt; &gt; where temp.rn=1) r &gt; &gt; on. r.[RunID]=m.[RunID] So close! We're missing a parentheses somewhere. 
This would be easier to figure out if we new what other fields are in your tables. I'm guessing that * iminvtrx is your inventory transaction table * imitmidx is your item table * humres is your employee table * cicmpy ???? * oerdhdr ---Um order header? whatever that means. So I would think your order header will also contain an Order ID number of some sort or a primary key that would like back to iminvtrx. If there is any logic to this, I would guess that your customer names are in either the order header OR another customer table. If that is the case, you would add this: FROM oerdhdr_sql AS o LEFT JOIN iminvtrx_sql IT WITH (nolock) --* Inner join should work, but just in case... ON o.«whateverId» = IT.«whateverId» LEFT JOIN [imitmidx_sql] IM ON IT.item_no = IM.item_no --LEFT JOIN iminvloc_sql L ON IT.item_no = L.item_no --* Sticking the rest of your stuff down here. INNER JOIN humres AS H ON OH.slspsn_no = H.res_id LEFT OUTER JOIN cicmpy AS DD ON Ltrim(Rtrim(OH.cus_no)) = Ltrim(Rtrim(DD.debcode)) Since the only other thing I see left is cicmpy that references your customer, I'm going to assume your customer name is in there: SELECT DD.CustomerFirstName , DD.CustomerLastName But I could be wrong and the name is actually in the order index SELECT o.CustomerFirstName , o.CustomerLastName Now, this is all inference since there's actually not enough information in your question to give you an absolute answer.
alright where's my error? /****** Script for SelectTopNRows command from SSMS ******/ USE [FineScanArchive] GO WITH cteRunSubSet AS( SELECT r.[Com1] as 'Part ID' ,r.[Com3] as 'Batch' ,r.[Com4] as 'Laser' ,r.[Com5] as 'Type' ,r.[RunName] FROM RUNS r JOIN ( SELECT [Com3] as 'Batch', [Com1] as 'Part ID', MAX([DispositionTime]) AS [DispositionTime] FROM [Runs] GROUP BY [Com3], [Com1] ) AS s ON r.[Com3] = s.[Batch] AND r.[Com1] as [Part ID] AND r.[DispositionTime] = s.[DispositionTime] WHERE r.[DispositionTime]&gt;'2018-04-15' AND r.[ProgramName] LIKE '%Comet_2017%' ) SELECT @@SERVERNAME as 'MachineName' ,db_name() as 'DatabaseName' ,m.[RunID] ,m.[GaugeID] ,m.[LocationID] ,r.[ProgramName] ,m.[MeasID] ,m.[Actual] ,m.[MeasX] ,m.[MeasY] ,r.[Com1] as 'Part ID' ,r.[Com3] as 'Batch' ,r.[Com4] as 'Laser' ,r.[Com5] as 'Type' ,r.[RunName] ,g.[GaugeName] FROM [Measurements] m JOIN [Runs] r ON r.[RunID]=m.[RunID] JOIN [Gauges] g ON m.[RunID]=g.[RunID] WHERE r.[DispositionTime]&gt;'2018-04-15' AND r.[ProgramName] LIKE '%Comet_2017%' 
Personal opinion with "the usual suspects" and what cat/subcat is associated to it then using a stored procedure to check for any un-categorized stuff and applying it would be my approach. Technically you could do it with a simple join and the cat/subcat wouldn't even need to be in your main table. The issue though is when you have places that sell more than just one category/subcategory like Walmart for instance because without some smarts in it, you won't reflect the right info. As a complete aside is this a project for the challenge of it because there's already free services out there that will link up to your bank account and do a lot of this stuff for you.
this has honestly never worked for me.
I think you meant this as a reply to the other guy
Perhaps use a group by?
I'm not entirely sure you understood the question...
Yeah, blob text, varchar(max), nvarchar(max) and varbinary.
 AND r.[Com1] as [Part ID] ... in line 19 should be: AND r.[Com1] = [Part ID]
&gt;Bad habits : Putting NOLOCK everywhere. Don't do this unless you know that you need to. We do this as a best practice when developing code and selecting from large tables... so other people can work. It's removed when code is promoted into production.
[Here's](http://web.swcdn.net/creative/infographics/1403_Confio_SQL_Server_Tuning_Infographics_8_5x11.pdf) a pdf I link to frequently. It's not the best or 100% fool proof document, but it's a great learning resource to start as a road map. The most important concept you will learn is that what you care about is time essentially. Are the users waiting too long? Are things taking too much CPU time where other things need those resources? Do other applications need the data faster? It's all about time. WHAT is taking the most time in your query? How do you shorten those areas and increase its speed? That's essentially what tuning breaks down to. Find what is taking so long and fix that piece. It's an over simplified statement, but at its core, that's query tuning.
Thank you both for responding and thank you for the PDF I will read it tonight. These are queries for work but I think I can post an example query tomorrow after editing some names to give an idea of what I am calling basic (no temp tables, or cases, or unions etc). Frankly I have had a tough time understanding how to insert a temp table that may be a huge part of my problem. 
Soooooo... having worked at a bank that did it for their customers, here's some basic requirements we covered: 1. By default, each transaction gets categorized automatically based on magic 2. The customer is able to override categorization for each single statement 3. The customer is able to override automatic categorization for each transaction receiver (e.g. recategorize Shell from Gas to Groceries if they only do groceries on Shell because they always get gas on Statoil) 4. The customer is able to split each individual transaction into multiple subcategories, specifying the amount belonging to each (so for a single $100 transaction on Statoil, they want to allocate $80 to Gas and $20 to Groceries) Implementation tips: 1. Leave the transaction table (raw statement data) alone, do not add any categories or subcategories or anything to it (let's call the table Statement) 2. Create a category subcategory dictionary table (let's call it Category) 2. Create a mapping table between vendors and categories / subcategories (let's call the table CategoryMapping) 3. Create an override mapping table for the customer, let's call it CustomerCategoryMapping 3. Create a split table between transactions and subcategories with the amount belonging to each category (CustomerStatementSplit) Categorization happens in order of precedence, based on: 1. CustomerStatementSplit 2. CustomerCategoryMapping 3. CategoryMapping So now you can satisfy the first requirement based on the CategoryMapping table, the second requirement (Customer overrides a single transaction) by creating a single CustomerStatementSplit record with the full amount and an overridden category, the third requirement (override categorization for a vendor) with CustomerCategoryMapping record and the fourth with multiple CustomerStatementSplit records. In the end, you have a query like so: SELECT Consolidated.Amount -- either the full statement amount or the split , ISNULL(Category.Name, 'Others') AS Category -- category resolved according to the described order of precedence , * -- bla bla FROM Statement LEFT OUTER JOIN CategoryMapping ON CategoryMapping.ReceiverID = Statement.ReceiverID LEFT OUTER JOIN CustomerCategoryMapping ON CustomerCategoryMapping.CustomerID = Statement.CustomerID AND CustomerCategoryMapping.ReceiverID = Statement.ReceiverID LEFT OUTER JOIN CustomerStatementSplit ON CustomerStatementSplit.CustomerID = Statement.CustomerID AND CustomerStatementSplit.StatementID = Statement.ID CROSS APPLY (SELECT COALESCE(CustomerStatementSplit.CategoryID, CustomerCategoryMapping.CategoryID, CategoryMapping.CategoryID) AS CategoryID , COALESCE(CustomerStatementSplit.Amount, Statement.Amount) AS Amount ) Consolidated LEFT OUTER JOIN Category ON Category.ID = Consolidated.CategoryID You don't need a trigger, a procedure, or anything for this kind of stuff. Unless your Statement table is huuuuge (like it is in a real bank serving this data for its real customers), you can do this on the fly in no time assuming the right indexes. If your rules are more complicated than just receiver =&gt; category map, then a trigger would be fine. Unless you have a huge influx of data that could pose performance issues, then post-import procedure execution would be a better pick. 
&gt; Different examples I found online on how to do what I want to do all use FROM in that location. they couldn't've been mysql examples, then, because [mysql syntax](https://dev.mysql.com/doc/refman/8.0/en/update.html) doesn't use FROM in the UPDATE UPDATE positions LEFT OUTER JOIN ( SELECT pnumber , COUNT(*) AS pnumbercount FROM sneeded GROUP BY pnumber ) AS i ON i.pnumber = positions.pnumber SET positions.numskills = COALESCE(i.pnumbercount,0) 
First, I would highly recommend before taking any outside courses in Oracle Database to do the \[2 Day Oracle DBA\]\([https://docs.oracle.com/cd/E11882\_01/nav/portal\_4.htm](https://docs.oracle.com/cd/E11882_01/nav/portal_4.htm)\) course that Oracle provides, it will hand\-hold you you with setting up an environment that you can work in without having to have a deep understanding of Oracle installation/configuration. Secondly, I'd recommend moving forward, you use something like \[VirtualBox\]\([https://www.virtualbox.org/](https://www.virtualbox.org/)\) or some other VM management application to create a Virtual Machine to do these installs on. Oracle and MS SQL Server both will inject itself pretty deep into the OS and going around just deleting folders isn't going to properly uninstall the DB engine and services the way you're expecting them to. When you start getting into physical disk management, like what a database does on the physical layer, it's not as simple as simply uninstalling the "application" from Control Panel to completely get it uninstalled. Troubleshooting wise, I would check to make sure the Oracle adapter is actually running. You can find this as OracleService in your Windows Services. The ORACLE\_SID is held in a file name called tnsnames.ora, you can find where yours is installed by referencing \[this\]\([http://www.dba\-oracle.com/t\_windows\_tnsnames.ora\_file\_location.htm](http://www.dba-oracle.com/t_windows_tnsnames.ora_file_location.htm)\) Honestly though, it'd probably be much easier to setup a VM and go through the 2 Day Oracle DBA course to get your environment setup. 
Blogspam. "There are a number of reasons why an execution plan might need to be manually rebuilt but the most common factor is data growth within the tables that the procedure references" Yeah, thats called statistics being out of date and dropping the plan is not the correct method to deal with that. It will still have the same bad statistics and will re-generate the same bad plan. No example queries to see what plans are cached. No examples of when a plan might be the problem and need to be dropped (bad stats is not a good example). "With a little luck a quick recompile of the stored procedure will resolve the issue. However, if you are still seeing performance concerns, continue to review the execution plan and step through the stored procedure. In many cases, a poorly written query will cause significant performance hits. If a query is poorly written then the execution plan will also be inefficient as it is relying on the poorly written query. Tuning the query may allow SQL to find a better road map for retrieving the data." This would be comedy gold if it was intended to be funny. Are you the author? IE the guy responsible for this gem http://jackworthen.com/2016/03/14/converting-rows-to-columns-using-pivot-in-sql-server/ storing months as strings? Seriously you are propagating bad practice and misinformation. Check out Brent Ozars website/blog for useful accurate information that adheres to best practice.
I like the suggestion posed by /u/Cal1gula, but as a side thought, why not try all of the solutions if time is not a factor? Unless you just want it to work and not to learn, I think it would valuable for you to build the solution in multiple ways and then compare the performance and architecture of them, it's good training. Teaches you the pros and cons from experience without killing an employer's db.
Sorry if I didn't. Just a suggestion. Tried to help.
+1 for this. If this was a production environment, I'd do it with the table join and not persist the category. Keep it as a fact table. That way when you join your categories, you can change it one place ... For instance, if you wanted to separate groceries into Multiple subcategories later. If you want, you could have an insert trigger that looks for new transaction types and puts in a record with a category of "unknown" so you don't end up dropping out records in a join if you forget to use a left join. 
thanks buddy .....yes the users table has a coulmn name Image. i take varchar size 20000 max. 
1. Read/learn about dimensional modeling, even if the DB that you'll e working on is not a DWH. I would recommend Kimball's books. 2. Learn about the DB engine internals - while not essential, it helps. 3. Read a book on data quality. Unfortunately, most of them are written about a castle-in-the-sky scenario, but knowing the 'ideal' state and some of the approaches helps. 4. Probably the most important - learn about the business/process logic. DB engines come and SQL standards change, deeper understanding of business/workflow drivers is a huge real-life candidate differentiation. 5. Network upward and out - your network tends to pull you. 6. Promote your work and results. It is NEVER enough that your manager knows about it.
My pleasure! Express has saved my butt (and wallet) quite a few times :)
You'll want to use dba\_tab\_cols \([https://docs.oracle.com/database/121/REFRN/GUID\-857C32FD\-AE30\-4AB9\-811B\-AC3A7B91A04D.htm](https://docs.oracle.com/database/121/REFRN/GUID-857C32FD-AE30-4AB9-811B-AC3A7B91A04D.htm)\) to get the table then wrap it with something like the previous comment. SQL would look something like select 'select '||owner||'.'||table\_name||' from dual where exists \(select X from '||owner||'.'||table\_name||' where X=Y\)' from dba\_tab\_cols where column\_name = 'X'; Then run the results or write some PL/SQL around it if you need to do it more than a one off.
Is there a way to pass all table names at once without typing them? 
Triple check your where clause (and verify you have one) every time you run an update and you'll be fine.
Navigating office politics and protecting your boss are almost as important as the skills you need to do your job. Three important rules that will help you avoid getting in trouble: (1) don't send or reply to an e-mail unnecessarily (2) don't speak in a meeting unless your input is solicited (3) don't take notes at meetings 
The third rule seem odd to me, cause when I was a QA in meetings, writing notes were always a good skill at meeting, helps me remember everything in a meeting and also remind people of they didn't remember notes. maybe it different at a government job but any more depth into that ? 
&gt; Network upward and out - your network tends to pull you. Can you explain this one?
I admit it's a little weird. It's particular to a previous employer. A project manager I frequently worked with occasionally asked if anyone took notes. Somebody gave my name, she asked to see them and my seemingly innocuous note taking turned into a political discussion about which team had what responsibilities and who said what. This happened several times.
You will get an error for invalid column
Lol I take notes during EVERY meeting and most people on my team starting copying me and doing the same.
I mean, you could do something as simple as where 1=1 and 'apples' != 'oranges' which are static conditions that are always true, and it would be valid sql containing data that don't relate to any columns in your sql statement. What do you mean by "element" in your case? You can also stick an entire subquery in a where statement that doesn't relate to your select data. For example: SELECT column1, column2, somedatefield FROM yourtable WHERE somedatefield = (SELECT max(someotherdatefield) FROM someothertable) In this example 'someotherdatefield' is not in the main select statement, but this is a common query that could select valid data. 
Always add begin Tran, rollback Tran, and commented commit tran to save yourself. 
You will get an error like "Unknown column 'age' in 'where clause'". You can play around with it here: http://sqlfiddle.com/#!9/5ebd1/4
1] Keep a couple of SQL reference works at your desk 2] Learn the structure of the databases you will be working with - do a little exploring and asking questions about what ambiguously named fields represent 3] Ask many naive questions about business processes that generate the data (especially during your first 3 months). This will help you build up a mental model of the business as well as give you a sense of which data sources are most reliable and which ones are almost certain to contain errors. 4] Ask to see standard reports from the data, you need to see both the queries/code that generate the report and a formatted ready to share output from those standard reports. This exercise will give you insight into the structure of the data and what has previously been thought important to business users. 5] Find some unmet need and fill it. Get a few early wins. 
Got any favorites references, I got the W3 schools one at least 
You could do a CASE WHEN (TimeStamp) = 1 THEN "SUNDAY" for each day and then group on that field while Averaging your Change. 
SQL Cookbook by Anthony Molinaro O'Reilly The Practical SQL Handbook by Bowman, Emerson an Darnovsky Online - Stackoverflow, W3 and more 
You're looking for the average net change for each day of the week? Maybe something like: SELECT DayOfWk = DAYOFWEEK(DayOnly) , AvgChange = AVG(DayChange) FROM (SELECT DayOnly = DATEADD(DAY, DATEDIFF(DAY, 0, TimeStamp), 0) , DayChange = SUM(Change) FROM table GROUP BY DATEADD(DAY, DATEDIFF(DAY, 0, TimeStamp), 0)) dc GROUP BY DAYOFWEEK(DayOnly) So you're first stripping the times off the dates and totaling the change, and then averaging those totals grouping by the day of the week.
I'd aggregate to day level in an inline view and then average that: SELECT dayofweek, TotalChanges / NumberOfDays as AverageChanges FROM (SELECT dayofweek(Timestamp) DayOfWeek, Sum(Change) as TotalChanges, Count(distinct date(timestamp) NumberOfDays FROM table GROUP BY dayofweek(Timestamp) ) Source
I was way overcomplicating things. I think adding group by does exactly what I needed. Thanks! 
So very generally speaking, if you've got a query like that you have to figure out what level you're outputting, and make sure you're joining and subquerying to reflect that. So in your example I think what you're trying to see is the total revenue for each userID from the orders table, and something (the number?) of engagments for that userID. To get that join working, you need to first get the total revenue for each userID from orders BEFORE joining to engagements. SELECT UserID, sum(revenue), count(engagements) FROM Orders INNER JOIN Engagements on orders.userid = engagment.userid This will not work - the sum(revenue) will actually be the revenue multiplied by the number of engagements. So sort it out like: SELECT x.userid, x.TotalRevenue, TotalEngagements FROM (SELECT userID, sum(revenue) TotalRevenue FROM orders GROUP BY userid) x INNER JOIN (SELECT userid, count(*) TotalEngagements FROM engagements GROUP BY userid) y on x.userid = y.userid By using subqueries* to make the IDs unique before joining you've ensured that joining them is not going to result in the join causing duplication. ^* ^(technically not a subquery but everyone calls it that)
 SELECT DoW = DAYOFWEEK(Timestamp) , Change = AVG(Change) FROM table GROUP BY DAYOFWEEK(Timestamp) would just be giving you the average hourly change for each day of the week, not the average daily total, though. I.e., if used on the data in your example you'd get DoW | Change ---|--- 2 | 7.5 Is that what you want?
Like I'm interested in selulium for web testing big I'm not sure it does database stuff? Is there a database SQL one I should look up ?? 
1) Start by formatting your code and separating it by command. Look at the sidebar picture, see how the select, the from and the where are their separate thing? each where clause, each join, orders, groupings should be it's own line at the very least. It helps US and more importantly it helps YOU read the code 2) Double check which tables you actually need based on the question and the database schema. 3) You are ignoring one of the requirements of the question, which is the where clause, fix that.
My other comment should give you what you need. Good luck! 
If an element is a column name, you will get “invalid column name” as an error. If it is a value of a column that doesn’t exist where the column you’re comparing against does exist, you will get “0 rows affected”. You could also get other errors like “invalid cast” if you try to compare a string value to a strictly typed numeric column. It all depends on what you mean by “element”
Are there more than just books that can be lent? Can you post a snapshot of the top 10-15 items of each table so we get sense of what the data looks like? 
[Here](https://imgur.com/a/Bk2Pg9X) 
Could you not get the information off one of the first screenshots in the description of the post?
This isn't a reason to not take notes. It's a reason to not work for/with idiots. It's also a reason to not show someone else your notebook (which they have no business looking at).
Tried responding earlier but computer crapped out. Had a long post with some additional information, but all i'm going to say is look on youtube to find SQL tutorials on joining multiple tables. Additionally, it's helpful to start with FROM statement, and fill in the other details from there. **IF ANYONE HAS ANY CRITIQUES, PLEASE LET ME KNOW.** Long time since I've joined 4 tables. SELECT C.Major AS \["Major"\], COUNT\(ii.InventoryItemID\) AS \["Total Checked Out"\] FROM Media M INNER JOIN Item i ON M.MediaID = i.MediaID INNER JOIN ItemInventory ii ON i.ItemID = ii.ItemID INNER JOIN Loan L ON ii.InventoryItemID = L.InventoryItemID INNER JOIN Customer C ON L.CustomerID = C.CustomerID WHERE M.MediaCode = 'Hardback Book' OR M.MediaCode = 'Paperback Book' **\(you can use a LIKE operator with '\%book' if you prefer\)** GROUP BY C.Major, ii.Status HAVING ii.Status = 'CheckedOut' ; Overall Tips: 1. Start with FROM to include all of your tables that you need. Think about what data you need, and what tables you need to get there. Since you only really need data from Media \(MediaCode\), InventoryItem \(status &amp; count\), and Customer \(major\), you start there. Then you realize you still need all the other tables to link this data together and join the others you need. 2. Don't forget to GROUP BY whenever you have aggregate functions \(Count, Sum, etc.\) 3. Youtube video on multiple joins: 
Sql express by default doesnt have the network bindings enabled. Go into sql config manager and enable tcpip.
\+
Never write a syntactically-correct `UPDATE` or `DELETE` statement right away. My process is: * Write a `SELECT` to figure out the data that will be affected. * Write the `UPDATE` as `UPDATE &lt;table&gt; WHERE &lt;where clause from SELECT` * Wrap the `UPDATE` in `BEGIN TRANSACTION`/`ROLLBACK TRANSACTION` * Complete the `UPDATE` statement * Test (with the rollback) * Change to `COMMIT TRANSACTION` Seems like a lot. It really isn't. And it's saved my hide enough times that it's worth it.
Thanks, this is what I was looking for.
What database are you using to execute the SQL code on? How and your ability to do this will be based on what tools/etc. you have available to you to be able to execute the SQL.
To add to this, make sure that Auto\-Commit is turned OFF in whatever IDE you decide to use \(if you use one\). 
I've used a little bit of MS, but won't OP still hav e to manually enter commas after each entry?
I have a couple of pointers - 1. See if you can use a Jupyter Notebook to upload the excel and then join. 2. A dirty way to do it would be to have your Excel column populated with the required MRNs, and in an adjoining column use the function CONCATENATE to add quotation marks and commas, and then copy paste. 
Nah you just build the string once on the first row then drag the formula down. I would use this method to build data injection / update scripts, opposed to fucking around with ssms import wizard. Bit ghetto in the eyes of some dB admins, but it works nicely when working with data from spreadsheets. 
I'm with ya. I just use python+snowflake to upload and query spreadsheets. 
dont do their homework for them
Idk I don’t post on this sub. Won’t in the future. 
If you're using report builder are you also using reporting services (SSRS)? If so, you should be looking at creating a data driven subscription. 
/r/datasets. I hate linking subreddits, but this subreddit is just perfect for your needs, including CSV files which is what Workbench works perfectly with.
you can just do a correlated subquery for each date, where you use ORDER BY, LIMIT, and OFFSET to get the first and second date.
I haven't had experience with limit/offset before, but it seems intuitive in a regular table. However, I'm not sure how to use it in a correlated subquery. Do you mind helping me out some more?
Awesome that looks perfect! Thank you!
thank you! I got it all working now! :)
thanks! I got it working
Do a row_number() and partition by part number, order by date ASC as 'rn'. Then you will select where rn = 2.
 SELECT p.id, p.name, p.description, p.quantity, p.category_id, This \^ selects the columns from the `product` table, it uses the table alias `p` to reference them. c.name as category_name This \^ selects the name column from `c`, then renames the column to `category_name`. Same concept as a table alias, but it's a column alias. FROM ". $this-&gt;table_name ." p This \^ says you're selecting all of the columns from `p` (product) LEFT JOIN categories c ON p.category_id = c.id This \^ says you're adding in the columns from categories based on product.category_id matching categories.id... the "LEFT" table is the first table in the FROM.. so `product`. 
Something like this should work for you unless I am misreading you. SELECT * FROM ( SELECT *, ROW_NUMBER() OVER(PARTITION BY PART_NUM ORDER BY PART_DATE ASC) AS 'RN' FROM PARTS_WE_WANT as A LEFT JOIN ALL_PARTS as B ON A.PART_NUMBER = B.PART_NUM AND A.PART_TYPE = B.PART_TYPE LEFT JOIN ALL_PARTS as C ON B.OLD_PART_NUM = C.PART_NUM AND B.PART_TYPE = C.PART_TYPE ) X WHERE RN = 2
You can add an Excel file as a linked server and treat it like a TABLE.
This looks great! I had no clue about the Row\_Number\(\)/Over\(Partition by\) functionality, that looks extremely useful. For the RN = 2, I am trying to build a case statement. If B.OLD\_PART\_NUM = C.PART\_NUM, then RN should be two like you wrote. If B.OLD\_PART\_NUM \&lt;\&gt; C.PART\_NUM, RN should be 1. The only problem is that I'm having trouble with the references. Apparently I can't reference table B or C in the outer query, since the inner query runs first. But I can't include it in the inner query either, since user\-defined RN isn't defined by the WHERE statement. Any idea how I could build that in?
yay glad to help 
Would it make more sense to have a master item table: ItemID: 1 ItemTitle: "Come to Florida" ItemDescription: "Come to Florida, dolphin design" Then have a table for variations: ItemID|VariationID|VariationColor|VariationSize|VariationStyle :--|:--|:--|:--|:-- 001|1|Red|Small|Shirt 001|2|Black|Small|Shirt 001|3|Red|Medium|Shirt 001|4|Black|Medium|Shirt
Paste your code.
this says you got an upvote
Don't forget you can do a second row_number() outside in the select and then wrap that up in another subquery (call it Y since the first one is X) and then pick where RN = 1, or whatever.
Correct. It takes a week to learn basic syntax. It takes months, probably years to put it together at an advanced level.
[Here](https://www.reddit.com/r/SQL/comments/8dgvmn/ms_sql_say_you_have_a_table_of_currency/dxn6c6w/) is a piece of code I wrote about a week ago that might help you. It uses row_number() multiple times in a CTE. Just a good thing to look at in terms of understanding how it can be used in the future. It's a good function to "master" and will get you around most problems you run into like this.
 Select * into newtable from oldtable where criteria
Absolutely! I'll go through it, but the advice you gave is super helpful to these types of situations. I'll definitely get familiar with it for the future.
Well, for one, you shouldn't if you only want records that share a value in your join to return.
1) Introducing records with no match when that behavior isn't desired 2) Performance loss (disclaimer: depends on a lot of things so this is a broad assumption) 3) Doing more logic on the result of #1 but then not accounting for null conditions that you introduced. Generally you only use an outer join in the scenario of "Try to match up my table with another table, but if it can't find a match, I still need all the data in my original table. The rest, of course, will be nulls. And then you gotta deal with those suckers because nulls will screw ya. 
I would use python or ssis to extract excel data.
You can PM it to me, but I don't see why you can't post it here. Clean it up if it has proprietary field names, etc.
Two major performance increases are from not indexing the table you are indexing into. If you index your "new" table, it will effectively double your inserts. If your running these queries on workbench on your own machine, you would benefit greatly from a SSD or a better HDD, which would increase your R/W speed massively.
Table-valued params. That's the native way to pass data sets to a proc as a parameter. You could also pass a comma separated list and parse it or send in xml but table valued params would be easier. 
NP, the thing to look at functions like this: Say you have a situation like the one you're in, and you want `RN = 2, but maybe some parts only have an RN = 1, and therefore you'd be missing those in your query. This is all happening because of the way you use the order by, i.e.: `ROW_NUMBER() OVER(PARTITION BY [Something] ORDER BY [Something] ASC) AS 'RN1'` So what you do is take everything greater than 3 (or all the 1's and 2's. Then what you can do is this: `ROW_NUMER() OVER(PARTITION BY [Samething] ORDER BY RN1 DESC) AS 'RN2') So what this will do is flip things around. Now the 2's will become 1's, and if there is no 2 then the 1 will stay a one. Then you can select `WHERE RN2 = 1` in the outer subquery, and `WHERE RN1 &gt; 3` in the inner subquery. Things like this are incredibly useful when you're trying to remove duplicates from data before a join, etc.
The Derek Zoolander school of SQL
Couple of assumptions: 1. This is for school. 2. In order to accomplish this you will need some kind of functions to sort your data to the most efficient structure based on the total number of rooms, hours in a day, etc. 3. The second point is the hard part, and where all the work for this project is. Since you have made no progress for weeks, you have not started that aspect. Arbitrarily say you have 1 room and a potential for 100 different events to be scheduled on any given day, each ranging from 30 minutes to 4 hours. Stacking them is arbitrary in terms of efficiency. It doesn't matter if you schedule event 1, then 2, then 3, then 4, or 50, then 1, then 4. The only difficult part is determining whether there are enough hours in the day to accommodate the scheduled events, and then if so, are there any events which are short enough to fill any of the unused time. Your problem is just compounded by having multiple rooms. You're probably going to need several things to solve this problem, a few sprocs, functions, or views. It likely isn't going to be just "a query" that does it for you.
Do you have a primary or unique key in the table to make finding a duplicate easy? sqlite has INSERT OR IGNORE to discard constraint violations... Not sure about other SQL engines.
I do too because I work with public health data that is dirty. I need all the records of all the tables and null values are acceptable.
Wow, this is genius! Thank you for taking the time to spell this out! Hey, it's alright. The code is immensely helpful regardless, and I'll fix it tomorrow in the data.
Only cross joins here bud 
It sounds really math\-y! I actually enjoy that, and believe it or not the data I was looking at is breaking so many conventions that I can use all the logic I can get. Thanks again for your help!
There is no data in y. I'm given a data and I have to model my data and insert into tables I made. The data is sitting in a sql server.
Some messages are super cryptic with SQL Server, this one isn't... It is telling you which table is throwing the violation and even tells you the value that is causing the error. What do your insert statements look like? What happens when you do: select * from dbo.Y where Z = 4963801 ? What does your create statement for dbo.Y look like?
&gt;2) Select all rows from db table. That's terrible. Try using a NOT EXISTS in your WHERE clause to exclude rows that are already present.
If I understand your question correctly, than no, sadly you will have to parse the text of dynamic SQL to find dependencies. The same way you can create a procedure, executing dynamic SQL doing a select on non existing tables, there is no checking for dependencies on dynamic SQL. 
I second this. In my experience if I were to write the same values, two rows, in one insert, it will take the first and fail on the next. 
This script does not "search for" but rather "displays something already found" if this makes sense? I.e. it selects dependencies of compiled objects that Ms SQL engine discovered. Dynamic SQL has not been compiled yet (duh?) so there are no dependencies.
Maybe you should output query results to a CSV, since your results are in the thousands of rows? Stack Overflow link: https://stackoverflow.com/questions/6076984/sqlite-how-do-i-save-the-result-of-a-query-as-a-csv-file
Full outer join just to be safe.
I don't think this is really a question of how to speed it up (there aren't a ton of ways to do that SQLite is pretty minimalistic). Seems like more of a question of why it's crashing. You can limit the number of rows returned but that may not resolve youur instability problem.
It gets everything.
But wouldn't that just be worse? Then that adds another step. You extract the data, but now you have to clean all of it and get rid of the stuff you don't want, which would add even more stuff to your query whereas you could just get what you want in the first place instead......
This is how you create a SQL injection. Don't do it this way. There are plenty of safer and more robust methods of importing a spreadsheet into SQL.
Yeah - I was just joking. How can you do a left out join in every statement ? The data I work with isn’t great but I definitely use inner joins especially on dimensions I’m building myself. You are just missing so much if you stick to one join type.
It's mostly SQL, with some extensions for their own particular concepts. They want you to build a Data Warehouse for them? TD is good for that, if a bit expensive, but your company should pay for some training on how to use it for that.
Oh, the usual. I bowl. Drive around. The occasional acid flashback.
Use the SQL Server import and export wizard. In SQL Management studio right click on your database in the object explorer. Then Tasks &gt; Import Data and follow the wizard. It will create a table with the same headers as your spreadsheet and import the data into that table. You may need to manually set the types and data lengths. Alternatively you could write a quick application in C# or VB using the Text Field Parser class (which exists inside a VB namespace but it works fine in C#) and parse the data into a SQL command. https://stackoverflow.com/questions/22297562/csv-text-file-parser-with-textfieldparser-malformedlineexception If you need to call a procedure to get the visit info them write a cursor to iterate over the rows and execute the proc. It is very bad practice to take user supplied data and construct SQL strings in the manner that others have suggested here, it will not work if there is a single ' mark and it introduces a massive security hole. What if one of your MRN values was ';DROP TABLE Users;-- ?
No they want me to just get data from it. So I need to be good with using SQL. What I don't get is if the SQL commands are different in other different database languages... Say I'm using Oracle or Microsoft SQL Server. I get the SQL is a language. Sort of. But is it always the exact same commands? For a select query does it change across teradata or Microsoft? That's my question I guess
&gt; Bit ghetto in the eyes of some dB admins Yeah cause its a massive security risk and really bad practice.
Look at his post history. This person frequently posts their blogspam here and I've had a look at a few articles and they are full of bad advice and bad practice. Take this gem for instance: http://jackworthen.com/2016/04/27/rebuilding-the-execution-plan-of-a-stored-procedure-in-sql-server/ Apparently out of date statistics will be fixed by rebuilding the procs query plan. "There are a number of reasons why an execution plan might need to be manually rebuilt but the most common factor is data growth within the tables that the procedure references. If a certain table only had 1,000 records when the initial execution plan was built but has grown significantly since, the existing execution plan may no longer be the most efficient means for collecting the data. This is generally a result of changes to the statistics of your indexes as a result of data growth. " 
SQLite performs pretty well, it's probably not where you should look for the problem. I suspect, as /r/Boxy310 does, that the problem lies in the large output. Try to add a `LIMIT 100` to your `SELECT` queries so the output is not overly long.
Good bot.
Thank you, fizzgiggity, for voting on BigLebowskiBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
[Huh.](http://i.imgur.com/0auxGaZ.gif)
This question is more appropriate for /r/cscareerquestions but one thing to consider is that they're going to ask you why you're qualified. You mentioned you've done some basic query stuff but plan ahead for when they ask you about more advanced concepts like stored procedures, transactions, database replication, etc. If you can talk about those concepts it should make you appear like a better fit for the position.
The language is very similar but there are definitely some nuances as to how TD stores data and also how it’s engine is set up. It’s been a while but if I remember correctly one of the things that got me was that you needed to set up temp tables to accept duplicates (I could be wrong though). I would definitely ask for some training ahead of time, especially if you’re building user facing objects.
&gt; I'm meeting with the director of a BI department about and intern position involving SQL. &gt;How can I communicate to the director that I know SQL and get that intern position? You are golden. The biggest mistake that I have seen candidates make is to just sit there and answer specific questions. Just wait for an opening. When the interviewer asks you to tell him about yourself, make it a point to mention that you have SQL experience and that you really look forward to getting back into the SQL space. Let he or she know some of the more advanced aspects of SQL that you enjoy using. For example, you might enjoy common table expressions, tuning select queries, etc.
I agree. 1) Convert the Excel data to CSV (I created a Python script for this purpose, and vice versa), then import it into your SQL database. 2) Or just do the whole thing using Python.
Thanks for the info. I still don't understand... &gt;The bit i don't understand is how it knows the reference to each table? May you explain please? Is the p reference to product as simple as the section "$this-&gt;table_name ." p", which would translate to "product p". And c reference to categories in the section, "categories c"?
Saying you enjoy CTEs sounds a bit silly; don't say that, OP.
Assuming SQL Server (folks, please tag your questions!), you should be able to use `SYSTEM_USER`. That'll give you the login associated with the session. This will break for you if you use `EXECUTE AS` to impersonate/elevate. If this is coming from another application (web apps especially), you're probably using a an application account (all traffic from the application uses the same login) so you'll need to pass the username in from the application to get the *actual* user (the human being performing the action).
\+
Maybe CTE will Help, long with a PARTITION BY keyword. Here’s a full query using your existing table. I used another CTE to generate your table, but You can use this and just replace the YOURTABLE name with your actual table. WITH CTEPARTITION AS ( SELECT ID, VALUE, COUNT(VALUE) OVER (PARTITION BY ID ORDER BY VALUE ASC) AS ROWNUMBER FROM **YOURTABLE** ) SELECT ID, VALUE FROM CTEPARTITION WHERE ROWNUMBER = 1 ORDER BY ID;
2008 huh. I'd probably just brute force it. DECLARE @json varchar(max) = '{"event":"loginTapped","properties":{"time":123456789,"distinct_id":"A123BCD4-5678-9EF0-12GH-IJ3K45L67M89","$app_build_number":"01","$app_release":"1.0.0","$app_version":"10","$app_version_string":"1.0.0","$carrier":"SomeCarrier","$city":"Pleasantville","$from_binding":true,"$lib_version":"1.0.0","$manufacturer":"Apple","$model":"iPhone8,1","$os":"iOS","$os_version":"11.3.1","$radio":"None","$region":"Ohio","$screen_height":667,"$screen_width":375,"$wifi":true,"mp_country_code":"US","mp_device_model":"iPhone8,1","mp_lib":"iphone"}}'; SELECT SUBSTRING(@json,CHARINDEX('"event":"',@json) +LEN('"event":"') ,CHARINDEX('",',@json,CHARINDEX('"event":"',@json)) -CHARINDEX('"event":"',@json) -LEN('"event":"')) AS [event] , SUBSTRING(@json,CHARINDEX('"time":',@json) +LEN('"time":') ,CHARINDEX(',',@json,CHARINDEX('"time":',@json)) -CHARINDEX('"time":',@json) -LEN('"time":')) AS [time] , SUBSTRING(@json,CHARINDEX('"distinct_id":"',@json) +LEN('"distinct_id":"') ,CHARINDEX('",',@json,CHARINDEX('"distinct_id":"',@json)) -CHARINDEX('"distinct_id":"',@json) -LEN('"distinct_id":"')) AS [distinct_id] , SUBSTRING(@json,CHARINDEX('"$app_build_number":"',@json) +LEN('"$app_build_number":"') ,CHARINDEX('",',@json,CHARINDEX('"$app_build_number":"',@json)) -CHARINDEX('"$app_build_number":"',@json) -LEN('"$app_build_number":"')) AS [$app_build_number]
lol nice
retail here... amen. need to join 10+ tables to get any decent item level report with how messy this db is. 
&gt; "Try to match up my table with another table, but if it can't find a match, I still need all the data in my original table." That is an accurate description of all my queries. I work in retail and generally am bringing lots of different tables together to describe a single item. 
Lots of left joins are very common in unstructured or denormalized data such as what you might find in a data warehouse or a "data mart" as some call it. You're basically fishing for matches, but you don't want to drop records when there is no match. Here's a real world example from a past job: I used to work at a financial institution. The base table was "customers", and we had separate tables for checking, savings, loans, mortgages, certificates of deposits, etc. So if I wanted to profile a customer for marketing purposes by listing out the balances for each product they own, if I were to inner join all the tables they would drop from my record set unless they own all ALL of those products. Hence left joining everything together. But at the same time, inner join was appropriate when answering the question of "For all active loan customers, give me their origination date and their current balance". In that case an outer join would give me everyone who did or did not have a loan, when I only wanted customers with loans. Each join type has its place. I've even used the rare "cross join" in certain scenarios. Usually because I'm generating possible combinations of data for some other use. 
I quickly started to ignore these posts, I mean I cannot fault someone for trying, but some of the content could be outright dangerous. One recent post advocates using xp_cmdshell to check for head blockers, with no mention of the fact it’s disabled by default, runs processes as the SQL Server Service Account (possibly a Domain Admin) and requires SA (or a proxy) to run. So technically he's not wrong, it will work, but there are other better and safer options. However that said, outright censorship is wrong, ideally, the mods will have some ideas how this sort of content can be tackled, education is better than indoctrination. 
So those in the same position are stuck scrolling through for dynamic SQL? No procedural way of doing it? That’s unfortunate.
Any way of compiling the dynamic sql into the dependency temp table or do you have to scan through each proc yourself for dynamic SQL?
I think mentioning CTEs is a way to help them casually assess your SQL experience. I would say you enjoyed first learning about them and then becoming comfortable using them. Over lunch they may not want to watch you take a SQL assessment so sharing enough details about your past experience will help them size up your desire to learn what they intend to teach an intern. You'll want to find out about that anyway since it's also about you interviewing them too e.g. will this be a good internship for you and your goals?
If it comes up in an organic way, sure. But saying, 'I like CTEs' is a lot like saying 'I like hammers.' Sounds weird. Even if the discussion gets steered toward how much SQL he knows, it's better to talk about problems you've tried to solve. It's a lunch interview for an internship. Imo, he's checking for personality more than technical chops. It's good to seem passionate, but don't force it with stuff like this.
So basically you have a query that comes up with a list, and you want to insert the rows of data that do not exist into a table, and not insert/skip the ones that do exist?
the purpose of dynamic SQL is that the code does not exist in runnable form until the run time. Also, each run it could have different dependencies (e.g, I can add a 'join X on X.id = FROM_Table.X_ID' to the query I get a relevant parameter). Or I can actually read the code from some configuration table. I dont think there's a general "solution" to this at all. Look at the stored procs in your databases and see if you can detect specific practices and maybe you can look for these. I.e. you might be able to search for 'join &lt;string&gt;' for tables and something like '[dbo].[uf_...]' for user functions, etc.
Exactly so. I'm really new at this and it took hours to get this working. Everything I'm seeing are completely different approaches to what I have here.
Bad statistics will not be fixed by a new plan. It will simply generated the same bad plan based on the same out of date statistics. An example: The table has 10000 rows. The statistics thinks it has 100 The plan assumes 100 cause thats what the stats say. You drop the plan. You re-create it. The plan is still assuming 10 rows. Updating the statistics will cause the plan to be re-created anyway. Recompiling the plan in the situation of bad statistics is not going to fix anything. Its absolutely false information masquerading as fact.
So before you do the insert remove the duplicates either using something like WHERE NOT EXISTS, a join, etc.
Oooh. So for the description of steps I mentioned, the ANALYZE TABLE step would update both the statistics and any precompiled execution plans, but the second step of rebuilding execution plans explicitly would be a no-op?
Hmmm... that’s where my mind went originally but it didn’t seem like the best practice. Thanks!
&gt;outright censorship is wrong I agree but I am not wanting to try take his website down which would be denying access to the bad information. I would instead prefer to view it as caring for our fellow users so they are not mislead by this person. As you have correctly identified, some of the posts are dangerously wrong and have the appearance of being correct to someone who is not familiar with the area. Most are filled with bad practice and the person really has no business running a SQL blog. 
Can't get my head around that. There are no duplicates until I INSERT. I'm either thinking or going about this wrongly.
Updating the stats mark any plan using those stats as out of date. You shouldnt need to explicitly drop the plans. 
You probably have your criteria and your column identified incorrectly. SELECT * FROM MyTable WHERE 'MEAT' = 'PRODUCT' or SELECT * FROM MyTable WHERE 'PRODUCT' = 'MEAT' For whatever reason its treating 'MEAT' as your column not your criteria. Don't qualify columns in mssql using single paranthesis ' use Brackets if you must. SELECT * FROM MyTable WHERE [PRODUCT] = 'MEAT'
I think elegant solutions tend to be overrated. Just because code is pretty or clever doesn't mean it's more efficient.
Makes sense. Thanks for explaining
Here's an elegant solution just for fun. I'd still personally prefer it spelled out. DECLARE @json varchar(max) = '{"event":"loginTapped","properties":{"time":123456789,"distinct_id":"A123BCD4-5678-9EF0-12GH-IJ3K45L67M89","$app_build_number":"01","$app_release":"1.0.0","$app_version":"10","$app_version_string":"1.0.0","$carrier":"SomeCarrier","$city":"Pleasantville","$from_binding":true,"$lib_version":"1.0.0","$manufacturer":"Apple","$model":"iPhone8,1","$os":"iOS","$os_version":"11.3.1","$radio":"None","$region":"Ohio","$screen_height":667,"$screen_width":375,"$wifi":true,"mp_country_code":"US","mp_device_model":"iPhone8,1","mp_lib":"iphone"}}'; SELECT [event] , [time] , [distinct_id] , [$app_build_number] , [$app_release] , [$app_version] , [$app_version_string] , [$carrier] , [$city] , [$from_binding] , [$lib_version] , [$manufacturer] , [$model] , [$os] , [$os_version] , [$radio] , [$region] , [$screen_height] , [$screen_width] , [$wifi] , [mp_country_code] , [mp_device_model] , [mp_lib] FROM (SELECT a.AttributeName , SUBSTRING(j.[json],CHARINDEX('"'+a.AttributeName+'":'+a.OpenDelimiter,j.[json])+LEN('"'+a.AttributeName+'":'+a.OpenDelimiter) ,CHARINDEX(a.CloseDelimiter,j.[json],CHARINDEX('"'+a.AttributeName+'":'+a.OpenDelimiter,j.[json])) -CHARINDEX('"'+a.AttributeName+'":'+a.OpenDelimiter,j.[json])-LEN('"'+a.AttributeName+'":'+a.OpenDelimiter)) AS AttributeValue FROM (SELECT @json AS [json]) AS j CROSS APPLY (SELECT * FROM (VALUES('event','"','",') , ('time','',',') , ('distinct_id','"','",') , ('$app_build_number','"','",') , ('$app_release','"','",') , ('$app_version','"','",') , ('$app_version_string','"','",') , ('$carrier','"','",') , ('$city','"','",') , ('$from_binding','',',') , ('$lib_version','"','",') , ('$manufacturer','"','",') , ('$model','"','",') , ('$os','"','",') , ('$os_version','"','",') , ('$radio','"','",') , ('$region','"','",') , ('$screen_height','',',') , ('$screen_width','',',') , ('$wifi','',',') , ('mp_country_code','"','",') , ('mp_device_model','"','",') , ('mp_lib','"','"}')) AS a(AttributeName,OpenDelimiter,CloseDelimiter)) AS a) AS x PIVOT (MAX(x.AttributeValue) FOR x.AttributeName IN ( [event] , [time] , [distinct_id] , [$app_build_number] , [$app_release] , [$app_version] , [$app_version_string] , [$carrier] , [$city] , [$from_binding] , [$lib_version] , [$manufacturer] , [$model] , [$os] , [$os_version] , [$radio] , [$region] , [$screen_height] , [$screen_width] , [$wifi] , [mp_country_code] , [mp_device_model] , [mp_lib] )) AS p;
OT - I own two for 8 years and I don't think I _know_ them.
I don't think you can own a cat, just have one that lives with you that tolerates your presence.
I'm not a fan of most of the articles, and posted some opinion to the most recent about using sql agent and xp_cmdshell to log blocking. I don't like that they post things like this and then rarely if ever come back and respond to comments made in Reddit. Google can be used to sort the value of the data on that blog, we don't need it spammed to reddit as well. Just my 2 cents.
If I'm reading him correctly there aren't any duplicates in his set, but there are records that already exist in the table he is trying to insert into. So he needs to "dedupe" his set (i.e. remove any of the rows that already exist in the table he is trying to insert into.) Is that correct /u/shalafi71
He could just do a join between the sets where scotttemp is null, no? Maybe right join or something like that? Antijoin? I can't test anything today but it shouldn't be difficult to do in a straight forward way.
I like this, because it allows me to add/remove data elements easily. Out of curiosity, why use CROSS APPLY instead of CROSS JOIN?
Except seems like the best route.
Something like this may work SELECT Code FROM TABLE WHERE CODE NOT IN ( Select CODE from TABLE where FK1 IS NULL) Group by CODE
Select a.Code from table A join table B on a.Code = b.Code and a.id = b.id where b.fk1 is not null On mobile, but if I'm understanding correctly, this is what you want. 
 SELECT code FROM datatable GROUP BY code HAVING COUNT(*) = COUNT(fk1)
I used CROSS APPLY out of habit without thinking about CROSS JOIN. In my head I was UNPIVOTing the data to apply standardized logic, then PIVOTing it back. The CROSS APPLY + SELECT VALUES is equivalent to UNPIVOT, but in this case it's simply duplicating the json column (like a CROSS JOIN) instead of splitting out individual columns. The database engine should understand them to be equivalent.
&gt; and a.id = b.id that's not gonna work
You're right. Take that part out. I thought they matched per row. Scanned it too quickly because it was not tabled format. 
Should be HAVING COUNT(FK1) &gt; 0 depending on ANSI_NULLS setting. 
&gt; Take that part out that still won't produce the desired result it won't return id 3 but it will return id 1 and 2 (incorrectly) 
&gt; ANSI_NULLS setting. has nothing to do with counting nulls see https://docs.microsoft.com/en-us/sql/t-sql/functions/count-transact-sql?view=sql-server-2017
UPDATE: We had the lunch and our conversation seemed to go well. Right now, there are 8 people being interviewed included me. The person I met with for lunch basically said he liked me but he can't guarantee anything because the position is really supposed to be for kids in college or coming out of college. Overall, I'm taking it as a learning experience and a networking opportunity. It would be great to get back into the SQL space. Now, I need to wait and see if I get a phone call from the manager that is doing the hiring. Oh, and we didn't really talk about SQL much. It was more generic questions about why data analysis, both of our backgrounds, the company, the goals, my 5 year plan, how communication is very important, and that the job would be a contract position. Maybe they are hiring a temp and intern. That's how it sounded but I wasn't exactly sure by our conversation.
dbo.scotttemp has no PKs, just made it to test with. The *actual* table throws an error if I try to insert a duplicate record.
Exactly so.
Thanks! This worked
Thanks!
Glad to help!
Read what this dude slingalot talking about. What you're asking for is fairly straightforward. If you can't get it I'll write it for you.
Yep, those two parts are how the tables and the aliases are referenced 
I'm glad that it went well - and good luck! &gt; he can't guarantee anything because the position is really supposed to be for kids in college or coming out of college It sounds borderline ageism. Maybe he meant that he expects young, recent grads to expect a lower pay rate?
I believe that did it! I'm sure it's not the best way but it will get me through this task until I learn more. Thank you from the bottom of my heart. 
That should just be a SELECT DISTINCT. You're not saving the query engine any work with a GROUP BY and it's less semantically clear what you're trying to do.
mytable = your base table with IDs and values mytable2 = CTE containing data where it's A mytable3 = CTE containing data where it's NOT A From that, we can construct a union. We want everything in my table 2, and then we also want to append everything that wasn't found on the first time through (NOT A), where the value is "B". Result: 1, A 2, B 3, A with mytable as ( select 1 id, 'A' value from dual union all select 1, 'B' from dual union all select 2, 'B' from dual union all select 3, 'A' from dual ), mytable2 as ( select id, value from mytable where value='A' --what is A ), mytable3 as ( select id, value from mytable where value!='A' --what is not A ) select id, value from mytable2 --we want everything here, this is your "first time through" union --combined with select id, value --IDs and values from mytable3 --from those not found in your "first time through" where id not in (select id from mytable2) --where ID is not in "what is found on first time through" and value ='B'; --and the value is B with the data not found on "first time through"
Some Intern positions are specifically targeting students currently attending college. Source: Was a summer employee for Lockheed Martin. They couldn't classify me as an intern because I didn't have enough credit hours. Given I had done the exact same job the year prior when I was in HS, they didn't want to train someone else so they ended up outsourcing me to a consulting group and paid me $5 more an hour than I would've made as an intern.
Man, this food is really good! I like Execution Plans! Have you tried the Caesar Dressing on this salad?
Damn these are some complicated queries people are giving you. You can just... (SELECT DISTINCT code FROM myTable) EXCEPT (SELECT DISTINCT code FROM myTable WHERE fk1 IS NULL)
Never knew you could use EXCEPT, or omit records like this. Neat!
At this point in my career I have at least 200 or so I use frequently. Small ones like searching all objects for text, Glenn Berry scripts, sp_whoisactive, biggest tables, free space (in db or on drives). It goes on and on.
I thought it was excellent, but note that it was written in 2005. If you consider yourself a novice, then there will be plenty to learn. SQL doesn't change that fast so the age probably isn't an issue. You'll just get some out of date information on what various platforms support (lead and lag for SQL server as of 2012 comes to mind). It definitely helped me improve. 
https://dbatools.io/ I have SQL queries saved or stored procedures saved for: * Getting me the sizes of all my data &amp; log files. * Basic backup/restore/copy of databases Rather than save individual queries, I have a few snippets in SSMS to take care of boilerplate for me. Most notably, to create a cursor &amp; the shell of the loop for it (I frequently work with large numbers of DBs on a single instance at once, so a cursor is a necessity there).
You're pretty much correct. (Microsoft's implementation of SQL is TSQL not MicrosoftSQL). There isn't a lot between the different implementations when learning the basics. The only thing I'd suggest is steer clear of Microsoft Access and SQL Lite. Microsoft SQL server is a good one to start with as you can get an express or development version free.
There is no such thing as MicrosoftSQL. Microsoft uses T-SQL (Transact-SQL) across their products i.e. SQL Server and it’s definitely not basic, probably one of the most advanced implementation taking into account different functions and options. MySQL is a different database engine, nothing to do with Microsoft. There is also a very advanced PL/SQL - ORACLE and bunch of others such as DB2, Postgres and more. Most if not all database engines fully support ANSI SQL which is the same across all products with vendor specifics deviations through their own flavours of the SQL implementation, i.e. ORACLE will have ORACLE specific things that T-SQL does not support and vice-versa
Why not SQLite? For learning the basics and smaller projects it's fine.
i use metadata a lot.
No window functions and lack of good community resources.
&gt; Most if not all database engines fully support ANSI SQL I think *no* database engine fully supports ANSI SQL standards. Which one supports `create assertion ...`? (That's in the SQL-92 standard.) 
You are right. Bad wording on my part. I meant they fully support ANSI SQL ie one can use ANSI but not that they support all of it. Thanks for correction. 
Would you mind sharing these?
Assuming you have a connection to SQL Server, Oracle, etc. you can use Access pass through queries to write SQL in the syntax of the linked database. Agree about Access' JET SQL being annoying/non-standard.
I use HANA Studio what kind of sql is that
I remember learning SQL through a combination of Access and SPSS database connections. Good times.
I think if you touch sql at all, having a basic understanding of the algebraic background is really helpful as it helps allude to thinking in sets and thinking how sql likes to think. 
Sorry, I'm not looking to be spoon-fed here but looking through the comments, yours seems to be the path that I should be taking. However, I have no clue what is relational algebra or anything like that. (When I meant beginner, I really meant it) I'm currently at the stage where I'm learning about **CREATE table ...., SELECT * from table** that kind of stuff. Could you point me towards any resources or programmes that I should use to learn all these syntaxes? I see a comment below saying SQLite. Is that the right way to go?
I recommend that you just pick a vendor, (a ubiquitous one, I mean — i.e., Oracle, Microsoft, etc.) Find a free/personal database to download and just get started. Once you’re SQL-literate, transitioning from one vendor’s version of sql to another is not the end of the world. The most important thing is to have a platform to understand the concepts and then learning to implement them. Once you’re comfortable with one “flavor,” you can try out the others to see where they’re different. That said, I do recommend beginning with a traditional, relational database. From there you can move on to columnar/in-memory option, but the basic principles of an RDBMS are still (imho) the best foundation for good SQL practices.
A temporal table keeps track of every change made to a record and keeps the history in a secondary, semi-hidden table. Then you can query the table as of a particular date/time (or date/time range).
Hi, A list of links of SQL exercises. All levels [www.practity.com/591\-2](https://www.practity.com/591-2)
Oh awesome, thank you.
The two dialects are 95% identical. MS lets you use variables, Teradata doesn't. That one hurt me deep inside. How TD works behind the scenes is very different than MS, but you never need to worry about that. You just run a query and then some data comes back. Just like you're used to. Random other things that come to mind: * Working with dates beyond simple comparisons is different. * String concatenation is || * Replacing a character is OREPLACE * No more IsNull, only COALESCE. * Teradata's SQL documentation really sucks. Overall the transition wasn't too bad, if you are relatively proficient with SQL, you'll be fine. 
Why is their documentation so bad??
I just left my company that's an SAP partner and working with HCI. It's like SQL and not and then it has it's own mind sometimes because random patches SAP puts out without notice.
One thing that helped me get in the door to have an interview was listing the programs that I was comfortable with on my resume. Then once I was on a phone interview or in person and they ask about myself (usually the 1st question) I mention that I enjoyed **** project/task/program and that is what made me apply for this position. Easy way to check it off before the non computer interview questions that are bound to happen, especially when HR is hiring instead of an IT manager.
For my work we write our own code for unit testing. But it's a custom application. At my wife's work I think it's built in with her tools like datastage. I don't know of any general purpose tools for it. Sorry.
There's also INTERSECT for matching records.
Red gate seems super cool. Unfortunately, I am poor and not yet doing SQL as a profession :/
Easiest way to get started in my opinion is the website data.world No downloads needed, just sign up, pick a dataset, and start writing SQL in a box against it. They have tutorials and everything, totally free, always recommend it to beginners. No affiliation or anything, just doing great work.
Probably saves them lots of money not having to provide documentation. 
What do you mean by no window functions? Like a GUI?
Why not build a few tables in ms access and use the querying tool to learn basic sql to start.
https://en.wikipedia.org/wiki/SQL_window_function
I really do appreciate your feedback regarding that particular article. After reviewing it I did come to realize that I did not specifically discuss updating statistics. Often when writing an article I try to focus on one particular task as attempting to cover multiple disciplines sometimes ends up muddying the water a bit. My goal with that post was to simply explain how to manually rebuild an execution plan. Though I tiptoed around statistics, I should have either gone into more detail or stuck exclusively to the concept of manual execution plan rebuilds. I will update the article to better reflect this. In regards to other concerns. I do agree, in some cases there are better options available and for some, these may not be considered "best practice" in certain environments. Perhaps in future articles I will attempt to make note of this and offer a brief description of alternative methods. However, many of my posts are responses to questions I receive on how to go about doing something a certain way. Dissecting a post to find reasons why one user might not want to handle it a particular way is easy. Again, I appreciate the feedback and will certainly take these comments into consideration when writing future posts but there will always be those that say "why not do it this way" or "why not do it that way." 
I totally agree except that SQL is in fact actually a full programming language. It's a bit pedantic I know 😆, but... Window functions and recursive CTEs make SQL Turing complete. SQL is first and foremost a query language, but it can actually be considered a full programming language now 👍
In my experience, SQL is in high demand at the moment, especially knowing proper implementation and security measures. I would for sure put it on your resume if you know it well. Knowing SQL and Linux seem to be both in high demand at least in my state.
Data Analyst. Hands down the best for entry SQL jobs with upward mobility. This job is crazy popular and accepts those from a vareity of disciplines. From here, you can be a senior, career data analyst/BI Developer (the charizard of data analysts), Data Scientist, database developer, data engineer, data architect, DBA, etc...
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 237.1 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about resume advice. But, I'm only ~6% sure of this. Let me know if I'm wrong! Have you checked out TalentWorks, /r/resumes, TIME? They've got some great resources: * https://talent.works/blog/2018/01/08/the-science-of-the-job-search-part-i-13-data-backed-ways-to-win/ * https://www.reddit.com/r/resumes/ * http://time.com/money/4621066/free-resume-word-template-2017/
&gt; career data analyst/BI Developer (the charizard of data analysts) Google says this is a Pokemon, but beyond that I have no idea what you're trying to say. Can you explain?
Just wanted to say that I 100&amp;#37; agree with this. My current job title is "Senior BI Developer" and I work at one of the largest technology companies in Utah. I have a high school diploma, and I started down this path as a data analyst. If you haven't already gotten good at using Excel, I'd say to add that to your to\-do list. Get down the basics of pivot tables, vlookups, building charts, and general excel formula syntax. Keep playing with SQL while you learn that. Once you're fairly comfortable, you can put that you have intermediate Excel skills on your resume, along with SQL skills, and you could very likely land a data analyst job. From there, the sky is the limit :\) At my company, degrees frankly don't matter. Sure it might help get your foot in the door, but being good at SQL is 90&amp;#37; of what we care about. We just interviewed a person with 16 years of SQL experience, and a masters degree in information systems, and didn't hire her, because she bombed our SQL whiteboard test. A lot of our hires are self taught people, who are passionate about SQL. Find a love for data, and keep learning, and you'll go far \(and frankly make a ton of money, because the demand right now is huge, and we can't even find enough good people to hire\).
What was on the whiteboard test and how on earth did someone with 16 years of SQL experience bomb it?
So he's saying data analysts evolve into BI devs? Ok that makes sense. 
Not the person you're replying to, but two options - the person got super nervous, or they were sixteen years doing some basic select statements, which is pretty common. I work with people who've used excel for ten years but couldn't do a sumif to save their lives. They know what a vlookup is but can't write them without the helper.
Good bot 
Thank you, AXISMGT, for voting on JobsHelperBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
What would you consider a complex join or subquery? I ask this because I'm still unsure what level of SQL I'm at
I think he means anything besides a simple inner join (complex join) or using a derived table (sub query) 
3/18 Want to hire me?
I don't know the notationd you're using... Select j.jname from j Inner join sp2 on j.j = sp2.j --(these are keys) Inner join p on sp2.p = p.p Where p.color = 'RED'
Thanks for this :) Saved so I can use it as a checklist
You've finally chosen to respond to me. Quite simply the majority of your articles contain bad practice or miss important information. Some of your articles contain completely incorrect information. I mean why did you not mention right click &gt; properties? http://jackworthen.com/2015/11/30/confirming-when-the-database-was-last-backed-up/ Or why is there no consideration for public holidays http://jackworthen.com/2015/12/20/creating-a-function-to-return-total-business-days-between-two-dates/ I've never seen a calculate working days lump of SQL that does not use some sort of holiday calendar table. Or http://jackworthen.com/2015/12/21/using-sql-to-create-a-randomly-generated-password-with-custom-length/ why are you suggesting using a unique ID to generate 'secure' passwords when a far more cryptographically secure method is SELECT CAST(CRYPT_GEN_RANDOM(16) AS UNIQUEIDENTIFIER) Which accomplishes more securely in 1 line what took you about 20-30 lines and 2 different objects. And your preference for functions is not good at all. They frequently break the sargability of a query yet you don't make any mention of this. Your website does not even have a valid SSL cert. I think you need to take a step back and refrain from blog spamming here as its clear by the other responses that other members of this community are not happy with the quality of your articles either.
Well, you're close but not quite there. Also, I'm not giving you answers, but rather suggestions, I guess? 1. It looks like you are using the symbol for natural join to mean theta. 2. Your schema has logically different attributes named the same. How do you 'fix' that? 3. You can't project average until it exists. P.s. I blame relational algebra for hammering the notion into students heads that a normal (natural) join is something that involves columns of the same name and equality condition. IMO, theta should be the 'base' and every other "variety" should be taught as special cases of theta. Outer joins, obviously, should be extensions of theta. I guess my point is, if you started to use (or you are encouraged to use) thetas - use them all the time instead of the natural join abomination. 
Yeap :-). I'm a Database Developer (SQL, PL/SQL, T-SQL) myself and work with BI Developers in my role. (Typically I'm involved in the ETL processes that prep the data they need using stored procedures). My undergrad is in Psych! I got my foot in the door and built my skillset with SQL (and other languages like R and Python) from there. 
What RDBMS are you using? Most will generate diagrams with their respective IDE so that might not be exactly what they're looking for
Agility for SQL workbench (redshift)
I didnt even know what you meant by theta join haha. I think the lecturer told me to use the join with conditions. I think I did that here but maybe it is hard to read for example there is "⋈ #j=#j J" the condition being that #j must equal #j in the separate tables '' and 'J'. Am I still on the right track? I will work through the other two parts of your answer when I get home. Thank you very much!
Analyst
What state are you in? Im in Florida and linux isnt as popular as i would like it to be.
Huh. Really cool. Thanks. Perl? Wow. 
How did you export the data? What I *suspect* you have is the number of seconds or milliseconds since some starting point. Your life may be easier if you can export the date/time as a text value and import it into Excel as text.
I thought the same. I originally thought it was a Unix Timestamp but the result was too large and was # out. As that's calculated as seconds from 1/1/1970 at midnight i'm thinking it must be a later date than that. 
Most databases store a date using a date time data type. One system I work with attires the date as a big int type. With some date (Jan 1 1980 I think) being 0. It's quite possible you're working with a similar thing. A second thing at play is that Excel is notorious for reformatting dates when you open files with it. Make a .CSV file and open that in notepad++. 
Just tried it with 1/1/2001 and it's the correct date. Frustratingly though, I don't know **why** that should be. Does it matter that i'm on a mac?
You're absolutely right. It's stored as an INT and by trial and error i've discovered that it's 1/1/2001 that is 0. Any idea why it would be that date?
SELECT * FROM INFORMATION_SCHEMA.COLUMNS This should work on any ANSI-compliant database.
As u/mamertine says it's often stored as a long integer counting the number of seconds from a particular date. In my instance, it was 1/1/2001. I used this formula to convert it: =\(A1/86400\)\+36892 where A1 is the date I want to convert. 
Damn. This is a wake up call for me. Should basic joins and select queries not even be considered SQL skills on my resume? I’m more of a general business analyst as opposed to data analyst so to be honest I haven’t needed to get this deep in SQL to do my job 
Awesome, thanks so much! 
Really, it just depends on where you are trying to move in your career. The list was what a Business Intelligence Engineer with 5+ years of experience is expected to know at my company (in terms of SQL) getting paid 6 figures. If you know at least have of these things, you are pretty good! 
Check out this article: https://www.mssqltips.com/sqlservertip/1816/getting-started-with-sql-server-database-diagrams/
Maybe start by learning window functions? Those can take you a long way! 
Try: SELECT [ID], [DATE], [2018-04] FROM TABLE SCHEMA.TABLE_NAME
So I tried running that; and I recieved an error in return. Code: SELECT [SF_ID], [2017-04] FROM DATATEAM.INVOICE_RAW_V WHERE INVOICE_INDEX = '1659201'; RESULT: The Snowflake database encountered an error while running this query. SQL compilation error: syntax error line 1 at position 7 unexpected '['. syntax error line 1 at position 13 unexpected '['. syntax error line 1 at position 21 unexpected '['. syntax error line 1 at position 42 unexpected 'SCHEMA'.
Thank you so much da_chicken, I did not know about this distinction at all. Worked perfectly.
What database engine?
Epoch reference on Wikipedia suggests it's tied to an Apple Cocoa epoch: &gt;January 1, 2001. Apple's Cocoa framework. 2001 is the year of the release of Mac OS X 10.0 (but NSDate for Apple's EOF 1.0 was developed in 1994). [source](https://en.wikipedia.org/wiki/Epoch_(reference_date\)) The epoch reference suggests that Postgres' default epoch is Jan 1 2000. Are you _sure_ your dates are referring to 1/1/2001 instead of 1/1/2000?
**Epoch (reference date)** In the fields of chronology and periodization, an epoch is an instant in time chosen as the origin of a particular era. The "epoch" then serves as a reference point from which time is measured. Time measurement units are counted from the epoch so that the date and time of events can be specified unambiguously. Events taking place before the epoch can be dated by counting negatively from the epoch, though in pragmatic periodization practice, epochs are defined for the past, and another epoch is used to start the next era, therefore serving as the ending of the older preceding era. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You could try a date literal such as {d '2018-04-01'}.
Thank you.
Thank you. Very helpful ! 
This is also the reason why they have this when submitting a question to this sub &gt; Don't forget to tag your post title with the correct platform MySQL], [Oracle], [MS SQL], [PostgreSQL] Each can do things differently depending on what you're trying to do.
Put another coalesce in the makedate. 
See this image; https://imgur.com/a/6YcuaOC. In the case where `avg_eng_min_to_solve` is 94, it means the data in the subselects was `null` and i used `coalesce` on that data. As you can see `wat_solved_at_year` nor `wat_solved_at_quarter` are not null, yet the `makedate` fails. Why would the `makedate` return `null` when the values being used are 2016 and 2? Hopefully that makes more sense.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/d4Ugix4.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dyydkez) 
You’re calling the values directly from the table, so the columns your seeing in your select results aren’t being used in the makedate function. If you want the sql engine to use the results from the previous select entries, you need to use their alias in the makedate function. 
Ah - that makes a lot of sense. So I would assume just dropping the `s.` should work - i.e. the query would be; MAKEDATE(wat_solved_at_year, 1) + INTERVAL wat_solved_at_quarter QUARTER - INTERVAL 1 QUARTER as "first_day_quarter_wat_solved", This way it uses the variables from the `COALESCE` above. However, after running it - `NULL` is still returned. Seems strange, am I missing something?
Depending on what sql engine you are using, there are different ways to explicitly define using the variable or not. TSQL is [] I believe. IMO, the easiest way is just to rename the coalesce column slightly and reference that new name. The other option is you can just wrap the entire current query into another sub query, and doing your makedate a level up. 
Can you just plop the coalesce into the makedate function as well? Sorry about 20 options here, I’m on mobile so I can’t do any testing. 
Well that worked; MAKEDATE(COALESCE(s.wat_solved_at_year, YEAR(c.quarter_created_date)), 1) + INTERVAL COALESCE(s.wat_solved_at_quarter, CASE WHEN (c.quarter_created) = 1 THEN 4 ELSE (c.quarter_created) - 1 END) QUARTER - INTERVAL 1 QUARTER as "first_day_quarter_wat_solved" I'm not exactly sure why the previous post doesn't though.
just put a select statement around all this stuff without the makedate field. and put the makedate field on the outer select so itll be select makedate... from (all your query without the makedate) src 
MySQL is weird sometimes - the entries in the select column aren't explicitly executed in the order which they are listed - in this case, the query engine was running the makedate prior to the coalesce, so that aliased column didn't exist yet for it to look at.
It might have already been answered - but you can *almost always* convert the column (in Excel) to Short Date for just YYYY/MM/DD format or Long Date for YYYY/MM/DD HH:MM:ss format (depending on format settings). Exporting to Excel usually saves the field in its "sequential serial number" format. Just a goofy Excel thing, your date is still there.
Is there any RDBMS engine that actually lets you use a column in a different column in a single `SELECT`? Like SELECT a+1 AS b , b+1 AS C -- FROM DUAL -- for Oracle I don't think this will evaluate on any of the major SQL platforms. 
I just tested in Teradata, and it works here, but yeah, it doesn't appear to work in MSSQL or MySQL.
Great explanation and I appreciate the alternatives. I never encountered this before I was a bit surprised! Thanks.
Cool. I'll probably wait to hear back from him, then. I would get errors like "users not found" because their domain users weren't available on my test upgrade VM. I nuked all users (because we can re-create them later). Then I got errors about views not able to be scripted for some reason or another, so I nuked those views, figuring I could re-create them again. Then I got errors about multiple timestamp columns in a table and threw up my hands and figured I'd reach out to somebody who knows more than me. &gt; If you want to do it yourself, you're going to need to at least provide us with some more detailed error messages I'll see what the DBA says first. And I figured you'd need more detailed error messages. I was just trying to get a general recommendation on what method of downgrade is best first -- if I should go with replication or scripting or export/import or third-party tools, then I would get into specific errors with whichever tool I used :P Thanks! It'll probably cost an arm and a leg, but the client is paying for it. We brought the DBA in and they agreed to pay for a couple of hours for him to look at the databases, then give them a quote on what it would take, so I'm probably good, but I figured I'd reach out to Reddit, because there are enough people here that have dealt with just about every stupid unsupported configuration :)
8,445 rows in 16 seconds? What is this, 2008? You need to download more ram.
If your delimiter appears in a value, the value needs to be quoted. This isn't really a "SQL thing" - it's the CSV convention. Ideally, all values in all rows would be quoted.
Nothing gives you that "I actually accomplished something today" feeling more than tuning something ugly and making it hum. 
That might be hard. The text file is half a gig.
So, standard optimization questions: * WHERE clause conditions changed? * Table join conditions changed? * Any column reindexes?
That's why you create the file with quoted values in the first place, instead of attempting to retrofit them into an existing file.
Well, it's SQL Server since you're using `GETDATE()`. I guess it could be Sybase but nobody uses Sybase except for SAP. Sure looks like K-12 and not higher ed with the first query being elementary and the second being secondary. It doesn't look like it's eSchoolPlus, though there's a lot of similarity. PowerSchool is Oracle and Illuminate (barf) is PostgreSQL. Skyward? 
So I started by adding more joins that I knew would allow me to narrow down the data. My case statement; *AND cal.endYear = CASE WHEN MONTH(GETDATE()) &gt;= 8 THEN YEAR(DATEADD(YEAR, 1, GETDATE())) ELSE YEAR(GETDATE()) END* Checks for the current school year against the calendar and reduces the total number of entries. I also did some things that are specific to my environment like filtering out trials from courses. I am currently reducing the triple nested select statement from the teacher query down to a double as well and making some more minor tweaks. Other similar queries I have changed in my department have been better than this one. Last week I turned a query that took 1min30sec to run down to 2 seconds so I still have some work to do but that is future u/fullyarmedcamel's problem he is a hero of the people always solving problems down the road. 
Not Skyward but you are on the right track, I mentioned in another comment that is an SIS and yes we are K-12. I had to sign a bunch of NDA's regarding sharing table structure and data formats when they gave me access to the database and we don't own the physical hardware the SIS is running on (locally). It made asking for help while learning SQL from the ground up very hard as I have to remove a ton of information from all of my queries and what not.
Synergy? 
Yeah, that's going to be something that needs to be handled at the time the file is generated. If they're not quoting values with the delimiter in them, you're not going to be able to figure that out after the fact in any reliable automated fashion, I'm pretty certain. 
Yeah, most people I know who've fallen into just-DB development haven't had formal training in it. Formal training can also have severe limitations, because each data environment is different and ends up being like teaching a foreign language without having any nouns you can use.
Never said don't post here, I said don't do people's homework for them. A person learns nothing from someone else doing their work except how to copy paste.
Or that sometimes multiple joins to the same table for data filtered by multiple columns takes less time that stringing together a giant "JOIN ON...OR...OR...OR..." clause that can't use indexes effectively? I just recently found a section of my own scripts where I "OR"d myself into an extra 1000+ reads that way.
Would it be possible to set up a temp server, install 2005 on it, move the data over (if need be), then point Dynamics GP at the 2008 server later?
You could declare a variable to store the result of getdate(), and use that in its place to ensure that it's not getting called many many times. I might store the case statement result in another variable, but maybe just replacing getdate() is enough for sql server to recognize the whole expression is a constant.
db is provided by your SIS? Could have sworn it was your MOM.
My district is eSchoolPlus and there's no NDA that I'm aware of and they give you a data dictionary. Not that it's perfect by any means, especially since the product just spent the last three years being handed around between vendors before PowerSchool bought it so support has been about what you'd imagine. I guess our SIS isn't running on a vendor appliance, however, so I suppose that changes quite a bit. We use a lot of third party reporting, however, so we essentially need that functionality. I know that PowerSchool doesn't allow DB access, either, though. One thing I did notice is that you're using single quotes for field names: CASE stu.schoolID --REMOVED FOR SECURITY ELSE 'No School' END AS 'ORGANIZATIONID', That should really be: CASE stu.schoolID --REMOVED FOR SECURITY ELSE 'No School' END AS "ORGANIZATIONID", Or, if it is SQL Server, you can use: CASE stu.schoolID --REMOVED FOR SECURITY ELSE 'No School' END AS [ORGANIZATIONID], You shouldn't use single quotes because the SQL standard defines those as used for string literals. I know SQL Server accepts it and it won't improve performance at all, but it could cause issues down the line if you update to a newer version of SQL Server if they ever carry out their threat of enforcing it. It only sticks around for SQL Server 7 compatibility. 
You're going to have to recreate the structure in a new DB and move the data over. You *can't* restore a backup made on SQL Server 2008 on SQL Server 2005. Edition doesn't matter. It can't be done. Honestly, though, you should be able to [script the database](https://www.easeus.com/backup-utility/restore-sql-2008-database-backup-to-sql-server-2005.html) and [run it on SQL Server 2005](https://codeshare.co.uk/blog/how-to-migrate-or-downgrade-a-sql-server-database-to-a-lower-version/). You will probably need to re-create all the server logins first (especially SQL Server logins) but it really should work if you get the order right and have all the Advanced options that your database needs enabled. It sounds like you've already tried this, but it's really the only solution that I'm aware of. It's not going to be straightforward and you will need to manipulate the files to get them all to work. You might need to create the schema, disable all triggers and foreign keys on each table, enable identity insert if you need it, then import all your data for that table. Then verify the data in the table and move to the next table. The only other option would be to do a new install of Dynamics GP 2016, configure it, and then import the data from Dynamics GP 8 somehow. You could also try contacting Microsoft support about it, although I imagine they'll either tell you the same thing or point you to a certified reseller. 
Thanks. Hiring the DBA is probably our best bet at this point, and contacting Microsoft is always an option. Just figured Reddit could help if I was missing anything stupid or obvious, but so far it sounds like I haven't. &gt; Honestly, though, you should be able to script the database Yup, [tried that](https://i.imgur.com/CElW9bZ.png), but when we started getting errors on the first database out of 20, I started trying other things. &gt; You can't restore a backup made on SQL Server 2008 on SQL Server 2005. Edition doesn't matter. It can't be done. Yeah, I've figured this much out so far. The reason I brought up Editions (enterprise) was because I was considering restoring the databases to a SQL 2008 Enterprise server, then using SQL Replication to replicate to a SQL 2005 target. If replication was successful, I figured we could then poing Dynamics GP to SQL 2005 to do the first few upgrade steps. After looking up the cost of SQL Enterprise, I trashed that idea, as that's pretty prohibitive, and the DBA consultant route will probably be cheaper in the long-run. Thanks for taking the time to reply!
I still live by that, unless there are locks on tables holding me up, that query should fly! 
You can also try aggregate function: with t as ( select 1 id, 'A' value from dual union all select 1, 'B' from dual union all select 2, 'B' from dual union all select 3, 'A' from dual ) SELECT id , max(value) keep (dense_rank first order by decode(value,'A',0,1)) value FROM t group by id
Your question is a bit vague? Do you want to sort the arrays or sort the objects within the arrays?
Oh I was not clear enough. Yes you’re right. I mean I will post, but won’t complete the problem in the future, just help the OP along to the right answer. No hard feelings at all. 
30 minutes down to 30 seconds. Client still asked me to justify the time spent. ¯\\_(ツ)_/¯
I have retrieved these for you _ _ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
If you know the under laying disk capability you should have a general idea how fast a query can go. If you are moving 500mb into a csv file it should take time it takes to read 500mb and write 500mb with little overhead latency. The problem is people do what I call the SQL hokey pokey. The data is garbage to begin with, it's structured poorly, it's constantly locking itself and then they start doing small subsets of data into tables. You puts some data in Select into #temp from table You take some data out Delete from #temp where x in (select id from #temp where x != 'value') You put some data in INSERT INTO #temp SELECT * from #temp where x = 1 UNION SELECT * fROM #temp where x != 1 THEN YOU SHAKE IT ALL ABOUT UPDATE #temp SET x = 1 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 1) UPDATE #temp SET x = 2 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 2) UPDATE #temp SET x = 3 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 3) UPDATE #temp SET x = 4 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 4) UPDATE #temp SET x = 5 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 5) UPDATE #temp SET x = 6 FROM #temp t WHERE x != 1 and x = (Select x from #temp t2 where t1.x = t2.x + 6) YOU DO THE SQL HOKEY POKEY AND YOU RUN YOUR BUSINESS INTO THE GROUND THAT'S WHAT IT IS ALL ABOUT!!!! The worst part is that usually I can take all that code and put it into a CTE and it runs in a few seconds tops.
This is when you shove this back on the person who created the file. It's called "bad data." Doesn't help, but if you don't push back, you're going to wind up dealing with this shit from clients over and over.
None. Sort the table based on objects in the arrays
Have whoever made the file remake it with proper data or a different delimiter. Personally I like using the tab character, it's *really* rare for that to be in a data value. Though the real right answer is to quote your data fields.
You can buy an [Oracle Press Study Guide](https://www.amazon.com/Oracle-Database-Guide-1Z0-071-Press/dp/1259585492/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1526325207&amp;sr=1-1&amp;keywords=1Z0-071) from Amazon for pretty cheap \- this, along with some practice and access to the Oracle Docs is all you need. You can find the list of materials that the 1Z0\-071 requires you to know [here](http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=5001&amp;get_params=p_exam_id:1Z0-071), under Review Exam Topics. My suggestion would be to print this out, stick it on a wall or something above your computer \- and go through each of the exam topics to make sure you know them inside and out. To the point that you could explain to other people what the exam is going to test under a given topic. 
Ok, then you probably want to use a lateral subquery: select s.* from stories as s cross join lateral ( select (el -&gt;&gt; 's')::numeric as s from json_array_elements(emo) as el where el -&gt;&gt; 'name' = 'A' ) as a order by a.s; 
I know this feeling well. At one of my jobs I optimized a query that took over 30 minutes to run because they were running a sub-query 4 times with UNION, each time the sub-query runs though it returned millions of rows so it was running DISTINCT on each row after the first sub-query (part of UNION's functionality). All that needed to be done was use AND in their WHERE clause ... got it down to 1.5 minutes. This was the best feeling I'd had in months :-D
You presented a database with shitty design and want a decent query out of it?
You inherit a DB with a 6GB audit table and no indexing...you're going to have a bad time. 
Hmm...i think ur probably more advanced than me then. I guess i can recommend sqlskills.com and brent ozar but i imagine uve been there alreadt :p. Best of luck! Hopefully others can add more stuff here
Yet after tuning, and hours of research and stored procedures and automating... All they say when you present is... "I have one SMALL change" 
"It's good....but can we [...]" 
looks like some code duplication there, can you use a with statement rather than a union? thanks. 
Yeah I had not thought about doing that I might make that adjustment later I am working on cabling projects for the next 2 months.
just inner i challenge you to try it with another type of join best way to learn something is to test it yourself
Clean up your data and get rid of that distinct and your execution time will be cut in half
Gotta love it when BAs describe the size of a change
This will be a manual clean but I wonder if it can be done more simply. Is there anything regular feature about the columns preceding or following that column? For example, if the data was consistently int|varchar|int you might be able to zero in on exceptions like int|varchar|varchar|int more rapidly.
I was messing about with it earlier, but honestly wasn't sure what data I needed to mock to test the other joins because I never use them. Thanks tho!
Ah, I see why. The unjoined rows are awkward to get. http://sqlfiddle.com/#!17/ba318/25
How would getting rid if distinct clean up my data, the point is to have a single entry per student and teacher and that is what the use of distinct is meant for plus in the logical processing order distinct is one of the last things to run and has little to no impact on the query. Cleaning up early joins is where the time savings could be made in this case.
Unfortunately it can't be done, working as a "low level" employee at a school district of 30ish schools each school has their own registrar and secretary and the data tends to break down at the entry level and I don't have the clout to be able to have them correct all but the worst entries. Office politics and all that, however we still have your have accurate rostering so you do what you have to in order to make it clean. I regularly have to use nested selects to clean up rostering but that is just the way of things.
There are at least 2 million records. There are bound to be mistakes..
How about when you reduce that 29 hour query to 45 minutes?
Would you be able to load the data into another table with 36 columns, correct the affected records, and then insert the corrected data into the final table?
Thanks to your post, I have newfound respect for something. I'm rather drunk now. Sorry
Not going to lie, having a customer treat me like a God because i brought their 12 minute report down to 1 second by applying the witchcraft called "indexes" made me feel good. I then implemented a spell called "SQL Job" that did some maintenance, and made sure the indexes were up to date each sunday night. Got an email where they had contacted my boss about how amazing I was. 
There's the deprecated \*= and =\* operators in sql server. 
one question I remember missing years ago was "you have a table with one field -- that field contains a number -- how do you multiply all the rows together?" I drew a blank -- had no idea what to do the answer ... select power (10, sum ( log (the_field ) ) ) ... log a + log b = log ab and here some from google https://www.google.com/search?ei=8pL6WriaE-Tm5gK-45qICQ&amp;q=sql+interview+questions+for+data+scientist https://www.google.com/search?ei=YZb6WvShAo-V5wLbmKnoDw&amp;q=sql+interview+questions+for+data+engineer 
Okay. Just trying to think of a way to zero in on bad entries. If the surrounding columns were constrained in the source DB it could work. Assuming the error rate is low, you could zero in fairly quickly on errors by simply opening in Excel with | delimitation and see where column alignment breaks. This would also give a sense of the error rate, as each bad record would offset columns by +1.
Oh, DAD. 
Yipes. I captured a plan of a long running query after the boss asked me to take a look. Two table scans taking up 80% of the plan's work... takes 45 minutes now!
Take a look at this: https://www.w3schools.com/sql/sql_join.asp That should help you. :)
Lol "SQL" magic
HAHAHAHA FUCK YOU 3 MILLION ROW PIVOT TABLE
Those would have come in handy so many times. Would have saved some typing. :(
lol, i did the same thing last week a report need 20 minutes after i looked into SQL i found there was two shitty functions for the report to take that long to execute i just made the into materialized vues with refresh each hour, and bang now the report need only 36 seconds. 
Your MCSA does not expire, there is a new MSCA for SQL Server though, 3 actually and a MSCE. Those are probably the best things to continue studying to get certified in. I would highly recommend Kimball + T-SQL Querying by Itzik, both are really good reads.
Read this: http://stackoverflow.com/questions/18750464/sql-left-join-losing-rows-after-filtering 
Feel free to correct me if I am wrong here but I believe the way the system would handle the **DISTINCT** operator would be like this; Pull data out of table in **FROM** statement Processes data in the **JOIN**'s Applies filters from **WHERE** clause Pulls data out of **SELECT** statement Applies filters of **DISTINCT** As **DISTINCT** runs last it would have the least overall impact unless you have many duplicates, in my case it is around 2000. You need to remember too that there is not just a single entry for students. Some students will be taking multiple classes where this product I am rostering for is being used irregardless of the filters applied the rest of the query. This document that I have has the purpose of creating a unique roster of students that can then be applied to classes within the application we are using multiple times over from a separate query that manages classes. I think in this case the **DISTINCT** operator is what is meant to be used it is doing exactly what I need it to, as a test I did run the query with and without the operator and it made no difference in run time. I think my next changes will be more focused around some of the joins and structure of the query. I can reduce from a double nested **SELECT** statement to a single. I can also do some thinks like using the school year detection **CASE** as a declared variable at the top rather than nested in the query.
so if I understand... you have *n* amount of any given product, noted by a productID. I would say for SURE separate rows and not keeping a count within the table. 
I'd been using SQL for a few years before I learned about that. If I were interviewing for a SQL position, that question would absolutely be on the list. 
Really interesting, thanks for the research. I just triple\-checked my data \- it is *definitely* 1/1/2001. Seems odd that they wouldn't use 1994 as their reference date OR make it easier and standardise with Microsoft on the 1900 or 1904 date. The data source is actually an Apple database stored on the Mac so it makes sense that it follows they're own date reference standard. Another thing to add to one's mental reference book \- Apple Cocoa framework = 2001.
 UPDATE yourtable SET username = 'mydomain.com\' || username
mssql, so + instead of || 
I did try this, but it didn't add the domain to the existing username, it over wrote it: update dbo.mytable set username = 'mydomain.com' where username = 'username' (I added the where clause so I could play with this rather than mass update the entire DB. Easier to undo.)
Ok first off - transactions can be used to test scripts and rollback without actually changing data. See below example. However please bear in mind that this will cause a lock. An update without the where clause will lock the entire table. This will mean any other commands hitting that table could potentially fail. BEGIN TRAN UPDATE yourtable SET username = 'mydomain.com\' + username where username = 'username' SELECT * FROM yourtable where username = 'username' --results: mydomain.com\username ROLLBACK SELECT * FROM yourtable where username = 'username' --results: username What you need to do is ADD the domain to the start of the username. So your set clause should look like this: SET username = 'mydomain.com\' + username See the '+ username' at the end, that's the bit you need. Please don't tell me you did this test on a live database ;)
That worked Like a charm! thank you
yeah, silly me for thinking MS SQL might have caught up to ANSI standard sql by now...
phpMyAdmin can import CSV files this big. I just recently had to write my own CSV to SQL converter to import more than 65,000 CSV files...but if I only had one or two, I'd have just done it by hand via phpMyAdmin.
Haha atleast you realise it. I think most experienced sql people have done something stupid (like a missing where clause on an update) at some point. 
Don't forget that SQL is really just set theory.
Here is the error I am getting when attempting to connect: A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: TCP Provider, error: 0 - No connection could be made because the target machine actively refused it.) (.Net SqlClient Data Provider) ------------------------------ Error Number: 10061 Severity: 20 State: 0 
This is simply beautiful. 
As a follow-up putting in the host file did not make any change. 
Are the connection properties changing for some reason? I would also double check the SSMS connection properties to be sure they’re the same. Are you using tcp/ip or named pipes?
They’re all normal. TCP and Named Pipes are both enabled. 
I assume it’s the default instance name. How about when you log in to the server via ip and run @@SERVERNAME Does it return the correct name?
It’s not the default instance name. Are you saying when I connect to the database engine to run tsql command @@SERVERNAME? The database is running fine, all users are connecting to the instance name. Assume the Database name is Database and the instance name is Database1. 
Yes, run that command and see what it returns. And if you try to browse for sql servers from the connection page does that server\instance show up as an option?
 File "H:/Python Scripts/untitled0.py", line 15, in &lt;module&gt; cnxn = pyodbc.connect("Driver={SQL Server};" SystemError: &lt;class 'pyodbc.Error'&gt; returned a result with an error set
I apologize, the full command is select @@servername You may also want to try running the SQL profiler and check for failed logins. It may give you a hint because this is a weird one. I was thinking that maybe the name of the server was changed at some point and SQL never completely updated it. I assume there's nothing in the windows event log either.
Honestly, once you get good at it there isn't anything faster or more easy to use than a text based query analyzer. Graphical query builders aren't worth a damn because they never account for all the nuance and flexibility that you need when writing a query. 
I can't find anything related in any logs. It's like I'm not even attempting to connect. The vendor disappeared on me because they cannot figure this out either. 
I also ran SELECT @@SERVERNAME and the results were exactly what they should be.
I understand the feeling of having a vendor bail on you. Been in the industry for a long time now, most of it as a consultant or vendor myself. I would never do that to a client. How about if you run sqlcmd -S servername from a command prompt? I'm curious to see if it works from the command line. Is there a specific port assigned to that sql instance? If so, can you use the hostname and that port to connect to the instance?
disabled
Yeah, the server name is right, I can check that in TSQL. I can also connect to any other method. I can use the FQDN, hostname,port\instance, localhost\instance but when I just user name\instance it won't connect. 
You don't by chance have some weird proxy setting configured on that box do you? I'm pretty much out of ideas on this one other than restarting the SQL service and\or server itself.
It was a weird one, but I found the problem and updated the OP with the solution. Thanks for helping me narrow down the problem. :)
Happy you got it worked out!! Yeah, normally it should be set to dynamic by default unless a port is specified for the instance. I learned something new today also :)
What /u/da_chicken said. Query builders in general are .... not good. It might seem complicated to express your "question to the database" in SQL, but once you get used to it, it actually is the easy way of doing it. Query builders never teach you anything, the queries built almost always are really damn bad, and are very damn limited, to what you can actually put in as "question". &gt; I usually use rank and with function This is pretty much already over the head of any query building tool I know off. Ranking window functions and with clauses, I have not seen those being implemented in gui query builders yet.
If you aren't willing to take the time to learn SQL and want an easy tool, your boss was probably wrong. You're not the person for the job. It takes a couple of weeks to learn complex queries. But the more important issue is that you don't want to take the time to even understand the queries you want to use...
Is SQL Browser disabled by chance? With a dynamic port, that service I believe needs to be enabled and running. 
These are the things you need to quantify and bring up at pay review time! 
Have you opened the "SQL Server Configuration Manager" then gone to TCP/IP settings and enabled TCP?
You try using a text qualifier? Double quotes usually make a good qualifier.
Depends on your rdbms. Toad and SQL operations studio both have code snippets functionality that allows you to reuse common scripts after you make them. I use those features daily. 
You're looking, probably, for an aggregate function. select country, count(*) from table where column_with_numbers &lt; second_column_with_numbers group by personid That will return something like: |personid|count|| |:-|:-|:-| |elbonia|10|| |frankfurtistan|3|| |tokyork city|15|| You might also be looking for something like this: select id ,original_column ,modified_value_column ,case when original_column &lt; modified_value_column then original_column else modified_value_column end as calculated_row from table The third column is going to give you the original value if it is less than the modified value, but otherwise will be the modified value. You might be looking for some combination of the two, as you noted it's hard to tell from your description but I think that points you in the right direction.
It's looking at a columns value, and counting the number of integers that are less than it's current value. For example if the value was 5, I would want it to count the integers 4 through 0, returning 5.
Yea, I know it would probably an aggregate, I replied to the first guy with an example worded a little better. Thanks for your help
Thanks you so much guys. You were really helpful :)
So if the value was 10, you would want it to count the numbers 9 through 0 and return 10? Is there any number where this formula returns a value that isn't equal to the input value??
You're going to need to explain why you need to do this as it currently makes no sense to me because the output of that operation is always going to be the same as the input value if the value is an integer type. The only reason I think you'd want to do this is if it's not and you're really looking to to get an output of 5 from an input value that is anything &gt;= 5 and &lt; 6 which is called flooring and you should just use the floor() function. But then I don't understand why you'd have used 5 instead of 5.684 as an example.
This is the correct answer, but with columns in the WHERE clause
Text file is third party. 
Did you follow the suggestion? I run into this often. Just open the script task, edit it, and build it. Click OK
I get namespace WinSCP could not be found on the script I'm working on, but not the other
1. \*\*Count\(\*\)\*\*. count is a summation function. 2. Looks good 3. That seems wrong. You're missing a filter \(WHERE\) 4. Looks good. 5. There's an error there. \[SET lname='Smith' WHERE lname='Doe'\] 6. It looks like you're not including customer information with the query, just ordering by it the id... You have to add a JOIN here to pull that customer information from CUSTOMER. 7. ...Hmmm... It looks like you're table is actually missing a column. Q7 seems like a question with missing information. Your solution may suffice. 8. That's one way to do it. Another way: SELECT table\_name FROM information\_schema.columns WHERE LOWER\(column\_name\) LIKE '&amp;#37;product&amp;#37;';
 I work in SQL Server so I'm not sure if my syntax is exactly correct.. But I think for Q3, you need to have a where statement - something like select fname from CUSTOMER where fname like 'A%' For Q5, I think it's a bit off; I would think something like this: update CUSTOMER set lname = 'Doe' where lname = 'Smith' I think Q6 is also off, you need to join the ORDERS table to the CUSTOMERS table.. select Ordernumber, fname, lname from ORDERS join CUSTOMERS on ORDERS.Customerid = CUSTOMERS.id The others I can't really look at right now. Again, I'm in sql server so the syntax might be different than what you're needing.
So I updated my answers. How is this looking: https://gyazo.com/d8a9ba92a38e64ca2efd000006073eff
So I updated my answers. How is this looking: https://gyazo.com/d8a9ba92a38e64ca2efd000006073eff
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/d8a9ba92a38e64ca2efd000006073eff.png Imgur mirror: https://i.imgur.com/sGpCGCD.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
I'd really recommend using the information\_schema for Q8. It's a little agnostic \(it would work in SQL Server, MySQL, Postgres\). I'm not sure that sysobjects is SQL\-flavor agnostic. Everything else looked good.
I saw that it was discouraged to straight up ask for help with assignments, but I think I figured it out. It looks at a column containing a rentals car level, for example a coupe is level 1, a four door level 2, a sports car level 5 etc. I forgot to mention that it counts the number of higher level upgrades available too, which would just be using an aggregate to subtract the highest level which I think is 8 from the column's current value. Sorry for any confusion, I'm still learning! 
Have you tried putting both DYNAMICS and the corresponding company database in 2005 compatibility mode and then upgrading to GP 9.0? I'm a former Dynamics GP administrator and seem to recall having to do this at one point. It can be a bit messy but could work in this case. Either way, you're definitely looking at a multi-tiered upgrade path to get the client on GP 2016. 
&gt;anyone know a site where i can learn sql? [SQL Fiddle](http://sqlfiddle.com/)
[removed]
http://letmegooglethat.com/?q=learn+sql+online As for learning on your workstation, download SQL 2016 (or 2017) express edition. All editions have all feature sets now, they are only capped on resources now. See here for resource limits https://www.microsoft.com/en-us/sql-server/sql-server-2017-editions
I think the Developer edition is free too. I don't think it's capped for the 2017 edition either. I'm most familiar with MySQL (which goes with the Workbench of HeidiSQL GUIs). If you learn syntax for both MSSQL and MySQL, you should be good. I use Postgresql, MySQL, and MSSQL at work. Postgres and MySQL overlap in their syntax in most use cases.
Developer is free, yes. 
Your answer will not work if there are any negative numbers in the table. 
For the Oracle fans: [Oracle Live SQL](livesql.oracle.com) 
I work with Progress OpenEdge database. If you do a left join and in the ON clause or WHERE clause you put the table on the wrong side then the left join becomes just a join. For Example. Select a.col1, b.col2 From a left join b On a.id = b.id Where b.col2 = 123 
Hackerrank is pretty popular for brushing up your skills
I used w3schools.com at work to learn the basics. It is extremely easy, lets you fool around with queries and their outputs, without having to download anything. I highly recommend it.
&gt; All editions have all feature sets now, they are only capped on resources now. Not exactly. There are still [features that are Enterprise (and Developer) Edition only](https://docs.microsoft.com/en-us/sql/sql-server/editions-and-components-of-sql-server-2016?view=sql-server-2017), but they're primarily management and performance features that are mostly used at "enterprise" scale - Resource Governor, some replication features, R Services, Service Broker, Data Warehousing, etc. For someone just getting started learning, those won't matter.
W3schools has been great for a long time. Otherwise, I suggest codecademy.com as they just recently revamped their SQL training. Should be pretty robust, and I’ve loved the site for other languages
These two sites helped me a lot with SQL and Python https://www.codecademy.com/ https://www.w3schools.com/ They provide coding and SQL tasks with tips, hints and examples but then give you another task that is related to what they showed you but they put you in the deep end enough to actually learn it, not just copy exactly what they put in the example.
only limit is it cant be \*in production\*
Are you able to get them to change their export to you at all? Like add qualifiers or change delimiter? Otherwise you may need to find instances and correct for them. Are you using ssis for the import?
order by
So you said sort, your before and after looks like you are trying to aggregate. You need "group by" and sum your amount column.
how do i get the table to add up the amount due for chloe in May?
&gt;Sorry but im new to using SQL, how do i query so that it sums the amount column?
Select Customer, sum (Amount) as AmountTotal, Month from table group by Customer, Month
google
You should start by learning the statements. SELECT FROM WHERE these should get you started. 
What if the table is pulled from SQL server? 
After the word from, write the table name those columns are in So from (Table Name) group by customer, month
Get really comfortable with basic set algebra: what sets are, what they guarantee, how to combine them (union or intersection), how to do a set difference. Bonus points if you understand the assertion that a function is a mapping between two sets (the domain and the codomain). If you’re comfortable with that, you can approach SQL as a language that helps you do operations on set-like objects called ‘relations’ (informally known as tables). The main new concepts to tackle will be keys and joins, but if you think of everything in terms of sets you should be OK.
I've been in sqlzoo.net for the past week, starting actually trying to really learn after I had been asked to do some things at work which required me to learn how to use stored procedures and triggers. I had fun with it so have pretty much just tried to find challenging questions, break them up into simpler questions, and then learn a bunch of discrete things about SQL and then learn something sort of complex at the end. 
The good thing with SQL is that you do not need any prerequisite for getting started with it. Indeed SQL is a [declarative programming](https://en.wikipedia.org/wiki/Declarative_programming) language, means a [high level programming language](https://en.wikipedia.org/wiki/High-level_programming_language). If you have some knowledge on [set theory](https://en.wikipedia.org/wiki/Set_theory) it may help, since the operations you do on records come from that latter theory. Most important concepts are table, record, primary key, association, foreign key, filtering, aggregate functions, join, grouping, having, subquery, derived table, sorting, limit. Go for a practice oriented tutorial. The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An advanced course is also available.
Just want to recommend data.world as they have datasets and interactive querying *and* tutotrials. Truly awesome site!
group by
Oops, I see what you mean. Fixed.
I can't recall what the error was, but we recently had a shit storm due to a windows update so it was probably the same issue. After the update installed my machine I rolled back the update for now and this weekend I'll install updates on our servers. I usually work late at night, so luckily I have admin on our hosts otherwise it would have been a very unproductive night. From what I read, there was an RDP vulnerability that was patched. But as a result, both the client and the server need the update. I believe it works fine if the server has the update and the client does not, however if the client has the update and the server does not then you will get the error.
SoloLearn (android app) gives a good crash course. It will have you SELECTing and INSERTing in no time. SQL is a great language to just dive into. Learn the basic syntax of a simple query and dive in. Initially think about it like a superpowered Excel Spreadsheet. If you can do it in Excel you can achieve it in SQL (Google is your best friend). Once your comfy with queries (will take less than a week if you're using it often), stop thinking with Excel-brain and begin to comprehend the power that is a Relational Database. It's a thing of wonder.
Yep, that's the one. I am rolling back my home PC as I type for this same reason ;) Not an option for me on work machines unfortunately. Ah well, I'll just have to work from home for the next fortnight...
https://www.virtualizationhowto.com/2018/05/windows-10-rdp-credssp-encryption-oracle-remediation-error-fix/
Try wrapping the email OR clauses in parentheses? WHERE (email1 OR email2 OR etc.)
That works - can you help me understand why? :)
OR statements will stop evaluating after the first match. So if you want to match multiple potential results you need to wrap those all in parentheses. The more complicated the logic the more important it is to keep track of your parentheses. I tend to use indentation to help me identify blocks of logic. 
&gt; OR statements will stop evaluating after the first match. Never knew that - cheers!
Hey, one more thing I am curious about. When I use a certain email in the `where` (just one email). I get two results instead of just the employees most recent role. Here is two columns from the result set; is_valid valid_from 0 2017-01-03 22:49:03 1 2018-05-03 09:41:15 How is possible that two rows are returned instead of the most recent one? My query says; LEFT JOIN `employees` `e2` ON `e1`.`employee_id` = `e2`.`employee_id` AND `e2`.`valid_from` &gt; `e1`.`valid_from` We can see that the second `valid_from` has a timestamp from 2018 and the other is 2017. Shouldn't the latter be the only row returned?
SELECT [Customer Name], SUM([Amount due]), [Date (Month)] FROM schema.table GROUP BY [Date (Month)] ORDER BY [Date (Month)]
The first part they're trying to remotely query their server from your server, so they can get the connection information. The second part they're trying to get the sa password and server directory structure.
Make all Joins - after the first INNER JOIN - LEFT OUTER JOINS. It's possible one computer could have multiple IP addresses and/or multiple users.
So the only problem is duplicate records? Use a Distinct or Group By clause
Are you only seeing this limit in SSMS? [That has character limits of it's own](https://stackoverflow.com/questions/3113360/for-xml-length-limitation). If you're querying from another SQL client, you shouldn't see truncation of this sort at all. 
V_R_SYSTEM should on be specified once Try this from V_R_System inner join v_GS_INSTALLED_SOFTWARE on v_GS_INSTALLED_SOFTWARE.ResourceId = V_R_SYSTEM.ResourceId left outer join v_GS_COMPUTER_SYSTEM ON v_GS_COMPUTER_SYSTEM.ResourceId = V_R_SYSTEM.ResourceId left outer join v_GS_NETWORK_LOGIN_PROFILE on v_GS_NETWORK_LOGIN_PROFILE.ResourceID = V_R_SYSTEM.ResourceId left outer join v_GS_NETWORK_ADAPTER_CONFIGURATION on v_GS_NETWORK_ADAPTER_CONFIGURATION.ResourceID = V_R_SYSTEM.ResourceId
Here's a hint for you. Can you show all of the supermarket/hall/side/shelf combinations that have products? What about all combinations whether or not they have products?
Of course i can, here is the excel with all the information that you need. In the located part, what's in red means that that shelf is empty
There's a thread on the r/sysadmin that has a lot of information on this, and a good fix for client machines. I worked around for awhile with a roll back of the update, but decided the registry fix was the way to go until the servers are updated. [https://www.reddit.com/r/sysadmin/comments/8i4coq/kb4103727\_breaks\_remote\_desktop\_connections\_over/dyov6iv/](https://www.reddit.com/r/sysadmin/comments/8i4coq/kb4103727_breaks_remote_desktop_connections_over/dyov6iv/)
Start by showing us what you tried then we can show you where you're failing.
How about starting with the query to show which shelves have a product. Create the query with as many tables joined as needed to also show the supermarket name.
I'm wondering: what solution did you pursue? It seems like your system is absurdly complicated -- that is, the requirements aren't that involved, but the solution is making up for a series of bad decisions. A badly flawed fundamental approach can't be fixed because layering more things onto it just makes it worse. 
select supermercado.nome from supermercado select distinct prateleira.nif from supermercado right join localizado on supermercado.nif = localizado.nif right join prateleira on localizado.nif = supermercado.nif and localizado.num = prateleira.num and localizado.lado = prateleira.lado and localizado.altura = prateleira.altura it's not working
 select supermercado.nome from supermercado select distinct prateleira.nif from supermercado right join localizado on supermercado.nif = localizado.nif right join prateleira on localizado.nif = supermercado.nif and localizado.num = prateleira.num and localizado.lado = prateleira.lado and localizado.altura = prateleira.altura This is the statement you posted, with proper formating... First failure point is you have two different select statements in there that will not interact with each other in any way shape or form. In statement 1 you are selecting names from your table,just names but all of them In statement 2 you are selecting nif from the same table, again just nif but all of them + a link to another table that will not really affect the number of records. So you will get one list of all names and one list of all nifs, completely useless. Your second Failure point is you are not using a [where clause](https://www.w3schools.com/sql/sql_where.asp) anywhere so how is it supposed to know what you actually want?
my guess would be these are number of days from 1/1/1900. Try dateadd, maybe?
Does this query give you the correct info? SELECT CONVERT(DATE,CONVERT(INT,column_name)); If so, I believe you just need to change the data type of the column: ALTER TABLE table_name ALTER COLUMN column_name DATE; I could absolutely be mistaken though, so hold out to see what others have to say if you'd like.
Yup. I probably should have clarified, did you TRY running some of the bits to see if you did HAVE any holes if they were successful?
Sounds like you had the data set up wrong in Excel and then imported them into an improperly-typed field in SQL Server. Are your date fields *really* `date` types? And are they real dates in Excel, or just text that look like dates?
As someone else mentioned, those are days since 1900-01-01, and it looks like your date column is not really a date datatype. You can fix this pretty easily. *I am not responsible if you loose your data* --Store your original data DECLARE @Temp TABLE ( [Id] INT ,[Date] INT ) INSERT INTO @Temp SELECT [Id], [Date] FROM MyTable --Remove your bad data from the real table UPDATE MyTable SET [Date] = NULL --Modify the datatype of your date column ALTER TABLE MyTable MODIFY COLUMN [Date] DATE --Put in an actual date to the column UPDATE mt SET mt.[Date] = DATEADD(dd,t.[Date],'1900-01-01') FROM MyTable AS mt JOIN @Temp AS t ON mt.Id = t.Id
I only suggested running xp_dirtree 'c$' but okay. if he's in charge of sql, and couldn't determine the impacts, then i say he got off lucky.
thank you so much for the hints, will do so and i'll tell you the result
hi. it definitely qualifies mentioning in your resume. I would say this has given you the minimum required knowledge of SQL any junior position on an apps team would need. there's still a ton to learn, but you've got the foundation now. my advice, for whatever it's worth, is to now try to develop something with a SQL database backend. if you're not into programming, at least learn enough of something like html and php so you can see how applications interact with and use a database. this will give you the insight you need to start creating database schemas or data models to solve problems instead of relying on a tutorial to explain everything. even if you never touch a programming language again, this knowledge will be useful as a DBA. 
k
make sure you have the column is set to not null you can't set a primary key on a column that allows nulls
Agile data warehouse design It's pretty good and straightforward https://books.google.ca/books/about/Agile_Data_Warehouse_Design.html?id=TRWFmnv8jP0C&amp;redir_esc=y&amp;hl=enq
I think a trigger could do this.
Unless it’s coupled with real world experience, then it’s a footnote. If your not able to use it in your current position then spend a good portion of your spare time developing your own applications using real world problems. It’s not having the skills that matter; it’s the correct application of those skills. And that comes from using them.
Maybe start with some regular expressions to see if you can strip out the javascript and HTML tags. Postgres, for example, has a "regexp\_replace" function. \(\&lt;script\&gt;.\*?\(\&lt;\\/script\&gt;\)\)|\(\&lt;.\*?\&gt;\) I think will find the crap you want to strip out.
Thanks! 
Bingo (☞ﾟヮﾟ)☞ Herein lies the real problem. Accepting Excel imports into finalized tables. This is the main reason i set up staging tables for excel imports, then use the staging tables to merge into production. The staging table will allow you to format the date as you want prior to pushing to the prod table.
&gt; I'm trying to avoid using more than one table Why? It seems to me like Assignment, Attempt, and Attempt_Grade should/could be separate tables. It would make it much simpler to understand your schema. If you do need to use 1 table for some reason, you could do your roll-up logic in a view, rather than a trigger. 
A view would probably be used to query to table. I'm talking about inserting and updating records. Multiple tables would involve a degree of redundancy that has been slightly problematic in the past. And I don't think it's needed. I think it may even simply things on the querying side, as we do need to show the correction history and all previous attempts to the user as well. I'm trying to avoid storing the actual grade twice and this came across my mind as a potentially more solid idea. Of course, I could be wrong and a separate table could be the infinitely better course of action. 
Hint: you need to use an aggregate function, either `count()` or `sum()` (it's solvable with both).
SELECT TOP 5 PERSON_ID,SUM(PASSING_GRADE) FROM TABLE GROUP BY PERSON_ID ORDER BY PASSING_GRADE That should get you the 5 that failed the most. 
Also if you ORDER BY PASSING_GRADE DESC That will give you who passed the most. 
Yeah if your data has too many dimensions for a single table then split it out. Triggers have a place and can be useful but this is not a good situation for them. General rule of thumb for triggers - if you can avoid them then do. They add performance overhead on any data change and this sort of logic belongs in a procedure. Using a trigger for this sort of thing is like using the handle of a screwdriver as a hammer, it may work but you are better off picking up a hammer. 
So, you would recommend splitting it out into three tables, like: - grade, where *all* grades and corrections live - grade_corrected, where only the last correction for all attempts live - grade_actual, where the currently highest corrected grades live I could consider taking out `grade_corrected` or just using a view for that, and only separate the `grade` and `grade_actual` tables. As for the logic, I'm starting to lean more toward doing it in the procedure. However, I'm still a little afraid that manipulation of the table records outside of the procedures by someone that does not understand could be a problem. As in, correct data is not enforced. 
I think r/MrDarcy87 's solution will not work since you need the top 5 that have failed THE MOST. A SUM will not work since summing 0 will get you 0 :) You don't specify the engine you're using, but taking a guess with VBA + SQL, I'd say SQL Server... Without giving you the actual answer... You'll need to filter on the PASSING_GRADE = 0 to get all the failures, and also a COUNT and a GROUP BY. You'll want to Order By the count, and use a TOP statement
&gt; A SUM will not work since summing 0 will get you 0 :) It will if you put a `CASE` inside the `SUM()` :) It's a twisted way to do it, but it works.
Sure, but if you use a CASE, are you then summing 0, or summing the results of the CASE :)
Something like that. I would personally change the procedure that inserts into grades to check the corrected and actual tables and update/insert as required. Another option would be to encapsulate the logic inline inside a view but you may find that could lead to performance issues if you have a lot of data and are not careful.
and stackoverflow ;)
Thank, yeah I have been searching, I'm mostly having a hard time with the granting these permissions in the "WebUI namespace" which is what my vendor has told me. So for your last statement GRANT ALTER, CONTROL ON WebUI::SALES to user In this example is SALES the database name?
apologies I changed the wrong bit from my google result. Should be: USE DBNAME GO GRANT ALTER,CONTROL ON SCHEMA::WebUI TO &lt;User&gt;
Separating your tables doesn't have to involve redundancy. Just don't put the same column in multiple tables, use foreign keys to join your tables when needed. 
Yes you should be using the correct database otherwise it will attempt to run them against the master db which you don't want. If its a domain account do it like so: USE DBNAME GO GRANT ALTER,CONTROL ON SCHEMA::WebUI TO [DOMAIN\USER] 
thanks so much for the help
I've had nightmares about multiple-column natural primary keys. Plus, this doesn't actually solve the problem, i.e. what is the highest grade of all the corrected grades? 
Then I don't see the point in having multiple tables. It would simply point to a record in another table to indicate the actual grade, exactly like an (indexed) `is_actual` column would do. What's better than: SELECT * FROM actual_grades INNER JOIN all_grades ON all_grades.primary_key = actual_grades.foreign_KEY over: SELECT * FROM all_grades WHERE is_actual = 1 
1-x? ;) 
Easy, group by testid and studentid with a having max grade. 
I'm afraid this could get slow very quickly, especially as there statistics calculations will be done on the actual grades and entire groups of people's grades will get queried. This is mostly an optimization method. It has to be reasonably easy and fast to work with large amounts of the actual grades. 
The last column being inserted is generated by a bunch of html, so you can't strip that out, right? But unless I'm missing something, the only sql here is at the very beginning.
I've never done inserting into a view. It feels kinda *wrong* and views are kinda limited. Would something like this even be possible? Not that I'm considering it. I think I'll get to work with the multiple-table, login in procedures approach. This stuff has a deadline, so I can't overthink too much. :) 
Not possible to insert in a view. I was suggesting you could potentially create a view that contains the logic to find the correct data.
Eh, I'm skeptical. SQL should be able to quickly handle most aggregations that are considered a "hack". I know people argue about something being sargable, but I've never seen extreme wait times on queries like this. For example, I queried our orders table (contains over 3 million rows) and it took less than a second to return the data. Below would be similar to what you want. Grouping Test and Student and giving MAX grade. I'm grouping Customer and Item and giving latest Due Date on the Order. SELECT a.Customer , a.Item , MAX(b.DueDate) DueDate FROM coItem a JOIN(SELECT Customer Customer , Item Item , DueDate DueDate FROM coItem) b ON a.Customer = b.Customer AND a.Item = b.Item GROUP BY a.Customer , a.Item ORDER BY a.Customer , a.Item
Good advice, thank you. Now here's my dumb question: What other client would you suggest? I've only worked with SSMS in the past.
Sounds like you probably want to take what the best ideas are and stage some test tables and try them all. See what feels and functions best. 
Results to file also truncates it :/ Ok, so here's a follow-up question if I do results to grid. The idea is to set this query up on a schedule to get mailed out as a report. Our customer specifically wanted this to display as an HTML report. Is there a way to get it to display as HTML if I do it as grid?
Is still truncated :/
Yeah, I'd love to, but I don't have much time. I just don't want to mess this up, because those grades need to be correct. I've explored real-time. Actually, in SQL Server 2008 R2, I've found using this trick: SELECT g.* FROM grade g LEFT OUTER JOIN grade g2 ON ( g.person_id = g2.person_id g.grade &lt; g2.grade ) WHERE g2.cijfer IS NULL ...performed a little faster than using MAX and a subquery. It's a neat little trick, but it has a problem when two grades are the same--both of them are joined. I wasn't able to really fix that in a good way. Also, this is after I've already filtered out corrected grades with a view. 
&gt;The idea is to set this query up on a schedule to get mailed out as a report Write a small C#.Net application or SSIS package or similar. With an app you have full access to SQL via the SQL namespaces without any SSMS text limits and can also access SMTP libraries for sending out the email. Your google searches you need are: How to read data from SQL using C# (SQLDataReader) How to send email using C# 
No you cannot. I suspected you would not be able to but thought I would give it a try. 1st insert works. 2nd fails. create table dbo.TEST (id Int, name nvarchar(100)) GO create view dbo.VW_Test AS SELECT * FROM dbo.TEST GO create view dbo.VW_Test2 AS SELECT T.ID,T2.ID as T2 FROM dbo.TEST T JOIN (select * from dbo.TEST ) T2 ON t.id = T2.ID GO insert into VW_Test select 1,'test' insert into VW_Test2 select 1,'test' drop view dbo.VW_Test drop view dbo.VW_Test2 drop table dbo.TEST Appreciate this is just theoretical but don't ever insert into views, its just bad practice.
I have been successful sending people over to [https://www.w3schools.com/sql/](https://www.w3schools.com/sql/) to get the extreme basics of SQL. The thing I like about w3schools is that they have a practice database and web based SQL client to get you started. I learned basic SQL by going to a library and picking up MS Access for dummies after my boss asked me if I knew anything about MS Access. I told him no but give me the weekend and I will see what I can do. I spent the weekend engrossed in the book and came back and accepted a project here and there and applied what I learned. I made a ton of mistakes but eventually I moved from MS Access to IBM Informix, Orable, MS SQL. The thing about the dummies book is that it started with Relational Database Theory. I think people who just want to jump into SQL don't want to know the theory. They just want to write SQL. However the more you dive into RDBS \(Relational Database Systems\) and how they work you will find what will work, what obviously won't work, and what will work but will produce wrong results. 
Right click, Save Result As text file, should work
Should be able to do it in around 100 or so lines of .Net code, most of which would just be copied and pasted from stack overflow. 
I love data world and sqlzoo. SoloLearn and CodeAcademy are great for you to see how the query should be structured and add the missing piece. But my biggest issue (after years of just having to edit a query to get the results) was remembering all the little operators. Those 2 sites force you to type out the query which helped me to remember to add them as I write new ones. 
I couldn't resist and did some testing, anyway (cross-posted from another reply). It's actually not that bad. The table has something like 2.8 million records. Querying the grades table outright without any joins or where clauses takes about 13 minutes. The query searching for the corrected, highest grades took around 27 seconds. I could actually go for it and see how it goes. This will be a new, empty table, so I could always rewrite to a multiple table solution later if performance takes a dive. I used a view that queries just the latest corrected grades in the process. Interestingly, it didn't really matter much if I wrote out the view query as a subselect; the view approach was even a bit faster. I thought that the query optimizer would be able to do a little more if written as a subselect in the same query, but apparently, a view really is just a query!
Maybe you should figure it out on your own instead of asking the internet to answer your test questions for you.
SSRS and subscription would be easier option.
Correct on both accounts. If you took the test twice, the highest of the two would count. If one test result has been corrected, the corrected result will be the only one that should be considered in this. I tried to put this in a view for easy querying, but querying that makes things slow again, as it has to be joined with other tables.
If there were two corrections would the second be the only to count? Sorry for all the questions, this one is interesting. 
Welcome to the SQL Collective, young padawan! You'll find SQL to be extremely easy to learn. Let me tell you a story. Once, I had to do a good set of SQL. I've been having problems with my back, not aware of the proper stance. Anyway, here is the best way to do a Squat, Quality Lift: 1. Keep your back straight, with your neutral spine, and your chest and shoulders up. 1. Keep looking straight ahead at that spot on the wall. 1. As you squat down, focus on keeping your knees in line with your feet. I hope this helps! Soon you too will be right on the path to doing proper S.Q.L.s! 
I started with the courses on sololearn, then picked up on the courses on w3schools. The w3schools also have a ton of exercises ( easily found on google search, sorry for not providing a link, am on phone). 
Because the column still has the old data type. I wanted to explicitly insert a date into a date datatype column. Never tried altering a columns datatype with values already there.
Good point! 
The link I posted includes instructions on how to increase the character limit for SSMS. However, if this is something that you run over and over, I'd probably write as a PowerShell script based on your description. I'd probably use ConvertTo-Html over FOR XML, even. So I guess my favored client for this would be System.Data.SqlClient or the SqlServer module's Invoke-Sqlcmd. For a query analyzer like SSMS, I use Aqua Data Studio (ADS), but that's $500 for a single user license and it's not that much better than SSMS for most people. I use it because I support several different RDBMSs from different vendors and ADS supports them all. I still use SSMS for it's server management capabilities, but for query execution and analysis I use ADS. 
Yeah, the second one, being the one that has been made after the first one, will be the only one to count. So, I have a view that has only the corrected (that is, the last correction) grades. Then the highest of those will be the actual grade. I think I explained it to the best of my ability in my original post! If this can perform well in real-time, I'm all for it! Maybe an indexed view? 
Very basic SQL (SELECT statements) are super easy to learn. It gets more complicated once you get into things like subqueries, which imo is where it starts to get really interesting. I'd recommend Itzik Ben-Gan's T-SQL Fundamentals to get a good foundation on SQL. It's published on the Microsoft Press, is really through, and a great introduction to SQL.
Well done! Be careful with sub selects, they can really kill performance. In one instance I dealt with recently adding literally 40 joins to a view instead of some awful sub query made it 600 times faster. The view was used by another view that was called from a procedure in a complex query with lots of joins. The query plan showed the sub select was running without considering the join clause several levels up which led to 6.75 millions rows being processed for a select top 1! 
Which is what I'm dealing with now. If I put all this in a general view that can be queried to find the actual grades, I mean that's basically a subquery. If I take that the view query and add a few joins and a where clause, it performs well enough. If I query the view, add joins to *that* and a where clause, performance is down the drain. My brain is starting to hurt, I'm way overthinking this problem. 
How about this then? Would this fit the bill? https://imgur.com/a/LyG2GzH
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/4zhGuzI.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dz71npn) 
Thanks all for the help. The tables have hundreds of thousands of rows and the conversion kept failing for one reason or another. It turned out to be easier to just clean up the csv and import it as the correct field type.
It may not be. I changed it around a few times to get the right results and then left it how it was when I did 😁 
&gt; If I query the view, add joins to that and a where clause Maybe split it out, use a temp table or CTE with as best a where clause as you can create. You can sometimes use vague where clause on a intermediary data collection to then run a more intensive where clause after. &gt;My brain is starting to hurt, I'm way overthinking this problem. Go for a walk or something. Too many trees to see the wood. 
OH, I thought take the highest grade of the group of corrections, not the grade from the most recent correction. I'll see what I can do. 
date part would not get you the past 12 months anyway it would only get you things from current year. Also the title and the question details don't actually match. If you actually want a 12 month trail then unless you clients very first visit EVER is January 2018 then you want more than just a count of 4 being put in the May 2018 Record. BETWEEN dateadd(MM, -12, **currentrecorddate**) AND **currentrecorddate** (Pretty sure the smaller date has to be first when using **between**, might be the opposite though.) Will get you a rolling 12 months constraint. You'll likely need to self reference in order to do it UPDATE Blargh Set blah.aptcount SELECT COUNT(appointments) From Blargh as blah Where Blargh.ID = blah.ID AND blah.aptDate BETWEEN dateadd(MM, -12, blargh.aptdate) AND blargh.aptdate
What I'm doing now is the following. Try thsi out. The issue here is real-time performance. Excuse the `DROP TABLE` in there, it's just the temp table. DECLARE @grades TABLE ( person int, grade int, attempt int, correction int) INSERT @grades VALUES (1, 80, 1, 0) INSERT @grades VALUES (1, 90, 2, 0) INSERT @grades VALUES (1, 100, 3, 0) INSERT @grades VALUES (2, 95, 1, 0) INSERT @grades VALUES (2, 80, 1, 1) INSERT @grades VALUES (2, 90, 1, 2) INSERT @grades VALUES (2, 89, 1, 3) SELECT b.* INTO #grades_corrected FROM @grades AS b RIGHT JOIN ( SELECT person, attempt, MAX(correction) AS laatste_correctie FROM @grades as b GROUP BY person, attempt) AS laatste_correcties ON (b.attempt = laatste_correcties.attempt AND b.correction = laatste_correcties.laatste_correctie AND b.person = laatste_correcties.person ) SELECT g.* FROM #grades_corrected g LEFT OUTER JOIN #grades_corrected g2 ON ( g.person = g2.person AND g.grade &lt; g2.grade) WHERE g2.grade is null DROP TABLE #grades_corrected 
 SELECT ... , TRIM(LEADING ', ' FROM CONCAT( CASE WHEN SOURCING_FLG = 'Y' THEN ', Sourcing' ELSE '' END , CASE WHEN ASSET_MGMT_FLG = 'Y' THEN ', Asset Management' ELSE '' END , ... ) ) "SERVICES_PROVIDED" FROM 
create table #apptXref (custID int, apptID int, apptDT datetime) insert into #apptxref values (1,1000,'2018-1-1'), (1,1001,'2018-2-1'), (1,1002,'2018-3-1'), (1,1003,'2018-4-1'), (2,1004,'2016-8-1'), (2,1004,'2016-9-1'), (2,1004,'2016-10-1'), (2,1004,'2016-11-1'), (2,1004,'2016-12-1'), (2,1004,'2017-1-1'), (2,1004,'2017-2-1'), (2,1004,'2017-3-1'), (2,1004,'2017-4-1'), (2,1004,'2017-5-1'), (2,1004,'2017-6-1'), (2,1004,'2017-7-1'), (2,1004,'2017-8-1'), (2,1004,'2017-9-1'), (2,1005,'2017-10-1'), (2,1006,'2017-11-1'), (2,1007,'2017-12-1'), (2,1008,'2018-1-1'), (2,1009,'2018-2-1'), (2,1010,'2018-3-1'), (2,1011,'2018-4-1'), (2,1012,'2018-5-1'), (3,1009,'2016-12-1'), (3,1010,'2017-3-1'), (3,1011,'2017-6-1'), (3,1012,'2017-9-1'), (3,1013,'2017-12-1'), (3,1014,'2018-3-1') /* first appointment within the last year */ IF OBJECT_ID('tempdb..#mindt') IS NOT NULL DROP TABLE #mindt select custID, min(apptDT) mindt into #mindt from #apptxref where apptDT &gt;= DateAdd(yy, -1, GetDate()) --exactly 1 year ago group by custID --partition by and order by - row number can be appointments select *, row_number() over (partition by x.custid order by apptDT ) from #apptxref x inner join #mindt m on x.custID = m.custID and mindt &lt;= x.apptDT --where x.custid = 2 
Ralph Kimball's ["Data Warehouse Toolkit"](http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/books/). Kimball literally wrote the book on dimensional modeling.
I haven't made any progress really. Booking a house isn't as simple as it sounds, mainly because of discounts. Fundamentally, a house has a base price (let's say $1000 per week), then there are usually 5-6 seasons, with varying validity dates spread across the year, which cause a depreciation in price. For example the house might cost 110% of the base price in July (the summer in my Country), but only 50% of the base price in the dark and cold November. Additionally, a house can have zero or more discounts associated with it, and the factors that can influence which discount should be offered to the customer (and always the one that offers the highest rebate) are: 1) Number of guests (for example 2-person discount) 2) Amount of weeks booked (for example 2+ weeks) 3) Amount of days until arrival (so-called "last minute" discount) The discounts can have any validity time span (for example a discount can be valid between 1st and the 30th of May), and this makes everything really complex. There's a "mini vacation" discount which is actually an opposite discount - for example, 3 days for 60% of a week base price. Then on top of this you have the whole availability aspect (is the house booked or not), and including homes which are available for arrival on the previous or coming day (surrounding the arrival date the customer searches for). There are also other business rules, such as not being able to book more than 5 consecutive, and no more than 9 total weeks during the winter period. Maybe I'm just missing the big bright idea, but I just can't see the light.
What would determine whether it’s True or False? I’m on mobile so maybe I’m missing something here but I don’t see that information available in either of the source tables. 
I can do it, but it's rather like doing it with your teeth, and there's probably a better way to do it. So, first, we want a list of the file and user ids, without duplicates. select FILE_ID, USER_ID from FILE_READ_AUTHS UNION select FILE_ID, USER_ID from FILE_WRITE_AUTHS, then we want to add two columns on the outside, maybe case statements. select FILE_ID, USER_ID, (case when (select count(*) from FILE_READ_AUTHS r where r.FILE_ID = one.FILE_ID and r.USER_ID=one.USER_ID)&gt;0 then true else false end) as READ, (case when (select count(*) from FILE_WRITE_AUTHS r where r.FILE_ID = one.FILE_ID and r.USER_ID=one.USER_ID)&gt;0 then true else false end) as WRITE from (select FILE_ID, USER_ID from FILE_READ_AUTHS UNION select FILE_ID, USER_ID from FILE_WRITE_AUTHS) one
My bff.
I haven't tested this, but I think this should do it with FILE_READ_AUTHS AS( SELECT 2 AS FILE_ID, 5 AS USER_ID UNION ALL SELECT 1 AS FILE_ID, 3 AS USER_ID UNION ALL SELECT 2 AS FILE_ID, 3 AS USER_ID ), FILE_WRITE_AUTHS AS( SELECT 1 AS FILE_ID, 3 AS USER_ID UNION ALL SELECT 2 AS FILE_ID, 5 AS USER_ID UNION ALL SELECT 6 AS FILE_ID, 9 AS USER_ID ) CTE_UNION AS ( SELECT FILE_ID ,USER_ID ,1 AS READ ,NULL AS WRITE FROM FILE_READ_AUTHS UNION ALL SELECT FILE_ID ,USER_ID ,NULL AS READ ,1 AS WRITE FROM FILE_WRITE_AUTHS) SELECT FILE_ID ,USER_ID ,MAX(READ) AS READ ,MAX(WRITE) AS WRITE FROM CTE_UNION GROUP BY FILE_ID ,USER_ID 
You can do this with set operations. Maybe not the most elegant solution, but pretty straight forward to understand: -- entries that have only reads (SELECT file_id, user_id, TRUE, FALSE FROM file_read_auths MINUS SELECT file_id, user_id, TRUE, FALSE FROM file_write_auths) UNION -- entries that have only writes (SELECT file_id, user_id, FALSE, TRUE FROM file_write_auths MINUS SELECT file_id, user_id, FALSE, TRUE FROM file_read_auths) UNION -- entries that have reads and writes (SELECT file_id, user_id, TRUE, TRUE FROM file_read_auths INTERSECT SELECT file_id, user_id, TRUE, TRUE FROM file_write_auths);
Got it, thanks. Totally didn’t notice the difference in table names. 
This is basically how *nix permissions work - I assume you know that since you use the same 2 and 4 values (albeit flipped) to represent the values. I'd consider this to be a neat solution, though - and if it's good enough for Linux, it's probably good enough for the rest of us.
Neat, I didn't know *nix permissions worked like this, but I did know of numbering settings or permissions in the 2^x series (1, 2, 4, etc.). For example, the game Quake 2 stored its user settings like that. For those who don't know, it means that you can store a slew of true/false settings in a single number, then use bit shifting to "extract" individual settings. For example, the binary number 01001 can be parsed as 5 different boolean (true or false) values, and simply be stored as the number 9.
If there's one thing I've learned about DB2, it's that everything I think I know about SQL is worthless when it comes to writing queries in DB2.
A full outer join might work. I'm using TSQL here: SELECT COALESCE(fra.FILE_ID, fwa.FILE_ID) AS FILE_ID ,COALESCE(fra.USER_ID, fwa.USER_ID) AS USER_ID ,CASE WHEN fra.USER_ID IS NULL THEN 'FALSE' ELSE 'TRUE' END AS READ ,CASE WHEN fwa.USER_ID IS NULL THEN 'FALSE' ELSE 'TRUE' END AS WRITE FROM FILE_READ_AUTHS fra FULL OUTER JOIN FILE_WRITE_AUTHS fwa ON fra.FILE_ID = fwa.FILE_ID AND fra.USER_ID = fwa.USER_ID ORDER BY COALESCE(fra.FILE_ID, fwa.FILE_ID)
Yes but this is even simpler than your example: SELECT * FROM all_grades_actual_v
So running a query against a single, relatively small table (&lt;3M rows) with no joins or anything takes 13 minutes?! There might be more problems than the SQL in that case. Was someone manually fetching the records by hand? 
i decided to do this as an exercise before reading the comments and i came up with the same thing (also T-SQL) SELECT ISNULL(r.[FILE_ID],w.[FILE_ID]) as [FILE_ID] ,ISNULL(r.[USER_ID],w.[USER_ID]) as [USER_ID] ,CASE WHEN r.[FILE_ID] IS NULL THEN 'False' ELSE 'True' END AS [READ] ,CASE WHEN w.[FILE_ID] IS NULL THEN 'False' ELSE 'True' END AS [WRITE] FROM FILE_READ_AUTHS r FULL JOIN FILE_WRITE_AUTHS w ON r.[FILE_ID] = w.[FILE_ID] AND r.[USER_ID] = w.[USER_ID] so clearly this is the best answer ;)
 with FILE_READ_AUTHS as ( select 2 file_id, 5 user_id from dual union all select 1,3 from dual union all select 2,3 from dual ), FILE_WRITE_AUTHS as ( select 1 file_id, 3 user_id from dual union all select 2,5 from dual union all select 6,9 from dual ), unioned_table as ( select file_id, user_id from FILE_READ_AUTHS union select file_id, user_id from FILE_WRITE_AUTHS ) select file_id, user_id, case when user_id||file_id in (select user_id||file_id from FILE_READ_AUTHS) then 'True' else 'False' end READ, case when user_id||file_id in (select user_id||file_id from FILE_WRITE_AUTHS) then 'True' else 'False' end WRITE from unioned_table order by file_id; Result: https://i.imgur.com/zvGQrDd.png Explanation: First two statements are just the "virtual" tables. The "unioned_table" table is a union and deduped table combing the distinct result of the tables. The final select selects the file_id, the user_id, then checks to see if the combination of file_id and user_id matches the combination of file_id and user_id in the READ table, then the next item selected uses the same method to select it from the WRITE table. Then I order by file_id because that's what you have. 
Puncuation some do some don't. But how does it make you feel on an emotional level pls comment do u luv d fast typin n flexibility of fone SP33K Or do you prefer clear, concise, and accurate communication that is quick and easy to process?
I don’t see you including the DISTINCT function in your second select. That and even if you did include it, since there are two rows of data, with different values in the columns, I think it would still return two rows each for 101 and 200. Using distinct is similar to adding a GROUP BY and stating every column, it will take all cases where the values in each column are the same on multiple rows and only return it once. If you added aggregation like summing it would apply that to the aggregated column while consolidating/grouping the non-aggregated columns.
I haven't written anything in a while..so I wanted some practice.. and I don't have a test db handy to test WITH Read_CTE (FILE_ID, USER_ID, READ_INT) AS ( SELECT FILE_ID, USER_ID, '1' AS READ_INT FROM FILE_READ_AUTHS), WITH Write_CTE (FILE_ID, USER_ID, WRITE_INT) AS ( SELECT FILE_ID, USER_ID, '1' AS WRITE_INT FROM FILE_WRITE_AUTHS) SELECT COALESCE(X.FILE_ID, Y.FILE_ID) AS FILE_ID, COALESCE(X.USER_ID, Y.USER_ID) AS USER_ID, CASE WHEN X.READ_INT = '1' THEN 'TRUE' ELSE 'FALSE' END, CASE WHEN X.WRITE_INT = '1' THEN 'TRUE' ELSE 'FALSE' END FROM Read_CTE AS X FULL OUTER JOIN Write_CTE AS Y ON X.FILE_ID = Y.FILE_ID AND X.USER_ID = X.USER_ID; I don't like the full outer join.. but, whatever, pretty sure this would get the job done... or it would go up a heap of flames
Ramrod45 \(Username reference to Super Troopers or to muzzle loading?\), I corrected the code adding DISTINCT to the sub query. It still gives me the same results with duplicates. . I added GROUP BY to the sub query and got the same results \(duplicate EMPLOYEE\_ID's\). Please let me know if I am missing your point. `SELECT *` `FROM JOB_HISTORY` `WHERE EMPLOYEE_ID IN` `(SELECT EMPLOYEE_ID` `FROM JOB_HISTORY` `GROUP BY EMPLOYEE_ID` `)` `ORDER BY EMPLOYEE_ID;`
The IN won't remove the duplicates. What are you trying to achieve? When there are multiple records for the same employee in the JobHistory table, what do you want the query to show you?
Your second query is effectively `SELECT * from FROM JOB_HISTORY`, because of course every employee id in the table is in the table.
Thanks man. It works perfectly 
A solution is to use a unique composite function based index. Make it so that if Local = No then both columns are null in the index (Oracle will not index it if they are both null). If local is yes, it will enforce uniqueness across that item. For example (not tested for syntax, I'm on my phone): CREATE UNIQUE INDEX unique_item_local_idx ON my_table (CASE WHEN is_local ='N' THEN null ELSE item_id END, CASE WHEN is_local='N' THEN null ELSE is_local END)
; becoming standard enables the language to use keywords more flexibly. Without it, it's more difficult to implement keywords in the middle of a statement, that alternatively could begin a new statement. That's why the previous statement must be explicitly terminated before using a WITH cte, because WITH could also be a hint.
The inner select retrieves a list of (distinct) employee IDs. ie duplicates removed. The provided outer query will then take that list and use it as a list for the ‘in’ of the where clause to the outer query. So it looks in job_history for any entries where employee ID is in the where clause. This could include multiple records. You’ve not asked it to remove duplicates. Perhaps if you explain what you’re trying to achieve, someone can help you with that. 
Oh this is kinda cool. I'm actually pretty used to *nix permissions and understood this solution ""easily"". Also I borrowed the whole file/authorities example from "unix world" since my real case is a bit different but the logic is the same.
It’s required when you make multi statement procedures in some systems so I just do it all the time now. Don’t really care now that it’s a habit.
It gives me a deep sense of fulfillment and purpose in life.
Your data model seems a bit off here. I am assuming that UserID is an identity column and is generated upon insert on the user account table. So why do you also have an ID column? Anyway, the user info table references the UserID column from the user account table, so that means the user account needs to be populated first. Try using scope_identity(). If you are using mysql it is last_insert_id(). Lastly, you can also choose to do a merge statement to capture the IDs if you are inserting many records at once. 
&gt; Your data model seems a bit off here. I am assuming that UserID is an identity column and is generated upon insert on the user account table. So why do you also have an ID column? This is my question as well. You need to create logic that does your first insert, captures the scope_identity value, then performs the second insert with the scope_identity value. To make sure both inserts happen consistently you can nest your query logic inside a transaction. Also, you could use an if statement to conditionally execute the second query if your first query returns a value for the @@rowcount. Try playing with a transaction solution until it is correct and you understand its use. 
Upon insert, it returns the last ID created as a result of an identity column. You can capture it and use it to insert the foreign key into your user information table. You might run into trouble with it because you have an ID column on your user account table. 
No I’m a beginner in databases....and I know this might sound dumb or stubborn but I really want to learn to set up n be fluent in Linux... It aggravates me that it’s taken me so long to set up and I’m Not the type to really give up when I’m so deep into something 
No it’s telling me mbstring is missing. I did the apt-get install mbstring It says it successfully installed I restarted it and still nothing smfh
That’s the beauty of it. A career in it doesn’t mean you have to excel at anything. Sounds like you have an interest in databases. I am a dba (database administrator) and in a professional environment segregation of Centers of excellence are mandatory. Learn Linux, sure. But rdbms systems are agnostic of os. A database should and if using a big 3 vendor, will work the same. Your host os platform doesn’t come into play. Same with php, it’s a programming language. Using any host os shouldn’t impact your system provided you’re not doing something like writing to /home/user/file. Don’t make your beginning journey harder then it has to be buddy. 
Never installed on Mac, not exactly enterprise grade rdbms vendor. Windows, Linux, etc. I primarily deal with oracle, on their site I know they have a virtual machine you can download with the os tuned and db installed with some tables to work with. I don’t know your level, what you know, your interest or even if you ever wrote sql before. When I hire an intern that’s what I make them work with. Again, this is oracle only. Pick your poison :)
Thanks for this mate...really appreciated. Blessings.
I'll throw in a slightly more generic solution that returns a row for each user and file, not just the ones with permissions granted. I assume FILE_ID and USER_ID are foreign keys to [USER] and [FILE] tables. SELECT F.FILE_ID, U.USER_ID, CASE WHEN R.USER_ID IS NOT NULL THEN 1 ELSE 0 END AS READ, CASE WHEN W.USER_ID IS NOT NULL THEN 1 ELSE 0 END AS WRITE FROM USER U CROSS JOIN FILE F LEFT JOIN FILE_READ_AUTHS R ON R.FILE_ID = FILE_ID AND R.USER_ID = U.USER_ID LEFT JOIN FILE_WRITE_AUTHS W ON W.FILE_ID = FILE_ID AND W.USER_ID = U.USER_ID
Let me know if you need any help
This isn't homework. I'm just trying to understand how to do something. As for your response, this make total sense now that I'vs slept on it. The sub query look for the DISTANT JOB\_ID. Once that is done, the outer query looks for ALL the JOB\_ID numbers that were found. This is what I am trying to do. Show all the columns in the table If there are duplicate JOB\_ID's, only show one. Which one is not important. 
This is what I am trying to do. Show all the columns in the table If there are duplicate JOB\_ID's, only show one. Which one is not important.
I'm with y'all in spirit!
Maybe you can't change it, but if a user can never have more than one accounts, which I assume not, there's no need to have a separate tables. Just combine them into one table. Only make two tables if there is a one-to-many relationship. I made this - not tested, but something like this. As others said, it makes no sense to have two auto-generated ID's on a user table. There's nothing wrong with scrapping the typical usual INT Id column and making your own, for example just "UserId". begin tran insert into Account (username, password) values ('johdoe', 'dongs') declare @id int, @userid int; select @id = @@IDENTITY set @userid = (select userid from Account where id = @id) insert into UserInformation (userid, firstname, lastname, email) values (@userid, 'john', 'doe', 'john@doe.com') commit tran
The way I'd normally do this is with a window function (i.e. row_number() OVER (partition by employee_id order by YourEndDateField desc) but you're unlikely to have gotten to window functions yet. Assuming there is a unique key on that table (e.g., Job_History_ID) you could instead use a GROUPed subquery: SELECT * FROM job_history j WHERE j.job_history_id IN (SELECT max(Job_history_ID) FROM job_history GROUP BY employee_ID) This way the subquery is returning the unique key of the Job_History table, and grouping it on employee_ID means it is returning only one of those IDs per person.
Are those all the columns in the table? Employee I’d, start/end date, job title, whatever the last one is? Is there a primary key?
I bet you could create a temp table, without th the two ids, read, write. Insert from either table and update the corresponding read/write column. Then merge into the temp table from the other read/write table. If matched update the read/write column, otherwise insert and set the /read write to true. Then select from the temp table.
The subquery only creates a table to check against, it doesn't restrict the main query to distinct records. You actually want to use only the subquery as below (untested): SELECT * FROM JOB_HISTORY GROUP BY EMPLOYEE_ID ORDER BY EMPLOYEE_ID;
Maybe you are trying to get this value for each batter_Id? In this case, since each query will return multiple rows (one for each batter_id), and you're not doing a join, it's not clear what to numbers to divide; hence your error. You need to restructure your query, so that you have: SELECT batter_id ,COUNT(*) in both of your two sub-queries, then LEFT JOIN those ON the Batter table. Then you can select your division on each row for each batter. SELECT b.Id ,pbp.Value / ab.VALUE FROM Batters AS batter_id LEFT JOIN (SELECT COUNT(*) AS hits ,batter_id FROM mlb.pbp_play_by_play WHERE event_type = 'single' OR event_type = 'double' OR event_type = 'triple' OR event_type = 'home_run' GROUP BY batter_id) AS b ON b.batter_id = b.Id LEFT JOIN (SELECT COUNT(*) AS AB ,batter_id FROM mlb.pbp_play_by_play WHERE hit_track != '' AND event_type != 'walk' AND event_type != 'intent_walk' AND event_type != 'sac_bunt' AND event_type != 'sac_fly' AND event_type != 'other_out' AND event_type != 'batter_interference' AND event_type != 'fan_interference' AND event_type != 'runner_interference' AND event_type != 'error' GROUP BY batter_id) AS ab;
There are tons of reasons why you might want to split that data into multiple databases. Example; you need to give read access to Accounting without letting a whole department see everyone's password data. 
What exactly are you trying to do?
Not sure why you got down voted. This solution actually performs really well. 1 logical read and 1 scan vs my 4 logical and 2 scans. Although it might be a different story for a larger dataset, this is still a pretty clever way of doing the query that performs much better than some of the other examples! 
Did it come with instructions on what to download?
Any version of python has sqlite3 linked in it, at least since 2.5, maybe even earlier? Anyway as others have pointed out, SQL is just a language. You're saying "I'm taking an online course in driving cars. The course says I need an out of date car carrying truck (python 2.7) that has a 1976 VW bug (sqlite3) already on it but you could also put any other car (mysql, posters, mysql...) on it to drive it around too. You need to figure out what database engine the course wants you to use to run the SQL queries on. Python in that context is just the thing you use to send the queries to the database. /r/LearnPython is a great sub to go to for help on installing and getting started with python.
 SELECT batter_id , COUNT( CASE WHEN event_type IN ( 'single' , 'double' , 'triple' , 'home_run' ) THEN 'hit' ELSE NULL END ) / CASE WHEN hit_track != '' AND event_type NOT IN ( 'walk' , 'intent_walk' , 'sac_bunt' , 'sac_fly' , 'other_out' , 'batter_interference' , 'fan_interference' , 'runner_interference' , 'error' ) THEN 'not hit' ELSE NULL END ) AS avg FROM mlb.pbp_play_by_play GROUP BY batter_id
Same missed it!
The syntax may be different for Oracle, but maybe this gives you a starting point - For all of your vehicle info columns I would do this: Isnull(table1.vin,table2.vin) as vin. 
That only makes sense if you're doing permissions per table, and offering direct access to tables either via management studio or such. Can't really imagine such a setup, but of course weirder stuff happens in real life...
How is it pricy? It's free. You can also downloaded SQL Server Development Edition. It is also free. It is not limited in any way - except that you should not use it in Prod. Dec and Test are ok.
You can download SQL Server Express for free. This comes with SSMS. I'd recommend downloading one of the AdventureWorks or Northwind databases for practice
&gt;Ive looked at buying a copy of SSMS but its a bit pricey for me. Say what? As of 2016, SSMS is a separate download/product from SQL Server and there's no charge. Even before that split, it was freely installable (just spin up the SQL Server install and only pick the Client Tools). But having SSMS isn't enough to work with T-SQL - you need to connect to a database. Fortunately for you, SQL Server Express Edition is (and always has been) free to download and use, and SQL Server Developer Edition (2016 &amp; 2017) is free for non-production use. But if you're still not convinced that you can keep practicing for free, Stack Exchange makes their databases freely queryable: https://data.stackexchange.com
SSMS is a separate download now. Northwind has been retired for a long time now; Microsoft's sample databases [are on GitHub](https://github.com/Microsoft/sql-server-samples/tree/master/samples).
Good to know. Is SSMS still free though? Was in December
SSMS always has been free and I don't see Microsoft ever changing that. Its apparent successor (primarily meant for developers at this point) is even [posted on GitHub](https://github.com/Microsoft/sqlopsstudio). MS wants you to be a customer of their databases (on-prem and cloud); putting a price on the client tools will just raise the barrier to entry, the exact opposite of what they want.
Its my understanding that if I don't already have a database to connect to\(I dont\), I would have to set up my own, which takes a $50 license
That's really cool of Stack Exchange, and I didn't know this previously. Thanks! 
I was under the impression SSMS was free but not sql server. It appears I was mistaken. So I can set up a local server using sql server express for free? That’s a whole lot easier than what I was envisioning 
You can set up a local instance using Developer Edition for free, if you want.
They even have periodic dumps of the database you can download and use offline.
SQL/Python installation trouble https://imgur.com/a/63ljmhU Here are a few pics from the installation including what I'm trying to download and the open Python window showing that it is indeed downloaded. 
It's free up to 10 GB.
Thanks for the response, I really appreciate it.
Thanks for the response, I really appreciate it.
i'll bet you say that to all the replies
Literally just tested yours, it works perfectly. Thank you so much
Another option besides what has been mentioned already - use a hosted solution like an Azure SQL database. You can also get free 180 day evaluation versions of Windows Server and.SQL Server
You are correct in that I don't know what a window function is. But I will now look it up and learn about it. Thanks for your help. I seem to move ahead while trying to do exercises I find online, currently at [hackerrank.com](https://hackerrank.com). I have also thought about my original question, and no one would ask this type of question in real life. The question would either be what was the last job or the first and I would have the query based on the date. But there could and will be something like it. Thank you for your help. 
The group by makes total sense. And you are correct, no one would write this query. They would probably want to know what the first or last job was, so the query would be based on the date. Thanks your help.
Here are the columns EMPLOYEE\_ID \- Complex Primary key START\_DATE \- Complex Primary key END\_DATE JOB\_ID DEPARTMENT\_ID \- Foreign key
We figured it out, guys. OP is trying to install MySQL. He thinks it's called SQL, so he's a little confused. So you know, SQL is the language you will you use to query the MySQL database after you get it installed. Looks like Python 2.7 is a prerequisite to installing this version of MySQL. You probably need the [MySQL Connector/Python](https://dev.mysql.com/downloads/connector/python/) If you are still having trouble, read up on a few threads like [this one.](https://stackoverflow.com/questions/12702146/mysql-for-python-2-7-says-python-v2-7-not-found?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa)
So instead of grabbing distinct employee ids and joining on that first do select employeeid, max(startdate) From job history Group by employeeid And then join to job history using the employeeid and the max start date . That will ensure you only get one row for each employee Id.
Getting Nicholas in this case should be pretty easy, however I assume you have to get more than 1 name/record with one distinct length right? For example, if that name was "Jake" the code it sounds like you're asking for would not work.
That's correct, as it has different length in names, I could not set a definite length in my charindex, I would really like to set up a query where it only extracts the value between first and date. much appreciated 
I would recommend Developer Edition over express since it includes the perf tuning tools that you can't get in Express.
 select distinct We might have screwed up a join or granularity someplace but who's got time for that when... AC1.givename, AC1.famname, AC2.givename, AC2.famname it gives you names. Of two people. Slowly. from academic AC1 What if you take all academics , author AU1 get every possible combination of authors and academics , academic AC2 throw another of those academics into the cube , author AU2 and kick it up a notch to the 4th dimension. If a monkey can produce a Shakespeare, this tesseract of gnosis gotta produce another "Origin of Species". where This is, literally, &lt;where&gt; we get to play the part of the Intelligent Designer and cull some of those undesirables via some unnatural selection. AC1.acnum=AU1.acnum and AC2.acnum=AU2.acnum and AU1.panum=AU2.panum The subjects of your inquiry besides being authors are linked via a mysterious property 'panum'. What is it? We might never know. and AU2.acnum&gt;AU1.acnum A and B or B and A? There could be only one, and an academic with longer tenure most likely will have a lower ID; so they will prevail since people with longer tenure need those wins. and not exists (select * Also, we couldn't care less for... from Interest I1, Interest I2 where I1.acnum =AC1.acnum and I2.acnum=AC2.acnum These people, or rather academics, who have.... (drumroll) WHO HAVE (drumroll intensifies) and I1.fieldnum=I2.fieldnum); COMMON FIELDNUM! ps. this query probably shows pairs of co-authors who don't share the same field or something like that.
Thanks! Your my saviour. Sending good karma your way !
You could just post them and someone would be happy to help.
Are you saying you are not able to login to the mysql command line client? 
no, I have been using the MySQL workbench GUI (also tried with sequel pro) since I am just trying to use the most simple approach.
Shut up, OP is willing to pay cash money.
Yeah that happens (in microsoft sql server) when you get no rows. Change your join to a left join, does it return results? If yes your join condition is wrong. Change back to an inner join then comment out the joins 1 by 1 to find what is wrong. If changing to a left join does not return results then its either an empty table or there is a where clause thats at fault.
One of the best online course. For beginners and for free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Contact me via pm :)
Many ways to do such a thing. If you intend to run the code multiple times then I would consider parsing it into proper XML and then query it using xpaths. With the right indexes it will be way faster than string magic. declare @XMLString varchar(1000) --its not properly formatted XML so make it proper by putting it all into a single user node SET @XMLString = '&lt;lastName="Mossin" firstName="NICHOLAS"&gt;&lt;date="1/1/2018"&gt;' SET @XMLString = replace(@XMLString,'&lt;lastName','&lt;user lastName') SET @XMLString = replace(@XMLString,'&gt;&lt;date',' date') SET @XMLString = left(@XMLString,LEN(@XMLString) - 1) + '/&gt;' SELECT @XMLString --change to XML type declare @XMLDoc XML SET @XMLDoc = @XMLString SELECT @XMLDoc --query with xpath SELECT @XMLDoc.value('(/user/@firstName)[1]','varchar(255)')
Post them. Or are you doing this for a school assignment and don't want to get caught?
HOST = mysql538.in.shared-server.net PORT = 47256
FYI: im doing this for a job assessment and really, really want/need this job
idk if this is unethical or what not :: i would ask my programmer friends but didn't really want to bother anyone
You may want to visit a PASS group/chapter meeting and see what members there suggest for training resources specific to your area. Aside from that..... Microsoft have some pretty good tools for finding a regional training partners https://www.microsoft.com/en-gb/learning/partners.aspx [UK site] You may also want to find out if you have Software Assurance from Microsoft as some of the courses can be exchanged with credits. https://www.microsoft.com/en-gb/learning/software-assurance-benefits.aspx [info] http://savl-catalog.microsoft.com/ [courses -make sure to set a language filter-] 
Try Pragmatic works. Our company has used them for us. They are little pricey though. See on google about them. 
PM me with Qns and your answers
Thank you so much for your reply! It really means a lot to me to hear back, especially when I know that I am practically like infant trying to drive a tank. I am so sorry to waste your time with such trivial questions, but I've really exhausted all google searches about accessing SQL Servers &gt;_&lt;. I am glad I assumed before that was the port, but even with the two hostnames, I get this error when I ping and try to test the connection in the GUI: Unknown MySQL server host 'db_name.mysql538.in.shared-server.net' (0) Unknown MySQL server host 'mysql538.in.shared-server.net' (0) Could it be that whoever configured the database restricted privileges for what remote users can access it? Even if I have the proper login credentials? Is there something simple I may be overlooking that needs to be done on my system for me to access a database remotely? There is almost no possible way the credentials are wrong since the website functions without any troubles now. Is there any other information that I may provide you that might make it easier for you to understand the issue? Thank you so much again for the help! 
My point is, I don't see why I should use a separate table to point to what is the actual grade, when I can also do it in the same table. I think that even having a redundant table containing the grade itself and all other needed information would probably be better. 
Not a mysql guy but if you create a dotnet embedded DLL (C# VSTO) you can write the mysql connection code yourself and call that DLL via a macro. Not ideal but it would work if you run out of other things to try. You can bundle it into clickonce for easier distribution aswell.
thanks guys. actually am also giving upon it.
Thanks! 
Not sure why you got downvoted for pointing this out.
There really needs to be a sticky post in this sub that redirects people who think "SQL" = "MSSQL"
Where are you looking for work?
Write a subselect where you count rows per ID where state equals 2. Then write a query around it where u use a case when count equals 0 then false else true On cellphone so i dont like to write the the statement
I'll check them out. Thank you!
I should clarify: I'm not interested in the actual SQL but more interested the best way to approach the problem, hence the screenshot of non-copyable text. That was possibly a dick move so I apologize. But I don't need code, just discussion and a good approach. I feel like this can be done with windowing functions by dividing each combination into partitions, but then I'm not sure what to do because that has to be in some kind of update statement. 
Thinking from a SQL server perspective, I would think row_number() or ranking function would work here to identify the non-conforming values.
WITH TMP AS ( SELECT computerID, count(policystate) as 'count' FROM table WHERE policystate = 2 GROUP BY computerID) SELECT tmp.computerID, CASE WHEN tmp.count &gt; 0 THEN 'TRUE' ELSE 'FALSE' END as 'PolicyState2'
Without the extra brackets, the order of operations is causing the OR to get applied first. In your query you are asking for records that meet one of two criteria: 1. The group contains Dr. Dre or 2. The artist contains Dr. Dre AND the year is before 2001 or after 2009. By putting the brackets in you're forcing the group/artist to be evaluated first, together, before the AND kicks in.
That clears things up for me, I appreciate your breakdown of the logic! Thanks for the response!
Your first statement is saying: group like dr dre must be true Or alternatively, artist like dr dre must be true, BUT thats not all, and in addition, if that's true then you must have one of these two conditions be true (year&lt;2001 OR year&gt;2000) Consider ID 5839 and go through the list. * Is group like dr dre? Yes --&gt; True! (note that nothing else matters because the condition is satisfied!) * Is artist like dr drew? Yes --&gt; True! * and in addition, is one of the following true? (year&lt;2001 OR year&gt;2000) --&gt; False. But it doesn't matter. Why? Because the first "like" condition was true. Your second statement changes the conditions. Here you are saying: group OR artist like dr dre AND in addition, the year must be &lt;2001 OR year must be &gt;2000) Consider ID 5839 and go through the list. * Is group like dr dre? Yes --&gt; True! * Is artist like dr dre? Yes --&gt; False! * okay, it's still a canidate for a true result, so lets move on to the second requirement: * and in addition, is one of the following true? (year&lt;2001 OR year&gt;2000) --&gt; False Therefore, it won't show up because the second part of the AND condition was not satisfied. 
http://sqlfiddle.com/#!4/f4a14/1
Thanks krankie! I'm going to have to hone my operator sequencing muscle it looks like, ha! Appreciate the help, have a great afternoon.
&gt; or ranking function Yup - I would try with RANK OVER PARTITION, but as my brain is shutting down for today I'm unable to engage any more... :(
 SELECT ComputerID, PolicyState2 = MAX(CASE WHEN PolicyState = 2 THEN 1 ELSE 0 END) FROM [your table] GROUP BY ComputerID
Neither &gt; mysql538.in.shared-server.net or &gt; shared-server.net responds from here so my guess is that the info you have been given is incorrect.
 Some sql engines will do that. I don't think mysql will do it though. But ummm just to state the obvious. It should not be like that when the data was imported / created. They should be in another table and be joined when you want that kinda query to run
Yeah unfortunately it’s all I have to work with at the moment. I can make it work manually using text to columns, and then transposing in excel., but it’s very tedious 
It is possible though... https://stackoverflow.com/questions/19101550/mysql-join-two-tables-with-comma-separated-values
SSMS is just the front end tool for SQL Server. You will need [SQL Developer Edition](https://www.microsoft.com/en-us/sql-server/sql-server-downloads). This will install a local instance on your machine that your SSMS can connect to. It is free to use for development purposes. Then you can create your own DBs (will only be available on that machine only) OR download the sample [Wolrd Wide Importers DB](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) to play around with. Used to be Adventure Works but this is the latest iteration.
His desired output is what I’m actually trying to avoid. My database currently outputs several values, separated by commas, in a single cell. Instead I’d like it to start a new row for each value
here's some code which will grab the first three id in the single cell follow the pattern for more note UNION DISTINCT because the 2nd and subsequent select will return the same value if there are fewer items in the list SELECT id , SUBSTRING_INDEX( REPLACE(company_IDs_whitelisted,' ','') , ',' , 1 ) AS singlecolumn FROM unconcat UNION DISTINCT SELECT id , SUBSTRING_INDEX( SUBSTRING_INDEX( REPLACE(company_IDs_whitelisted,' ','') , ',' , 2 ) , ',' , -1 ) AS singlecolumn FROM unconcat UNION DISTINCT SELECT id , SUBSTRING_INDEX( SUBSTRING_INDEX( REPLACE(company_IDs_whitelisted,' ','') , ',' , 3 ) , ',' , -1 ) AS singlecolumn FROM unconcat ODER BY id test and seems to work, but i would look for a more elegant solution, because this is a hack
Thank you, I’ll give this a stab in the morning when I have access again
I remember using Navicat for the databases of my Aion server, that should suffice too, right?
 Well as you can see its a complain pain in the arse to do. Cause database were/should never be designed to operate this way. There are other questions like that eg https://stackoverflow.com/questions/17308669/what-is-the-opposite-of-group-concat-in-mysql ^^ I have never ever had to actually join a stored procedure like this! Expect to get a coffee everything you run a query though!
Navicat will allow you to manage the instance once you have set up the server, but it serves the exact same function as SSMS. You will still need to install SQL server in order to use either frontend.
You should post what you have to do for your assignment and ask for help. That will get you more help.
Can you post your assignment questions?
I have too much trouble with procedures and functions to even begin asking for help, plus the resources we have to work with are horrible, database from 2010 that haven't been touched since, its a real mess relations are all screwed up with I've spent nearly 2 hours today working around a relation that ended like half way through because at some point from 22 records there were only 2 that have been filled with actual data and the rest was just NULL.
Yeah I need to prepare the documents so it can't be linked back to me in anyway. The assignment itself it's actually very easy 3 questions, summary report(sproc), insert procedure and view trigger.
Almost sounds like it will be more work to cover your tracks than to just spend the time to learn these fundamentals of SQL? Your only cheating yourself and the profession...
Hey, Thanks for your suggestion, I changed my join to join left but it didn't work. Turns out one of the cells had like a spacing at the end but for some reason the join left didn't cut it out, so I Left(abc,3) it and it worked
Dude 3 weeks ago I had no idea what SQL give me some break, I'm not trying to cheat myself because I want to learn these things, thats why I'm here at 3am even tho I need to wake up in 4hours and go to work. Its just the course I enrolled to it's pretty garbage and I'm not fully happy with how its done, we got old database from 2010 with no ERD to even show the relations.. and trust me if you actually saw the database and what is going on there you would understand. I started learning database design 3 weeks ago and I can already tell this design is just pure garbage. The design is what I have the biggest issue with, because it's so hard to put everything together when you haven't created the database and you don't know what is what and what is where.
Indentation and bracketing like that are great for readability in any type of code. The world would be a better place if everyone did something like it. IMO what you've got there is already the proper formatting! 
Sounds like a good assignment to get you ready for the real world. Part of the awesomeness of doing what we do is starting with a shit design. Figuring out what is going on and making the database meet business requirements. Not saying you haven't come anywhere in 3 weeks but that's not really an excuse for a school assignment. If the entire class fails then you know it's something wrong with the class. If it's just you, then you haven't tried hard enough. Maybe a tutor from your school as well that won't violate any honor code that you may have agreed to for each assignment? That being said. The community here is awesome and I learn something every day based on submissions and other contributors. Sometimes I can help... But if you don't post it here then the community can't help. 
Thank you cheesus, sorry I am fairly new in writing SQL. I don't think I have the authority to declare set the string? no idea
Thank you dan_au, much appreciated for your effort and explanation. I have used your suggestion and tried digging further, however, the result doesn't seem to end at the '"' position. have I written it incorrectly? tried both below, it still doesn't end at " Thanks again &gt; SUBSTRING(@str, PATINDEX('%firstName="%',@str)+11,CHARINDEX('"',@str,PATINDEX('%firstName="%',@str))-PATINDEX('%firstName="%',@str)) AS 'firstName' &gt; SUBSTRING(@str, PATINDEX('%firstName%',@str)+11,CHARINDEX('"',@str,PATINDEX('%firstName%',@str))-PATINDEX('%firstName%',@str)) AS 'firstName' 
tbh I feel like sproc is a bunch of bollocks every scenario I've seen so far that was given to me as a part of exercise I could've easily solved with a view. 
&gt; Do you guys think this is a good first step? Yes!
Awesome! Thanks for your comments hokies
Nvm got it working.
Stored procedures have their benefits over views for certain applications and can have much more complex logic including creating, dropping tables and modifying or inserting data in other tables. But if you have the answer in a view you could probably easily wrap that into the sproc...
Yeah you would think so right, but the scenario is actually so stupid it's hard to do it with a sproc and I've done it with 1 view
When you are querying your view are you passing any where clauses or is it a select * kinda thing? Are you joining tables to your view in the query?
What I mean is that it starts off looking like this: LEFT JOIN [AnalyticsMapping].SLA.[mpGoals] Z2 ON Z2.SLAType = ''HARD CODE VALUE'' AND ( (Z2.Client = Z1.Client AND Z2.SLALabel = Z1.SLALabel) OR ( Z2.Client = ''DEFAULT'' AND Z1.Client NOT IN ( SELECT DISTINCT Client FROM SLA.[mpGoals] WHERE Client &lt;&gt; ''DEFAULT'' AND SLAType = ''SAME HARD CODE VALUE'' AND SLALabel IS NULL ) ) ) 
It doesn't matter now I just realized I'm fetching category name instead of categoryID so this whole sproc won't even work, I would have to redo my joins and views to make this work which is I don't have time for now, need to get some sleep have to go work in 3 hours. but yeah all the information i need in that sproc i can easily grab from view with simple select 1,2,3 from view which makes me question this whole scenario...
Yeah, I use a lot of white space when I work with OR.
You'd hate my code. I use a *new line* and include comments when it gets complicated... then I leave it that way (gasp). It's a little gift to my future self for when the end user inevitability wants to change the logic or questions the results. That being said, I tend to do some wacky things to avoid OR.
Curious how 
In 5 years you're going to have the same problem, find this post in a search, and curse yourself for not mentioning how you solved it.
Yep, got that setup downloaded and went through the installation process (topmost option, installing a new instance or options onto an existing one) but I'm not exactly sure which functions should be selected to have a server instance specifically for a certain (or multiple) MMO games, so I ticked all of them and left most options displayed during the process (database module, distributed replay etc.) unchanged for now. I'll see how it goes and either report back or trash the whole thing and then report back to make a new instance.
it's a table-valued function, so it should be in the FROM clause (not in the column list like a scaler function)
It really sounds like a dream first job if you want to get on the BI/analytics path. I don't even understand why you're uncertain about it!
&gt; which obviously won't work because you can't assign multiple values You can if @variable1 is a table. 
Hey, it’s a bit of a hack but use cr/lf? You can use char(13)+char(10) to simulate a new line... @multiline = @title+CHAR(13)+CHAR(10) ... @multiline = @title2+CHAR(13)+CHAR(10) Etc 
This is exactly what I want to do, and I even got a table function written for it 