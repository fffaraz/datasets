This is needed only once for an already existing table/dataset;a trigger would not work .
what about case when (select host_name from v$instance) = 'qa' then [logic stuff] else when (select host_name from v$instance) = 'dev' then [more logic stuff] end; sorry im on mobile and I've been drinking but you get the drift. 
So here's the problem, what you are trying to do is technically impossible given the data set you presented. I say impossible if you require 100% accuracy. Here's the problem: SQL Server will not read records from the database *in the order they were created* but rather in the order that the query optimizer will read the data from disk and/or memory. Often times, especially in a small table, they very well could come out in the order they were written. But it's never a guarantee. Without a PK that's sequenced in the correct order based on inserts, or a date/time stamp, you can't get something 100% right 100% of the time using this data set.
There is probably a better way to do this, but this should work. I think you were on the right track with the recursion but there needs to be some extra logic to make sure you are getting the max step and have everything ordered. Can relook at it tomorrow but this works on my end with some test data: WITH cte AS (SELECT OldName ,NewName,0 as countn from #namechanges1 UNION ALL SELECT a.OldName, z.NewName, z.countn + 1 FROM #namechanges1 a join cte z on a.NewName = z.OldName ) SELECT OldName, (select NewName from cte innercte where innercte.OldName = cte.OldName and innercte.countn = max(cte.countn)), (select max(countn) from cte innercte where innercte.NewName = (select NewName from cte innercte where innercte.OldName = cte.OldName and innercte.countn = max(cte.countn))) FROM cte group by OldName order by (select NewName from cte innercte where innercte.OldName = cte.OldName and innercte.countn = max(cte.countn)), max(countn) desc; 
That logic should work to infer the latest name and is what I used in my solution. So in the recursion you get the max NewName in the chain based on what has the largest count.
I think it's not entirely correct what you say. The rows are inherently unordered, it is true, but through the `( old, new )` relations, they do have a temporal ordering. E.g. the data above gives us the sequences `1 -&gt; 2`, `3 -&gt; 4`, `5 -&gt; 6`, `A -&gt; B -&gt; C -&gt; D`, `X -&gt; Y -&gt; Z`, regardless of the relative ordering of the tuples themselves. The problem still needs some sort of recursion to be solved in the general way, though, and I think the traditional approach to this is indeed building temporary tables and the re-applying a search for the next or previous result until a given condition is met. FWIW in PostgreSQL you'd probaly want to do that kind of thing with a recursive CTE.
How about trying PostgreSQL? It's arguably the best freely available RDBMS out there. It does take some time to get into, tho, as it is a really complex product. That said, SQLite must be the most Absolute Beginners Friendly RDB in the world. I just did this using Python3.5 and taking advantage of the standard libraries `sqlite3` module (no installation required!): ```py from sqlite3 import dbapi2 as sqlite db = sqlite.connect( 'intershop.db' ) cursor = db.cursor() def q( query, *P ): cursor.execute( query, *P ) return cursor statements = [ """drop table if exists texts;""", """create table snippets ( line text );""", """insert into snippets values ( 'こんにちは' );""", """insert into snippets values ( '數據庫很棒阿' );""", """select * from snippets;""", ] for statement in statements: # print( statement ) for row in q( statement ): print( '77628', row ) ``` Output: ``` ➜ sqlite-demo python3.5 japanese-demo.py 77628 ('こんにちは',) 77628 ('數據庫很棒阿',) ``` 
Decode is king and even handles nulls niceley.
[SQL 2012 onward has a function PERCENTILE_DISC](https://docs.microsoft.com/en-us/sql/t-sql/functions/percentile-disc-transact-sql) that should be adaptable to your needs.
Take out the comma before your AND
I'll preface this with that I'm not an access expert. The first thing to look at would be the comma right before your AND. Psuedo-code would be something like this: Where [patient_state] IN ( ' Maryland ',' Virgina ' , ' District of Columbia ' ) AND [patient.copay] &lt;= ' 20% ' ; 
Thank you! Unfortunately I think this has brought me to another problem. I was trying to build a custom expression to calculate monthly copay (I did this in the expression builder). We were told to do so by doing 30 days * copay * doses per day * average price per dose. I don't think I've properly entered the 30 days correctly as when I try to run the query it asks me to enter parameter value for day(30). I believe this is the code that I have for the monthly copay [Day(30)][copay][doses_per_day]*[average_price_per_dose] Would you have any idea as to what I've done wrong?
Yes that was it. Now I'm encountering other problems if you could refer to my reply to /u/ickies 
I'm guessing Day() is a function that probably returns something from the input, so that's probably part of the issue. It sounds like from your description you just want to get 30 X copay X doses_per_day X average_price_per_dose? In that case it's just a simple chaining of them all together: 30 * [copay] * [doses_per_day] * [average_price_per_dose]
I've made that change and now when I try to run it, a box comes up asking me to enter parameter value for 30. Regardless of the number I enter, it gives me an error saying the expression is typed incorrectly, is too complex, or contains too many complicated elements.
OK. So I've made that change to Day('2018-01-30') and now I'm getting a data type mismatch in criteria expression. Thank you for all your help but I feel like going back and fourth over reddit to help troubleshoot this problem is a really inefficient use of your time. I'm going to get help from my TA. 
I guess I should of clarified. You are correct if this is the only data. But as a developer or architect I wouldn't assume that. I would assume this is a much larger data set where assumed order wouldn't exist. I.e. what if D was previously used and then C -&gt; D? In the context of the *sample*, then yes, if each Old value was unique then that would be true.
We're probably both wrong 'coz when you go by the data that was given and assume those display all the pertinent properties of the real data set, you can give a much dumber solution by just select all the names that are in the right column but not in the left. That will miserably fail if any row was repeatedly renamed back and forth, or if any name got recycled. Then again, it's all the data we have, so it makes little sense to throw in one's own concerns...
A pointer and understanding about the relationship between Group By and Having, the "HAVING" clause can be thought of as an additional "WHERE" clause but reserved for use only after a GROUP BY. And you'd only use a Group By to group your data when you're using an aggregate function such as count() or sum(), for example. So you'd use it like: Select thing, another thing, aggregate_function() From table(s) joined on columns Where condition(s) Group By thing, another thing (often, but not always, everything that isn't an aggregate function) ------ okay, now you're sort of there but you want to filter more things with respect to after you've grouped HAVING condition(s) (this is like a where clause after the first where clause) ORDER BY thing, another thing;
Assuming this is Microsoft SQL?
Well that was a bunch of crap. Didn't realise I was reading a badly written advert until the end, I thought it was just a blog post from someone with massively limited knowledge of databases. 
No ideas, but I would just advise it's not worth the effort. If you need to black box logic from your customers, you should just offer a cloud solution running on your hardware. A savvy customer could decrypt the procedure text anyways: http://www.sqlservercentral.com/blogs/ganapathis-mssqllover/2016/05/20/simple-way-to-decrypt-sql-server-stored-procedure/ 
 WITH cte_maxID AS (SELECT PM_Project_RecID,MAX(PM_Note_RecID) as maxNoteID FROM PM_Note group by PM_Project_RecID) SELECT DISTINCT m.PM_Project_RecID, maxNoteID, notes FROM cte_maxID m INNER JOIN PM_Note n ON m.PM_Project_RecID = n.PM_Project_RecID and m.maxNoteID = n.PM_Note_RecID
 /****** Object: Table [dbo].[PM_Note] Script Date: 2/23/2018 1:51:38 PM ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [dbo].[PM_Note]( [PM_Project_RecID] [int] NULL, [PM_Note_RecID] [int] NULL, [Notes] [varchar](50) NULL ) GO /****** Object: Table [dbo].[PM_Project] Script Date: 2/23/2018 1:51:38 PM ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [dbo].[PM_Project]( [PM_Project_RecID] [int] NULL ) GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (1, 1, N'I shouldn''t return 1') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (1, 2, N'I should return 1') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (4, 2, N'I shouldn''t return 4') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (4, 3, N'I shouldn''t return') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (4, 10, N'I should return 4') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (2, 12, N'I should return 2') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (3, 10, N'I should return 3') GO INSERT [dbo].[PM_Note] ([PM_Project_RecID], [PM_Note_RecID], [Notes]) VALUES (3, 2, N'I shouldn''t return 3') GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (1) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (2) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (3) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (4) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (5) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (6) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (7) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (8) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (9) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (10) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (11) GO INSERT [dbo].[PM_Project] ([PM_Project_RecID]) VALUES (12) GO 
in your query, change this -- FROM rating r, movie m, user_profile u INNER JOIN to this -- FROM rating r INNER JOIN
You CAN’T decrypt a stored PROC once encrypted. That’s not what I’m asking really. Can the INPUT parameters be encrypted of a stored proc? 
** Query ** Your syntax is off. And your column names aren't correct. Here's how'd you join tables: SELECT column_name(s) FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name; So, you `INNER JOIN users_profile u on u.user_profile_id = r.user_profile_id`. Only that won't work because `rating.user_profile_id` doesn't exist. And there is no `movie_id` in the ratings table. So lots of clean up needed in that query. ** `movie_genre table` ** To create an index in Postgres: https://www.postgresql.org/docs/current/static/sql-createindex.html . `btree` is the default. To populate, that's a bit trickier. You're going to need to use a couple of functions: `string_to_array` and `unnest`. E.g., given a movie table that looks like this: id | title | genre ----+-------+----------------- 1 | Spam | food|meat|email 2 | Ham | food|meat|pig 3 | Eggs | ovum|bunny (3 rows) You can use those functions to expand each '|' delimited column into a row: user=# select movie.id, genres from movie, unnest(string_to_array(movie.genre, '|')) genres; id | genres ----+-------- 1 | food 1 | meat 1 | email 2 | food 2 | meat 2 | pig 3 | ovum 3 | bunny (8 rows) That should get you going in the right direction.
Thank you both for replying! I follow this explanation, but I might have oversimplified my issue. I'm trying to add this join to a query that's over 200 lines long and contains 20 existing joins between PM_Project and other tables. Is there a way to approach this problem without touching the existing "FROM PM_Project" statement?
Wow thank you for the detailed explanation :-) i will try it out and lets see how i progress. 
You would probably want to use a stored procedure so you can pass in the current user. Then I would probably try to use where statements to do each of the filtering options you require: - If the project has zero requirements, the project is visible to everyone. - If the project has one or more requirements, the user viewing the site, must meet all the requirements. Try to make yourself think of other ways to accomplish the goal other than using iterations. SQL is supposed to be used like that, at least I avoid cursers at all costs. Also, more information and a clearer layout of the table information would make this much easier. Maybe a starter select statement or something you have already come up with?
 SELECT m.title, m.genres, r.rating, u.gender FROM rating r INNER JOIN user_profile u on u.user_profile_id = r.user_profile_id INNER JOIN movie m on m.movie_id = r.movie_id ORDER BY r.rating DESC LIMIT 5; Thanks for explaining it :-) this worked perfectly, and im so excited ill try the indexing part now :)
You want to use "exists" and "not exists". Something like this: exclude records select * from projects except select * from projects a where exists (select 1 from project_requirements p where p.project = a.project and not exists (select 1 from users_requirements where user = [input] and requirement = p.requirement)) No iteration is necessary. I've given up on formatting on reddit so sorry about that.
It’s Friday, and I’ve already had beer, but if I were you, I’d set my SSIS package to handle the table delete and load the data, thus leaving he SQL Agent Job to only execute the SSIS package.
+1 for the best answer. Best to exclude the projects that should be hidden rather than find the ones that should be shown given the requirements. I formatted this query and aliased a bit different but its ultimately the same query. SELECT * FROM Projects EXCEPT SELECT * FROM Projects p WHERE EXISTS( SELECT 1 FROM Projects_Requirements pr WHERE pr.ProjectId = p.ProjectId AND NOT EXISTS( SELECT 1 FROM Users_Requirements ur WHERE ur.UserId = @UserId AND ur.Requirement = pr.Requirement ) )
First... don't use `DELETE FROM` to clear a table. This action gets fully logged and will take a long time depending on the number of rows. Do `TRUNCATE TABLE database.dbo.Table` this action is still logged, but it will _instantly_ clear the table, regardless of how big it is. &gt;Warning, the following steps cannot be reached with the current job step flow logic [1] ClearTable. This is because you have to modify the job so that `[1] ClearTable` is the first step: http://social.msdn.microsoft.com/Forums/getfile/120385 The change that step's "On Success" to "Go to the next step". Then when you save the job it'll ask if you want to set the last step to "complete job reporting success" just click yes/ok. Edit: yeah, as stated elsewhere, I would modify your SSIS package to include the "EXECUTE SQL STEP" to truncate the table instead of leaving it in the SQL Agent... but it doesn't really matter which way you run it.
* https://www.khanacademy.org/computing/computer-programming/sql * https://www.codecademy.com/learn/learn-sql * https://www.w3schools.com/sql/ * http://www.sqlcourse.com/ * https://mva.microsoft.com/product-training/sql-server#!lang=1033 * https://www.coursera.org/ * https://www.edx.org
To expand on this a tad... you'll want to add an Execute SQL Task to your SSIS package and drop the truncate or delete (whichever you're sticking with) code into there and assign the appropriate database connection to the task. Then connect that SQL Task to your Data Flow Task (or whatever) ensuring in the workflow you run the data removal then the load in that order.
When you say &gt; Furthermore, in SSMS, I have created and saved the SSIS package which imports the excel file to my SQL server. do you mean you ran the import wizard then saved that output (SSIS package) to be used with your job? If so, you might hit a snag doing what people here are suggesting... Typically Visual Studio handles the SSIS development piece through [SQL Server Data Tools (SSDT)](https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt) which is what you'll need to be able to do what people here are suggesting regarding modifying the SSIS package.
Given that I think OP was using the import wizard to generate said SSIS package, I think this answer is more correct given that it works as OP is requesting it work. But yeah, editing the SSIS package would be the best solution assuming OP knows what they're doing in SSIS.
How are you able to SELECT notes FROM cte_maxID if notes wasn't selected when you created the CTE?
I agree about the timing oddity. It’s the thing that’s making me confused. Your point about potentially having to populate the DB makes sense. Thank you for the feedback!
I don't know if this helps or not, but I just did a test a few weeks ago. I had 4 hours to do the test and it was 6 questions. The coding questions were actually incredibly straightforward and easy, so I think that they gave all the extra time to account for things like the pressures of testing, etc. Perhaps that's what they're doing here? Giving you lots of extra time to make sure you're not rushed. I mean, they don't want you to fail. They just want to see if you can do it. 
Thanks! How long did you take to answer those six questions?
Thank You very much six36, I appreciate this.
thanks man!
FSResidential? 
Vacasa
Could you expand on your second bullet?
I did a 2 hour test with 2 questions for my current job. The questions weren't too difficult but had a few potential answers. I remember one of them a cte, nested select or cross apply would have got the right result but the returned result also had an execution time allowing you to test a few methods before submitting. The call back wanted me to discuss my answer and my choices. My advice would be not to worry if you feel you have loads of time left and you have a 'correct' answer maybe just think about whether there is a more optimal way of doing it or if what you have could be simplified to make it more maintainable. As with most sql tests though as long as you can join union nest group order and filter you'll do at least ok. Sorry for wall of text, good luck 
Basically, get used to not using while/foreach/etc loops that process things one result row at a time. SQL is optimized around performing tasks in groups/bulk; individual tasks are best handled in code after pulling from a database. For example, find the total sales sold at each store in the last year: Linear Solution: var totalValueStoreOne; var totalValueStoreTwo; foreach(s in allSales) { if(s.store=1 AND s.dateProcessed &gt; '2017-02-23') totalValueStoreOne += s.saleAmount; else if(s.store=2 AND s.dateProcessed &gt; '2017-02-23') totalValueStoreTwo += s.saleAmount; else -- do nothing } Set Solution: SELECT storeId, SUM(saleAmount) AS totalStoreSales FROM allSales WHERE dateProcessed &gt; '2017-02-23' GROUP BY storeId 
Oh, that makes sense. Thanks!
Awesome list! I'd like to add one more -- [https://www.edx.org](https://www.edx.org/)
Something like: SELECT address, date FROM clients WHERE code IN (SELECT code, MAX(consumption) FROM consumptions GROUP BY code)
Google the the top 50 sql questions I bet you will get a good chuck of the same questions. 
I haven't stepped through your SQL but something is seriously wrong if you're getting division by zero errors when attempting to calculate SD. Just think about how the SD formula works, you're taking the difference between a data point and the mean, squaring it and diving by the number of data points. Can you export your tables easily? This looks like a pretty straightforward problem to solve if we can see your raw data. My gut feeling is that you have dramatically overcomplicated the problem. Is there a technical requirement to produce this information into a secondary table like [Metrics_InventoryAvg]? It seems like you're trying to create a really mini version of a data warehouse. Given the scope of what you're after we can probably just return all of this in a single select statement. 
Also make sure you know the basics....obviously. Never had an sql tests, but plenty of other times in interviews I psyche myself out trying to remember as much complicated shit as possible, only to realize I may have remembered something simple wrong. 
Probably expected to write a fairly complex multi table join, some conditional logic, maybe an aggregation. They can't seriously expect people to come in and write the more exotic SQL stuff off the top of their head - did you know you can parse JSON in TSQL? - so they probably just want you to display knowledge enough to solve a data exercise independently.
Thank you corsair130, I really appreciate you all dropping these links for me. I'm just trying to learn as much as possible to help improve my skill-set and ultimately help secure my job and lighten work load on my co-workers. I'm going to take full advantage of what you guy's put in this thread.
There's an instructor named Simon allardice I think on Lynda.com. He has a sql basics or something like this. I don't remember the exact title. But he goes through the basic concepts of databases, normal forms, keys and all that stuff. It's a great video. I'd start there. Lynda costs money but you can get a month free and binge watch. 
I would have no doubt in my mind if I over-complicated the problem, haha. In the standard deviation formula, you divide the sum by n-1. So if there’s only 1 data point, it’s trying to divide by 0. I’m inserting the values in the [Metrics_InventoryAvg] so we could have historical data on averages, etc. But if we could get to a point, like you suggested, where we could return everything I need in a select statement for any given date, then I can just make this a view and not bother with the extra table. I can give you a sample of data, but due to strict company policies I would need to do some scrubbing first. 
Will do Corsair130, my supervisor told me that if I'm able to take to SQL easily that the opportunities for increasing my salary is huge. don't get me wrong, lol more money is always lovely, but mainly I want to just know as much as possible about my job/job tasks so I can be a knowledgeable technician
I'll wait for your data if it's easy enough to send it through. FYI for SD you only use N-1 when you are calculating based on a sample size, not the entire dataset. [This](https://www.mathsisfun.com/data/standard-deviation-formulas.html) page explains the math. 
Study up your joins and subqueries. 
The % sign is a wildcard of any length. If you changed to two underscores and no %, then the string would have to be exactly 3 characters in length. Depending on the SQL dialect, you could also state this as WHERE LEFT(CustomerName) = 'a' AND LEN(CustomerName) &gt;= 3
If you used “LIKE ‘A_’ “ you would only match when the value started with an A and was two characters long. If there was a third or more characters or started with a different letter it would not match. % - The percent sign represents zero, one, or multiple characters _ - The underscore represents a single character I would check out this link to find some more information. https://www.w3schools.com/sql/sql_like.asp
Do you have the data still in SQL? If so you could do an export and upload it to dropbox or the equivalent. Otherwise at worst if you gave me the data in Excel, I could probably import it into a table, assuming you don't have anything unusual in your table setup.
As I mentioned, you can use that CTE to get the IDs of the latest notes, then join back to the notes table to get the note itself. 
&gt; having columns twice in group by select, e.g. I agree with this very much. Neo4j's Cypher query language introduced implicit `GROUP BY`, [and it works quite fine, could work as well in SQL as you're suggesting](https://blog.jooq.org/2015/05/05/how-sql-group-by-should-have-been-designed-like-neo4js-implicit-group-by/) &gt; I mean when you go like select a, sum(b) sum from TAB where sum &gt; 100 the intent is also clear? You're partially right. [I've explained the order of SQL operations here](https://blog.jooq.org/2016/12/09/a-beginners-guide-to-the-true-order-of-sql-operations/). The problem is that the `SELECT` clause is the only place where we can alias expressions. Yet, it often does not make sense to allow referencing expressions from the `SELECT` clause in other clauses. For instance, you cannot reference your `sum` expression from `WHERE` or `GROUP BY`. Neither can you reference a window function expression from `HAVING` So, the real feature here would be to have a section where expression aliases could be declared (hypothetically re-using the `WITH` clause): WITH sum AS (sum(b)) SELECT a, sum, FROM tab HAVING sum &gt; 100 The difficulty is, however, that `b` within the `sum` expression is not defined, so it works more like a macro than like an expression. I guess there are too many open questions to answer here.
Don’t know why Where CustomerName like ‘a__%’ /* (two consecutive underscores) */ wouldn’t work. 
You can use python in jupyter to query your databases. https://github.com/catherinedevlin/ipython-sql
You are right they indeed are different. Since SQL is declarative language compiler should be able to work out which filter to execute when from "where" clause alone. I have to disclaim that I'm writing SQL compiler and the reason I ask is that I have strong desire to simplify syntax and ditch "having" altogether. I can and am work everything out from where so that "select a, sum(b) sum from tab where sum &gt; 10" yields the same plan and result as "select a, sum(b) from tab group by a having sum(b) &gt; 10". I wanted to gauge if this is a welcome simplification?
It's actually actually SQL in jupyter. Althought it does return results in Python dict format In [9]: %%sql will@shakes ...: select charname, speechcount from character ...: where speechcount = (select max(speechcount) ...: from character); ...: Out[9]: [(u'Poet', 733)] I've used it, it's good.
&gt; It is declarative language yet it is explained and thought of as procedural No you didn't understand my explanation (and others), then. There's nothing procedural about the fact that there is a logical order of operations. That's just syntax. And you happen to not particularly like it. The actual execution order (execution plan) may be vastly different.
Sorry, I am new to Sql, but enjoy the challenge. Maybe something like: select cl.address,co.date from clients cl, consumptions co where cl.code=co.code and cl.consumtpion=max(cl.consumption);
Thanks (to you both). I can kind of see what's going on, but will have to let it sink in a bit more. It's been kind of sobering to say the least, as I thought I had quite a good grasp of SQL. Looks like I will have to study some more advanced stuff so this won't happen again. BTW, if anyone else has trouble running this in MySQL, just swap out `EXCEPT` with `WHERE NOT EXISTS` (I hope). So something like SELECT * FROM projects WHERE NOT EXISTS( SELECT * FROM projects p WHERE EXISTS( ...
You should check out beaker notebook. It's a package for phython and is a notebook style development environment that supports SQL. I've used it and I'm thinking it is the closest thing to what you are looking for, but I think something like Squirrel SQL or pgAdmin is really better for your case.
Apache zeppelin
Thank you all for the help!
This was part of the problem I ran in to. What I ended up doing was counting ALL work days, so if it didn’t ship on one of those days, I counted it as 0. If it’s a new item that we started to ship in the middle of the week, I would only start to count from the first time it was shipped. I did this by using a CASE statement.
&gt; boilerplate and redundancy i don't like. Not particular syntax. So... Syntax :-P
&gt; Since SQL is declarative language compiler should be able to work out which filter to execute when from "where" clause alone * No it shouldn't * WHERE occurs as a filtering (on scan/index/etc) * HAVING occurs after aggregation, where no prior knowledge of the result set at compile time could be known. * They are necessary to separate as attempting to perform them at the same step would cause non-determinism in the result set. * SQL is about as close to a complete grammatical language as most IT people will encounter in their life; changing it to the way you are suggesting would completely break that. It would necessitate turning SQL into an RBAR solution instead of a set one. 
To me, the logic makes sense to me but I am having difficulty seeing why it would be wrong, which it seemed to is from the wrong output. Any feedbacks will be great, thank you. 
Oh wow. I was focused on the [!] portion that i forgot i put 'NOT'. Sorry about that
There are some analytics SaaS companies that offer shared SQL like notebooks like Mode Analytics and Periscope (both offer quite a bit more but the basics of sharing SQL is there, presented somewhat differently). I also just stumbled across this 'app' popsql.io which is more simply collaborative real time SQL not sure if you can create a view with multiple queries. I've always thought it'd be cool to give my less technicaly skilled employees access to a wiki-like listing of SQL, maybe organized by 'Question' (how many users over 30 open email last week) with the SQL below to build off of or run
I believe you need a distinct as well... it says no duplicates
You are right
&gt; SQL is about as close to a complete grammatical language as most IT people will encounter in their life; changing it to the way you are suggesting would completely break that. It would necessitate turning SQL into an RBAR solution instead of a set one I think we agree to disagree here. Grammatically complete SQL certainly is not. Gramma is missing for so many thing so don't even know where to begin? There is no gramma for time series, geo-spatial data, nothing for text processing. You can't even pivot a result set for christ sake? What I'm suggesting almost doesn't matter in context of those things. Does mean it isn't a good place to start a discussion though ;-) 
W3schools.com
Thank you. Great alternative
I think you confuse common linguistic grammar and [computational grammar](https://www.cs.utexas.edu/~cline/ear/automata/CS341-Fall-2004-Packet/1-LectureNotes/25-TuringMachinesHandout.pdf) and leads into completeness. The differences between the two are where I believe your frustrations with the SQL language come from. They exist for a very good reason; without them, modern computation could not exist. Rules and structure are required at some level.
I understand where you coming from now. Thanks for the link. Perhaps I should clarify that i'm proposing to unify `where` and `having` as just `where`, let `where` use aliases and let optimiser decide on order of predicates. I shall expand on my silly example to illustrate that for query such as: `select a, sum(b) sum from TAB where sum &gt; 100 and a &lt; 10` optimiser will split predicates into `pre` and `post` base on type of expression they reference. I would be genuinely grateful if you help me understand how this breaks the language. 
Sqlzoo
Sent someone an email about this today! I really like https://www.khanacademy.org/computing/computer-programming/sql
The best one I've used is hands down Microsoft SQL Server Management Studio, followed by Oracle SQL Developer. Unfortunately neither of those are made for MySQL or PostgresSQL. 
When an execution plan is create for the SQL engine to utilize for a given query, and a query is ran, a few things happen: * The query is broken into it's constituent parts * The first thing it runs through is the parser, this runs linearly across the keywords/column names/etc and classifies them into tokens and their types. * When building the language, there is what amounts to a 'follow set' for each of these tokens. Simply put, each token type is expecting another token of an allowed type to follow. (*ex. SELECT will have column names, static values, etc after it*). This is how IDEs know syntactic errors before compile/run-time . * (*Implementation detail, how this occurs varies on platform*) If successful, any statistics about the data are evaluated, including indexes. * (*Implementation detail, how this occurs varies on platform*)A best effort for the available request is built using heuristics/etc * The SQL engine goes to the database and retrieves the information requested based on the instructions provided. This is the FROM and WHERE clauses are evaluated. * Additional operations such as windowing and aggregation begin based on the additional criteria. These require the content of the rows/run-time decisions to be made GROUP BY/ROW_NUMBER/etc * On completion of aggregation, additional steps to determine filtering based on the contents/result set. Operations here can overlap the previous filtering step (WHERE). Should this occur, the returned values would not be definable. * Any static operation, such as formatting/etc occurs * The result is returned Basically, one major problem is that the parser would be unable to determine the operations that need to occur, and that the same wording would produce different results on the same data set if this was allowed to happen. One step depends on information impossible to obtain without the other. Would the WHERE contain both the filtering, the engine would be unable to build a standardize an answer to which operation should occur first (since certain operational keywords can be present in both the WHERE and HAVING). In Computer Science/Engineering, a single input set to a function should always produce the same results. The ACID principle would be exceeding difficult/impossible to follow. For simple operations like the SUM example you gave, what you ask would likely be possible since solutions could be pre-determined by designers and hard coded. But when complex queries come into play, no bueno. This is a readers digest version of a readers digest version explanation. If you're interested in more, look into compiler design. The theory behind them has more math and proofs than most people realize. *TL'DR - Computer Theory and Math says so* 
OK I have a solution JFC tl;dr: On Mac the secure_file_priv variable is set to Null. I think this means that MySQL can neither import nor export. I set it to a path by adding this line: secure_file_priv="/usr/local/mysql/lob/" in my my.cnf (on Mac it checks in [several locations](http://geodatawrangler.lazym8.com/blog/2017/02/16/secure-file-priv), one of which is /usr/local/mysql/etc) How I figured this out: Wasn't sure where the mysql import directory was so I tried exporting a table. Then I discovered the secure_file_priv variable was not set because I couldn't export. This was after making sure that all the files I was trying to read were owned by mysql: chown mysql:mysql "whatever file" And they had 755 permissions. Anyway the last step definitely solved it. But not sure which other steps I took were necessary as well.
Thanks for the suggestions. Per your last point, in a previous company we had SQL queries for questions like that saved on redash, then connected that to slack, so employees could message the bot asking 'how many new users joined yesterday' etc. and get an instant answer. Pretty neat. 
Hey I only just saw your comment now because you responded to the thread as a whole. This should be fine, but I'm just about to head to bed. I should have something to you before long. 
You are not wrong that the data could be inconsistent but its worth noting that using a function in that manner is going to destroy sargability and stop the query from using indexes. 
Just use a cron job or task scheduler with the NOW() function. That's really the only way without going into dimensional table structure. The query won't be fast but that's also kinda a run at 11 when everyone's gone kinda job
there doesn't seem to be anything here
Any decent database management system will have a utility to analyze queries and show you the details of the execution plan. 
"Execution plan" (or sometimes "query plan") being the key phrase(s) here. That's the succinct way of describing what you're trying to learn. Hopefully your DB has good documentation on it.
I know sometimes it's necessary to view a query's explain plan, but it isn't far-fetched to think that some veterans are able to view a query without looking at one and say, "Yeah, that's gonna run forever"?
At least part of that is going to come from familiarity with the database(s) in question, rather than the DBMS in general. As a basic example, there's a huge difference between a query that runs five different subqueries against tables with only a couple hundred rows each and a logically identical query, but with the subqueries hitting million-row or bigger tables.
Oh yeah definitely, you are right in that experience with the system and its data definitely plays a part.
In a vacuum I'd guess that it's rather difficult to tell how long it'll take based strictly on the query, unless you also recognize some part of the query itself that might be prone to long processing. Computed columns spring to my mind but even those can generally be optimized by things outside of the query. Lots of external factors (what's indexed and where, retrieving vs updating, how many hits the DB's getting at a given time, etc.) will inform your decision on performance too. And there are always trade-offs. Lots of indexes on frequently-queried columns can make data retrieval fast, but at a significant cost when it comes to storage. That sort of stuff isn't immediately available by just looking at a query, but it is available in an execution plan. I don't know about other DBMSs, but in my environment (MSSQL), SSMS comes with both a run-time execution plan utility and a "preview" that can generate a possible plan before the query runs. It's a tool at my disposal--there's no harm in using it!
Have a look at my free online book Use The Index, Luke! https://use-the-index-luke.com/
Oh dang. I’m on my phone so I didn’t even notice I was replying to the thread instead of comment. Thanks again for helping me on this! It’s very much appreciated!
It's often easy to tell the difference between a query that will run quickly vs a query that will run slowly, but it can be very difficult to tell the difference between a query that will run slowly and a query that will run *SLOWLY*. 
I think I've got you covered here fam: &gt;Describe a project you worked on with sql? So for this one, you have to think back to a time where you were working on a project, and then describe that project. &gt;What were your day to day tasks working with sql? Pretty similar one really, what you want to do here are to consider the kind of tasks you did fairly regularly, and describe those. &gt;Description of working in a group with sql? Alright this one is getting trickier, but I think what the interviewer is looking for is for you to think where you were working in a group (of people I think), and used SQL in that group, and then describe it. Not sure what else you were looking for, you want us to provide examples of your experiences?
This is a good approach. I find that it's beat to use the Access GUI as opposed to typing in the query directly. So I always break it down and then build it using the Access query builder.
I would disagree with that. The speed of any query will depend on the data being queried, where it is (maybe on multiple servers), and how it is indexed. 
Just looping back with everyone that I passed the 70-762 and earned the MCSA. Looking forward to moving onwards to the MCSE! Thanks!!!
Have you tried using temp tables? 
I just published this the other day: an SQL Roadmap. It shows the kind of things you can and should know as you learn more about SQL. https://www.databasestar.com/sql-roadmap/ There's quite a few topics there that I don't have articles for, but it should give you an idea of the kinds of things to learn about or research. Hope it helps!
It looks like you're trying to store valid combinations of DEFID and ATTRID. I would suggest going one step further and creating a table that stores the valid combinations of these two values. CREATE TABLE filter_values ( defid INT(10), attrid INT(10) ); Then, your filter_values table would have data like: 123, 2 123, 3 123, 4 3112, 3 3112, 30 3112, 34 3112, 23 3112, 4 ... Then your query would be: SELECT * FROM variables_table WHERE (defid, attrid) IN ( SELECT defid, attrid FROM filter_values ); Or it could be done with a JOIN: SELECT v.* FROM variables_table v INNER JOIN filter_values f ON v.defid = f.defid AND v.attrid = f.attrid; Hope that helps! 
[removed]
- What's the size of your redshift cluster? - What are are the distkeys and sort keys of the customers table? - Whats the volume of this data? - How long does it take to just select the customer ids? I probably would just do it all in one step, the CTE abstraction seems minimal for something that could be done with a single nested subquery. 
You did it again :P Ok so I'm almost there but I'm still slightly unsure of the final output you want the data in. I don't even mind jumping on a quick call, depending on what timezone you're in just to clarify. I'm written an (ugly) proof of concept below, that returns the AVG, stddev (will return 0 if there's not sufficeint data, and I've hard coded it for a single value for simplicity. (Note I am in Australia so I have a different date format to you, just enter your own values for the top dates part) DECLARE @dtEnd date = '2017-12-30', @dt date = '2016-12-01', @Startdate datetime , @Item varchar(20) = '2511' select @Startdate = min (convert(date,shipdate)) --Find the first date the item was shipped FROM [ThinkAutomationV42].[dbo].[ShippingHistory] where fk = @item select @Item as Item , AVG(b.No_Shipped) as Avg, CASE WHEN (count(b.No_Shipped) &gt; 1) THEN STDEVP(b.No_Shipped) else 0 END as Std_Dev, @Startdate as [First Date], @dtEnd as [Last Day] from ( SELECT DATEADD(DAY, number, @dt) AS [date] FROM ( SELECT DISTINCT number FROM [master].[dbo].[spt_values] WHERE [name] IS NULL ) AS n WHERE DATEADD(DAY, number, @dt) &lt; @dtEnd and DATEADD(DAY, number, @dt) &gt; @Startdate --Restrict the days to the range supplied, but only start from the first ship date AND DATENAME(DW, DATEADD(DAY, number, @dt)) NOT IN ('Saturday', 'Sunday') ) a LEFT OUTER JOIN ( SELECT fk, count(fk) as No_Shipped, convert(date,Shipdate) as Date FROM [ThinkAutomationV42].[dbo].[ShippingHistory] where fk = @Item group by convert(date,Shipdate), fk ) b on a.date = b.Date This will output Item | Avg | Std_Dev | First Date | Last Day ----|---|-------|----------|-------- 2511 | 2 | 2.042 | 2017-06-15 | 2017-12-30 I was going to be lazy and just make a CTE or something to feed all of the items into this so you can get the output for multiple 
Try "view source" in your browser. echo "&lt;tr style='margin-top: 25%;&gt;"; One possible explanation is that you're missing the closing `'` here
I appreciate your help so much my dude :D you made my day &lt;3 
You'd probably want two tables. One for the constants (ID, country name, etc) and one for the variables (countryID, year, population, foreign reserve). Then join the two when querying.
No, could you explain how that would help? Thanks.
New to SQL so probably off on this one, but wouldn't this select cities where the last character was an a,e,i,o, or u? My instinct would be to say WHERE NOT right (City,1) in (A,e....etc 
The customers table is the output of a subquery, which joins 3 large tables together. When I run it to output just the customers table, it takes ~20-30 min. I haven't tried just selecting customer IDs so I'm not sure how long that takes. The customers table returned from the subquery is 3 million rows with 10 columns, mostly binary values. Thanks for the help.
Have you ever worked with SQL before? How long have you been learning SQL and how have you been learning SQL?
That would be correct. It should be not in instead of in. I was more concerned with showing the simplified logic than actually paying attention you the question. 
Thanks for clarifying, in any case I learned from your syntax and appreciate it. 
Why does my phone keep deceiving me?? lol This definitely looks more promising than what I had. The output would be close to what you have: part number, average, then standard deviation. The only reason I was going to put it in a table was for record keeping; being able to look at the output for any given time. But you’ve got that taken care of by being able to put in any date range and getting the desired output. I haven’t tested this with my data yet, but will today and let you know how it looks. I’m in GMT -6 timezone, so +17hours for you. 
One might consider the second table a FACT table, with records that represent the values for each year for each country. The former table you could call a DIMENSION or REFERENCE table which will be much smaller and contain the FOREIGN KEYs from the FACT table for the COUNTRY information which does not change year-to-year.
Definitely, although I steered clear of using those terminologies, since they always tripped me up in school. Technically name are 'facts,' although [a FACT table is not necessarily a table of facts](https://en.wikipedia.org/wiki/Fact_table).
I use the truncate / insert method for speed as well. This will theoretically always be faster since MERGE will read every record. Either way, switching to merge won't help your issue. You simply shouldn't have other jobs running against the DW during the maintenance window. You are guaranteed to get deadlocks or ghost reads. If you can't reschedule them, the other option would be better error handling of the other jobs that are deadlocking, and setting up retry attempts and extending the interval as needed.
Good bot
&gt; I would advise against updating the data every minute. This could have a big drain on the system. Yeah, but the "current hour" and "current day" count will likely need to be live. Or if I can convince the business side otherwise
Yes, that's exactly what I mean... and yes, I'm hitting a snag. I'll try what the other commenters have recommended and see if I can un-snag it
Great suggestion, thanks. Now, I do have the ClearTable set at Step 1 in the job. I have also ensured that the On Success action leads to next step. I am not sure how to add a SQL step in the package I have saved. I don't see that anywhere as an option. Can you direct me to that option? I've gone through the Import Wizard a few times, but don't see that 
If you have 2 steps, one being run SQL code to delete/truncate which then leads to step 2 being run your import package then you shouldn't need anything else.
&gt;But seriously though there's nothing wrong with seeking a career in a vocational trade lol god damn. r/murderedbywords
I agree, in a relational model I'd usually label the table that just has country information something like Reference.Country or dbo.refCountry or dbo.lkpCountry to indicate that this is a reference/lookup table. Philosophically, these "ref" tables are closer to schema than data, because they must be populated prior to any transactions hit the system. The other table I'd probably name dbo.AnnualCountryStatistics.
&gt; And DateID =1513 And DateID=1514 WHERE clauses are evaluated one row at a time how can a single column value be equal to two different things at the same time?
BI Analyst, Database Analyst, Report writer
&gt;new line character in the database entry. If I had a nickel... Where's it coming from? Is it in a field that should be numeric, but is actually `varchar()`?
Probably this whole database is a shitshow. So far, every field I've come a cross is stored as a varchar, no matter what a reasonable person would store it as. It's horrifying.
You could do 1-way replication
If you do a SELECT CostCode, LEFT(CostCode, 1), RIGHT(CostCode, 1) FROM [JPA COST JC2 TOTAL] WHERE ISNUMERIC(LEFT(CostCode, 1)) = 0 OR ISNUMERIC(RIGHT(CostCode, 1)) = 0 SELECT CostCode, LEFT(CostCode, 1), RIGHT(CostCode, 1) FROM [JPA GL KEYS] WHERE ISNUMERIC(LEFT(CostCode, 1)) = 0 OR ISNUMERIC(RIGHT(CostCode, 1)) = 0 do you get any results?
What do you see frequently that you believe you are missing on SQL developer postings? I'm thinking you're seeing a lot of requests on the developer side? C#, Python, etc?
I already fixed it, but thanks for the response! There were only like 5 entries I had to change in the DB
Backup and restore. And the testing of those procedures. As well as integrity checks. If all that isn't being done properly (it's not enough to back up your databases - if you can't restore, it's pointless), nothing else matters.
Time to get your /r/powershell on, along with https://dbatools.io/. dbatools even has a function specifically for testing your last backup. https://dbatools.io/functions/test-dbalastbackup/
Something I've seen a fair number of people get caught by is the [Instant File Initialization](https://www.brentozar.com/blitz/instant-file-initialization/) and/or weird fill factors. Also worth considering, is disable all non-required indexes for the merge import operations, then rebuild/update statistics afterwards.
Ooh ty. Pretty new to the SQL/DBA life, much appreciated.
Yup C#, Python, R, .Net I'm confident I can use Google and learn how to code during work, but obviously that won't fly on a resume.
I feel like there are three roles you will see usually. 1. SQL Developer - Programming oriented. This is what you're seeing. They want someone who can program and their primary application interfaces with a database and you need to know how to access that database with your programming. 2. SQL Developer - SQL oriented. You will write primarily SQL and occasionally interface with programming languages. Writing in SQL is your primary concern. 3. SQL people write SQL and programmers write programming code. Programmers and SQL folk work together to come to the understanding of how the application should work and then unite. The SQL folks will typically write stored procedures, functions, views, etc and the programmer will implement those calls and worry about the coding. The SQL folks worry about the database performance. You are probably looking for 2 or 3, those are usually not what you see available from a programmer perspective. I'm still looking for that opportunity that is basically creating new amazing fun products all day in SQL and SSIS. That in reality, amounts to 5-10% of my job, maybe less. 90% of the time it's fixing shit people made or adhoc requests. I'd look to something DBA-esque. Business analyst / sys admin are good entry points for that. 
Yeah there is definitely a smarter way to make this more efficient, but honestly, if it solves the business's needs to have it updated once a day It's probably fair to just put it into production and run with it. Sounds like it's all sorted.
I've seen junior level SQL jobs that don't list much knowledge outside of SQL. If there's one specific thing you consistently see and are missing, start working on it on your own. I recommend expanding your search, there should be listings...maybe less depending on area, but it's not a niche field or anything 
Put the various sub queries into temp tables and query off that. 
Well database developer and database administrator are sort of two different career paths - have you tried looking for entry level database administrator jobs? Which are you looking for? Most of the database development roles are more in the bi/data warehousing space (BI developer, ETL developer, data engineer). You will also have some OLTP SQL developers, but a lot of places leave their developers to write their SQL code (no comment). A database administrator might do a decent amount of database development as well depending on size of the company, but they are the ones responsible often more for things like user permissions, back-ups, replication, patching, releases on higher tiers and the like. In general DBA positions are more ubiquitous. 
You can create a temp table to solve this, in your temp table you can add a rank for each row base on multiple columns.
I was gonna come in here and throw dbatools some love, but you beat me to it. This does everything OP could want, and does it with ease. Here's one of the core contributors talking specifically about the command referenced above: https://sqldbawithabeard.com/2017/03/20/testing-your-sql-server-backups-the-easy-way-with-powershell-dbatools/
Yeah I'm seeing than more and more, I take classes on lynda.com so that'll be the next one.
Investing and developing quants there’s a ton of instruments to use pick your poison 
 Just pulling customer customer_id from the source table would be pretty quick, but this particular subset of customers is selected by filtering on a column that is a fairly long character string, so I think that is what makes the subquery take upwards of 30 min. I made the 'customers' subquery a temporary table with customer_id as distkey, then did the calculations all in one step and was able to run it in ~70 minutes, which is passable, though I am worried that I might have to increase my sample size and push the limits a bit. I don't quite understand why that second pass takes so long. Thanks a lot for your help. 
Thank you, that helped speed things up. 
Exercise and course for free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Reddit basically uses PostgreSQL as a SQL and NOSQL database (kind of). They only have 2 tables, a "Things" table which stores every item with common attributes, and a data table which stores the data for that item (I would assume it is JSON or some other parseable structure) - this data is basically like a document in a NOSQL database. Doing this, they are able to query and sort "Things" pretty quickly on the relatively small "Things" table which is likely indexed very well. Then they use an id to relate that "Thing" to its corresponding data in a separate table when the data is needed. They lose the power of joins on all of the data related to these records, but they gain access speed due to less joins - the only join they ever do is on a clustered index. The data itself is likely parsed on the server or client side. Here is an article explaining it a bit: https://kev.inburke.com/kevin/reddits-database-has-two-tables/
That's exactly correct. That third table is a junction table that describes the combinations of valid user-subreddits. The table might be larger. The row count would be bounded by 0 on the left and the number of users multiplied by the number of subreddits on the right. But that's not necessarily an unweildy data structure to work with, depending on what you're using it for. 
Voat is an open source implementation of reddit. They're keeping their database sources here: https://github.com/voat/voat/tree/Core/misc/SqlScripts If you're interested, you could have a look at how they do it (Although it might different from the reddit implementation)
Check Brent Ozar's blog. He has 10-15 blogs about getting into the DBA business.
Wow thanks, that's definitely helpful!
Hello, thank you for the response. That is correct, I wish to do a one time pull at certain intervals straight from our database. I use the power query just like in the link provided, however, I get a list of incomprehensible object names. I x-posted to Excel and one user suggested that I consume the data using powerquery and use table.schema or table.profile, however, I cannot pull all the tables (we have a massive database) -- I need a way to define what these objects are to know what exactly i'm consuming. Thank you,
Not fussed about getting any exams done but interesting videos. Cheers
That's an odd name for a table. Or anything. I gives me deja vu. Is the database part of some packaged product created by a third party (like Microsoft or Oracle)? Maybe that database is used by some kind of reporting tool, like Business Objects? Sometimes, tables with weird names are rollup or grouping tables that were built by an automated tool. Those names weren't meant to be accessed by humans, just other software tools that know how the magic works. Sometimes, third-party developers purposefully obscure the meanings of tables and fields. Sometimes, developers have funny ideas as to what makes a "logical" acronym or contraction of a series of words. I once worked on a project where all table names had to be four characters long and the first two characters had to be "tb", meaning "table". When you have a couple of dozen tables with (effectively) two character-long names, you run out of clever combinations really fast. At a minimum, I think what you need to do is to find the people responsible for creating or maintaining the database and ask them for a "**data dictionary**". This describes the meaning of each field in each table in the database. If you can get that, I'd also ask for an "**Entity Relationship Diagram**", aka "ERD", which will show you which tables relate to other tables.
Add payment date to the group by clause. 
Ok, that makes sense, but my data are not dates, but timestaps. Is there a clever way where I can still use group by?
Cast the payment time stamp to a payment date in your sub query and group by in your top level query. 
Also your group by should be in the outer query, whereas the raw where clause should be in the inner. (Select customer, amount, paymentdate::date paymentdate between x and y) group by ... Etc. 
Thanks! That worked. I also wanted to display the sum from another group of costumers next to my first group, however I am not sure which function to use, neither union nor join seem right.
Ouch, you have my sympathy. Worked on a database once for a financial accounting application that stored the monetary values as floating point numbers. The guy responsible for that 'design' choice literally hissed like a cat when asked why it was done that way. 
With calendar as ( Select cast ('1/1/2018' as datetime) as calendarDate Union all Select dateadd (day,1,calendarDate) from calendar Where calendarDate &lt; '1/1/2018' -- when exceeding 100 iterations, remember to set your maxrecursion option. Apologize for formatting as I did this from my phone. 
thank you!
A couple points: * Reduce your code to the minimum required to reproduce your issue. We don't need/want to scroll through a pile of CSS to find a few lines of PHP * Ignoring *everything else*, your code is wide-open to SQL injection. **Fix that first** by using prepared statements and PDO.
Oh my bad! I'm a beginner, so this is purely hosted locally for beginners, so nothing is hosted onto a website. Neither am I gonna publish this, I will remember to cut it down to the php only :) 
Doesn't matter. Learn and get in the habit of doing it the right way now. It's easier to learn it right first, than to learn it wrong and then have to un-learn it.
I will check it out, thank you :) But any idea for my syntaxes :O? 
Why do you think it's the database portion at all? You've provided no information that points at that.
 INSERT INTO fjell (fjell, moh) VALUES ('$fjell', '$moh') syntax looks fine... 
You are missing a bracket after &gt; if(isset($_POST["leggtil"])) 
Do the inputs need closing tags?
Um... Find/Replace (ctrl+h) won't work?
If there are always 2 values (and 2 hyphens) subsequent to the text you want back, you could do something like (x being your column): SUBSTRING(x, 8, LEN(x) - CHARINDEX('-', REVERSE(x), CHARINDEX('-', REVERSE(x)) + 1) - 8) If there aren't always 2 values subsequent to the text you want back, you're probably SOL because there would be no way of knowing whether a - is a field separator or part of the text you want back
True, true... My solution will only work if the subsequent fields will never contain dashes
What might work is if you know all the values for Value 1, Sample 1, Example 1, etc. Then attempt to find the index of those values in said string and then use that index as your stopping point. I'm always more comfortable doing things like that in multiple steps so would be something like Attempt to find index values for "column 3". If found, parse string based on associated index value. If not found, skip (or dump into analysis table for manual evaluation)
You could perhaps use a synonym to avoid code change at all. create synonym dbo.TableA for dbo.TableB (if Table A is unused).
Does your platform have regular expressions support? In Postgres, you would want to look through pg_proc and change the prosrc column, but that's not helpful if you're not using Postgres. This is assuming find and replace isn't viable for whatever reason lol.
I've been able to compare manually calculated averages/std devs to the output your provided query spits out. Looks like we have a winner! The only thing I've adjusted in your query was to wrap most of it in a cursor loop that fetches each individual part number from our catalog table and calculates all the numbers for it. After that, it inserts the results in a table that I am using to store the current values to do views/reports on. Thank you again for helping me out with this. This is very much appreciated! :D
well, in theory, and i stress that i can not recommend that.... in MS Sql Server, there is a system table.... sys.sql_modules, where you can find the sourcecode of stored procedures .... One could go ahead, and do some string replacements, and recreate every stored proc in a given database. There even is chance things would still work afterwards. Ask yourself this question thou.... do you REALLY want to try something like that? Do a full backup before at least.
Option(recompile) looks like MSSQL, so ill answer you for that DBMS. When you use option(recompile), the query's execution plan will not get cached. It will have to be compiled on any execution. So yes, it will not look at any cached plan. If I am on a dev environment, id just nuke the entire plan cache for testing a query rewrite, if im to lazy to look up the plan handle of the query im rewriting, which i usually am to lazy to do (on a test / dev server).
Awesome, glad to hear to worked out.
So, if your "2nd field" i'm calling it, the text you want back, does not allow space and dashes inside it, for instance: if you can't have 'Text - I - Want - Back'... then this should work? I your sample using a table and added a new row as well. create table #blah (string nvarchar(1000)) insert into #blah (string) values ('2018 - Text I want back - Value 1 - More text'), ('2018 - TextIwantback - Sample 1 - More Text'), ('2018 - Text-I-want - back - Example 1 - More text'), ('2018 - Text-I-want - back - Example 1 - More text'), ('2017 - A-I-A - back-true - Example 1 - And even more text') select charindex(' - ',substring(string,8,len(string))) AS StartingPosition --starting position for next "field" , charindex(' - ',substring(string,8,len(string))) - 8 AS LengthOfField2 , string , substring(string,8,charindex(' - ',substring(string,8,len(string)))) as Field2 from #blah; drop table #blah;
Are you restoring the backup to a new server or are you restoring it on top of the original database? 
New server. It's a test/dev/reporting environment with different drives. 
If the drive lettering and paths are the same on the new server, just use the basic restore command shown in the examples. If the paths are different, you will need to provide a properly constructed -FileMapping parameter to indicate where the different files should be placed. 
I am rewriting/optimizing procedural queries, and yes I'd normally nuke the entire cache. 
Have you thought of tracking your query plan using the query store and forcing it to use a more optimal plan?
No, because I am taking a very large query and forcing it to take sequential steps which are individually optimized to achieve the final set.
I've used with recompile for a stored procedure where between steps data was being written to temp tables to further break up the query. The thing is that we only found recompile to be beneficial if there was varying amounts of data that would be pulled back from the query on subsequent executions which would make a cached plan not good. Normally having a good cached plan is a good thing ;)
This process is using #tables but I'm more experimenting with segments of code, i.e., are three joins faster than two joins into a #table and then a left, or is it faster to do two joins into a #table and then an index before doing the left, or forgo the index? Once the entire process is segmented down I'm happy with it to use the cache and optimize, etc.
I prefer WITH XMLNAMESPACES as it lets you declare namespaces for the whole SELECT statement, not just one XML query.
Pretty sure In the case of this select statement it's a field name. sometimes people use reserved words as field names because they don't know it's a reserved word. Try Select Type from Activities and you will see Hike, HorseBack and probably a bunch of other activities. 
So I know that SELECT does the attributes and FROM does the table names and WHERE kind of filters it. But what is type doing? It appears to just be another type of filter like WHERE but its using "in" which ive never seen before (in my very short SQL time). 
It is not DOING anything it is a field name, like rdate and ppp
Oh, i'm understanding i did that code and i just get (idk the specific word) hike, horseback, rafting ect, just whatever can be entered as a different activity, so yep your are correct. So because its blue it means its a reserved word? Can you elaborate on that? 
You are correct it was a field name. As another user suggested, i did: SELECT type FROM activities and got various different activities. When you say field name, is that the same as an attribute? I'm just coming from building ERDs so I'm not certain if that is interchangeable. 
That was me by the way :P Do Select * from activities You will see the entirety of the Activities tables contents in a Column/Row format. Each Column will have a Name, that is the field name or column name It's the thing you select AND the thing you use to specify your where clause and joins. in the list of columns you'll see Type, you'll see ppp and you'll see rdate and likely a bunch of other things. Then each row will have a bunch of values. 
Sorry, I didnt notice the user names! &gt;(which is why I want to punch whoever called one of the fields of that table of yours "ppp" in the dick, just as an fyi) haha, that would be my instructor. I think she intentionally named a few sections "dumb" names cause shes trying to enforce that sometimes what goes on in modeling creates a hassle for the coding end. &gt;While you cast your sum of ppp as a 10,2 decimal it's actually pretty likely it was already defined that way unless the designer of that table is more of a moron then what has previously been shown by his naming convention. I think you are correct, PPP (price per person) all have 2 decimal places already. 
I think i get it. So someone can actually name a field name "from" and while SQL will apply it correctly, it will still be colored blue because of its reservation status. Makes sense, I just never knew type had any sort of significance. 
I'm not sure how then, but the code executed fine and did give me the answer i was looking for, which was the sum that each person paid from horse back riding and hiking. 
I doubt it gave you the right amounts.
When i take the last line out and leave it at the '%17%', it does seem to return a better number. 
Start with this. Select top(5) * from activities Select top(5) * from reservations. Take a screenshot of both and post it here. 
https://imgur.com/a/idWQm
I appreciate the help, and honestly yea, i'm probably too tired to do SQL right now since im about to head to sleep too! I kind of know join commands, we just went over that in class, but we do not use the actual word join, here is an example of code from an earlier problem i did that is how we did joins. SELECT distinct rfname ,rlname FROM guests G, families F, activities A, condostays C, reservations R WHERE C.guest# = F.guest# and R.guest# = F.guest# and G.guest# = F.guest# and R.aid like '%R%' and startdate &gt;= '1-July-17' and enddate &lt;= '31-July-17' and hrs &gt; 3 ORDER BY rlname; So yea i'm definitely still learning the finer points of join, but its not completely new. I'll have to take another look at this problem tomorrow though. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/wmeRfzl.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dv0bhph) 
Not sure what database type you are using, (MS SQL, MYSQL, ORACLE) but that looks like a horrible way to do joins. For one you can't specify the direction(INNER, RIGHT, LEFT) of the join this way and two while it might seem less wordy is also more confusing at a glance than SELECT G.rfname ,G.rlname FROM guests G JOIN families F ON F.Guest# = G.Guest# JOIN reservations R ON R.Guest# = G.Guest# WHERE R.aid like '%R%' and R.startdate &gt;= '1-July-17' and R.enddate &lt;= '31-July-17' At least here you know what is what.
Reservations is what you need to join and I kinda gave you an all caps hint on the join field.
I think for the nature of what we are doing, the instructor is trying to keep joins as simple as possible for now. That is the only method we have gone over in class so far, so I dont want to be throwing up all this crazy code we havent gone over. Also, the database is MS SQL server. 
Thanks very much for an informative reply :D I will troubleshoot1
In SQL server, we put field names on square brackets in our sql code if they are easily confused. We have a field called year in one of our tables, so we query with [year] 
Thanks, I think I was having problems with the hashtable syntax. I have it working now! 
I would suggest three tables: title (title_id + title data); title_genre (title_id, genre_id, weight) genre (genre_id, genre_name) 
Ssis is very efficient at moving large data amounts. That is why it exists. Do your conversations in SQL and just use ssis to push from a query (or staging table) to another table. Doing conversions is trickier than just pushing a data set from server a to server b.
When you are trying to troubleshoot are you using SQL server data tools or is it after the package has been deployed to the server? If you have a package deployed already, you can execute it via the UI and it will prompt to display an execution report afterwards which runs though the steps and can help with error messages and source of failures... If you haven't deployed the package yet and are still developing it, SSDT has a fairly decent debug... Sorry I'm not addressing the larger discussion of the need of one tool over the other as for us as professionals we have a variety of tools in our toolbox and we should use the right one for the data scenario. That being said SSIS will allow you to pick up many data sources easily and even run your own code.
Classic normalization question. You have one table for housing your full list of movies, where movie (as in, whatever constitutes a unique movie) is the primary key, and one table for housing genres, where genre is the primary key. Then you have a movie_genre table where movie + genre is the primary key, allowing you to add as many genres per movie as you like. The important part is that you set up foreign keys for this movie_genre table, so that the "movie" in this table has to also be in the master movie table where movies are uniquely identified, and same for the "genre" in this table. Then for reporting purposes you do that string aggregation thing in format 2. Format 1 isn't unworkable, but who's to say how many genres are possible for one movie? And dealing with the inevitable null genres in that table is a pain. And, probably most importantly, what does "Genre 1" even mean? If you want it to be "the most important genre," you have to set up a whole other series of controls to make sure that works as expected. Can you give an example of how you want to use the weighting of the genres?
Format 3 to me seems like the most striaghtforward way to make it into a web application, but also the kost difficult to maintain
&gt; Which of these three formats **none** of those four is any good... in fact they're all terrible see answer by /u/lemon_twist don't forget PKs and FKs -- CREATE TABLE title_genre ( title_id INTEGER NOT NULL , genre_id INTEGER NOT NULL , PRIMARY KEY ( title_id , genre_id ) , weight INTEGER , CONSTRAINT title_genre_title FOREIGN KEY ( title_id ) REFERENCES titles (title_id) , CONSTRAINT title_genre_genre FOREIGN KEY ( genre_id ) REFERENCES genres (genre_id) ); 
SSIS works as a whole solution. So you can complete all of this in SSIS, the task that transforms the table of data could be an execute T-SQL task within SSIS so it's all compact into a single entity rather than a hundred scripts in a hundred places.
I consider SSIS to be an ETL tool, you definitely can accomplish the same thing with python or other tools. &gt; I find SSIS to be poorly documented and an absolute nightmare to troubleshoot. There's a ton of MSDN articles on SSIS and lots of tutorials everywhere. As for troubleshooting, it can definitely be horrible to troubleshoot, that's where SSIS best practices come into play. You kinda have to create your own ability to troubleshoot it to a degree. 
Parallelism, not something easily coded in tsql or using service broker. You want to loaded and transform 250 files, you can easily do that simultaneously in ssis. Do it in tsql or service broker, good luck. 
Personally, I wouldn't want anyone adding information to the database to be limited to just three genre items and three theme items. The way to fix this is to have a table for the movie, in this case, giving it a randomized, unique id number. Then a table for the genre items that are available as choices, and a table for theme choices. Now, you will need a separate table where you can add the id for the movie and one of the genre items, if any. Another genre space is needed for the movie, simply add another id for that movie and another genre item. This way, a movie might have from zero to infinity genre items, as long as there are infinity genre items to choose from. Same goes for themes. This is a many-to-many relational setup and I use it often for complex data systems and it works swimmingly.
 WHERE MONTH(something) = MONTH(CURRENT_DATE - INTERVAL 3 MONTH)
The column would be the metric for the previous 3 months of the month in the month column 
did the best i could with your nebulous posting
You could try something like: WHERE MONTH = GETDATE(month, -3, GETDATE())
hmmmm
If you wanted proof that SQL is turing complete, here it is. Also, combine this with something that can compile to BF, and a true thing of beauty emerges. For example, https://github.com/arthaud/c2bf or https://esolangs.org/wiki/Brainfuck_code_generation Note that the "width" parameter controls both the width of entries on the tape, but also the size of entries in the jump table. If you have a thousand or more jumps, you'll need to increase width [in the initial program CTE] correspondingly.
Can you give an example? Conditional splits are relatively straightforward in SSIS as long as you know the syntax. 
Agreed. OP, you need to give us a sample of your data and expected results, or go to DBA.stackexchange.com You’re asking for a lot and providing us with very little.
Nice work, you absolute monster. 👍
I do what I can!
Honestly, this is cool. 
Thanks :-)
Everywhere I've worked has essentially used SSIS as a 'skeleton', which moves the data and does some basic initial cleansing/error handling (i.e. for data truncation, data types) but calls stored procedures for any complex business logic. SSIS is good at batching/parallelism, reading things like csvs/excel files (much easier than bulk insert), error handling on a row level (i.e. want to write your row that triggers a truncation error to an error table but load everything else). I wouldn't try and do a bunch of data manipulation via GUI. So a basic outline might be - SSIS takes data from source to a staging table (or series of staging tables), with logic to redirect (and ideally have some automated fixing) for truncation and data type errors. Then it calls are series of stored procedures that do some transformation, auditing on the staging tables. Then it moves the data to final destination tables(s).
OP did not specify the platform so why suggest a solution that works in only one?
IF Im grasping your question correctly... Your select statement should include the “file name” column as well as a concatenated colum. That uses the “file name” column to generate a string that is the true address. SELECT FILENAME, ‘Http://EnterprimaryAddressHere/MovFolder/‘ || FILENAME AS FileAddress FROM TABLE
I got to this after I made the post. echo "&lt;td&gt; &lt;a href=" . $row['FilePath'] . "&gt;test&lt;/a&gt;&lt;/td&gt;"; It makes a downloadable link, but fails and says no file... 
I know sql but not to this level. Can anyone eli5? 
First five sentences do a good job, I didn't know what this was either. https://en.m.wikipedia.org/wiki/Brainfuck
Non-Mobile link: https://en.wikipedia.org/wiki/Brainfuck *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^155179
**Brainfuck** Brainfuck is an esoteric programming language created in 1993 by Urban Müller, and notable for its extreme minimalism. The language consists of only eight simple commands and an instruction pointer. While it is fully Turing-complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
please, show the actual sql otherwise we're tilting at windmills
Parallelism is a great point, I would also like to add error handling/restartability and multi environment set ups.
&gt; So because its blue it means its a reserved word Yes, if it bothers you either put the name in brackets or change the column name SELECT [Type] FROM [Activities];
SSIS is an enterprise tool. It adds efficiencies for data transport and manipulation which are key to enterprise ETL. As /u/BiggRigg mentioned, parallel processing is very much key to SSIS and is able to be implemented as easily as just dragging and dropping two data flow tasks to the same canvas. It promotes team development standards for ETL processes - it's toolbox is filled with data specific tasks, which makes code maintainability easier if you have multiple engineers working on the same codebase. It's more of a 4GL language compared to Python's 3GL - which means more rapid development, end-to-end. It natively logs metadata, which comes in useful in determining what happened. As others have pointed out, it does have lots of documentation (as is typical of most all Microsoft products). It's not a goofy tool at all, but it's something that's been around for 13 years and used by probably millions of ETL developers. I've created over 500 SSIS packages myself, and the workflow does get a lot easier. In your scenario, if my entire scope was to move data around for just a couple of tables, and I didn't need an enterprise tool, I would definitely just stick with Python.
I'm with /u/r3pr0b8 here - a better stated question and/or having some examples would be easier to answer. I am tempted to say there's no effect whatsoever but that sort of depends on what are you asking exactly. For example, if you refer to a subquery as an input data set that's used in your main query (as in "select a.x, agg(a.y) from (subquery) a group by a.x) then per se there's no effect on the "(subquery) a". If you have a subquery or, especially, a correlated subquery in the select list (as in "select a.x, (select b.y from something b) from a group by a.x") then the sql engine will need to ensure that there's only a single value returned per a "group by" value. So it would probably require that all correlated values are coming from the group by list. What would MySQL do in this case I don't know and I don't have an instance to test.
Human Resource Machine was awesome. I'm just a couple of things away from getting both optimisation credits on all of the levels.
Well my conditional split works in SSIS. i'm just looking at it from a maintenance perspective. To do all of my transformations, conditional splits/ statements, and movement in SSIS requires quite a complex packages. Whoever comes after me if I ever leave the position I'm in is going to have a nightmare trying to troubleshoot issues if they occur. this is my main reason for doing transformations and conditional splits etc. in raw SQL and just using SSIS to "push/pull" data. It will make this analytics system I am building much more intuitive to those who come after me.
I get `NULL`.
instead of IF EXISTS (which is going to return TRUE or FALSE, not a value), try actually running the query and then do an IF on the returned value
Is there a reason for the IF? Sorry, I work in T-SQL. I would do it something like this: (A quick search showed IFNULL for mysql instead of ISNULL from T-SQL.) DECLARE DiscountVar FLOAT(28,10); SELECT DiscountVar = IFNULL(user_discount.percent, 0) FROM user_discount WHERE user_discount.link_id = wo_id AND user_discount.pacc_id = pacc_id AND UTC_TIMESTAMP&gt;=user_discount.timestamp_valid ORDER BY user_discount.timestamp_valid DESC LIMIT 1 DECLARE DiscountVar FLOAT(28,10); SELECT DiscountVar = IFNULL(user_discount.percent, 0) FROM user_discount WHERE user_discount.link_id = wo_id AND user_discount.pacc_id = pacc_id AND UTC_TIMESTAMP&gt;=user_discount.timestamp_valid ORDER BY user_discount.timestamp_valid DESC LIMIT 1 
There are multiple conditions we need to check for and return the value. Last I tried was this (returns `NULL`): IF EXISTS ( SELECT @DiscountVar := acc_user_discount.percent, acc_user_discount.link_id, acc_user_discount.pacc_id, acc_user_discount.timestamp_valid FROM acc_user_discount WHERE acc_user_discount.link_id = wo_id AND acc_user_discount.pacc_id = pacc_id AND UTC_TIMESTAMP&gt;=acc_user_discount.timestamp_valid ORDER BY acc_user_discount.timestamp_valid DESC LIMIT 1 ) THEN SET discount = @DiscountVar;
Tried. Returns 0 rows that way.
why are you doing IF EXISTS?? that will not return a column value
This looks an awful lot like homework; at least attempt to show us what you've done already. Ddidn't specify platform either.
It's for a SSRS report I'm trying to make on log files, currently I can show the cells that don't match but I'd like to add somewhat of a text highlight on the strings to show the difference.
I googled this question but didn't specify I wanted a SQL server answer. There are several examples in other programming languages such as VBA and Python. Perhaps you can borrow the logic from those examples and apply something similar in T-SQL.
https://stackoverflow.com/questions/12089967/find-difference-between-two-strings
I'm only familiar with TSQL but are you running that exact script and expecting an output? because that looks like it only creates the stored procedure. You have to execute it to...well... execute it. Something along the lines of EXECUTE show_job. I'm not sure what the exact syntax is for MySQL
There wasn't one, simply nothing happened. And no, I was just expecting it to say something along the lines of "successfully executed", and with a worrisomely major tweek, it's now working. However I'm trying to get this to work with another procedure that actually does the information updating through java and then displays the update with the procedure I was describing above, and that's not running and updating on the java IDE I'm using and nothing changes when I execute the show_job procedure (it executes and shows data, but not the changed data from the java program that makes the change). Not sure if you know what to say here, though
What does the other proc look like and how are you calling it from java? It doesn't sound like it's making any changes to the database. 
Here's the procedure: CREATE PROCEDURE`mvu`.`UPD_JOB_LOC`(pJobno INT(2), pLocal VARCHAR(13)) BEGIN UPDATE job SET local=pLocal WHERE jobno=pJobno; END; Here's the java program: package jdbc3040; import java.sql.Connection; import java.sql.DriverManager; import java.sql.CallableStatement; import java.sql.SQLException; public class CallableMain { public static void main(String[] args) { // TODO Auto-generated method stub try { Connection myConn = DriverManager.getConnection("jdbc:mysql://localhost:3306/mvu?useSSL=false", "student", "mysql"); CallableStatement cs = myConn.prepareCall("{call UPD_JOB_LOC(?,?)}"); cs.setInt(1, 50); cs.setString(2, "WashDC"); cs.executeUpdate(); } catch (SQLException e) { e.printStackTrace(); } } } It's also worth noting I had to omit the DEFINER = `student`@`localhost` part from the show_job procedure in order for it to be created so that may be an issue, but I'm wondering if there are other issues and how I can fix it, I don't know what to fix syntactically but I'm pretty sure that part is necessary
My suggestion is to test the connection first if you haven't. See this https://stackoverflow.com/questions/2839321/connect-java-to-a-mysql-database. Next, profile your database to see what your java program is actually querying. Basically, SET profiling = 1; ...(do stuff)... SHOW PROFILES; (https://stackoverflow.com/questions/14727432/free-mysql-profiler) You're probably right about omitting the DEFINER. The java program needs security context to execute the proc. Your code looks correct otherwise
You could select all max appointment_durations then union to all min appointment_durations then join that to the person and physician tables to get those details.
I'd highly recommend learning ssis at the very least. As the comment above, adding skills and looking around for another job will help. 
now go and implement a relational DB with this...
&gt; I'm not sure what your specific job title is I think that's part of the problem. I don't have a relevant job title because I started in a clerical role and taught myself everything I know. It doesn't help that my supervisor isn't technical at all. He's never written any code, never had to hire or manage any developers, and I'm the only person in his department who does any kind of development. There's basically no way to have an objective discussion about salary. I'm not even sure what title I would give myself. What differentiates a 'Report Writer' from a 'BI Developer'? Or what makes someone an 'ETL Developer'? I could write an ETL process with stored procedures, jobs, CLR functions, and .NET applications to pull data from non-SQL sources, but most people seem to equate SSIS with the 'ETL Developer' role. I think my only choice is probably to start learning SSIS and look for a new job. My boss doesn't even know what ETL is so it's kind of hopeless trying to convey my abilities to him... 
For BI development you need to be good with a variety of ETL tools (start with ssis), data modeling, writing queries, data visualization and reporting/ dashboarding (PowerBI and ssrs). With that said, you can easily fake it till you can make it. Target 100k a year in 5 years.
Senior level DBAs, even mid level, make 6 figures. Sales engineers &amp; consultants too. 
Most likely because your while they have multiple booking entries, they are for different flights? Your trigger seems to cover one passenger and one booking. Are you saying that one booking can have multiple flights? Also, please switch your JOINS to ANSI-92. Make the world a better place.
Wow thats pretty cool. So there are multiple options career wise if I continue my lessons. Based on you flair, you seem to be in one of these fields.
thanks the union helped separating the max and min, but how would i get the corresponding info. because the physician name is in another table and the patient name is in another.
would it be the exist command i would be using ? 
I can’t speak for everyone but my SQL knowledge was limited to a single class and I got a job as a Technical Analyst (pretty much systems analyst) right out of college and it pays well. After some time people generally get promoted to DBA or something similar. 
But that seems so pointless! 
Levenschtein is your friend here
Most of our developers are SQL heavy, starting pay for entry level is high $50's, most of our guys are in the $70's, senior guys $90's. I do 100% SQL, in a leadership position, and I'm healthily into the six figure range.
Flightcode =flightcode means that for every time the trigger runs, you are only updating the record for that particular flightcode.
a sample of the insert statement that I am dealing with is select 'JA100','11/30/16',81108 union select 'JA110','11/28/16',33850 union select 'JA110','11/28/16',89902 union select 'JA110','11/28/16',83620 union select 'JA110','12/02/16',87416 union select 'JA110','12/02/16',59366 union select 'JA130','11/29/16',81108 union
How much SQL do you need to know 
How long would it take to be knowledgeable enough to be employable
I appreciate you taking the time to help me very much, as I have been stuck on this for the past 8 hours... I'm pretty sure I need the f.flightcode = i.flightcode restriction, as if I omit it every passenger that flies has the exact same miles column
Did you study it in college. I know you don't actually need college for success in the field since it's skill based.
You’re right, you do need it; but it may be the cause for your trigger failing to update. I’d start with rewriting your statement to have ANSI-92 joins and work from there. You may find that a join you think is happening, isn’t actually happening at all because the supporting data doesn’t exist yet (due to transaction timing and such). Use left joins so you can tell.
Yeah that can be very frustrating. My first job out of college was similar in that the title didn't translate well to other companies. I started looking for something else when I stopped getting raises, because top-level management thought that they were paying us what we were worth. As I interviewed, I actually made it to the final round a few times. I had lots of skills and picked up on things quickly, but ultimately, because I didn't have experience with the specific tools they were using I was passed over. As for differences in the roles: * Report writer - focuses on writing reports, either with SQL or with tools such as PowerBI, Tableau, Excel, etc. * BI Developer - Designs and develops analytical databases. The full skillset can include requirements gathering, dimensional modeling, ETL, reporting, and presentation * ETL Developer - Specializes in building ETL pipelines Lastly, I think learning SSIS would be a great place to start. Honestly, it sounds like you are a go-getter. You learned all this stuff on your own, so I'm sure that you can get the big salary bump if you make up your mind to swap to a SQL or BI or ETL developer. As the other comments suggest, learning SSIS will be a big help. You may be tempted to start learning SSIS and spend many months learning it before doing an interview. You should start learning it now, but I would also suggest that you should start networking now, and prioritize that. Look over LinkedIn and reach out to your contacts and let them know what you are interested in. Try and grow your network by connecting with other data professionals, especially those that currently have the role you are most interested in. A lot of companies would love to have a go-getter like you. Some might pass over you, but if you have a good recommendation from a colleague many would go with you I'm sure. Just curious, have you ever had a SQL/data interview? You may want to prepare for that a bit.
For data analyst, you don't need to know that much SQL, but you should also know Excel and be pretty good at structuring, organizing, and presenting data. A typical job duty might be "Here is whole bunch of sales data in a SQL Server database. Tell me a story. Are we up, are we down? Which sales people are doing well? Which products are doing well?"
so I revised my statement to be UPDATE Passenger SET miles = miles + distance FROM Passenger p LEFT JOIN inserted i ON p.passenger_id = i.passenger_id LEFT JOIN Flight f ON f.flight_code = i.flight_code WHERE p.passenger_id = i.passenger_id AND i.flight_code = f.flight_code and nothing has changed. The result is the same as it was from my post
As someone who spent 7 years in college ending with a graduate degree in a STEM field, I’m envious of your situation.
SQL in itself is very easy to learn. I teach beginner and advanced sessions at conferences and have mentored people from no experience to entry-level abilities in a few weeks. The biggest thing is to understand the fundamentals of relational databases. The SQL syntax has a finite number of keywords/commands. Once you grasp the basics of relational databases, and can write out what you want to do in plain English, then Google can literally show you rest. Then, just practice, practice, practice.
Okay cool. Now go ahead and grab the distance value and pop it into a variable. Then use RAISERROR to print that variable or just insert it into a table. Make sure distance is actually populated. If not, that explains why the value isn’t getting updated. Do the same for miles before and after you update it.
I'm training my staff to use SQL, hired internally, we use it about 50% of the time in database marketing (queries, views, sp's, SSIS, and jobs development). We are all paid very well, but are also responsible for email, SMS, and direct mail campaigns.
Sounds complicated upon initial grasp. But I'm sure in time could become second nature knowledge. 
I'm sorry, I don't understand how to grab values and place them into variables. I put this below my code declare @test int; select @test = distance; RAISERROR ('TEST VALUE IS %d', 11, 1, @test) 
Select @test = distance FROM [this part is exactly like your update FROM, including the joins]
I would do the variable check/display before my update, but this should still technically work. What did your results yield? Here is a very crude table and trigger I just wrote which captures kind of what you’re trying to do: sorry it’s in photo but I’m on mobile.
Consulting 
I know there are SEO consultants, didn't know about SQL consultants. I assume its basically the same thing with different goals 
so doing that makes the passengers that already had 0 on their miles column have 99999... There are some passengers that do not have any bookings and therefore their miles are at 0, as they should be. Those are now set to 99999 with the ISNULL I think perhaps I did not explain clearly what my problem was. I receive the correct amount of miles for Passengers that have a single booking with my original code. My problem is that if a Passenger has say two bookings, each with distance x, y, my return value for that Passenger's miles is either x or y, when it should be (x+y). 
Went from DBA to Sr DBA to Lead DBA in the last 3 years. I am very happy with my compensation. Keep in mind there are a lot of factors that go into one's salary: Size of company/firm Years of experience Industry Geographic location Manager/non-manager Whether your employer values i.t. I suggest you look up what DBA's are going for on job sites in your area.
If doing that makes passengers that have 0 miles suddenly have 99999, then your tigger is updating the wrong records and your ISNULL() is catching a NULL miles value. This trigger will run once per record inserted, and should only be updating one row at a time right? If that is the case, you’re doing something iffy in your selection of records to update.
Yes, I believe that f.miles being null at the time of insert would make sense, as those passengers did not have bookings. As they did not have bookings, they did not have flights, so it makes sense that f.miles would be null for that row. and yes I understand that something is wrong with my selection in updating, but that is what I am unsure about. It works for Passengers with singular bookings, but multiple bookings it does not add them up. When I inspect the code of SET miles = miles + F.distance FROM Passenger P, inserted I, Flight F WHERE F.flight_code = I.flight_code AND P.passenger_id = I.passenger_id it honestly looks like it should do what i want it to, but it does not
Here is a new image, of a trigger I made very similar to what you want. Be sure to pay special attention to my SET SET p.miles = p.miles+ I.miles https://i.imgur.com/wmRJZBi.jpg Hope this helps, but I got my desired results.
Why aren’t you just using I.distance instead of F.distance? I.distance is the actual value that is being inserted while using f.distance is relying on the transaction to be complete before you get a value there to insert. My hunch is that you’re not using I.distance is causing issues.
Ahh, I see after rereading that your trigger is on booking. I’m sorry, it’s very late (midnight where I am) and I’m not thinking straight. Yes. Your trigger should technically be running for each inserted record. Maybe using a variable to get the SUM of miles for that passenger at the time of the trigger firing. Declare @totalMiles INT; SET @totalMiles = SUM(f.distance) FROM inserted INNER JOIN flight ON .... INNER JOIN passenger On ..... GROUP BY p.passengerid; Update p SET p.miles = p.miles + @totalMiles FROM ...
thank you for your patience trying SET @totalMiles = SELECT SUM(f.distance) FROM inserted i, flight f, passenger p GROUP BY passengerid; gives me the error INCORRECT SYNTAX near keyword SELECT
&gt; a report writer is basically entry level analyst that has migrated from Excel reporting to some sql. That's what I was thinking, but it's hard to find a standard definition for any of these roles. Report Writer (to me) sounds like an entry-level job for someone who knows how to write queries but not much else. I think my manager believes that's the most appropriate comparison to my job when talking about salary. And since he doesn't understand any SQL terminology other than the extreme basics, it's difficult to convince him otherwise. 
I am a pseudo DBA for a 100ish size tech company in Sacramento and make $60k for a relative figure. I also am a systems engineer/analyst as well. I am probably a little underpaid but I get a lot of liberties as well.
Report writers can be senior level if that’s all they’ve aspired to do, and do it very well. We had one where I worked that was good, but I absorbed their role when I got Hired and the position was made redundant. I’d say you can be any of those positions I listed with zero issues at this very moment. And your current situation is seriously hurting your career advancement and projection. Unfortunately your current team doesn’t deserve you if they can’t take 2 seconds to research your career path. Get out and go make that bank.
IT DOES!!! my god this is the first time in 10 hours or so something actually has worked as intended
Hahaha progress! Well, all that’s left to do is translate that into your assigning of @totalMiles. Copy pasta that junk in there and slap a SELECT = On it. SELECT @totalMiles = SUM(f.distance) FROM inserted .....
It sounds like you were in an almost identical situation. I applied to 2 jobs a few months ago and only got a technical phone interview for the first one (web dev role that was definitely a stretch for me) and made it to the final round of interviews for a reporting/BI role. I'm going to see if my employer will at least spring for some of the [MS training](https://www.microsoft.com/en-us/learning/course.aspx?cid=20764) if they won't budge on the salary. That second job is still open and they added a line that says 2 years of SSIS experience is "critical" to the role even though it was never mentioned before. I know a lot of people say the MS certifications aren't worth much but I think they may be more useful for me since I don't have any kind of degree.
How do I get the f.distance if I do not join it to Flight? really confused.. Side note: I may or may not be a new employee at a certain airline and as you can tell it has been a struggle..
Ahh, there’s the thing. If one booking has multiple flights on it, you may have to come up with a way to say “give me all the flights for this booking, and the total distance for those flights” So the question is: aren’t you firing this trigger for every single booking insert? If so, a booking record contains ONE FLIGHT, ONE PASSENGER right? Or does it contain ONE PASSENGER, X FLIGHTS? Isn’t booking just PassengerID, FlightId? Don’t be discouraged. The struggles are why we get the big bucks. I’m a huge fan of said airline so I’ll do my best to see this through with you. You can do this.
Great explanation
Just know I am very appreciative of your help. A single booking record contains one flight, one passenger, one departure date(which is not referenced in this trigger)
My pleasure. I do this for my team all the time and find it fun/rewarding. Okay. Then you may have something odd about your insert samples as you mentioned in your edited comment. Try inserting 2 booking records for the same passenger. Different flights. Simple as that. The trigger will fire once for each inserted record, and should update your passenger’s mikes to be the sum of the distance of both flights.
oh hang on running SELECT SUM(f.distance) FROM Booking i INNER JOIN Flight F ON i.flight_code = F.flight_code INNER JOIN Passenger p ON p.passenger_id = i.passenger_id WHERE p.passenger_id = i.passenger_id GROUP BY p.passenger_id; as a separate query gives me exactly what I need in a table with just the distance each passenger.. is there a way for me to set that as miles in my update? 
Also, here is a bit of reference: https://docs.microsoft.com/en-us/sql/relational-databases/triggers/dml-triggers https://docs.microsoft.com/en-us/sql/relational-databases/triggers/specify-first-and-last-triggers 
Yeah. Just use Booking instead of inserted in your trigger. I think you just cracked it.
No no, the setting of @totalMiles is where you should be using booking, not the update statement. Change it and see.
AGH. Did as above and it's not happening still.. please help Declare @totalMiles INT; SELECT @totalMiles = SUM(f.distance) FROM Booking b INNER JOIN Flight F ON b.flight_code = F.flight_code INNER JOIN Passenger p ON p.passenger_id = b.passenger_id WHERE p.passenger_id = b.passenger_id GROUP BY p.passenger_id; UPDATE Passenger SET miles = miles + @totalMiles FROM Passenger P INNER JOIN inserted I ON P.passenger_id = I.passenger_id I am STILL getting 34 for all passengers! Yet when I run my query above on this database I get the columns I want. Truly befuddled
Your booking to flight join is saying: For Booking 23 (flight 300, passenger 201) Give me the Sum(distance) of the flight (300) for passenger 201. The distance is 34. Weird. Any chance your server is set to explicit transactions? Try just one passenger with multiple bookings, like 2 bookings. Also, try using transactions for each insert. BEGIN TRANSACTION Insert into booking Values (blah blah blah) COMMIT TRANSACTION; BEGIN TRANSACTION Insert into booking Values (blah blah blah) COMMIT TRANSACTION;
I'm truly out of ideas at this point as it feels like it should work but it isn't.. Is there no way for me to just replace the column of miles in Passenger with the column that comes out of the query using an UPDATE?
Yep, this has to be it. The trigger is firing ONLY based on the last record is inserted. Use the begin/commits between them and you should get the values you need.
Did you try the transactions? And yes. That update statement I had you do on it’s own? Turn that into an update. UPDATE p Set p.miles = SUM(f.distance) FROM passenger ..... Have the update run after the records are inserted, and you should be good. That’s what I’d do in Oracle anyway.
SO close I feel like it's almost there Doing that givse me the error Msg 512, Level 16, State 1, Procedure tr_Booking_MilesUpdate, Line 11 Subquery returned more than 1 value. This is not permitted when the subquery follows =, !=, &lt;, &lt;= , &gt;, &gt;= or when the subquery is used as an expression. The statement has been terminated.
Just means it found too many values in SUM(f.distance) and can only update one passenger with one value. Make sure your standalone query only pulls ONE record per passenger. Remember the group by has to be there. Paste the update statement and I’ll check it.
That’s fair. I wouldn’t want you to change anything on my system either. The thing is you should return ONE value per passenger for SUM(f.distance). Make sure you’re still using your group by.
Wait, and this is a new procedure? You’re using inserted here when you don’t need to I think. Remember that query I had you build on the side that just had a SUM(f.distance) for a specific passenger? Change that select to an update. Keep the passenger join but get rid of the WHERE passengerID= ###. It will update all passengers in the passenger table to their current sum(f.distance) values.
This is also what I'm looking for since potential for growth and having it on your resume is invaluable. I think the most eye catching to great employers is scripting Auto SQL queries and iirc the next big thing will be anomaly detections in the universe of data we have mined. 
They must be the same because they both start with an S right? 
https://www.brentozar.com/archive/2017/12/tell-us-make-2018-data-professional-salary-survey/
Are you trying to get the longest appointment by physician or by patient or by a combination of both or just the longest appointment ever and tie it to the physician and person? 
So I’m trying to get the longest appointment and shortest appointment in one table but as well show the names of the physician and patient with the longest and shortest appointment that corresponds with the longest and shortest appointment , the issue is that in the appointment table gives only the foreign key and not the actual names which I pave to pull from different tables , one from a patient table and another from a physician table . I was able to get help with separating the max and min to be shown at the same time in different sequence with the UNION command but I just don’t know how to pull 2 different values from 2 different tables . Included that I need the first and last name for both physician and patient 
If you’re a beginner programmer, keep in mind that the job and the fit may be more valuable to you than the money. A company that offers more money may be a short term gain if the work itself and overall fit is not as good. You’d be better off making less money in a company where you can learn and grow more allowing you to make a much bigger jump in pay in a few years. Just food for thought. 
&gt; I think the most eye catching to great employers is scripting Auto SQL queries Had to look that up as I've never heard of that product. That is not something that I would allow or encourage my end users to use. It looks like a data governance and system performance disaster waiting to happen. Not to mention that it would encourage important business processes to be run on peoples' desktops, which are unreliable by nature. And then the DBAs will inevitably have to answer for why Important Client Smith didn't get their data feed last week while Joe was on vacation.
You're not going to make six figures JUST because you know SQL. But, knowing it will give you the ability to learn other skillsets, which will eventually make you very valuable. I started making 6 figures about 5 years after learning SQL, but not until I also learned SSIS, SSRS, SSIS, MDX, DAX, PowerBI, etc., along with Agile product management, client management, requirements analysis, and a number of other soft skills. BTW, I'm a Sr. Business Analyst. Good path to go down if you're interested in SQL. 
Yeah, job-title wise, things were similar. My background is a bit more technical, I did get a degree related to databases, and by the time I was interviewing other places I had passed both the 70-461 and 70-462 MS certificate tests. Certificates are definitely useful, mostly because you will be more confident in answering interview questions. If you do want to do a cert, I would highly recommend the 70-461 test. The book is really good for that one, and it will set you apart from lots of others in your experience range. You could start by asking your boss if he would pay for the book (https://www.amazon.com/Training-70-461-Querying-Microsoft-Server/dp/0735666059). It will take some time to study it to be ready for the test. I think you should be able to get a new job without having passed the test. From my experience, not too many employers know about/are impressed by me having the 70-461 completed. But they are impressed by the amount of knowledge I have about databases, most of which came from that book. Start studying, but really, networking is going to be very very valuable. You made it to the final round of interviews, imagine if a buddy was telling them that they HAVE to hire you? You would probably be a shoe-in. How does your network look on LinkedIn right now?
Unfortunately with 2008 you don't have the nice built in Windowed Functions LEAD and LAG, which makes operations around this type of thing much easier. In your case though, you could just join the table again. *** SELECT t1.* FROM (SELECT ID ,max(date) AS MaxDate FROM Table WHERE Date &lt; 'whatever-my-cutoff-date-is-here' GROUP BY ID) AS t2 INNER JOIN table AS t1 ON t1.id = t2.id AND t1.date = t2.maxDate ***
Analytics. Easy to find a job that pays well.
The question is whether I could learn all that by 30. 
It depends. Are you 29? ;)
I am 26. 
Possibly. Also depends on where you live, if you could get a job with access to those technologies, etc. Mine were just an example, because all my work was for Microsoft, I got trained in the whole MS BI Stack, but you might go a different route depending on your career, such as Oracle, PSQL, Tableau, Hadoop, Machine Leaning, etc. I would recommend self-training for a BI certification of some kind and that'll give you a big head start. Don't worry too much about pay right off the bat. Just focus on gaining experience, and the pay will come. Good luck!!!
Consulting exists for just about everything. The job function is to provide (typically) short term expert services for whatever field you are consulting for...
The pattern I see is that consultants make good money. That's what I'm intrigued by. 
This comment is right on the money. Personally, I use SQL every day in my job, though it is certainly not what I do full time (I manage a team of technical folks). It is one of several arrows in my quiver. Tools. It's all about the tools. A couple of years ago, I bought a copy of Database Workbench with my own money and have never regretted that purchase (the Data Pump feature alone has saved me many, many hours). Is it the best tool? I don't know. But like SQL, it's one more thing I can use to get stuff done quickly. 
I’m a data analyst. We use SQL to store all of our data, and then build reports with it in SSRS, Tableau, or our own custom built websites. 
Being a "consultant" isn't a license to print money. Consultants tend to command a higher price because they're often hired for a short period to fix a very specific problem for a company, and a solution to that problem will earn them (or save them, or protect them from losing) *far* more than they've spent on the consultant. If you're an independent, there's a *lot* of time spent building (and maintaining) relationships, finding new clientele, managing the business, marketing, etc. There's lean times and strong times. If you're working for a firm that does database consulting and/or staff augmentation, there's a lot of that as well, but less business management. Either way, you have to be able to learn your clients' business and systems quickly and the soft skills are *hugely* important.
Developer / Architects make six figures. West Coast.
Sorry for bombarding you with questions, but here goes...what kind of data? Is it CRM or sales or spending? Does your company has a tool to generate all the necessary data, or do you manually gather the data? So SQL is mostly just used to store data?
We deal with all sorts of data, from customer metrics to network outages (I work for a cable company). Some data comes from outside sources (other SQL servers, oracle databases) which we have automated tools to import, and other data comes from internal tools we’ve built to gather it. SQL is the only place we store it, but throughout the company there are multiple methods of data storage. 
Data analyst for a marketing team here. We use sql as our data warehouse. We use it for all of our data from crm to sales and inventory. We have everything in processes that load it into sql automatically. We also use things like Tableu and sad to do analysis and report building. 
I'm a database analyst. I use SQL to import data (medical - claims &amp; clinical) and clean it for the statisticians on my team. However, data is data at the end of the day. SQL is useful for any kind of manipulation of data (I don't mean modifying the data to say something it wouldn't otherwise but rather making formatting consistent, widening or lengthening data, etc.). Basically anything you can do in Excel, you can do in SQLite or better yet something like PGSql. (SQLite doesn't handle dates very well, but then again, neitehr does Excel! ;) )
ah so SQL is mostly used as data warehouse. do you do any analysis or data manipulation on SQL at all? just getting worried that my learning SQL wouldn't help if i want to become a datalyst...
Start using it at work to develop and automate reports that people are doing. Definitely a lot of opportunity in marketing industry for sql data analyst.
There is definitely analysis that can be done and manipulation. For what I do more of a statistics software like sas is better. But there are people on my team who work strictly is sql for analysis and data manipulation. I think learning any type of language is beneficial, that way you have a sense of coding and can probably pick up other languages quickly as well. 
Whatever your business does, think about the data that could be tracked - sales transactions, customer info, email interactions, etc. there’s a ton of it. A SQL database is an efficient, organized way of storing all of that data. But, just storing it doesn’t offer much insight, so people have to pull the data out of storage and come up with better ways of looking at it that offer deeper insights. If there is a really common way of presenting the data, the business will probably have some standardized way of presenting the data - let’s call it a report. Maybe like weekly sales by category. Usually most job functions get the data they need from standardized reports - maybe tableau or excel dashboards someone made or some online software. Sometimes, the data you need isn’t cleanly presented in a report and you need to go to the database to get it. That’s where SQL comes in handy. You can write some code that collects and organizes the data you need to answer whatever business question you have that can’t be answered using standard reports. If you are in marketing, you might want to know which types of customers tend to respond to which types of marketing for specific products. If your business collects all of that data, you can just go get it in the way you want it. In a lot of companies, the ability to just go get the data you need rather than relying on someone else is a really important skill. 
What do you do now?
Food service. Yeah, I know. 
i see. thanks for that hiring tip. everyone here is also saying that they use SQL mostly for data warehouse. does that mean after learning how to extract data via SQL, I should learn other languages? (actually leaning towards R after reading a mountain of comparisons vs Python)
I went from food service to phone support to IT, you can definitely do it! 
Check out Brent Ozar, he has a lot of blogs about entry level in SQL. That list is definitely for the high senior end of things and didn't list architecture or administration which are more critical for the DBA. The things listed are technologies, you'll want to be decent at SQL and theory for 6 months to two years entry level. 
How many years of experience? That seems way, way low.
I'm cringing at some of these responses that say they use SQL to store data. Don't be confused, SQL is used to *query* data that is stored in a database, which could be SQL Server, or Oracle. 
Been working there a total of seven years. No schooling, all self taught through employment. Have only been doing it the last three or so years. First couple years there I was doing general IT / help desk stuff.
Yeah, I'd definitely recommend getting the basics of SQL and then moving to a language like R or Python. R is my pick over Python, but both serve the same general purpose: a scripting language you can use to analyze, visualize, and transform data. The Johns Hopkins Coursera sequence is pretty good, but there are a bunch of good online classes out there.
&gt; First, I think that you're better off using a foreign key on Jobs referencing Employees with ON DELETE CASCADE. That accomplishes exactly what you're trying to do, if you change your query to DELETE FROM Employees WHERE EmployeeID IN (SELECT EmployeeID FROM Jobs WHERE Active = 'N'); You could even rewrite this DELETE query to use a join. For reasons that would take too long to explain here, that isn't possible. Obviously that would work in this generic example - but our real problem is much more elaborate. &gt; That said, even if you weren't, the better option with temporary tables would be to create a global temporary table once outside of the procedure, make sure it's not preserving rows on commit, and use a transaction to insert into the temporary table, perform your deletes, and commit. I don't disagree with this either - but creating anything outside of this procedure isn't possible due to typical red tape sort of stuff. I'm not trying to be a hack and get around anything - we just need to get a temporary solution in place while we attempt to resolve a more proper way to do this. I realize what I'm asking looks goofy, but that's only because I've used a generic example. If Oracle truly has no way of creating a temporary table structure inside a proc for a session, I'd be really surprised and disappointed. 
Well, a temporary table is a permanent object, not a per-session object. Are you looking for a cursor? It kinda sounds now like maybe that's what you're looking for.
Changing every couple of years is perfect in my opinion, but it's real easy to forget stuff so if you wish to keep your skills at a high level remember to refresh every once in a while. Be aware of the automatic garbage collection ;-)
I agree. From the outside it's hard to see me liking or sticking with SQL development for a long time. I really like making mobile and web apps, I'm slightly afraid I'll get less personal satisfaction out of SQL development. 
"What kind of computer do you use at work?" "I use a C++" ...
I don't think there's a lot of personal satisfaction in SQL compared to java. Generally if you're are doing advanced level things that involve SQL, it's mostly back end stuff. Optimization, write a procedure/package, fulfill some business requirement, do a new ETL process, etc. Mobile and web devs interface with the end-user directly, so I think Java devs probably experience more personal satisfaction. I've never made a GUI anything. Although some satisfaction is gained through iron-clad ETL processes, where you might get some praise from end users if you simplify their job, and praise from management because time is money, and you've figured out how to save time. But I will forever argue that SQL is not very exciting. As far as pay, it can be fairly lucrative depending on what you're doing. Just like with Java, the sky is the limit. 
The more you know, the more employable you become. It can't hurt to get a couple of years of backend development under your belt. To someone that comes from high level frontend / middleware developent into SQL development, I would say, you start knowing SQL, when you begin to understand how the DBMS executes your query. As in what algorythmns are used to actually perform a join. THEN you start knowing SQL, everything else is just syntax.
[Perhaps look at populating a collection, and then using that collection to drive the deletions](http://www.oracle.com/technetwork/issue-archive/2012/12-sep/o52plsql-1709862.html)
There is so much missing in your question. Is the only unique set of columns *all* the columns, or is something else (a subset of columns) unique? Is this read heavy. or write heavy? If this has child tables, and you need unique parents, then you'll need a join table. But if this is some sort of static lookup you might not even need a relational database. 
I had that IDE program print the results and it looks like it's connecting to the database insofar as the ide program is outputting something that makes sense, but nothing it being updated when I execute show_job on SQL Developer. I'm going to ask someone on this subreddit about oracle sql syntax I guess?
There's no natural candidate key, just storing relationships between a base and available configurations of inserts. Let's treat base as the parent table. It's not read *heavy*, writing is seldom, but new configurations will be handled by admins. Fewer than 1000 rows. 
It's not necessary to know what they are, just that there are multiple bases, and multiple inserts and some bases allow 1, 2, or 3 inserts, and we need to keep track of the inserts used in the same configuration. 
Your failure to explain the problem clearly is a reflection of your own failure to fully grasp the relationships between these things. Hope that helps!
No. You can be employable as a Jr Data Analyst with just SQL and some solid Excel skills. Those other technologies were just an example of a set of skillsets you'd gain over the years in order to make $100k+. For me, I started an entry level job as a data analyst making about $55k. As you gain more skills, your worth increases dramatically, and I made $100k after roughly 5-6 years as I acquired these skills. Your path might be different, but the concept of acquiring more arrows in your quiver will make you not only worth more at your existing role, but much more attractive to other companies, requiring your employer to pay more just to keep you. On a side note, I also recommend interviewing at other jobs every year to test your knowledge, see where your gaps are, and test your marketability. 
Are you kidding me? Are you here to seem smart and arrogant, or do you want to help? There's no further details about what inserts and bases are themselves that can help you help me. Let's call it an equation of variables *x* and *y*, ask about the relationships, don't ask me what color or shape they are, just how they relate. We're not building the base table, so from a relational database standpoint, all we need from it is the primary key, which we have.
There are probably many ways to do this, but based on the information you provided. I still don't really understand the assembly part unless you mean an assembly is a configuration of base + x inserts? In any case, I'd do something like: Base table with Id as the primary key Base 1 12 15 Insert table with Id as primary key Insert 14 15 17 23 then a join table: BaseId | InsertId ---|--- 12| 14 12| 15 15| 14 15| 17 15| 23 Then you can cascade insert/delete/updates to the tables to make sure that you don't have any nonexisting bases or inserts in the join table. 
You'll have to use a split string function, see the example below. @MultivalueParam = 'Apple,Mango,Banana,Guava' Where [field] in (SELECT Item FROM dbo.SplitString(@MultiValueParam, ',')) https://www.aspsnippets.com/Articles/Split-function-in-SQL-Server-Example-Function-to-Split-Comma-separated-Delimited-string-in-SQL-Server-2005-2008-and-2012.aspx
Aha! You meant "matched an insert to a base" in the OP, not "to an assembly." I assume the order of insertid doesn't matter? So insertid1=10 and insertid2=20 and baseid=1 is the same as insertid1=20, insertid2=10, baseid = 1? 
Does this allow for the end user to select from values I specify in a separate dataset?
Thank you this is a great response. I'm really open to trying new things and honestly think that experience on front end and back end is a good thing. I'm taking the interview and I'll hopefully get a chance to see what their devs work on and how they interact. The company and location is perfect I just need to get over the hurdle of fear of the unknown. 
Sorry I reacted to your being dismissive and a little condescending, and yes I did use "assembly" and "configuration". The former being the noun I decided to assign to our new object, and configuration in a generic way to describe the combination of multiple generic components. I don't feel good about exchanges like that.
So it's not the position that matters as much as the combination. Base 1 could have combination 1, 2, or 3, Combination 1 could have inserts 5,8, and 11, where combination 2 could have 6, 68, and 413 At some point I need a primary key to refer to, and I just keep finding many to many relationships... 
The rub is that I need to track the grouped inserts in discrete groups to a base. A base could belong to multiple config groups each group containing different inserts. 
I was about to downvote and ignore you, but you realized your exchange with sts_clover was unproductive and admitted it. Kudos! I think what you are missing is a table to store the different configurations of assemblies. Call it "Base" if you wish, like in your description. It has ONE row per configuration. You have another "child" table called "BaseInsert", that contains one row for EACH insert that is in each base. At that point you can then just add "BaseID" to the Assembly table. Join from Assembly to Base to Insert to get all the inserts in an Assembly. This allows you to have normal PK and FK relationships. Your problem was that you attempted to model "Base" as one row per configuration. This is actually a parent-child relationship, with many Inserts per Base in the child. Hope that helps!
Then make a combination table? i.e. CombinationId|Insert1|Insert2|Insert3 :--|:--|:--|:-- 1|5|8|11 2|6|68|413 And then a separate base to combo table?
It's late so this is the best I can think of off the top of my head, but you could do something like this: Base - primary key Id 1 12 15 Insert primary key Id 14 15 17 23 Insert Group - Primary composite key InsertGroupId, Insert Id InsertGroupId | InsertId ---|--- 1 | 14 1 | 15 2 | 14 2 | 17 2 | 23 BaseId| InsertGroupId ---|--- 12 | 1 15 | 2 
Yes
So i would do something like: Select customerkey, count(amount) From Purchases where amount &gt; $500 group by CustomerKey Having Count(amount) &gt; 4 How would I move forward from here? Say the customer had multiple purchases in excess of $500 but I only wanted those that occurred within a 24 hour period with a count of &gt;= 4. Of course, purchase date is part of the table. I could put this all in a temp table and filter further from there, but I'm unsure how a selfjoin would work here. Thanks for the help btw. 
On Ubuntu if you are logged in as the owner of the installation, try: sqlplus '/as sysdba' 
That's what already exists. It's not 1NF. 
If you have a known timeframe to view then you could just add a between clause to the where statement. I’m starting to think the timeframe (start and end) is unknown. Just spitballing, but I’d try to list out all purchases over 500 then use LAG over 4 rows partitioning by the customer ordering by date as an inner. The outer query can take the diff from lag value (first order) and current order where lag val is not null...obviously edge cases if they have &gt; 4 orders etc. But maybe worth a shot. 
If it matters, it is in 1NF and by itself it's actually in 5NF, if there are no known dependencies between InsertX columns. Regardless, what do you perceive to be the problem, specifically? Is that the fact you need to address different 'InsertX' columns separately while the order (Insert1,Insert2, etc.) does not matter? If so, simply have 2 tables - "Base_T" with let's say BaseID as the PK and "Insert_T" with the InsertID as the PK, then simply create an associate/link table BaseInsert with PK of BaseID and InsertID. You can add a counter attribute if the 'insertID' items could be present in multiples in a 'base'. 
Here's my understanding of the problem: - A base can have one, two, three, or possibly more inserts. - You want to store these relationships without any of them pesky nulls. - The order of inserts doesn't matter. You need two tables. One that maps groups a set of insertIDs, and another that maps them to the base. Assembly table: AssemblyID|insertID :-:|:-: 1|1 2|2 3|14 3|15 4|14 4|17 4|23 The primary key is composite: (AssemblyID,insertID) BaseAssembly table: base|AssemblyId :-:|:-: 1|1 1|2 12|3 15|4 The primary key is composite: (base,AssemblyID) You didn't mention whether there could be multiple of the same inserts attached to a base. If there could be duplicates in the Assembly table simply add a column to track the position of the insert (1,2,3, etc) and include it in the primary key. I just noticed that /u/mmo115 already wrote up something very similar, but I'm posting this anyway as I already wrote it up.
Don't forget XML - you can then parse the values directly into a temp table variable.
Adding DB skills to your CV will only increase the opportunities available. I know you asked in terms of your career but I also will answer regarding your technical ability. If you are going from an object oriented language to SQL then you need to unlearn a lot. If you apply the same approach you will produce SQL that performs terribly. For example, in java stuff like loops are fine, in T-SQL you have cursors instead or even a while loop. Both are terrible performance. 
I really don't like embedding queries. It makes performance tuning the query more convoluted (especially when it comes time to deploy).
So going into looking at this, I have a bias against the design of our database, and have been conditioned to assume that things have been done incorrectly. When I saw this table with columns that are Insert, Insert 2, Insert 3, I instantly thought that alone was a problem. Then there is the fact that EVERY field in the ID4 column is null, and MOST are null in ID3. When retrieving the data, I was concerned that we would be performing operations on every record and having to check if a value was null. It seemed better to me to just relate the base to one or more "groups of inserts", with each group being a mapping between group ID and insert ID. The problem with that is I can't use the group ID as a primary key if it appears on multiple rows, each with a different insert assigned to it. So I still don't have the answer, but this may be an "if it ain't broke don't fix it?"
Well, if you'd know everything you wouldn't be asking, would you? Anywho, what "other table" are we talking about. Going back to your original data, it has the single "currentTable" that hosts "base" and "insertX" data. this is the ERD (-ish) diagram of what is being suggested: Base_T (PK: BaseID) &lt;- BaseInsert (PK: BaseID, InsertID) -&gt; Insert_T (PK: InsertID) So, there are 2 FKs, both in the BaseInsert associate table. One is referencing Base_T via BaseID, another is referencing Insert_T via InsertID. If you need to reference a 'configuration' from someplace (table) else, you'd use BaseID (the 'header' record). What scenario do you envision that would need to reference the associate entity directly?
 select TYPE, Count(*) as 'Count' from TABLE group by TYPE Didn't get to see the link but this should be the query that gives you your ideal output 
I assumed that you have a datetime column and value for each order. Is that not correct? You're trying to find orders in the last 24 hours without those orders having a date already assigned?
Oh interesting I didn't see that function yet, I'll investigate!
 SELECT x.SCHOOLYEAR, x.[ROLE], x.LASID, x.LASID, x.FIRSTNAME, x.MIDDLENAME, x.LASTNAME, x.GRADE, x.USERNAME, x.[PASSWORD], x.ORGANIZATIONTYPEID, x.ORGANIZATIONID, x.PRIMARYEMAIL, x.HMHAPPLICATIONS FROM ( SELECT RANK() OVER(PARTITION BY ssh.sectionID ORDER BY ssh.sectionID DESC, ea.assignmentID DESC) AS 'priority', cal.endYear - 1 AS 'SCHOOLYEAR', 'T' AS 'ROLE', sm.staffNumber AS 'LASID', sm.staffStateID AS 'SASID', sm.firstName AS 'FIRSTNAME', ISNULL (sm.middleName, '') AS 'MIDDLENAME', sm.lastName AS 'LASTNAME', CASE sm.schoolID ... END AS 'GRADE', ua.username + '@sd25.us' AS 'USERNAME', NULL AS 'PASSWORD', 'MDR' AS 'ORGANIZATIONTYPEID', CASE sm.schoolID ... ELSE 'No School' END AS 'ORGANIZATIONID', ua.username + '@sd25.us' AS 'PRIMARYEMAIL', CASE sm.schoolID ... END AS 'HMHAPPLICATIONS', ssh.sectionID, ea.assignmentID FROM Table1 AS s JOIN Table2 AS c ON s.courseID = c.courseID AND c.departmentID IN (...) JOIN Table3 AS cal ON c.calendarID = cal.calendarID JOIN Table4 AS sp ON s.sectionID = sp.sectionID JOIN Table5 AS t ON sp.termID = t.termID AND GETDATE() BETWEEN t.startDate AND t.endDate JOIN Table6 AS ssh ON s.sectionID = ssh.sectionID AND ssh.endDate IS NULL JOIN Table7 AS sm ON ssh.personID = sm.personID JOIN Table8 AS ea ON sm.assignmentID = ea.assignmentID AND ea.endDate IS NULL JOIN Table9 AS ua ON ua.personID = sm.personID AND ua.ldapConfigurationID = '2' WHERE sm.endDate IS NULL AND sm.teacher = 1 AND NOT (ssh.[role] = 'N' AND ssh.staffType = 'T' AND sm.title != 'Long Term Substitute') AND ssh.[role] != 'C' AND ssh.sectionID = 439047 ) AS x WHERE x.[priority] = 1
Because you are using the rank() to filter in the outer WHERE clause, you have to use a subquery. Keep it as-is.
I'm not seeing any easy and for-sure way to improve performance but there could be a number of things that could be causing performance issues that are not just cause of the structure of the query. The main tool here is looking at the SQL execution plan. I never used this before, but here is a cool service for sharing: https://www.brentozar.com/pastetheplan/ there could be a number of things that are "wrong" or could be improved that the execution plan would detect. It should show you a all the parts of how the query will be completed and then give an estimate about what steps will take the longest and those steps can be looked at to see if there are any improvements needed. Without seeing the execution plan it is hard to give guidance though.
Either choice can make for a fulfilling career. Do what you love. Will you love being a SQL developer? Maybe. If you do SQL development long enough, it can transition into a DBA role. Will you love that? Maybe. I do. Don't worry too much about the money, as it'll be there in either case.
I optimize a lot of queries like this in my role. Generally speaking you want to break it down and force SQL to handle it in chunks. Do not listen to the optimizer until you've done this/started doing this because you are smarter than it. Real quick example here because your joins are not descriptive, so I can't tell which is an INNER vs LEFT, etc., but say you have a query like this: select ( select *, case when, case, case, case from table inner join left join left join inner join left join where blah blah ) as x where blah = 1 So the first thing that will *generally* improve performance is something like this: begin select *, case when, case, case, case into #table from table inner join left join left join inner join left join where blah blah end begin select * from #table where blah = 1 end Now you can start to get creative. For example you might want to do a select * from table + inner join, then in a second step do your case logic (or cross applies, or whatever you are doing, row_number, etc.), then in a third step add an index to your #table2, then in a fourth step do all your other joins, then in a fifth step select * where blah = 1. You could get more creative and do the joins one at a time and continue to add indexing, although this generally isn't optimal unless each table is absolutely massive. Sometimes doing 5 joins in one step is better, sometimes doing 1 or 2 is better. Sometimes you don't even need a join and can just do something like a WHERE EXISTS or WHERE NOT EXISTS. General rule of thumb is that sub-queries, multiple joins, etc., are all bad practice. They work great when you're first writing something and trying to get it accurate... but generally speaking if you break it down after the fact it will perform much better in sequential chunks, and then you can use the optimizer on those chunks to really optimize the sub-processes or blend two sub-processes together. It really depends on how long the parent query is taking to run. Less than 10 minutes and you only need to run it once and awhile? Probably not a good candidate. Does it take several hours to run and it needs to run daily? You can probably get that down to running in half to a tenth of the time. It really depends on how your data is modeled, specifically what you're trying to do, etc., but you can start to play with the code when you have it broken out into chunks... for example is it better to do a where date between x and y at the top of the chain, or at the bottom? So you just run each chain and see how long it takes until you find the one that is really taking a long time... then you get creative/clever.
&gt; I work a support job on an ERP system This is pretty close to my dream job. Would you mind sharing what skills and education helped you get that job?
DECLARE @TableName nvarchar(400) DECLARE @TestSQL nvarchar(MAX) Set @TestSQL = '' DECLARE cursor1 CURSOR FOR select name from sys.tables where name like 'Datatable_%' OPEN cursor1 FETCH NEXT FROM cursor1 INTO @TableName WHILE @@FETCH_STATUS = 0 BEGIN Set @TestSQL = @TestSQL + 'Select * from ' + @TableName FETCH NEXT FROM cursor1 INTO @TableName If @@FETCH_STATUS = 0 BEGIN Set @TestSQL = @TestSQL + ' UNION ALL ' END END CLOSE cursor1 DEALLOCATE cursor1 Print @TestSQL exec sp_executesql @TestSQL OUTPUT
The query below shows what I came up with(Thanks for the Help!!!). I am able to see the results I want, but I have not figured out how to store these in a view. 
So one have a little bit of a weird background. I started off as a Computer Science Major. The classes I took during that were SQL, C++, Java, and Unix related. From this background really the SQL only applies. Knowing a bit of Unix helped because I understand how to navigate a command prompt and understand the basics. We use DOS commands on customer systems to check connections, kill programs and etc. I graduated with a degree in Business Adminstration I took some project management, database, and Buisness analysis classes. Among other Buisness stuff. Overall understanding of SDLC, ERD Diagrams, and background on DB's is what helps me perform my job. I also got a another minor and a certificate but those aren't really relivent. Big thing for me is people skills, communicating with customers, time management, and basic good employee skills. If an organization is good, they will hire someone they see as trainable even if you have talent gaps. Remember you'll always have resources of some kind to refer to when you work. For us specifically, the skills we would require are SQL and Crystal Reports. That's about it on the tech side. This truly depends on the company and postion you're going for.
Okay, you have given me a lot to work with here let my try and break it down a bit. All of the JOIN's are INNER I wanted to use CASE to try and fix this issue but here is the problem; When there is a Long Term Sub I want ONLY that subs records for the given class. When there is no long term sub I want the teachers records for the given class. The problem I ran into is those will be kept as separate records for the same class IE Class number 1 Teacher of record/primary teacher is Bill Class number 1 Teacher/Sub is Susan If Susans records are active then Only give susan else give bills records. In addition I can't return that RANK column as it has to match up with what the vendor needs.
Hey man, I tried to tell you how to fix this below. The problem is "GroupID" should be in one table, and all the associated InsertIDs should be a SECOND table. Classic parent-child relationship. You are skipping creating a unique identifier for each group, then complaining that it doesn't exist. Don't skip that step. Proto SQL: Create table Insert (InsertID PK)--defines unique inserts Create table Group (GroupID PK)--defines unique groups of inserts, ONE row per umique group Create table InsertGroup (InsertGroupID PK, InsertID FK, GroupID FK)--one row per group per insert in that group Create table Assembly(AssemblyID PK, GroupID FK)--Associate assemblies with a set of inserts That "GroupID" on the Assembly table is the missing piece of the puzzle, I think.
To be clear, you made a table with just one column?
No dude...you didn't post your schema so we have no idea what entities you're modelling. These are just the keys for each table. 
I use database queries in my role but only have a very brief knowledge of the admin side of things. I'm in a data analyst role so all the admin work is handled by the database admin people. It all depends on what role you want to get into. Don't worry too much about not knowing everything at the start. It'll come with practice and it's very difficult if not impossible to learn everything especially as the technology of each organization grows and changes. 
Can you explain what type of parameters you're talking about? Because in reality the answer is to not do this at all and design a source table where everything is pre-calculated.
FWIW /u/Beeranator's suggestion worked. And no, we don't have our users manually enter values. We normally add additional datasets to query dictionary tables and create a dropdown list of available parameters for the user.
Pardon the formatting, I'm doing this on my phone. And I've not actually used MySQL before... But this seems to work... Select typelist, count(id2) n From ( Select id2, GROUP_CONCAT( distinct type SEPARATOR ', ') typelist From docs Group by id2 ) d Group by typelist
I keep text files of some of the ones that it took me a while to build or were interesting. Best I've done is one that concatenates the results of a specific column if every other column can otherwise be grouped. 
Yes of course datetime of transaction is a datapoint. 
If you're strong at SQL, but not strong at developing in 3GL languages, you could always go for a job in data warehousing/business intelligence. There are entry-level jobs in that function.
Maybe find some datasets or think of a fictional company and create a database with tables. Then make up some reports perhaps taking some inspiration what you can find online or maybe YouTube tutorials. Here is a real world example I guess to help provide some inspiration of something to maybe build for a portfolio. Not sure if interesting or not but I just did a report at work for the first time in a couple years which was the most complex I've done so far. Basically think of kpi's and production statistics for a manufacturing company. Essentially I created parameters to choose from such as date range, product type, plant, production line, material, and shift. I have multiple datasets which are filtered based on the parameters a user selects. I then show show or hide individual matrix's based on a selected product type so it is like I have multiple reports in one. Each matrix has around 50 calculations grouped by the selected production line. For this I created views and had to union multiple tables and added calculated fields which were frequently used in different calculations to build out comprehensive datasets for each matrix. This could be build upon further with some graphs comparing the performance of individual shifts. For example if one shift has more scrap consistently then an investigation could be performed. 
Can you show us what you mean? An anonymous block is “anonymous” because it’s not an object like a function or a procedure would be. It’s a query block meaning it doesn’t get “deployed” as an object on Any schema. It runs as whatever schema you’re logged into. To answer your question, no, you cannot see anonymous blocks on a schema, because they don’t exist past runtime. Technically they don’t even exist at runtime. However if you use SQL developer to run SQL history you should be able to see most queries you’ve run in the past. That’s not really related though. Your table is showing extra rows because they were inserted at some point, in my opinion. Whether it’s by a trigger or another insert statement is TBD. Further reading. http://www.orafaq.com/wiki/Anonymous_block “An anonymous block is an unnamed sequence of actions. Since they are unnamed, anonymous blocks cannot be referenced by other program units. In contrast to anonymous blocks, stored/ named code blocks include Packages, Procedures, and Functions.”
Sr BI Engineer with 12 years experience here. I've literally never been asked for this. I don't save old client or employer code and I don't discuss my work with former clients or employers except for at a very high level. I'm really surprised to see this is a standard ask. Where do the rest of you work, that this kind of behavior is normalized? What geographic region or industry?
I've not experienced this myself. I think a portfolio is more common for web development or perhaps projects in github for software development positions. 
Also by the time you create a portfolio another potential employer won't give a rip but at the very least hopefully some knowledge is gained which can be used during an interview to demonstrate a basis of knowledge if called upon.
I would try the PIVOT statement. The pivot syntax (comes after main select) would look something like PIVOT (COUNT(*) FOR DateSold IN (20180305 as Yesterday, 20180306 as Today)); --Adapt to MS syntax
Thank you. I'll test it out in the morning.
Thank you for the suggestion. I'll look into doing that.
This would exclude products that didn't have a sale today from yesterdays results because of the left join...
Not the best code, but it should give the results you want, if it needs running for any two dates you can add a third parameter to limit your upper bound in the where clause. Declare @Today DATE = getdate() , @Yesterday DATE = getdate()-1; `SELECT Product, ISNULL(COUNT(CASE WHEN DateSold &lt; @Today THEN 1 ELSE NULL END),0) As YesterdaySales, ISNULL(COUNT(CASE WHEN DateSold &gt;= @Today THEN 1 ELSE NULL END),0) AS TodaySales FROM Table WHERE DateSold &gt;= @Yesterday GROUP BY Product`
if you are using MySQL database, then you should use MySQL syntax. Oracle SQL developer is just an interface to the database. Don't know the MySQL syntax so I can't help you. Just use the tools that come with MySQL database, i think its called mySQL workbench.
you can look at this https://docs.oracle.com/cloud/latest/db112/LNPLS/dynamic.htm#LNPLS011 The dbms_sql package is good. You can also read from the system tables and construct what you need. These are tables like user_tables, user_tab_columns, ...
There are all kinds of reasons to use them. Some of those reasons: * You need to perform other operations independent of the outer query (ex Aggregates) * The query needs to be legible * The query needs to be broken into parts (ex the SQL engine of choice is doing a poor job of building a proper plan, and subqueries can help hint it in the right direction). Not saying subqueries are the only way to solve issues like this; there are other methods like CTEs/temp tables/table variables/etc.
I find they're most useful when you have to select a ton of stuff. For example, say I have table A and table B. Table A has the data I want, and table B has the unique identifier as well and date ranges. I could do something like select * from A where unique_identifier in (select unique_identifier from B where TIMESTAMP &gt;= &lt;date&gt; and TIMESTAMP &lt; &lt;date&gt;) without the nest, I would need to manually enter each unique identifier in the "in" clause. Sometimes this can be thousands of recods
thanks ! really like the sample code
TBF, your example could just as easily be done by `inner join`ing `A` &amp; `B` on `unique_identifier`. Possibly a more efficient query too.
np. Nesting is something that comes in handy and its easier to grasp then joins (though you should learn those too, they're way more powerfull). I probably use nesting at least once a day. Its also easy to exploit. like...in Oracle DB, you can't put more than 1000 elements in an "in" clause. so I do something like this select * from A where unique_identifier in ( &lt;select statement with a thousand elements in an in clause&gt; &lt;union select statement with a thousand more elements in an in clause&gt; &lt;union select statement with a thousand more elements in an in clause&gt; &lt;...and so on&gt; ) You're going to have to resort to hacks like that if you work for a company that doesn't understand how relational databases work when they designed their apps.
Nah, I think you're 100% right. If I'm scripting something that gonna run every couple of mins, I'll write it as a join because generally you're right. They're more efficient. Me personally though, if I'm gonna run a quick query I find it easier to nest.
You are right. One benefit of nested query is that you can run it on its own and see what comes back. Its not useful if you are writing a stored procedure, but when you are conducting ad hoc query it might come useful. Other than that inner joins are what I use in those situations.
In this case it's a matter of aesthetics. I prefer EXISTS and NOT EXISTS as they work regardless of NULLs.
If it's just a matter of which filters are applied based on which parameters have values specified, rather than use dynamic SQL, an approach I've used in situations like this in the past is to just ensure all my parameters have default dummy values if they're not specified in the frontend. (I had to take this approach in a situation where it wasn't possible to use NULL because it had a special meaning and needed to be specifyable as a WHERE - but you could do the same with the defaults being NULL if it didn't matter). e.g. the default value was ` '---NA---'` for each parameter, and they all got specified in the query like so: and ((Name = v_NameSearch and v_NameSearch &lt;&gt; '---NA---') or (Thing2 like '%' || v_Thing2Search || '%' and v_Thing2Search &lt;&gt; '---NA---' and length(v_Thing2Search) &gt; 0) or (v_NameSearch = '---NA---' and v_Thing2Search = '---NA---')) Of course you can get crazy complicated like this, eg multiple separate queries that all return the same columns but from vastly different tables, all `UNION ALL`ed together, and each subquery defined to only return rows if a certain value was set. 
Not too familiar with the function itself, but in SQL 2012 and newer there is a LAG() function that might just be perfect here.
Everything is working right now, my upload has 0 errors so now I want to improve the functionality and hopefully learn something in the process. 
That will include the sales data from yesterday, but they still wouldn't have a name. You'll want a COALESCE(a.product,b.product) to catch those if you are using this approach.
Let me see if I can think of some use cases off the top of my head. How about if you want a query to only run in a server named production? select thing from table where (select host_name from v$session) = 'production'; Sometimes I might want a query to select a different column from a table depending on what server it's running on. You can stick some subqueries in the case-when statements to handle that requirement: select case when (select host_name from v$session) = 'production' then (select attribute1 from table) when (select host_name from v$session) = 'test_env1' then (select attribute2 from table) when (select host_name from v$session) = 'test_env2' then (select attribute3 from table) end as some_alias from dual; You can use a subquery anywhere. Here is an example selecting the hostname with the record set: select (select host_name from v$session) as hostname, thing from table; You can use a subquery in your where clause to define your own "table" as a subset of the main table: select thing, mytable.otherthing from table, (select otherthing, linking_id from someothertable) as mytable where table.id = mytable.linking_id ; --oh no, an ANSI-89 join, run for the hills! And, specifically regarding subqueries in the where clause, there are two types called correlated and uncorrelated. Most of the time you see uncorrelated subqueries. Those are queries that work from the inside query to the outside query, such as "select thing from table where value in (select value from sometable)" The values in sometable are gathered, then passed to the main query as part of the "in" clause. But sometimes you want the innermost query to constantly communicate with the outtermost query. That's a correlated query. Example from wikipedia: SELECT employee_number, name FROM employees AS emp WHERE salary &gt; ( SELECT AVG(salary) FROM employees WHERE department = emp.department); In this example, both the main query and the subquery are using the employees table. For each iteration of the loop, the inner query has to be re-executed. In other words, there is a dependency between the employee table and the employee table that is aliased as emp. You can also use a subquery to check existence of data in another table : select * from sometable where exists (select 1 --you can select *, select null, select 'bologna sandwich', or anything you want from someothertable where sometable.x = someothertable.y); 
Your query is pretty clean looking so I'm not sure how much you're going to get optimizing it. Like I said, look at the SELECT and see where you're doing your heavy lifting from (row_number, cases, etc.) and then see if you can break that down and do it earlier in the process, then join to the rest of the tables, or do it at the end and just do a select * in the first step (i.e. select all the columns you need for your cases, etc.,. but don't do the case logic until you have the data segmented into a #table first). The problem with SQL when it comes to advanced queries is that they'll sometimes cause the server to hang. Optimizer won't tell you anything or be of any use beyond creating a new index.
FYI that's not MySQL you may want to delete and re-post as **[MS SQL]** in /r/SQL 
That worked exactly as I needed it to. Using ISNULL wasn't necessary for my data set so I just used COUNT (both ways returned the same results though). I really appreciate the help!
 from ' + @TableName + 'where looks like there's no space between the table name and the WHERE keyword
Thanks Alot I thought that, but did not put a space between single quote before. I will blame it on at work so early on a Wednesday.
Not familiar with Salesforce, but we use this TSQL function: -- USE [dbname] GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO --CREATE FUNCTION [dbo].[Word_Cap_Funct] (@str varchar(255)) RETURNS VARCHAR(255) AS BEGIN declare @pos int declare @res varchar(255) select @res ='' select @pos = patindex('%[- ()/;:,.&lt;&gt;?!@#$^&amp;*\|~{}]%',@str) while @pos &gt; 0 begin if @pos &gt; 1 select @res = @res + Upper(substring(@str, 1, 1)) + Lower(substring(@str, 2, @pos - 2)) + substring(@str,@pos,1) else select @res = @res + Upper(substring(@str, 1, 1)) select @str = substring(@str, @pos + 1, 255) select @pos = patindex('%[- ()/;:,.&lt;&gt;?!@#$^&amp;*\|~{}]%',@str) end select @res = @res + Upper(substring(@str, 1, 1)) + Lower(substring(@str, 2, 255)) return LTrim(RTrim((@res))) END 
VirtualBox to run your VM. You still need a license for Windows though. Express Edition will be more than adequate for starting out. But Developer Edition is free, so why not jump in with that?
Don't underestimate the power of convenience. I used to have a VM on Azure for my "home" use of SQL Server (as I too am a Mac person) but after a while, I realized I wasn't using it because it wasn't sitting right in front of me all the time and when I did use it, I had to start it, wait, do my stuff, then remember to deallocate it when I was done so I didn't keep getting charged. Having a local VM, I'm much more likely to use it regularly.
Wow, right on the money. We do use a third party app that accesses this database. Unfortunately, anything that requires anything more than a click of a button requires me to be the one to administer the report (I already have enough on my plate). I will ask for a data dictionary and a entity relationship diagram. Thank you so much for your insight!
Sum the total spent, keep column customers, count column customers then use the datediff on the dates like u were... Group by customers and then use a having clause where customer count &gt;4. SORRY IF THIS DOESN'T TRANSLATE. I had to hurry 
Yep, this 
If you know how to power other applications in your suite with SQL, you could land a data analyst role. SQL Server + Excel landed me such a job, and now I get paid to learn Power BI, too. The only "coding" I do is with T-SQL, DAX, and Excel formulas. These days the BI platforms are designed to cut out the guys that you traditionally consider to be "IT." Instead the keys are with staff on the admin side who happen to have a little technical acumen.
This probably won't matter much but it should be noted that a UDF will result in serial execution, and will slow down the query in some workloads.
 How are you not getting syntax errors on your first select into? Your group by specifies columns that are not in your select portion. 
I was actually just re-reading it and realised I hadn't renamed some of the columns. Will update.
On the CTE, don't inner join on the second join to PlacementList. Make that a LEFT JOIN. Then include in the JOIN AND z.PlacementID &lt;&gt; zz.PlacementID
Thanks for the tip. Is that from an efficiency view point? Is where I'm at with the query about as simplified as I'll get, or is there another way to tackle it?
Are you using the 'year' function I had in the sample?
&gt; Select count(*), year(time) from quakes where type='earthquake' group by year(time) Yes, it says "[00:10:17] Error while executing SQL query on database 'Earthquakes': no such function: year"
'Year' is in Microsoft SQL, which flavor are you using? Try datepart(year,time) instead.
I'm using SQLite Studio and it says no such function for datepart :( lost cause
Self join on userid and where placementID don’t match.
Strftime('%Y',time)
&gt; Strftime('%Y',time) ahhhh this worked! Thanks so much genius stranger
are both those colums in the same table? then just: SELECT Agents, calledNumber FROM callRecord 
Well you can not create indexes on CTE, so that's quite big. Temp tables are also session based so you can load in once and reuse it without calling your original query each time. I personally use CTE only when I need recursion, for everything else I use temp tables.
yes bro
Blitz_Index for starters, check wait stats, check index fragmentation, monitor the biggest cpu hitters. Check the TempDB files, how many have you got? Just overall check if you're suffering from IO or CPU. Please do not do this backup malarkey. I don't see how this won't have downtime? Look into Ola Hallengren, the scripts for maintenance are even online. Identify the problematic indexes, and try reorganising them first, put lock timeout, low deadlock priority and look into maxdop. 
wait times seem extraordinarily high..? What wait times? CXPACKET, IOLATCH? https://www.sqlskills.com/blogs/paul/wait-statistics-or-please-tell-me-where-it-hurts/ - Paul Randal is your man. The DB seems tiny, so most likely statistics. We can go into bad words like bad histograms and the f word - fragmentation, but it won't help much. Identify the most "executed" queries, and update the underlying tables' statistics and sounds like that some of the rows statistics are skewed when you do a select. It sucks that you got hit by everything all at once, but you have to love it, it's good shit, that's what you should live for, you will learn a lot by the end of figuring this one out. 
Site is down
also, since it looks like you have your answer, flair this as solved
Also check that tempdb is properly configured. 4 or 8 files equally sized and equal growth. you just wrote a great sale point to boss. if you can close for week and save that time in performance, there is time to schedule an outage. what business does not have any windows for scheduled downtime? can you get a copy of the server? P2V or a clone? as for Ola scripts, they will run in a 'examine' only mode. and for your jobs sake: SMALL managed changes over a long period of time. 
also /r/sqlserver 
i dont know your platform or sql variant but i'd try another letter than P again for the table name. also im not sure why you need the subselect. why not SELECT ... FROM ADDER.POS_RATE_ACTIVE P LEFT OUTER JOIN ADDER.DWSOF_ALL D ON P.POS_NUM = D.POS_NUM LEFT OUTER JOIN ADDER.APPOINTMENT_ACTIVE A ON P.POS_NUM = A.POS_NUM} WHERE ...
yw but i'd actually need an example of how those tables actually look like to get an overview and your new exact code. also, i can only guess that what you're doing with those left joins is actually what you want to do. and i'm not sure what you mean by duplicates like are there more of the same pos_num in D with different dates? there are multiple ways to do this. it also depends on your platform/sql syntax. you could also get duplicates because you didnt adress all key columns. but if i guess in the dark id say try by subselect in the where clause and d.date = (select max (x.date) from ADDER.DWSOF_ALL x where x.pos_num = d.pos_num) 
instructions are a bit unclear but does this help? select a.last_name ,count (a.*) Amount1 ,count (b.*) Amount2 ,count (c.*) Amount3 from table a, table b, table c where a.last_name = b.last_name and a.last_name = c.last_name and a.last_name = '%varLastName%' and a.SRC_SYSTM = 'Source1' and b.SRC_SYSTM = 'Source2' and c.SRC_SYSTM = 'Source3'
I think the % is part of the program that I am writing the query into for surrounding variables, but I'm not sure. There is only one TABLE here that the three sources are combining into. I'm not sure if that changes anything. There are four or five other variables that I left out of the query above that I am also checking. 
I mean, probably not the best/fastest way but if you have three queries that are working just fine and you want to combine them into a single pull, just put them all in your code separated by "union" in between them. Just make sure all of your top select statements are the same for each. 
Example of TABLE: LAST_NAME|FIRST_NAME|SRC_SYSTM ------------|-------------|------------- JOHNSON|ADAM|Source1 JOHNSON|ADAM|Source2 JOHNSON|BETTY|Source1 SMITH|CATHY|Source3 I would expect count from Source1 for Johnson to be 2, from Source2 to be 1, and from Source3 to be 0.
oh ok then you need to use "=" instead of "LIKE" rest should work tho. alltough im not sure what result you're expecting. maybe try to tell me what the program does. do you for example insert a gender and want to see the amount of the occurencies in those sources?
I needed all the entries from the P database, but only those entries from the other databases that have actual entries in P. Left-Joining seems the best way to do that. In the D database there are multiple titles for a given position number. It is a quality control problem from history. As a result, only the most recent entry can be seen as reliable. Is possesses personally identifying data, though, so I can't really share contents. BUUUT it looks like it worked (minor tweaks due to weird structure on the database). Dude, you are awesome!
 SELECT src_systm , COUNT(*) AS cnt FROM table WHERE last_name LIKE '%varlastname% AND src_systm IN ( 'Source1' , 'Source2' , 'Source3' ) GROUP BY src_systm 
yeah the code should work then just replace like with = you wanted to join these queries together so now you get 3 results at once. the code creates a new table for each source and counts the amount of your last name in it.
&gt; just put them all in your code together separated by "union" in between them that won't work too well in this case as proof, i offer you one possible result set from your union query -- 34 47 52 in the worst case scenario, two of the counts will be equal and you're get this -- 34 46 
All I need returned from the query is the count from each source. I have different logic in place depending on if all results are from one system or if I have results from multiple systems.
All I need returned from the query is the count from each source. I'll be adding text to a file that already contains Name, Gender, DOB, etc. that indicates which source if count&gt;0 for only one source else other text if multiple sources or no matches.
I know there are only three sources though. Currently we have two and are in the process of adding a third. I'm concerned about performance though because there are a lot of similar queries to the above that are checking different criteria and I don't want three queries for every condition. Currently I have two.
Supposing, sure. Use a different solution in that case. OP said there were only three sources though, and one query for each.
Mmk. This should return three columns, one with a count from each source, plus whatever other columns you specify, for each last name. u/r3pr0b8 is correct in that if you have more than three sources then this is wasteful. But I also assume that this is a somewhat ad hoc query that you're not likely to store as an object in the DB.
yes then either use my code and fetch the result into 3 variables at once or use r3pr0b8s code and fetch each result for each source into the variables using a loop in the program
if you re-run a script several times (say during testing), you need to drop/re-create those temp tables every time, but not CTEs
I'm an accidental DBA now managing a 25 TB SQL server (running 38 databases in that space). We have one that is 7 TB. If you can spare some money, SolarWinds DPA has been my friend for a year now. I can identify pain points fairly quickly. I was also able to show very quickly what queries are slamming the CPU vs Disks, found bad query plans, borked stats, etc.. Brent Ozar - scripts for databases are a real time savers. SP_Blitz gives you a great place to start. Ola Hallengrens maintenance scripts are fantastic. TempDBs, should have 1 per CPU, they should be set to a specific size with no growth/shrink turned on. Try to separate the MDF and LDF into different disks. LDFs should be on fast disk too. TempDB's should be on a different disk, preferably SSD or fastest disk you have. Find most executed queries and do update stats on the tables those hit. I'm sure I'm missing a ton, but I feel your pain. Good luck
id probably go with the array solution and use this code: SELECT COUNT(*) FROM table WHERE last_name = '%varlastname% GROUP BY src_systm 
&gt; I've simplified the query a bit here. this is my excuse and i'm sticking to it
 select a.group, (select min (b.block) from BlockGroup b where b.group = a.group) as Block1, (select min (c.block) from BlockGroup c where c.group = a.group and c.block &gt; (select min (b.block) from BlockGroup b where b.group = a.group) ) as Block2, (select min (d.block) from BlockGroup d where d.group = a.group and d.block &gt; (select min (b.block) from BlockGroup b where b.group = a.group) and d.block &gt; (select min (c.block) from BlockGroup c where c.group = a.group and c.block &gt; (select min (b.block) from BlockGroup b where b.group = a.group) ) ) as Block3 from BlockGroup_Containers a where a.container = 15 group by a.group
That's wonderful and amazing, and waaaay more overhead than I thought it would be! Thank you very much!
wow! Thanks very much, I thought this would be simpler for some reason... but thank you! this will do the trick!
My attempt: SELECT groupId, [1] Block, [2] Block, [3] Block FROM ( SELECT groupId, blockId, ROW_NUMBER() OVER (PARTITION BY groupId ORDER BY (SELECT 0)) num FROM #groupBlock WHERE groupId IN (SELECT groupId FROM #groupContainer WHERE containerId = 15) ) SRC PIVOT ( MAX(blockId) FOR num in ([1], [2], [3]) ) PIV 
I’m thinking I should probably just clone the most recent rate period and then create a table to normalize the rate period moving forward and just abandon all the old data.
I think the point is that the select on tables just takes the data it needs. The view takes ALL the data, stores it and after select again to get the data you want. 
A view can be indexed; however, views that use views can't be properly optimized and will fail to utilize the indices. 
Again, here is where my knowledge is limited. Are the values being returned in the array guaranteed to be in the same order every time? I need to know the first value is Source1, the second is Source2, and the third is Source3.
I mean there is no reason you can't push all of the data into sql server as a one time load right? It sounds like your question may be really specific to your environment and less SQL related. In any case, storing the data in a MSSQL backend and using access as a front end is perfectly fine. It's concerning that there is a multi billion dollar program without any real db support though.
You can't use a column alias like that--you'll either need to make the `CASE` statement a subquery / join on it or just repeat your case statement as the `concat` parameter: SELECT DISTINCT PRIUS , HYBRID , CASE WHEN PRIUS = '1' THEN 'A' WHEN PRIUS = '2' THEN 'B' WHEN PRIUS = '3' THEN 'C' ELSE ' ' END AS BIN , CONCAT( CASE WHEN PRIUS = '1' THEN 'A' WHEN PRIUS = '2' THEN 'B' WHEN PRIUS = '3' THEN 'C' ELSE ' ' END,HYBRID ) AS BIN
If you have any sway persuade them against using MS Access at all. I cringe every time I see that. Port the data over to SQL Server and offer them SSRS or a basic .NET client to view and modify the data. 
A view is just a stored query
While I totally support other commentators saying just go to sql and write a new front end, I'm going to play as if you've got a ton of forms, querydefs and reports in access that you are not going to be able to abandon in a reasonable time frame. If you had to, upsizing the data into sql and using dsn-less connections to make linked tables of the same names as the original access tables is possible, and will usually allow you to leverage that existing stuff. I made an 11th hour change to an about to ship product that was access based but prone to corription this way ( a long time ago), and the use of an mdb as a pass-thru shell to the underlying SQL database worked fine while we worked on decoupling the querydefs to procs, reports to Crystal and forms to (shudder) VB4. I won't swear this is all still possible with whatever version of access you're using but it all sounded familiar.
So it seems like you have two issues right? The date filter and the agent Id variable? It looks like you are declaring the variable as integer (I usually just use INT but that might be the RDMS) and when you set it you are adding the X character in quotes. That would fail in MSSQL. That should be a integer without quotes? As far as the date you could use get date()-120 Might need to cast that as date otherwise you'll get up to this moment. Please forgive typos as I'm on my phone.
That's not how views work. 
LAG is the ideal solution, but OP if your RDBMS doesn't have LAG, then inner join your existing table_name with another table_name ON the first table's event_date = the 2nd table's event_date - 1 day.
now its up bro. for some reason site down. sorry for it
Is the Oracle Table changing in any way? Not nessasarily the columns that you are specifically retrieving. I’ve had similar issues with an ODBC driver when I was using the columns tab to specify which columns to select. Instead I switched to a SQL select statement to only retrieve the columns I needed. My issue may have been different to yours though. I’d get it to work, and then any change in the metadata in the entire table would throw up that error. 
are you trying to create an sql procedure or do you just need a query? looks like you need to set this part in brackets ... AND (APP_STATUS = '0' OR APP_STATUS = '1') AND ... what is x0660? it doesnt look like an integer. heres how you get all dates that are 120 days in the past: AND date (DATE_CREATED_TS) &lt;= current date - 120 days
 SELECT a.event_date, a.section_id, a.metric, (metric - (select max (metric) from table_name b where b.section_id = a.section_id and b.event_date = a.event_date - 1) ) as Metricdiff FROM table_name a WHERE a.event_date BETWEEN CURRENT_DATE - 8 AND CURRENT_DATE
When you recreated the package, did you copy and paste any of the components from the existing package to the new package? If so: I found a bug back in 2012 which had this issue. Also, is the version of visual studio used to create the package different from the version you used to modify/update? Metadata gets recompiled every time a component/task is opened and “saved” by hitting OK. Double click every component, change something, change it back, and hit OK so it gets recompiled. In the case of the script task, open it, recompile the C# or VB Code, then close and hit OK.
Same result, I used the new table button on the destination without any luck.
Which statement is actually on line 9? If there's more than one, separate them. Did you check the table?
Yeah good catch on the APP_STATUS I changed it from INTEGER to a STRING, but it still won't recognize the variable. Hmm... I am about to try your date trick. How do I make that date code spit out the format I need? 
Thank you for the response! If I need it to be a mix of alpha and numerics should I use STRING?
the format you need looks like a normal db2 timestamp. i dont have access to a db2 system atm. but does current timestamp - 120 days work? then you could remove the date () function.
What about just a multicast step as the destination? Also how are you selecting the columns in the ODBC source? Table Name with Columns checked or SQL Command?
maybe this? DECLARE v_date Timestamp SELECT CURRENT_TIMESTAMP INTO v_date FROM SYSIBM.SYSDUMMY1 try char(max lengths of agent_id in table) instead of string declare v_agentsid char(8)
is DATE_CREATED_TS an actual column in db2prod.pa_iba_base? or are you trying to show a specific date there? does the first query you mentioned work without an sql error? if its an actual column: SELECT NAME_FIRST, NAME_LAST, DATE_CREATED_TS, APP_STATUS, DELETED FROM db2prod.pa_iba_base WHERE AGENT_ID = 'X0660' AND (APP_STATUS = 0 OR APP_STATUS = 1) AND DELETED = 0 AND date (DATE_CREATED_TS) &lt;= current date - 120 days --- or if it works: AND DATE_CREATED_TS &lt;= current timestamp - 120 days ORDER BY DATE_CREATED_TS DESC will spit out all names that arent deleted with status 1 or 0 on all dates in the table that are more than 120 days in the past if its not an actual column and you are trying to simply show a date 120 days in the past you need: SELECT NAME_FIRST, NAME_LAST, (select current timestamp - 120 days from sysibm.sysdummy1) as DATE_CREATED_TS, APP_STATUS, DELETED FROM db2prod.pa_iba_base WHERE AGENT_ID = 'X0660' AND (APP_STATUS = 0 OR APP_STATUS = 1) AND DELETED = 0 ORDER BY NAME_FIRST DESC this will spit out the same date 120 days ago for all names... or is there another date column in the table that youre trying to substract the days from? 
The date is a column, it is the date that an item was started. The scenario is we have an app that is pulling in the items from the last 120 days by default. There is a filter that can change the sort by 90, 60, and 30. The SQL query is proving that the app is pulling in the correct data after the app went from being hardcoded to using actual data from our DB2s.
I will try that and let you know, thank you!!
YES! The date worked, ha! My many thanks! Here is the successful code: SELECT NAME_FIRST, NAME_LAST, DATE_CREATED_TS, APP_STATUS, DELETED FROM db2prod.pa_iba_base p WHERE p.AGENT_ID = 'X0660' AND (p.APP_STATUS = 0 OR p.APP_STATUS = 1) AND p.DELETED = 0 AND date (DATE_CREATED_TS) &gt;= current date - 120 days ORDER BY p.DATE_CREATED_TS DESC Also, how can I make it so the AGENT_ID can be a variable I can plug in at the beginning of the query. Not important, but would be nice to know for the future.
THIS! This is what I was expecting to see. Most closely resembles the examples I tried to model my own attempt after. Thank you!
Analytics Analyst at the American Automobile Association. AAAAA. Please only hire somebody whose first and last names begin with the letter A.
Someone asked me if there's opportunity for remote work: Not right out of the gate. The people here that have remote access appear to have worked here for 6+ years.
looks like the link has expired, btw?
Not exactly sure why that happened, but I'll leave this up for a little while in case it comes back up. Anyone reading this keep an eye out on indeed or glassdoor for something similar to marketing analyst for autoclub of southern california.
Granted I don't really know the San Antonio job market that well, but 70-80k seems low for this skillset.
Why do you require the ability to operate a motor vehicle? 
you can read my book it is for beginners : [The technical language Transact SQL (MySQL)!](https://www.amazon.com/dp/B01LDB3934)
it's simple : Select Count(*) as cnt, LAST_NAME, FIRST_NAME, SRC_SYSTM From TABLE group by LAST_NAME, FIRST_NAME, SRC_SYSTM or Select Count(*) as cnt, DISTINCT LAST_NAME, FIRST_NAME, SRC_SYSTM From TABLE [url for ebook!](https://www.amazon.com/dp/B01LDB3934)
thanks but they don't have book titles in the option to generate 
lol you're right, thanks anyway 
This is written by an HR director. It’s just over specified. The pay is way more negotiable and the requirements are actually more lenient. I generally re write these but didn’t have a chance today
Stay away from this one like the plague. First hand experience. Just don't.
Can you elaborate a little more on where I'd set that up? I'm done for the night and going to come back at it again tomorrow. 
So if you look at the SQL, nothing is over 255, and in fact none of the 255 columns are even close to 255 but I was having issue before with one of the columns set to 143. These are irrelevant alias types and there is no reconsidering anything. The CLAUSE column though on mpVariable is a true (MAX), it could be huge. &gt;But there's literally just a task called 'Data Conversion' in SSIS you can add to the data flow between the source and destination in SSIS and alter the fields there. Syntax/set-up can be a bit odd so you might want to google it (and make sure you stay with the unicode ones (dt_ntext/dt_wstr) if you are using nvarchar). Thank you. This version of SSIS I'm using is integrated with Visual Studio and a bit different than the version I was used to using. Everything is kind of the same and I've managed to work my way through it fairly well, but this issue has been lingering. 
It's odd as they got rid of ntext/text from sql server itself years ago, but it's lingering on in SSIS. But the data conversion task is straight forward. Also probably want to set up your truncation failures (unless you want it to fail on truncation, which you might) in case some of the memo fields are &gt;255
You’re close. Lookup the HAVING clause. Depending on the size of your data, probably the easiest solution. https://www.techonthenet.com/sql/having.php
of course not
your JOIN is missing the ON clause
No
Meal name is decimal. I would use one table for Stretch and LiftingSet and name it exercise and have Type column that would determinate what kind of exercise it is. That way you can easily add running, swimming and another kind of sports. I would drop LiftingRep table as it way to normalized, it can be stored in same Workout table so each exercise would have it's value. You can also think more abstractly about it and name it "Repeat" or similar instead of LiftingRep, since then it could be reused for other kind of activities (Running laps, no of stretches etc.) The rest looks fine for me. 
I turn caps on for SQL keywords (select from where) but I'll use only lowercase for object names, like the fields and tables. It helps me visually break it apart, I also format a particular way for my own sanity. But to each their own, I have maintained plenty of other peoples code, the only thing I absolutely can't live with is all lower/upper written in one single line. If it's more than a few fields and joins then it becomes unintelligible to me, I'll drop it in an auto formatter to give it some kind of organization. 
Nope. I write my code and then use RedHat to apply the formatting that I like before deployment.
Some SQL tidying utilities: https://stackoverflow.com/questions/1093106/tidy-for-sql 
Everything in caps?? You monster.
I can't even remember the last time I used caps lock.
No and I stopped using caps in general years ago.
Some people give me shit in the office for doing this, but I actually type all lower case, but I took the dictionar of all reserved words and wrote an AutoHotKey script to replace all of the lower case spellings to CAPS. It looks nicer. I also did this for common mistakes I make, such as select 8 from::SELECT * FROM
I use my pinky and left shift. I'm not an animal. 
Uppercase for SQL things such as GETDATE, FROM, WHERE, AND, OR, DATEADD, smaller case for things like mm, dd, mi, ss, etc. Proper case for objects, i.e. dbo.CurrencyConversion. Upper case for A.Alias. In the beginning I always write in lower case which I call, "angry SQL," and my management is keenly aware that anytime they receive work from me with an attached query in lower case that it hasn't in my opinion yet been fully validated or allowed to ferment. When I am finally happy with something I will go back and rewrite/format it. I refuse to look at code by other coworkers unless it is formatted to some degree, and will constantly lament how we have production quality code that is in a state of total disarray. Someone has to be that guy. It doesn't take that long to format your code to some kind of standard and make sure that it is legible and easily transitioned to new developers. If I inherit some old code that is written without formatting then that is the first thing I do before making any modifications, trying to understand it, etc. 
Totally agree, except the rdbms I use ignores case so I always use underscores instead of camel case on naming things 
It isn't about whether it ignores case or not, it's a function of whether another developer can sit down and start working with your code right away without having to format it. If your code is finished and production ready then it should be formatted to some kind of group standard. You could use all lower case and trailing commas, or all upper case and leading commas. Just need to pick something and generally stick with it.
I mean, they do give us the shift key for a reason. 
Caps for command words, lower case for objects. My coworker with 25 years of SQL experience enjoys all lower case except for field names that he capitalizes the first letter of each word and made the database case sensitive. I fucking hate it.
I switch it on for generics and ok for objects
Having some random issues. Should I only bother doing the data conversion for the offending columns and then importing those into the Excel file, or map/convert every single column? So my process involves truncating tables and in a perfect world this would be rolled back if the process failed so that the SSIS packages can never leave the database empty. Are there any other clever ways I could configure the export/import process. Currently both are separate packages that have separate directories. The export job drops a file in its directory called "INCOMING" where I can go in and make edits, when finished I can save the file to the separate job's folder in "OUTGOING." It's been awhile since I've used SSIS and I wasted a lot of time fucking up the file templates and not backing them up, metadata going missing, etc. I have both jobs (their entire folders) archived in RARs in case something goes wrong so I can delete them and just extract the most recent working version.
Here, just created a [github repo](https://github.com/timeddilation/SQL-AHK-Script) to toss it in. This is my short list, which is usually the one I run, but I removed the other parts of the script not related to SQL. Also, you'll see I've made my set of keywords. We use soft deletion so I deal with those columns a lot.
So it seems that the memo file type is not working with any of the data conversion options, so I am going to try rebooting my export job and changing it to long text (which is what it was originally, five or six tries ago) and then see if I have any luck.
You should only have to map the types you want and everything else flows through unconverted. The Data conversion task section of [this access instructions](http://oakdome.com/programming/SSIS_DataConversion_AccessToSQL.php) sort of lays it out, or here's [another overview](https://www.tutorialgateway.org/ssis-data-conversion/). Transaction handling in SSIS is pretty complicated iirc (and it's not on by default). I never actually used transactions in SSIS. I'm not that fond of it in general, and just usually use it for basic stuff (like from csv files), move them to 'staging' tables in the DB, and then do more complex transforms in the database. You can have a sql job call the SSIS package if you are trying to automate things. You can also have SSIS loop through all the files in a directory, though I haven't implemented that.
Same here, but my IT group has asked for deliverables in Excel and it is literally making me go fucking insane.
Doesn' intellisense automatically capitalize most SQL keywords?
UPDATE: For some reason something in the package was automatically switching the connection back to the prod environment which didn't have the new column added. There is no other expressions in the connection then to a connection string which is linked to project parameters to quickly switch it back and forth from dev to prod or uat. The package is being rebuilt from scratch complete to overcome the problem.
Good to hear. Glad you got it sorted. 
I use sql prompt so it formats it for me. So I don't worry about it.
Shit’s expensive but I couldn’t manage without Toolbelt.
Append this to your query. AND NOT prod_id = 58
Here. https://docs.oracle.com/database/121/SQLRF/ap_keywd001.htm
I always uppercase the reserved words, but I never use caps lock. I just use shift. To me, caps lock is for continually typing in uppercase, and to use it for just one word, like select or delete, is slower than using shift.
I think the 071 looks like a better scope of topics and you should just focus on that. Certs aren’t super valuable (anywhere I’ve been), but you’ll have to learn that content anyway so why not. At first though I wouldn’t go back to school to get into a SQL career. I also wouldn’t mentally lock yourself into a dba role, there’s other areas using sql (think a standard biz analyst running queries) to get an entry position. I would get super comfortable with the topics they have, using both an existing db - not sure your setup but Oracle generally comes with an HR scheme that’s good for practice, and also create your own data model and “re-implement” everything with that. Not sure what field you have experience in, but more of it will translate to your next job that you anticipate so don’t undervalue historical/any project experience. Hope this helps—
Quickest way is to remove the joins one at a time till they work. On mobile so can't see the above in a useful format.
note the comment where you`re missing the FROM clause -- SELECT m.* , c.country_id , c.country , cat.cat_id , cat.genre , act.act_id , act.actor , cr.crew_id , cr.crew_member , cr.job FROM lu_movie m INNER JOIN ( SELECT mc.movie_id , c.id as country_id , c.name as country FROM movie_countries mc INNER JOIN lu_country c ON mc.country_id=c.id ) c ON m.id=c.movie_id INNER JOIN ( SELECT cat.movie_id , GROUP_CONCAT(DISTINCT cat.cat_id ORDER BY cat.movie_id SEPARATOR ';') as cat_id , GROUP_CONCAT(DISTINCT cat.genre SEPARATOR ', ') as genre /* YOU'RE MISSING THE FROM CLAUSE HERE */ INNER JOIN ( SELECT mg.movie_id , c.id as cat_id , c.name) as genre FROM movie_gender mg INNER JOIN lu_category c ON c.id=mg.category_id INNER JOIN lu_movie m ON mg.movie_id=m.id) cat ON m.id=cat.movie_id INNER JOIN ( SELECT mp.movie_id , mp.person_id as act_id , per.name as actor FROM movie_people mp INNER JOIN lu_proffesion p ON mp.profession_id=p.id INNER JOIN lu_person per ON mp.person_id=per.id WHERE p.id='1' OR p.id='2' ) act ON m.id=act.movie_id INNER JOIN ( SELECT mp.movie_id , mp.person_id as crew_id , per.name as crew_member , p.name as job FROM movie_people mp INNER JOIN lu_proffesion p ON mp.profession_id=p.id INNER JOIN lu_person per ON mp.person_id=per.id WHERE p.id!='1' AND p.id!='2' ) cr ON m.id=cr.movie_id
Thanks so much for the thorough response! My career counselor recommended that I get a certification before I start looking, which is why that's been my focus. I'm not super locked into a DBA role either; I just wanted to jump in and learn as much of the basics as I can. I figure I can specialize later since right now my goal is just to get a new job. I have Oracle 12C and the HR schema installed, but I'm not sure I set up the permissions correctly since there are some things I haven't been able to do. I've been able to practice basic SELECT statements on it though. I was thinking about maybe trying to do a clean install on virtual box or something, but it's been kind of tough to figure out how to do this on my own. I've also played around with a Pokemon database in SQLite, but I got stuck when I realized SQLite didn't have some of the functionality I needed for the queries I wanted to write. Also, if it helps, my background is in biology. I'm currently a research technician. One of the reasons I started getting interested in databases is because I keep a spreadsheet with all the different proteins I've expressed for different people, and I wanted to be able to be more efficient with looking through how much I have of each protein, in what types of cells they were expressed in, and when I expressed them. Thanks again!
I prefer lower case everything except column names. Column names are typically upper case if db isn't case-sensitive. I guess I'm backwards from most folks.
Everything seems to be working fine, but when I go into the file and add some new data and run the job the new data is not making its way into the database. Any thoughts?
I'm whole now. Learned that you have to point SSIS to the Excel tab with the $ at the end of the name and not the regular tab name in order to pick up any changes. 
Do you have the rows set to fail on things like truncation errors? Or continue? (fail is default so if you haven't changed it, it should fail)
Wow, I hope to never have to deal with that!
Ugh, a CSV would have been so much simpler and always what I've used in the past but for some reason our "ETL team" has decided to use Excel files as our standard... and for some reason they use SAP to do the data integration instead of SSIS... but they use SSIS to run the materialization process once the data is loaded using SAP... and they want me to be able to save data from my workspace environment to Excel and email the data so that it can be imported to the production environment...instead of just copying the tables and their contents from server to server... and want me to use SSIS to automate the process so that the datatype conversions are consistent. Seriously.
Didn't know that had a name. I "dude shift" all the time. I only use capslock when I'm staying in caps fir a while (which isn't often).
Absolutely not, in fact I use plenty of lowercase. 
Thank god for Redshift and Postgres
Don't show this to any perspective employer. Save it though. I've saved some of my scripts I wrote during my first days with sql.. They are horrendous. It's quite an eye opener to go back and realize how much you've learned. 
[removed]
you can either write a program or an sql procedure and use variables in that sql and then call the program/procedure with input parameters from another one. theres also dynamic sql where you can put together the whole sql code into a string variable and then call it. i dont know what system you're on and what programing language you use. 
- are you sure the sql is correct? just because it worked before doesnt mean it must still work. maybe a table changed or theres new/different data in it. maybe try a command that should work 100% or check if you get an error if you run the sql on the system you're connecting to? - maybe recheck ODBC connection (including port) - check if antivirus/firewall blocks you - check if mysql service is running - maybe reinstall mysql ODBC driver
thanks i fixed it, it was the firewall blocking the port i just created a new rule to open the port and fixed it I installed xammp and sql workbench recently, that might have messed things up with the port thanks again 
how do your tables look like?
&gt; And then remove the parenthesis using De Morgan’s laws best example i've ever seen to describe how that works nice article! 
I cannot take this article seriously just based on the icon 
Just read an article this morning that addresses this. No bronies, though. [Consider using [NOT] EXISTS instead of [NOT] IN (subquery)](https://www.red-gate.com/hub/product-learning/sql-prompt/consider-using-not-exists-instead-not-subquery)
Depending on your database it'll be possible one way or another to import a CSV, but if you're familiar with basic excel formulae you might find it easier to just generate SQL update statements from your worksheet directly, for example update clients set name = 'John Smith' where id = 99; which would be from a formula looking something like this, if you've got the name in cell A2 and the id in B2 ="update clients set name = '"&amp;A2&amp;"' where id = "&amp;B2&amp;";" You can then just copy that formula for the 500 rows, and then paste the whole lot into your DBMS and run them. This isn't necessarily best practise but if it's a one-off job it's a quick and easy solution.
Mysql has a LOAD DATA from local infile command. You could load your mapping into a temporary table, then update with a join?
I've seen CASE syntax be used in an update query to accomplish this.
Wouldn't you just join on the ID and then update the Client name after you load the file into the database?
If I understand the problem correctly, I think this is the query you want to run: UPDATE t1 SET t1.whole = t2.whole, t1.retail = t2.retail FROM table t1 INNER JOIN ( SELECT manufacturer, style, color, MIN(whole) AS whole, MIN(retail) AS retail FROM table WHERE item_size &lt;&gt; 'QUOTE' GROUP BY manufacturer, style, color ) t2 ON t1.manufacturer = t2.manufacturer AND t1.style = t2.style AND t1.color = t2.color WHERE t1.item_size = 'QUOTE'
You are absolutely amazing, thanks! 
I've never used a Join like this... Any idea on why it is giving me incorrect syntax near INNER? 
Maybe try dropping the word "inner" from the query? JOIN is shorthand for INNER JOIN in MySQL anyway. I'm afraid I'm not as familiar with how to do this in MySQL specifically so I probably messed the syntax up somewhere... [Here's the link to a tutorial on UPDATE JOIN syntax in MySQL](http://www.mysqltutorial.org/mysql-update-join/)
sqlbolt.com, w3schools.com/sql, sqlzoo.net
codecademy
Ah so the syntax is the same as T-SQL. I don't know why I got mixed up when I was reading the MySQL syntax for it... Glad you got it working. Thanks for sharing the final answer
https://www.1keydata.com/sql/sql.html
Install SSMS and import a database, then start writing queries and asking it questions. Group by, etc. It really isn't that hard if you have a programming background. Joins are a little weird, and NULL's have some weird behaviors, etc. but as far as basic proficiency it shouldn't take more than a few hours of working in a meaningful context.
Sqlzoo is how I learned. 
A book.
One of the best online course. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Install MariaDB/MySQL/PostgreSQL/etc... and learn SQL for free. 
For things like that use exists. Here is a nice explanation of the diference. [count(*) vs exists](https://blog.jooq.org/tag/count-vs-exists/) select case when exists (select * from tuples) then 'Not Empty' else 'Empty' end from sysibm.sysdummy1
Use a formula to create values table and join to the table that needs to be updated. Excel Formulate: =",('"&amp;A2&amp;"','"&amp;A3&amp;"')" WITH Client_new AS ( SELECT * FROM(VALUES (&lt;UniqueCode1&gt;,&lt;newName1&gt;) ,(&lt;UniqueCode2&gt;,&lt;newName2&gt;) ... ,(&lt;UniqueCodeN&gt;,&lt;newNameN&gt;) )a(UniqueCode,NewName) ) --SELECT o.UniqueCode,o.Name,n.NewName UPDATE o SET Name = n.NewName FROM Client_new n JOIN Client o on n.UniqueCode = o.UniqueCode 
If they dropped the education / certs / sharepoint / Telerik, then it seems about fair. I'd say the education and certs should boost it up 20k at minimum. Sharepoint if you are good at it, should really boost it up 10-40 depending how much you'll have to put into it. And I've never heard of Telerik.
https://community.modeanalytics.com/sql/tutorial/sql-in-mode/
You are trying to insert 5 fields into a table that expects 7 without telling it where things are going. While ID may be identity and timestamp may have an auto value, how do you expect it to know you're not trying to shove '28392' into ID? UPDATE [TABLE_NAME] SET [FIELD_NAME] = 'new value' OUTPUT '28392', 'TABLE_NAME', 'FIELD_NAME', deleted.[FIELD_NAME], inserted.[FIELD_NAME] INTO [log_activity](APPLICATIONNUMBER,TABLE,FIELD,OLD_VALUE,NEW_VALUE)
DB2 (at least starting from version 10) has "fetch first X rows only" that you can use in a subquery. Simply fetch a single record - if you get a result the table is not empty.
It would
First, that would throw an error, you can't insert a null into an identity column. Second in order to even be able to try to insert a null into an identity column you'd have to set identity insert on and you would STILL have to name your insert fields, so you'd be adding ID to the beginning of that list and timestamp to the end for absolutely no reason. 
key to this problem is finding the average salary first SELECT AVG(salary) FROM daTable next, using the above as a subquery, grab the absolute difference, and sort descending, taking the highest one SELECT salary , ABS(salary - ( SELECT AVG(salary) FROM daTable ) ) AS abs_diff FROM daTable ORDER BY abs_diff DESC LIMIT 1 
You'll need SQL server too, not just ssms. Development edition is free and doesn't require a license
Jesus Christ, thanks a ton. Desc 1 didn't work for some reason. I made another subselect with where rownum = 1 and got what I wanted :))) 
Yeah my example wasn't well thought of to be honest, ID is an identity so it expects to not be given a value and to not be referenced on an insert unless you set identity insert on. In fact it will throw an error if you try to reference it in an insert. It doesn't "count" in the ordinal positioning. But all other fields do and they either require a value be thrown at it or to be omitted from the insert by specifying the insert fields. That should make things clearer I hope **Edit:** Quick addendum, this will only work for fields that have default values which in the case of your table is all of them because all of your fields except ID are Nullable since you didn't define them explicitly and NULL is the default for everything unless otherwise specified. 
&gt; Desc 1 didn't work for some reason that's because it's `DESC LIMIT 1` rownum? really? can i see what you did? i'd like to confirm you're actually running MySQL
That was what I meant to say instead of SSMS, thanks.
SELECT * FROM (SELECT employee_ID, abs (salary - (SELECT avg(Salary) FROM people)) AS abs_diff FROM people ORDER BY abs_diff desc) WHERE ROWNUM = 1 Sorry if I made a mistake when I said mysql. But it works :) 
The where conditions are effectively making some of your JOIN's an INNER JOIN instead of a LEFT JOIN. your problem is probably there. Use a ISNULL or move condition to the left join ON area. 
here's an alternative, assuming MS SQL Server -- SELECT TOP 1 employee_ID , salary , ABS(salary - (SELECT avg(Salary) FROM people)) AS abs_diff FROM people ORDER BY abs_diff DESC
This is one way using "simple" sql (wrote it for oracle) http://sqlfiddle.com/#!4/df273/5
Here's another sample. It contains two SQL statements for you to use http://sqlfiddle.com/#!4/df273/7
No. The idea is to find out which certifications are best recognized and work my way backwards into studying for it and hopefully have an entry-level analyst position again. I'm a lawyer at the moment and have not had any luck finding work in that field.
I'm curious what that does
SQL Fiddle of this: http://sqlfiddle.com/#!18/9767d/1
Hi there, thank you so much for the tip! I moved all my WHERE conditions into the Case When, played around with the code for about another 2 hours, and I finally have the result I need, but ultimately you were correct. Thank you!
This is a query I often use to gauge applicants ability. It's a recursive CTE table H that cross joins to itself forming N which ends up containing column i with values 1-10,000. The resulting table n is often referred to as a "tally" table or "numbers" table. A proficient SQL user would have come across it and used or abused it at some point. Here's another version nof it with dates: [SqlFiddle](http://sqlfiddle.com/#!18/9eecb/7038).
&gt; A proficient SQL user would have come across it and used or abused it at some point. I've got 30ish years of programming experience with dbms's from informix to Oracle. I'd self identify as a proficient SQL user. I have not come across any such thing. I would have to puzzle through that from the inside out and I've never used any such thing in my actual day to day SQL work. Mostly it looks like the kernel of one of the "Lets do this fancy thing because we can" that's a complete headache to tear apart when the original author is long gone and they hire a consultant to come in and enhance it.
Sounds like you'll want a cascading trigger... looks like there's Postgre examples here: http://database-programmer.blogspot.com/2008/05/database-triggers-encapsulation-and.html
oh cool, i was looking into using the ON CONFLICT action after an insert to recursively trigger itself. but this looks like exactly what i was hoping for.
Sweet join syntax, bro, where'd you get it? The early 90's?
My time to shine! I've used them a few times. They are great for converting strings of varying lengths to single values. "3435 2315 5688 3457 4568" becomes "3435" "2315" "5688" "3457" "4568" And you can do it with a very fast SELECT rather than looping through the string looking for spaces or commas or whatever. 
I got the MTA from Microsoft. I wouldn't call it "respected" but the fact that I worked for it at all combined with extensive Excel experience was enough to get me offers for two entry-level(ish) data analyst positions. Learning SQL (and specifically T-SQL and the SSMS environment) was a not-insignificant time investment. I studied at least four hours a day, every day, for a month before I felt confident enough to take the exam. (In retrospect, I crushed the exam, so maybe I didn't need to study THAT much.) After the MTA, MS offers certs (MCSAs) that are open to everyone but marketed toward people with some work experience in the field, and these can carry some weight. You can do these from the ground up too (and you don't even need the MTA to pursue them), but they're noticeably more difficult, and typically you need to pass 2-3 exams before you get the cert. So you're probably putting 3-4x more work toward these certs than the MTA. You probably don't need the MSCA (or even the MTA, or any other vendor-specific cert) to land a job in the field. That said, the MTA was relatively cheap (&lt;$300 after exam + prep material costs), and the material learned along the way made me reasonably competent in SSMS.
Very helpful! This is exactly the type of answer I was looking for.
You are doing the lords work. If you want to get good then keep up what you are doing, my son.
This is a *liiiiiittle* off topic for a SQL forum (you might have more luck with Python forums), but pandas is fantastic and definitely not just for data science. There are plenty use cases for it in a BA sense and being able to create visualizations to bridge between the raw data and the analyst is key. On the SQL side, setting up a strong data warehouse is key to making your experience go smoothly.
I'm a sort of hybrid developer / fisheries biologist. I use pandas and scikit-learn statsmodels etc on a near daily basis for statistical stuff related to what I really should be doing in my job (data wrangling, regressions, etc...). One interesting use I found for it was writing ETLs coming from our mobile data collection. We're contracted with a mobile survey company to use their software which has major constraints to it so anyone can develop (much like access) a mobile survey form. With biologists designing these forms with no idea of proper database design all kinds of interesting data formats come over. I can do most of them with straight python but some need joins, merges, custom functions etc to be put into a normalized database. pandas is fantastic, I like it better than R which is required in my job. It's definitely worth learning. I'd recommend getting started with anaconda environments, Jupiter notebook, and a good book (Wes McKinneys python for data analysis will save you a lot of heart ache). It's primarily a 'data wrangling' tool and all the data science stuff is mostly offered in separate libraries. In my opinion as far as data manipulation it is FAR superior to straight SQL. 
true, but what I found was most people on the python forums are mostly web developers; although some use it for data analysis. I figured this would be my best shot at getting good responses. I am going to post this on the python soon to get some other responses. 
I am a DBA (Db2), and have been working on learing Jupyter Notebook. I have only scratched the surface of pandas, but do hope to learn more.
I am in the same boat. From what I hear, python is hard for anyone that over thinks it. I still have trouble...
For Oracle I think you just need to insert some 'FROM DUAL's and change the aliasing.
just curious what your specific questions are?
Have you looked at Colaboratory? It's just Jupyter Notebook that essentially runs on Google's cloud and links seamlessly with Google Drive. Was designed for easy collaboration on TensorFlow projects so it's fantastic for that kind of stuff
&gt; Can you tell me what this does? It proves you can't even write 4 lines of SQL without monumentally fucking up the formatting. Next!
The only way to learn how to code is to do it. Start with FizzBuzz, and then find other programs that you can implement in Python. It's not magic. It's just a skill that you can learn. 
&gt; ',' is much shorter than 'CROSS JOIN' But it's also extremely antiquated and relies on arcane knowledge that no one born after 1980 would have. If anything, it shows what a poor job you do of designing practical, useful screening problems.
You know. I have never been able to put my finger on it. It feels like the answer is that many of the libraries one would use are just really abstract? Like everything is done in the background and hidden behind a layer. I just want someone to sit with me as I go through some code to answer silly questions. And of course if I could help someone else learn sql or as is then it would make me feel better about the whole process.
I am, just so slow going ;) 
Several years ago marketing paid good money for a vendor to create a predictive model for potential customers. They gave us a .sql file with a giant case statement applying weights and values to attributes. It didn't look like an interpretation of a proper model either. The weights were very simple, like 0.2, 0.5 etc. It was curious, disappointing, and impossible to tell how much work was put into it's creation. Not really similar to the OP other than a "model" in SQL.
Not sure if you are kidding or not...but this works for me. I have my own methods of formatting...empty lines between joins, subqueries, etc. But case is not an issue for me.
I'm learning python and trying to teach two other guys at work, one who is a DBA, the other who is a software developer /dba - both much stronger than me with SQL. Do you have experience with other programming languages? I learned SQL first but can write c#, python, php..etc
That's how you start! Python is easy to get started I feel. I would suggest Automate the boring stuff (book) if you want a decent intro. For me, I hate those hello world projects as they are useless. I need my own projects, I need to build something I want to use. I start with a problem I want to solve then Google " how to do x with python" and I'm off to the races. I've automated stuff, built web sites and even scripted out stuff at work with python (where it's all a Microsoft stack). I have a podcast that we could record and build small things on. We've done a couple of episodes with python. 
Sorry, I meant to say I'd like to help but my time is limited and I wouldn't want to commit and let you down. 
Try grouping by `pair_id` in your subquery, so that the engine understands it's unique.
What you may consider is putting your data in a [fiddle](https://sqlfiddle.com) and edit the question with what you've tried.
r/datascience
Nope, not kidding at all, I write all my SQL with little regard to case, my IDE colour codes and formats for me when necessary so I see no reason to bother myself!
Don't ever post on stack overflow with this syntax or someone will find where you live and leave a horses head on your bed. 
 SELECT i.itemnumber , p.price FROM ( SELECT itemnumber FROM price GROUP BY itemnumber HAVING COUNT(*) &gt; 1 ) AS these INNER JOIN item AS i ON i.itemnumber = these.itemnumber INNER JOIN price AS p ON p.itemnumber = i.itemnumber 
much appreciated :) , i removed the join and even without it, it does the nested loop which you can see here https://explain.depesz.com/s/lv7p
Thank you everyone for reading this, I found the solution myself, this is going to sound really ridiculous but if you add a duplicate column in both the tables and do an INNER join instead of a CROSS join, it works 10x faster lol because the query analyze eliminates the nested loop. I have updated my question to include the answer but seriously does that simply mean you can replace your cross join with an inner join by adding a duplicate column in both tables, that's the most absurd thing I've ever seen
Solid book and I also recommend learning by a project. I don't mind a basic 3-4 hours of intro crap on sites that gets me kinda used to syntax, the program, etc, but then it's best to really learn with projects. 
To be honest it's for simplification and I was wondering if there's a more elegant solution than writing CASE WHEN ELSE multiple times on the same condition
Curious. I tested this concept in MSSQL and found that nested loops (23s) outperformed a hash join (25-27s) for a cross join. Assuming the db engine can repeatedly read a table from cache, nested loops has less overhead when joining every record with every record. Nested loops skips calculating hash values and organizing records into a hash table. DECLARE @starttime datetime2(3) = SYSDATETIME(); //hash join SELECT max(a.ID + b.ID) FROM dbo.Number AS a INNER JOIN dbo.Number AS b ON a.ID*0 = b.ID*0 WHERE a.ID BETWEEN 1 AND 10000 AND b.ID BETWEEN 1 AND 10000; SELECT DATEADD(millisecond,DATEDIFF(millisecond,@starttime,SYSDATETIME()),CONVERT(time(3),'00:00:00.000')); SELECT @starttime = SYSDATETIME(); //nested loops SELECT max(a.ID + b.ID) FROM dbo.Number AS a CROSS JOIN dbo.Number AS b WHERE a.ID BETWEEN 1 AND 10000 AND b.ID BETWEEN 1 AND 10000; SELECT DATEADD(millisecond,DATEDIFF(millisecond,@starttime,SYSDATETIME()),CONVERT(time(3),'00:00:00.000'));
You could write out several queries and UNION the results together. Depending on your case, that may be faster than the CASE statements. It'll also be easier to follow what's happening for the next guy who picks up your code. The head developer at my current gig loves case statements. I rewrote one of his queries to ditch the case statements just so I could easily follow what the code was doing. It ran in under 1/10 of the time of the old one...
What about this: create table #alpha (cola int, colb char(1), colc int, colx int , coly int , colz int) create table #beta (cola int, colx int, coly int, colz int) insert into #alpha(cola,colb,colc,colx,coly,colz) values (101,'Y',3,7,9,2) insert into #alpha(cola,colb,colc,colx,coly,colz) values (102,'N',4,7,8,3) insert into #alpha(cola,colb,colc,colx,coly,colz) values (103,'Y',5,7,7,4) insert into #beta(cola,colx,coly,colz) values (101,2,3,8) insert into #beta(cola,colx,coly,colz) values (102,4,5,1) insert into #beta(cola,colx,coly,colz) values (103,6,9,5) select A.cola, A.colb, A.colc, colx = isnull(B.colx,A.colx), coly = isnull(B.coly,A.coly), colz = isnull(B.colz,A.colz) from #alpha as A left outer join #beta as B on B.cola = A.cola and A.colb = 'N' 
since this is being run on merely a local mac installation with default settings, i doubt it is getting parallelized but i have no idea why the system in postgresql can be fooled into thinking that an inner join is better than a cross join, did you observe something like that in mssql server
I tricked MSSQL into doing a INNER HASH JOIN instead of a CROSS LOOP JOIN, but it wasn't better. In both cases, the query was paralellized.
care to share some examples..? :D
in side my e1_alerts table i currently have a primary key and then a column that contains strings of the form XYZ:ABC where XYZ is found in sources and ABC is found in destinations, i could have 2 separate columns in e1_alerts where one contains XYZ and the other contains ABC. e1_alerts is a table that could potentially have millions of rows at a time, e1_sources will always have around 2000 values in it and e1_destinations will always have around 100 values in it, each, while dealing with XYZ:ABC from e1_alerts every value before the : will be found in e1_sources and every value after the : will be found in e1_destinations, while i havent tested your theory, if we assume 1 million rows in e1_alerts 1500 in e1_sources and 50 in e1_destinations, stage 1 would be an inner join with e1_destinations giving one set of values, stage 2 would be an inner join of the result having 1 million columns with e1_sources giving another set of values, and then i ll have to check the condition, will give this one a whirl, thanks for the suggestion
This is how I'd do it.
 SELECT A.a, A.b, A.c, A.x, A.y, A.z FROM A WHERE A.b = TRUE UNION ALL SELECT A.a, A.b, A.c, B.x, B.y, B.z FROM A INNER JOIN B WHERE A.b = FALSE Or maybe something like this SELECT A1.a ,A1.b ,A1.c ,COALESCE(A2.x, B.x) ,COALESCE(A2.y, B.y) ,COALESCE(A2.z, B.z) FROM A A1 LEFT JOIN A A2 ON A1.key = A2.key AND A1.b = TRUE LEFT JOIN B ON A1.key = B.key AND A1.b = FALSE 
How are you joining Table A to Table B? If, as you describe, x, y, and z are the only columns they share and those are the conditional columns, exactly how do you know which row from Table B corresponds to Table A? 
Thank you! I was able to get it to work using ROW_NUMBER.
Thank you, I was looking into using CASE as well. This helps a lot!
So is this better: WITH H AS ( SELECT i = 0 UNION ALL SELECT i+1 FROM H WHERE i&lt;99 ), N AS ( SELECT i = ROW_NUMBER() OVER (ORDER BY (SELECT 1)) FROM H CROSS JOIN H a ) SELECT * FROM N
If I am reading this correctly you are probably wanting to group the data by product and use an aggregate function to sum up the total number sold. Additionally, the product name is held in a separate table so you would want to join the table in that contains the name. Is there a date column in the checkitem table so that you can limit it down to items sold today? Something like: SELECT I.ItemName, SUM(CI.Quantity) AS QuantitySold FROM dbo.checkitem AS CI INNER JOIN dbo.item AS I ON I.itemnumber = CI.itemnumber GROUP BY I.ItemName ORDER BY SUM(CI.Quantity) DESC
Hi Ash0r, First thank you for replying. As far as your question there is a date column in table checks column name date. You can join table checks with table checkitem with column checknumber as its in both tables. I am having a hard time figuring out how to sum up when the item is sold individually and when the item is sold in quantity because of the way its stored in the checkitem table. For example Table checkitem column checkitemnumber column quantity 1 1 1 2 
LOL, that's great.
 select sum(quantity), item from sales where salesdate &gt; date(current_timestamp) group by item order by 1 dead limit 20 Your db syntax may be slightly different.
Glad to see you liked it! Learned about 2 new things today as well so it was a fun little project.
Yeah, me too. Ultimately I plan to develop this idea quite a bit and plug it into a Tableau dashboard where I can show it off to my boss' boss, and the rest of my coworkers. He has a bit of a problem of telling people he'll call them in 5 minutes and it taking an hour.
Try changing the = sign to LIKE
Is this really what you want? It seems like you could end up with a lot of columns in your result.. and you wouldn't know how many columns at design time - which could lead to some dynamic SQL. what is it you're trying to do?
Thank you for the help but didn't work. I get the column headers and a single drop down box.
Yeah I realize one reason to why this isn't ideal is storage. But I was really curious if someone could think of an easy solution to this problem. I was trying to figure out a query to get values in field1 that doesn't have this one particular value that is really popular in field2.
Are fields 1 and 2 on the same table? We should start there by defining this or show us what you've tried. 
Weird. It’s almost like you are querying the form - not the table that the form is based on. 
How big is the table? How many records match `Price = 3725`? How many match `Price &gt; 3725`? 
The obvious way to do it is: SELECT s.subredditName FROM RedditUsers ru JOIN SubredditUsers su ON su.userId = ru.id JOIN Subreddits s ON s.id = su.subredditId JOIN Posts p ON p.subredditUserId = su.id WHERE ru.username = 'morpen' GROUP BY s.subredditName HAVING COUNT(*) &gt;= 2 As far as "what's the *fastest*" that depends on: * How big each table is * Where the indexes are * How fast the storage provider is * How much memory is available * What the overhead on the system is * Etc. In other words, it's impossible to say without knowing a lot about the system. You can't tell much from the basic schema and a query. 
The table has 10 records One match `Price = 3725` Six match `Price &gt; 3725` 
That's really far too small to draw a meaningful conclusion about how the query planner works. Try adding 100,000 rows. 
The purpose of the SUM aggregate is to give you the total quantity sold instead of just the number of records. If you can imagine for a moment, when you group the results by the ItemName you will get a single row returned per item. This grouping allows you access to special aggregate functions. SUM, COUNT, AVG, MIN, MAX being the most common. If you were to use COUNT against the quantity column it would just tell you the number of rows that were returned. Max would give you the highest single quantity in the result and SUM would give you the...sum :) What I supplied above should get you most of the way there. You will need to add a where clause to filter by the current day and a top to limit to 20 results. I am probably missing on some of the names here, but something like this? SELECT TOP 20 I.ItemName, SUM(CI.Quantity) AS QuantitySold FROM dbo.checkitem AS CI INNER JOIN dbo.item AS I ON I.itemnumber = CI.itemnumber WHERE CI.Date = CAST(GETDATE() AS DATE) GROUP BY I.ItemName ORDER BY SUM(CI.Quantity) DESC
I've worked with Oracle SQL/MySQL and Python pretty much every day for the past few months at my job (as a DBA). I love using Python connectors like cx_Oracle/MySQLdb, because they allow Python code to replace the PL I'd have to write otherwise (namely sequences, triggers and stored procedures). I'd recommend trying out one of these packages to ease your transition into Python programming. There are many things that Python does much better than Client software.
the result of char(0) is based on the charset and collation settings of the server therefore your mileage may vary https://dev.mysql.com/doc/refman/5.7/en/charset-server.html
Thanks i will try that. Interesting that removing the `order by Price DESC` part from the second query changes its plan to a clustered index scan. 
Just add a `LIKE` check to your where clause... --MySQL SELECT * FROM Students WHERE FirstName like CONCAT('%',NickName,'%')
thank you
Probably not exactly what you are looking for, but something like this? DECLARE @StartLoc int , @LineLength int = 150; WITH cteFixedWidth AS ( SELECT CHARINDEX(CHAR(10)+CHAR(13) /*end of the previous line*/ +' 12345' /*right aligned 10 character integer 12345*/ ,FixedWidthData) AS StartLocation , 150 AS LineLength , FixedWidthData FROM FixedWithFiles WHERE FileName = 'Something') SELECT SUBSTRING(FixedWidthData,StartLocation-LineLength,LineLength*2) AS Result FROM cte;
Don't use SQL to manipulate this unless there's a gun to your head. SQL isn't for manipulating data like you will need to do here. Write a script with PowerShell, Python, or C# (in SWISS perhaps).
Awesome! I'll try this tomorrow and report back.
Possibly with a PIVOT operator
You need to pivot into your age categories... https://technet.microsoft.com/en-us/library/ms177410(v=sql.105).aspx.
but it gies the "no authroized routine" error
In my experience floats run into conversion issues more than precision issues. I like to store SLAs in the same type as the data (or calculation). Otherwise the same calculation can come in above or below SLA depending on when the data is converted. SELECT SUM(CONVERT(money,100*RAND(CHECKSUM(ID,RAND(ID))))) AS AboveSLA , SUM(100*RAND(CHECKSUM(ID,RAND(ID)))) AS SLA , CONVERT(money,SUM(100*RAND(CHECKSUM(ID,RAND(ID))))) AS BelowSLA FROM dbo.Number WHERE ID BETWEEN 1 AND 13; 
Yeah, I'm with you, but the variance you're talking about is .00000n% across the 20+ million rows that my dataset comes out in. That is not at all relevant to me and gives me an ease of integration advantage. 
Sure they can, but not in the way that I am using the data. The values are being input in Excel and then uploaded into the database and simply used as values for third party consumption.
as stupid as my comment is gonna sound, i'll still add it, why cant you convert all of them to big integers and do whatever operation you want and then divide them again, lets say, you want to store 3.14159625 , simply store it as 314159625 and do whatever math and then convert it back, any problems doing this
dude that worked!!!! you rule!!! while its a 3 table join, it works waaay better than nested loops, is it advisable to overburden the database with 3...n table joins, i mean dont they burn up all their RAM doing this,i was wondering if i should materialize view with a primary key on 2 tables and then join on the view all the time vs doing a 3 table join but maybe i am overthinking things
Integer division always truncates and is something to generally avoid. Suppose you want to divide pi by 5: SELECT 314159625 / 500000000 AS [Integer] , 3.14159625 / 5.00000000 AS [Decimal] Integer|Decimal -:|-: 0|0.628319250000000000
&gt; pseudo DBA for a 100ish size tech company in CA and make $60k for a relative figure I hope you are in LA, or anywhere besides the Bay Area, because one would expect you to be making nearly double that
Sacramento.
But I'm connecting to the same mysql server. The client host is the varying one. Would charset be affected here?
If you know the really popular field2 value, the EXCEPT clause comes to mind. SELECT field1 FROM table EXCEPT SELECT field1 FROM table WHERE field2 = popularValue;
&gt; But I'm connecting to the same mysql server. That's not what you said on your original post though.
I think you are missing some line breaks. Your Question is unreadable.
Sorry I've been trying to format it as a table but now I've just created it with line breaks.
https://dba.stackexchange.com/questions/138672/fill-in-missing-dates-with-data-value-from-previous-populated-date-for-group
Not following the last statement but how about a cross join to get you to the second table? Could filter the date dim by dates between a and b then cross join to the first table (filtered to one product if needed)
It sounds like you either need a WHERE clause WHERE ProductID = ? AND MAPPING_CODE = ? or an ORDER BY clause. ORDER BY ProductID ASC, MAPPING_CODE ASC, DATEID ASC If the product 123 does still exist BETWEEN 20180202 AND 20180228 but you only want it where the dates are unique, then there'd be a couple of ways of doing that.
Sorry I didn't make it clear, I am connecting with different client hosts, but to the same server. Doing ('test','','test') gives me testtest. I created this 'test', char(0), 'test' as an example so I can share it here. The original problem I have, involves reading raw binary data from a blob field, that happens to have some null characters. Once I hit a null character, the stream stops.
Solved! I'm just joining the queries and saying if q1.A is null, use q2.A, else q1.A (similarly for B and C). And then pulling T1 and T2 (and getting NULLs when appropriate) Thank you!
Sounds about right. CASE is more flexible. IIF is just less typing if it's simple. Both perform similarly. 
I think you need to use a while loop. Like WHILE date_id &lt;= 02022018... This will cycle through the process and stop when the date gets to that, then start over.
A WHERE clause wouldn't work for this schenario as we have a 100's of products. This was just an example data set. For example product 456 would need to be displayed with all the dates until a new file is loaded or todays date.
We don't have while loops in the database we're using unfortunately.
you need a date table with all the dates. left join with the date table with a where clause. Where date &gt; start_date and &lt;=end_date. example: Select p.ProductID , p.MAPPINGCODE , d.dateid from products as p left join dates as d on d.dateid &gt;= p.DATEID and d.dateid &lt;= getdate()
use the COALESCE function
Then I would check what's different between the two hosts For instance I recently had an old piece of software that's been working for well over 10 years suddenly stop functioning on one clients machine. After adding a bunch of message boxes all over the place I finally isolated the error to a cdate() statement that was trying to convert a hard coded future date (no clue why since it's not my code), thing is that cdate() worked everywhere else. Turns out the client had changed their computers region settings to french so the cdate() did not understand the "31 Mars 3011" response it was getting. 
I’d been putting it in my select clause this whole morning! Talk about a case of the Thursday’s haha thank you man. Solved! 
Remember your WHERE is your filter part of your query.
 SELECT column FROM table WHERE substring(table.column, 9, 1) = '9'; Something like this would work. You'd be able to select the entire text of the column, by use of the substring function.
 DECLARE @Table TABLE (ProductID INT, MappingCode INT, DateID DATE) INSERT INTO @Table(ProductID,MappingCode,DateID) VALUES (123,45,'20180101') ,(456,33,'20180202') ,(123,45,'20180228') /*** Use this if you have access to Lag/Lead funtions ***/ SELECT T.ProductID, T.MappingCode , DATEADD(dd,ROW_NUMBER() OVER (PARTITION BY T.StartDate ORDER BY (SELECT 1))-1,T.StartDate) AS EnumeratedDate FROM ( SELECT ProductID, MappingCode, DateID AS StartDate, ISNULL(LEAD(DateID) OVER (ORDER BY DateID),GETDATE()) AS EndDate FROM @Table ) T --Convert source table from a series of dates to a series of date ranges CROSS APPLY ( SELECT TOP (SELECT DATEDIFF(dd,T.StartDate,T.EndDate)) 1 AS n FROM sys.all_objects s1 CROSS APPLY sys.all_objects s2 ) S --BS SQL into giving you a variable number of rows --WHERE T.ProductID = 123 --Use a where clause to limit results if you want ; /*** Use this if you do not have access to Lag/Lead funtions ***/ WITH TL AS ( SELECT ProductID, MappingCode, DateID, ROW_NUMBER() OVER (ORDER BY DateID) AS n FROM @Table ) --Create rowIDs to spoof Lag/Lead functionality SELECT T.ProductID, T.MappingCode , DATEADD(dd,ROW_NUMBER() OVER (PARTITION BY T.StartDate ORDER BY (SELECT 1))-1,T.StartDate) AS EnumeratedDate FROM ( SELECT T1.ProductID, T1.MappingCode, T1.DateID AS StartDate, ISNULL(T2.DateID,GETDATE()) AS EndDate FROM TL T1 LEFT JOIN TL T2 ON T1.n + 1 = T2.n ) T --Convert source table from a series of dates to a series of date ranges CROSS APPLY ( SELECT TOP (SELECT DATEDIFF(dd,T.StartDate,T.EndDate)) 1 AS n FROM sys.all_objects s1 CROSS APPLY sys.all_objects s2 ) S --BS SQL into giving you a variable number of rows --WHERE T.ProductID = 123 --Use a where clause to limit results if you want ;
Format shouldn't be an issue in how you store dates on your table. Store as the appropriate [date data type](https://dev.mysql.com/doc/refman/5.7/en/datetime.html), and format when presenting to the user. If you are storing a date as a string, you're doing it wrong. When inserting, use a standard format like ISO8601 or variable of the appropriate type in your [Prepared Statement](https://dev.mysql.com/doc/refman/5.7/en/sql-syntax-prepared-statements.html) (instead of concatenating strings). Don't use a proprietary convention within the database itself, else you'll be reformatting &amp; checking all the time.
This sounds like a limitation of your query analyzer, not the Oracle database.
I've never seen SQL in excel like that. I myself do a lot of VBA/SQL Scripting. Why don't you just set it to something like 
could join to a subs select, selecting the last event date. Something like this. inner join (select eventsno, max(eventdate) latest_event from dbo.events group by eventsno) latest_data on latest_data.eventsno = e.eventsno and latest_data.latest_event = e.eventdate 
&gt; SELECT WHATEVER &gt; FROM WHATEVER &gt; WHERE ID IN (ID1,ID2...ID15) AND WHATEVER I'm not sure if I follow. I am not sure if I follow... I am a rookie when coming to VBA and SQL
A cte would work. I didn't look too hard at your tables / columns so will need adjusted appropriately, but something like: WITH RecentMatterDate as ( Select row_number() over (partition by mattercode order by eventdate desc) as RecentDate , mattercode from matters ) Then in your where clause: ... AND e.MatterCode in (select MatterCode from RecentMatterDate where RecentDate = 1) ... 
Assuming what you mean is that you type in 15212 and 15592 in the green fields in your screenshot and that returns all the id's between 15212 and 15592. What you want to do is: In D18 write: " and egait2 in ( " In E18 write the ID you want so in your example: " '1000', '1043', '1209' In G18 write: " ) " leave H18 blank. The SQL TRIM field on row 18 should say: "and egait2 in ('1000', '1043', '1209'). What this does is it filters the query to return only rows where the column egait2 has 1000, 1043 or 1209 in it. The ' ' around the numbers signifies that the value contains letters, not just numbers. In your example it is used for the numbers also which suggests to me that the column datatype is some kind of string. The egacdt looks like a date column so you can also filter by date the same way as above if you want. 
Open another sql developer instance. I agree with /u/da_chicken it’s an IDE issue. Opening another SD instance and connecting should be an okay workaround.
For ages I've used a CTE to handle this scenario. Never thought about putting it in a join. 
I'd suggest you look up the modulo function in SQL. That and [NEWID()](https://docs.microsoft.com/en-us/sql/t-sql/functions/newid-transact-sql) should get you a start to how I'd try this. I'm assuming this is homework so I'm not spoon feeding you a solution.
Even easier would be to write an excel macro that connects to the DB and refreshes the data of an excel table based on any of the columns that contain your parameters getting changed...
It's actually not homework. I have more complicated scenario, but this was the easiest way to describe what I needed. I basically need to input values from table 2 randomly into table 1. While Table 1 has millions of records, the second table only has about 300k.
I'm not familiar with Python. I do have a loop running to basically add in these "Teams" into the "Students" table for each day, but I end up with some days having null Teams or when I union the Team tables together to generate a larger pool, I end up not hitting every team.
 SELECT destination_id, count(id) as count FROM my_table_list WHERE t_l_date BETWEEN #03/03/2018# AND #05/03/2018# GROUP BY 1; ^ Pretty sure this should do the trick. 
As a recovering developer and current DBA, I *loathe* code stored in databases. Doubly so when it's stored as fragments that are later pieced together.
Same. I'm stubborn and love ctes though. Plus a join can be a lot more complicated in different situations
Won't get pushed to production using that. Ultimately I will need to trigger an email and save a copy in a network drive to have the mapping values consumed. I specifically want to write to Excel from SSMS.
You can connect Excel to your db via the data tab and run a sql query (an sp or view would be cleaner) to update specific tables, and simply hit refresh in Excel to obtain new data from the db. Inserts from excel to sql are a little more complicated. But with ssis and a consistent file protocol (shared folder and consistent naming conventions), are pretty straight forward. No need for macros
As a dba, why would you loathe code stored in the database? Maybe I'm misunderstanding your reason but that's exactly where I'd want it.
This sounds about as time consuming as copy and pasting. I was looking for a more automated source. I already have the SSIS package built.
You want data in the database, not code.
 AND ISNULL(b.RowNumber,1)=1
Data entry grid like MS Access?
If code is the main entity of your database, then it is data. It will likely complicate things, though.
Looking for a web based solution. They won't use access,excel,Google sheets, ect. 
The cte would just be the same as the sub query above, and (in either case) you could put `b.RowNumber = 1` in the join criteria. That would avoid needing to test for null in the where clause. 
In this case you could make the argument that the code itself is the main entity of the process. We have clients across the globe and very different people attached to very different clients, but all of them have contractual KPI's written into their agreement... the catch being that a) different salespeople may write different KPI's/requirements/use different language, b) different clients will also ask for different KPI's, requirements, or use different language. The actual query to get the correct answer is a fairly simple `sum(x) / sum(y)` but each client can have dozens of KPI's... so all in we're talking about hundreds if not thousands of micro-customizations in order to come up with a table where all the results are stored and a senior executive can look to see exactly how the entire collage of clients are performing relative to our contractual obligations. In my opinion it is not realistic to create 1 sproc per client per calculation, or to write 1 query per client per calculation and execute them all in a chain in one huge sproc... especially if I need to update 50 calcs in the same way for 5 clients and switch a field to a new field, etc. With this process I can do all of those things very easily and SSIS pretty much automates the entire up and down process. The big loop runs in about 15 minutes and produces ~20 million rows (we only have 10 calculations written so far and anticipate writing another 30-80.)
There is a php admin tool for mssql. I've never used, but I have used the one for MySQL. It had a data grid. It's clumsy, but maybe you could cut it down to just the parts you need. Good luck!
I was in a grocery store a few years ago. Went from intern to Sysadmin to DBA in 2 years. If you do good work, the only thing that matters is getting your foot in the door.
I used to be like you. Then, instead of doing just my own development, I started building solutions around existing products... closed-source, proprietary software whose code, queries, and user interfaces are _hot fucking garbage_. When those shitty vendors let me play with their data processes by rewriting a stored procedure, view, or trigger I have control over as a DBA, I am a hero to my clients and employers, and have made their awful product slightly (or even markedly) less terrible. And, I can still version control the definitions for those database objects, as long as I put a modicum of effort into change management and migration scripts, where necessary. So now I prefer that versatility, and I still architect great software that includes adaptable, extensible code that customers can bend to meet their needs, even if it's not open-source, and even if they have SQL pros on staff but nobody fluent in other languages.
&gt; As a recovering developer and current DBA, I loathe code stored in databases. Doubly so when it's stored as fragments that are later pieced together. I'm not entirely sure what all tricks SQL Server offers, but as a database developer - I LOVE code in the database, if it's the RIGHT code to be stored in the database. Since we're an Oracle shop, I can take advantage of Oracle's packages. Any calls/access to the database for our applications each gets its own API package written along with a few utility/common APIs we've written for ad hoc report writing and common calls. This ensures I don't have some app developers hitting our databases with some shit-ly written inline SQL queries or an ORM that throws us a jumbled mess. Additionally, with this - we can take advantage of these reads/writes to the database being optimized and cached in the database. 
Yay for self-joins! Another way is a recursive cte.
You definitely need a cross join, which is basically a Cartesian product.
The Ace ODBC driver can query Excel files as well as write to them. I don't recommend it as there are too many limitations.
I don't think that will work, I'm purposefully using 97-2003 XLS files and the Jet driver.
I agree with everything you said, but I think OP would also need to make F18 blank for your instructions to work. 
Have you looked at Google forms? Entry like an application form that fills put a google sheet.
If I have to join on a subquery I turn it into a CTE. That's my stylistic rule.
Can you use staging tables, and add a step to the SSIS package to add the staged rows to excel and then delete from staging? 
I think I threw up in my mouth a little reading this. 
Sounds like your current job is a nice fit - that's worth a lot. If you have the interest, though, with your you could be making 90k+ I think 
If you're using SQL server have a look at the lag function, that will allow you to get the value of the previous row so you can then do a comparison test to see if one value is greater than the other. I don't know if lead and lag are available on other SQL providers unfortunately.
mySQL, unfortunately -- don't believe those are supported. Am also trying to nest some statements with criteria a.date &lt; b.date and a.ID = b.ID to calculate a difference from previous. Then, trying to select all columns where count of negatives = count of items within ID.
Do you know in advance what val2 and val3 are, or are you just looking for matching pairs in general? 1) You can try a subquery with INTERSECT on Val2 and Val3, then return the rows that returns. 2) Join on CAST(m.Val2 as varchar) + CAST(m.Val3 as varchar) = CAST(n.Val2 as varchar) + CAST(n.Val3 as varchar) 3) use EXISTS 
I do not think it matters DB optimizer will probably do the same thing in both cases.
 SELECT val2, val3 FROM M INTERSECT SELECT val2, val3 FROM N
How about this? SELECT * FROM ID AS ID1 WHERE EXISTS ( SELECT * FROM ID AS ID2 WHERE ID2.A = ID1.A AND ID2.C &lt; ID1.C AND ID2.B &lt; ID1.B ) Query assumes that (A,C) pairs are unique.
 SELECT TO_CHAR(e.timestamp, 'DD MM YYYY') as date , COUNT(DISTINCT e.user_id) FROM events e WHERE e.user_id IN ( SELECT user_id FROM events WHERE event_name IN ( 'user_create' , 'room_message_send' ) GROUP BY user_id HAVING COUNT(DISTINCT event_name &gt; 1 ) GROUP BY TO_CHAR(e.timestamp, 'DD MM YYYY')
Code sitting in a stored proc is miles away from code stored in fragments in a *table*. This looks like a nightmare to maintain.
Hi, that does in fact produce the perfect return! Typically it is much simpler than what I was attempting as well :D Unfortunately I have been an idiot and missed a requirement from the OP. Because this return is going into an Ember model, I am required to also return a column called 'id' in the response. In cases where I do not have an 'id' column I have been producing a fake one with the following: ROW_NUMBER( OVER (ORDER BY t_l_date) as ID When I try and add this to the above I run into either 't_l_date must be part of the group_by' error or 'window functions are not allowed in group_by' depending on how I try to change it. Any ideas
This may sound crazy, but I would look at Powershell first if I was handed a project like this. I think Powershell makes a great bridge between SQL and readable formats like Excel, csv, etc. 
Smssbost
Is this SQL database hosted on your local machine or remote? In either case, you can find out the hostname by typing 'hostname' in the command prompt (on your local or remote machine).
Do you connect to your database through another client tool currently? If so it has the connection info you need like the server and database name. I'm guessing you're not the admin of your PostgresSQL server. Do you know who is? You could ask them for the connection info. Feel free to msg or chat me if you need more direct assistance. 
Oh, I wholeheartedly agree. I was speaking to the general case, from before /u/alinroc made his edit.
I think I'd try: DECLARE @COuntTeams INT SELECT @CountTeams = (SELECT COUNT(1) FROM Teams) SELECT * FROM Students JOIN Teams ON (StudentID % @CountTeams) = TeamID
There are some pitfalls here: you'll actually want to load Teams into a #Table and join that way as you'll need a teamID to = 0 and there to be no gaps in Team ID for this to work correctly.
Yeah, I worded that poorly. Love stored procs. **Hate** code fragments in tables.
Stored procs are fine, it's code in tables that I despise. If you're storing code in a table, you've got multiple layers to dig through just to find your source code. I worked on one platform many years ago that stored **all** the code (TCL or VBScript, dealer's choice) in the database. Manual versioning. If you had a runtime error, you couldn't get a usable line number to trace back to. You couldn't step through the code via a debugger. Another system use a proprietary language where you could embed SQL queries (or SQL fragments) in the code (which then got stored in the database). Again, craptacular debugging. All manner of issues resulting from commingling your code and data, in part because the system was designed to execute anything that looked like SQL as SQL - or stripping out reserved words or symbols, mangling the user's input. Straight up SQL fragments in a table? Now you're two layers deep into dynamic SQL here. I don't really have a problem with dynamic SQL; I use it every day. But putting the query fragments into the table itself means more concerns about properly handling your code so you don't risk SQL injection, and you're (as above) going to have a hell of a time figuring out where to start debugging because the issue could be in the SQL fragments, the code you use to glue them together, or the resulting SQL - once again, several levels of debugging and picking things apart just to figure out where things are going wrong.
Thanks! I knew there were multiple ways to skin a cat, but I figured it might make for an interesting post. The first option makes more sense to me, but not sure it really matters.
you gotta format that sample data.
good catch :)
The first option is usually cleaner, and there are some corner cases with nullable columns that don't always give you the results you expect with the second option.
There are other fields and information that is important about the middle day. I agree with what you are saying, but just for this business case it doesn't work :( 
Is this homework or are you actually a hotel employee tasked with doing this? If this is homework, show me what you have so far and i will help.
It won't work because some of the fields are identity fields, you can't insert on them. I have to change the tables after the data is imported. 
Or if the day is not equal to the min/max of the stay; then it's a middle.
So what? If it can accomplish what I am trying to do then it can be part of an automation process. I have no problem abusing technologies to get them to do what I want, but you seem to have an issue with it. If it solves a problem elegantly and improves a process, then it is a good thing.
I have that set up already, but that isn't exactly what i'm asking.
If they're duplicate, would not a distinct on the insert resolve it?
Actually no, I have a specific table set up where each fully completed and compiled query is stored so for trouble shooting, modification, querying to see all of the queries that use FieldN, etc., there is a one stop shop for all of your needs. Debugging in this world as opposed to maintaining hundreds of similar but different queries and storing them in a long sproc, or having dozens to hundreds of sprocs for each single one of them are not viable solutions. 
It isn't an ideal solution, but it is a solution that works quite well and solves for a large organizational problem without having to change the way multiple global teams change the way they operate. From a cost management perspective it is the best solution. If you have another one that will achieve the same end results I am all for hearing it. If you don't and just want to talk about best practices then I am the wrong person, and this is the wrong thread.
Unfortunately not. The issue is not that my query is returning multiple results that are duplicates. But it's trying to insert results into the reference table that already exist on that reference table. I need the insert to ignore those rows if they already exist.
Then just left join the reference table on the values that are duplicate and only insert where the values are null. Example: *** SELECT a, b FROM sourceTable AS src LEFT JOIN targetInsertTable AS trgt ON src.a = trgt.b AND src.b = trgt.b WHERE trgt.a IS NULL AND trgt.b IS NULL ***
I'll give that a try, but there are several hundred tables, if I have to do it for each one it might not be worth it. Thank you though.
Yeah that’s absolutely true. I thought OP was running into an issue where the query progress was in a pop up like TOAD (Non-DBA) does. 
so is the id column correct in this sample data? what is id?
Anybody? 
one day stay = last day? two day stay = first day and last day. three day and plus = first day, mid+0.X, last day is that the pattern rule?
With a single script you should be able to turn identity off (or on, in always forget), insert the data, and then flip it back after the historical inserts with the same script, just reverse of what you did.
You have two options it sounds like: 1. You can simply get rid of 'as count' from the SELECT statement and the column will render as "id" since that's what we're counting. 2. If you want the date column to render as id: SELECT destination_id, count(id) as count, ROW_NUMBER( OVER (ORDER BY t_l_date) as ID FROM my_table_list WHERE t_l_date BETWEEN #03/03/2018# AND #05/03/2018# GROUP BY 1, 3;
Hi, thank you for responding. I have posted again simplifying the question if it helps. 
Are you trying to copy values from each row in the Excel B &amp; C columns into SQL A column? 
Correct. Thanks.
For this portion: ( Select distinct T024.A, T024.G, T024.D, T024.I from T024) q2 on q1.A = q2.A and q1.E = q2.D inner join
Is this MSSQL? Import export wizard, import into a new table, update via join, drop table.
 SELECT b.GuestID , b.StayDate , CASE WHEN LAG(b.GuestID) OVER (ORDER BY b.StayDate) &lt;&gt; b.GuestID OR LAG(b.GuestID) OVER (ORDER BY b.StayDate) IS NULL THEN 'First Day' WHEN LEAD(b.GuestID) OVER (ORDER BY b.StayDate) &lt;&gt; b.GuestID OR LEAD(b.GuestID) OVER (ORDER BY b.StayDate) IS NULL THEN 'Last Day' ELSE 'MiddleDay' END AS FirstMiddleLast FROM Booking AS b; http://sqlfiddle.com/#!18/e0e60/6
If it’s ms sql developer edition, the easiest way would be to use an SSIS package. You just have to download SQL Data Tools for free. It’s a lightweight version of visual studio. It will be a pretty simple package to make. 
I would probably use `FIRST_VALUE()` and `LAST_VALUE()`, but otherwise, yeah, this is roughly how I'd do it.
I'm not sure those would work without creating a field identifying continuous stays.
Hm, I think you're right. I guess I assumed there would be a reservation ID in the table to partition by, but I guess that's not how it's set up.
Select field1,field2,field3,field4, max(field5) as lastestdate from yourTable group by 1,2,3,4
https://www.reddit.com/r/excel/comments/81eas0/need_calculation_to_find_firstmiddle_and_last/ 2 Weeks ago they worked for a healthcare company... https://www.reddit.com/r/SaltLakeCity/comments/7g6gko/best_way_to_get_part_time_job_at_ski_resort/ Annnnd there it is.
Honest question, but why does it matter? I mean I remember when I was learning SQL. If reddit was around then and I was stuck on something, I'd ask too.
Thank you both. I'll give these a try and let you know if I was able to get it to work.
Amazing. Thank you. 
One day stay is a first day
Totally unrelated but tacking on... Visio can be used to do some impressive things with SQL where you can set up a diagram or flowchart (e.g. for call centers) which break out each step in a process, and then you can just populate the metrics for each step in the flow from data in SQL. Really works well and I've never seen anyone do something like it before.
Try r/postgresql
What data types? Mostly text and numbers? I'd convert all to csv then combine, then go back to Excel. 
Thanks!
Will x-post there, thanks stranger!
I did this with excel power query (get data from folder, where I had the files stored) and then loaded the combined file to be used in SQL. Had never done it before and after a few YouTube videos it was super quick. 
Thanks but if I run that, I get an error "Each GROUP BY expression must contain at least one column that is not an outer reference" If I remove the group by, I get an error "Column q1.field1 is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause Any ideas? 
Are you typing in exactly what I wrote and only what I wrote?. Also regarding your edit, you make no sense. Why do you want the latest date if you’re gonna trash it at the end?
Or append the files first, if the total doesn't exceed a million rows.
Why would you convert to dev just to go back to excel? Surely just appending straight in to excel would be better. Not having a dig, just wondering if I'm missing something.
If this is to be a one off, then all the good advice offered below can help you. In general, though, xls / csv files are of course human-based and so can be subject to structural and data type changes without warning. To preserve your sanity, consider writing some basic source validation code that will check the structure of your files before loading them if this is to be a recurring load process. 
I’ve always consolidated data in excel, concatenated an insert statement for each line in excel, then just copied and pasted those inserts into ssms. 
I'd stick with import utility if you need to crack on and just get done, will create the table and you can ensure your data types are respectable. You'll end up spending more time reinventing the wheel over just completing the exercise.
This is also what I do. First I copy out columns that are strings into Notepad++, use regex or multi-cursor to add single quotes around them, paste back into the sheet. I save it as a CSV, actually separated by commas, open whole file in Notepad++, again use regex or multi-cursor to add the INSERT statement at the beginning of each line, then save the file from .CSV to .SQL I typically find this to be faster than the import wizard.
Postgres 10 now offers logical replication. I haven't used it but read in about it. Before pg10, they only offered physical layer reocation. Postgres uses MVCC where it keeps dirty data unless vaccum is run. There is very good blog on this by Uber engineering team. They used postgres for replication but switched to mysql because postgres mvcc was causing them problem. Please refer to that blog, it is very informative. 
Pluralsight offers some content on more advanced SQL stuff such as window functions like you listed. There is a lot of intermediate to advanced topics covered across a lot of videos. I created playlists for all the different stuff I want to learn.
Convert all xls to csv, then use log parser 2.2
something like this? http://sqlfiddle.com/#!4/bdd0b6/1
This doesn't check if user 12345 bought the same three products as 54321 
 SELECT a.CustomerID , b.CustomerID FROM CustomerItem AS a INNER JOIN CustomerItem AS b ON a.ItemID = b.ItemID AND a.CustomerID &lt; b.CustomerID GROUP BY a.CustomerID , b.CustomerID HAVING COUNT(*) &gt;= 3;
This works great, thanks!
Loops are bad use a tally or a calendar table to unpack the dates
I would start with database design before moving onto SQL. I started learning mysql without really knowing how to create a database properly and didn't realized until college...
Read every "Third Normal Form" beginners' resource you can find.
If you have/can get Access, you should find that you can happily migrate Excel workbooks into Access. Access also holds your hand when it comes to developing queries, and allows you to view the generated SQL and/or write SQL directly. Much like the Excel macro recorder doesn't generate the best VBA code, and saving a Word file as HTML doesn't generate the best HTML, Access doesn't generate the best SQL, but the application itself will feel somewhat familiar coming from Excel, and you can build some powerful tools with it
IMO the best way to learn SQL is https://sqlzoo.net/
Use Airtable.
Get some books by Itzik Ben-gan. I have been writing sql for almost 20 years and just started in one of his books. It’s solid.
If your solid in Excel why not learn VBA? You don’t master Excel until you master VBA. I learned SQL and then a few years later needed to turn thousands of Excel reports into SQL inserts in order to create a database. After learning VBA I learned Python. I am now learning PowerShell. The point is that you shouldn’t put all your programming eggs in one basket. SQL is probably the easiest to learn and will give you a good background in how programs utilize data. However, with your background in Excel I would recommend starting in VBA.
First be sure that you really need a database. There's a shit-ton more to database work than just writing queries, and a table of 30,000 items is frankly a drop in the bucket for what a good RDBMS is designed for. That said, Access is a very good place to start. Ignore stuff like forms at first and just work on getting data in place. Play around with data types and constraints. Refer to the SQL Editor often. Read up on logical query processing (Itzak Ben-gan has some great articles on this). SQL is not processed from top to bottom like your typical procedural languages. Understanding this will go a long way toward writing good queries.
Is this homework? I'll lead you down the path of.... Sum Over ... (Partition By). They are called windowed functions, and will do exactly what you want. If you show me what you've tried, I can guide some syntax.
&gt;Its same to this but what I'm trying to do is add up all the records that has the same "time" value. That's what the first example in your link is doing (the one he asks for help with), grouping by time reqardless of the date, no? I'm confused at how what you want differs.
Sorry, I'm drunk and it's late but I think this is a terrible idea. I have been well exposed to normalization, and I will agree that it is an important concept, but it is materially irrelevant to someone new to databases. In fact, it is materially irrelevant to someone who uses databases on a daily basis. I break normalization rules *all the time* because it saves me time, helps integrate with integration, etc. Normalization is an important concept, but if you are just starting out it is just something worth introducing as a general idea... it isn't at all relevant to using databases to automate your work. All it's going to do for you is save disk space, and improve server performance. Given an unlimited amount of disk space and an infinite number of servers then normalization has no practical use at all. For someone who is transitioning from Excel files to the world of databases this is (by definition) a meaningless concept.
Time to get into some PowerShell. Check out [dbatools](https://dbatools.io/) and specifically [Restore-DbaDatabase](https://dbatools.io/snowball/). Grab the list of backup files and either loop through or pipeline them to restore one at a time.
A window function will work, but is not necessary, you can just sum your values and group by datepart hour. You'll need to concat a string for output presentation Select sum(occurrence) occurrences, datepart(HH,timestamp) + ':00' occurrenceHour From Table Group by Datepart(HH, timestamp) + ':00'
second powershell the only way to manage such an operation. structure the code so you can handle relocate of db files if needed
&gt; structure the code so you can handle relocate of db files if needed The module/function I pointed at already does it :)
Try this https://stackoverflow.com/questions/19428189/best-script-to-restore-multiple-databases-with-sql-server-2012 
I really don't know, it's been a long time since I read anything and I could use some advice too.
Third vote for powershell! 
&gt; Assuming what you mean is that you type in 15212 and 15592 in the green fields in your screenshot and that returns all the id's between 15212 and 15592. That is correct Thanks, this sounds correct, I wasn't aware that it would be this simple though. What about cell B18?
spRestoreGene... But really... Dbatools.
&gt; I need to restore these backup files to the similarly named databases. I have done some restores manually by using MSSMS restore option, finding device, choosing the option overwrite with replace and no tail log backup. This works, but will take a lot of time. Here's the old school way to do it if you don't want to script it with PowerShell. Configure the restore process once for the first file. When you're ready to restore, instead Script To New File. Now, copy the RESTORE statement and use it as a template for the other operations. You can do that with Excel and the CONCATENATE() function, or any other various methods. 
Yep this is the easiest way. Just add "where timestamp between (start date) and (end date)" to get your range. Then you can add "order by timestamp desc" to sort the output
Thank you so much for this! I'll definitely give this a try but just in case, where would you put this code along with the sample code in the link I provided? Thanks again! :)
I highly recommend SQL for Mere Mortals by Viescas and Hernandez. I was in similar situation to yours and found it very straightforward. Basic principles of database design are covered there. Authors guide you from very basic concepts to more advanced ones with clear explanations and lots of examples. 
&gt; You can do that with Excel and the CONCATENATE() function, or any other various methods. I'm not ashamed to admit that for a one-off kind of repetitive thing, I've used a spreadsheet to create the commands to paste in a shell. I often like to set up the command, change the bits that change to things like XXX, YYY, ZZZ, then a chain of `SUBSTITUTE()`s to make all the permutations. It especially helps if your command needs quotes around filenames and such.
Sorry for the late response, I forgot I've put this question here. I'm using an anonymous block because I'm using CQN. In this block I call a Procedure that inserts a row into a table when CQN sends a notification. The thing is, when CQN calls this Procedure, instead of inserting a single row was inserting 4. I believe this is because I created 4 anonymous blocks calling the same procedure. I didn't meant to do that, I just realized what I did when I was getting those 4 inserts instead of one. So I was wandering if it was possible to see those anonymous block and deleted them. At the end I changed the procedure name and created a new anonymous block to call this one. That solved the problem.
How much do you make?
Came here to ask the same question!
Check to see if there is an active SQL Server User Group in your area. If there is, find out when and where they are meeting next and go to that meeting. Usually they'll have a live or remote speaker who will teach a topic for an hour or so. So... free training, and sometimes free food. Very good resource to start networking around.
Also, sqlpass dot org is a fantastic resource for finding a user group as well as online material.
Honestly, this isn't a bad response, but it's the wrong one. Networking is good, don't get me wrong, but instead of networking the OP needs to learn how to market himself. All you're going to do when you network are meet other jerkoffs who are doing the same thing as you. What you want to be doing are meeting senior managers, vice presidents, and other jerkoffs who make decisions. Here is a good example. I regularly frequent an upscale bourbon bar and a few months ago I met a guy who was the COO of a 100million dollar company. First words out of my mouth were, "let me build your entire analytics platform for free of charge." Now this sounds stupid, but it's going well and isn't taking much of my time. My title for this project is **architect** and my reference is a C-suite executive who fucking loves me. This is going to look **great** in my portfolio and easily going to help me command a 20-30K salary bump at the end of this year when I finish my current contract out and move on. That's about a 4 year transition from "analyst" with no SQL experience to "architect" and a salary range north of six figures. You aren't going to meet guys like that at the local SQL group, and frankly from his post it sounds like he would be one of, if not the most experienced person at the group... meaning he'll likely be networking with juniors.
I dont know if that helps, I was looking a few years ago (2013) for some jobs in the east (specifically one in NY, one in NJ and on in Florida). Im not from the US though. I was then a sr BI and ETL developer with 10 years of experience. speciality of Oracle, Informatica, SQL, PLSQL. anyway, the NY job gave an offer of 130K, NJ was 110K, the Florida one was 122K all of them included a 401K plan and health coverage for me and my wife. I eventually rejected them all cos I found the offers too low. I worked some time at a big company everyone knows placed in San Jose, I was an outsource there for 6 month, everyone around me at the same job were earning around 150K and someone even stated that I should be earning 180K. though, you know they may have exaggerated. Today, I am earning around 100K, but I do not live in the US and here health and other stuff free or really low. so its not comparable. Also adding to this, I have no degree in computer science or anything, I just have vast experience working for me. 
The absolute first thing you should do is type "technology recruiter in __________" into Google. Start talking to about 4 recruiters. Tell them you're looking to make a move. Get your resume into shape fast. If you're not good at resume writing pay someone to do it for you. (My person that does this for me has been working with me for 5 years and she's worth her weight in gold, if you want her contact PM me.) Keep in mind that not all recruiters are created equal and you should try to do some vetting. If you live in Milwaukee hit me up, I know the best ones. Also my company is hiring two BAs right now. Being in Tech allows you the luxury of having other people do your job hunting for you...for free. You should utilize this perk.
Didn’t know such a plug-in exists
Get rid of MAX and use order by for the date. Something like ORDER BY activitydate desc
Hm, thank you for the suggestion. I will look into that idea, just having key words to search for is very helpful, so thank you.
That could be done, but the issue after that point is that I need to return only a single row per Person. So if I did that, I'd still need a step afterwards to select the row from the order by with the most recent date, per person, which boils down to the same roadblock in the original problem.
It exists for SSMS too!
So look into sub queries and that should get you there
It looks like it isn't used. Does your sheet return the correct data? If it only returns ID 15312 then delete the code in B18.
Sure, I suppose there’s no reason he couldn’t go to a user group meeting and then afterwards ALSO drunkenly commit to free work to climb his ladder of success? Am I right?
Yeah you can do both, sure, I'm just pointing out that networking isn't really the solution long term. You need to learn to market yourself, regardless of whether its working for free, etc. You need to show it on your resume, carry it with you and show it when you interview, etc. I mean he is here right now networking, and what advice is he getting? Stop networking and start marketing.
Yeah it's really a fast way to bodge one time batches together. I haven't tried `SUBSTITUTE()` before. I'll have to remember that one.
So... have a resume and a portfolio. Skip the networking and be a blowhard at a bar?
Blowhard? No. Be confident. Get a card for yourself. Give a card and get a card. Email someone and send them your portfolio. You could actually call that "networking" if you want, and it certainly qualifies as networking in the pre-LindedIn digital age. But its marketing. You need to market yourself. If you want to make money you need to market yourself. If you don't want to make money and you want to learn and make friends and "network" then do that. I'm not going to be angry at you.
If you have a unique primary key you could change your first query to insert a list of unique IDs that you want to delete into a #temptable. Then just DELETE FROM [T] FROM [TABLE] AS [T] WHERE EXISTS (SELECT ID FROM #temptable [TT] WHERE [TT].ID = [T].ID)
Assuming rowID is stored in the table and is unique for every row in the table, then yes.
Select * From Person p Outer Apply (Select Top 1 * From Activity a Where a.PersonID = p.PersonID Order by ActivityDate desc) act 
That's hilarious. I can't believe it didn't occur to me to just rename one of the other columns to id. Thanks for the help, much appreciated
$43,000
I believe the closest group is 70 miles away. I should have mentioned that I live in a pretty low population area. My town is around 12,000 and 20 miles from a town of around 40,000 and 70 miles from 170,000 and 100,000. I don't think there are a ton of SQL job opportunities without long commutes or moving. (but I do not know this for sure, since I really have not looked)
I used to be good at resumes. I have not done one in a while though. I have considered a recruiter. I should have mentioned that I live in a pretty low population area. My town is around 12,000 and 20 miles from a town of around 40,000 and 70 miles from 170,000 and 100,000. I don't think there are a ton of SQL job opportunities without long commutes or moving. (but I do not know this for sure, since I really have not looked). A recruiter could tell me, if I could find one. I do not live in Milwaukee (but I do love that area). I am not really in a position to make a major move right now. I would have to wait a few years until my wife retires from teaching and my daughter graduates. Then I think we are open to changes.
I do get what you are saying and appreciate all angles of advice, but I just don't think this is me. If it works for you, that is awesome. I suppose in the right crowd, maybe with people I already know, I could do something like this. But just "cold" with strangers? Not me.
Yeah, I joined PASS a few years back and never really did anything with it. The closest chapter is 70 miles away. The closest SQL Saturday is 2.5 hours away.
I was able to restore with dbatools. Thanks!
I really want to use this as it looks powerful and is super customisable. However, it always seems to mess up tabs and spaces in SSMS for me, automatically converting them in strange ways. 
Try this: https://whoishiring.io/ there's a decent amount of remote work going for SQL server. If you don't like the idea of working from home I suppose you could frequent a cafe or rent an office. Probably the best chance at maintaining or boosting your income / work challenge without moving. In terms of your resume, I think your post almost a cover letter. To "market yourself", add some description of what the impact of your work was on the business, e.g. made reporting more efficient saving what used to take X hours, improved data integrity by combining data in a repeatable and testable way that used to be done manually, etc. Good luck!
It didn't used to be me, either. Change.
He's 43 and makes less than 50K a year. Going to a SQL forum is the wrong advice. &gt;I may be wrong, but "get a business card", "resume", "portfolio", "market yourself", "be confident and get yourself paid" isn't exactly the sage advice that he came here for. I'm 35 and make twice his salary with less experience. I don't have a college degree. It might not be the advice he came here for, but I assure you it is the advice he needs. Tell me I'm wrong.
Time to relocate. You can literally triple your salary or more if you market yourself.
If you're in the Midwest doing everything a BI administrator or BI developer does then you should be earning at least twice what you're earning now. Go to dice.com and upload a resume as soon as you have one finished. You'll have recruiters calling you constantly within days, if not hours. If you want some assistance, PM me and I can send you mine along with some other resources. I need polish mine up anyway. Best of luck!
not currently hiring, but in the past our technical interests focus on: - knowing the number of databases / servers, sizes of databases and large tables... are you working on one server with a few million rows or less, or are you used to 50m+ tables... we *will* ask how you address performance, expecting familiarity with several concepts. - knowing your DTS environment... how many packages, what are their designs, how many rows and what runtime (thus throughput)... this mainly boils down to knowing how to get the most performance out of your platform, but since we have so much going on, we expect our folks to know how to write *good* processes (we don't care about 100% ideal, but at least 80% would be better than a lot of the sucky things we've seen / had to fix) throw in some soft skills, etc.
Time to relocate. You can literally triple your salary or more if you market yourself. By the way, being in a small market without many resources around is a HUGE plus if you market yourself towards freelance work. Why? Because it's hard to find resources, and a lot of smaller companies are afraid of being taken advantage of. You have an extremely lucrative skill set. Boutique consulting firms in this field charge well over $100/hr and can go as high as $300/hr. In the Midwest. But you have to learn how to market yourself.
It isn't about that. The man is here asking for advice about making money. You want to make money? This is how you do it. Not just in our field, but in any field.
I've been using "[SQL Pretty Printer](http://www.dpriver.com/products/sqlpp/ssms_index.php)," which I like. I tried downloading this poor mans formatter for SSMS, and I'm not sure how to use it... I don't see any buttons or options. Any advice?
So it sounds like you are inserting into a table where all the fields are nvarchar(255) but then it sounds like there is a FLOAT in one of the tables you are selecting from. you can just CONVERT(nvarchar(255),columnName) on the fields you are selecting and it should fix the issue rather then combing through all 40 tables trying to find the FLOAT.
Are you familiar with Brent Ozar dot com ? First of all, great technical resource. Second, salary survey. My first instinct is that you're under paid. https://www.brentozar.com/archive/2017/12/tell-us-make-2018-data-professional-salary-survey/
My code looks like this: Insert into AA select * from ( select * from dbo.sales_201718$ UNION select * from dbo.sales_201719$ UNION select * from dbo.sales_201720$ UNION select * from dbo.sales_201721$ ... ) Is there any way you can tell in the select part of the code to just read the values on those tables as nvarchar(255)?
step 1.. have you verified the SELECT part works?
Yea the select part works, every select individually works, when i union some pair of them it doesn't work for the same reason the big one doesn't work
yeah, you have at least 1 float column in one of the tables. There are a few ways you can do it... Insert 1 table at a time until you get an error. Then you will have to list the columns and CONVERT the FLOAT column. You could do the same but try several tables at a time. For your sake, I'm kind of hoping all the tables have the same column names in which case you could get creative with Replace All
it is a 1 time query, i loaded all the excel files into SQL and i just want to join them up
when i get this error : Msg 8114, Level 16, State 5, Line 37 Error converting data type nvarchar to float. which one represents the column? the Level or the State?
&gt; Msg 8114, Level 16, State 5, Line 37 None, that is the error message for trying to convert a varchar to a float. Are you sure there are no floats in the table you are trying to insert into?
Oracle uses a two-tier processing model. The users and user processes make up the client tier. SQL*Plus is one tool that is used for establishing user sessions against the server processes. The server tier is made up of the mentioned server processes (the "workers" that actually execute the SQL statements you send from the user session in the client tier), the database instance, and the database itself. You mention Ubuntu so I'm assuming you don't have a database client installed on your local machine. I'm assuming the database you need to connect to is on a server somewhere else. You'll need to get the host name or IP address of that server and your log in credentials from whoever installed or manages that database. The [Oracle doc - SQL*Plus Getting Started](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sqpug/getting-started.html#GUID-0A5854C2-ECDD-4068-9A9F-4BDE8AABEB51) is also a great reference tool. Play around on that site. A lot of goodies hide there -- especially in other docs. :) 
i found the problem, the previus tabla had 1 field in Float formar, the table giving me problem has the same field in Nvarchar(255) , so i need to ethir change all the previus one to NVARCHAR(255) or change that one to float
Okay, there's kinda 2 ways to go here. SQL Server is a RDBMS made by Microsoft, but a SQL server could just be referring to any server with a database on it. The actual database program is something you generally don't interact directly with. They usually run as a service and connect to them with an "IDE". SSMS is one of these IDE's and it's for Microsofts product SQL Server
There ya go! Just FYI, this happens frequently when importing spreadsheets using the Wizard. Save some time and give a quick glance before you run the import.
Thanks for the explanation. I'm trying to learn SQL myself and found out that writing query is the easy part. I got stuck at how to make use of the database that sits here. And when I try to connect with visualization tools it seems that I have to have a SQL server to do that. But I dont know how to do that... Anyways, is it correct to say that the database I have on postgreSQL is not a server, and SSMS is not a server either, but a service to manage database/server? So what do I do to get a SQL server?
Put all of this on LinkedIn, be very specific of each skill you have :) Recruiters will search based on skills, and you will start to pop up more and more. Data is the new gold :) you are in the right place, just need to market yourself. If you get approached by contracting companies DO NOT RUN AWAY, most of the people i know have started as contractors and have been converted short after (if your good). Either way its a good path into the field, you can go from being a contractor at one place to leveraging a full time position at another, just get out there.
I was able to do it !!! , i just had to list all the variables in all the tables and just Cast(variable x as nvarchar(255) ) and it worked xD. Thanks a lot for your help, i need to find a reliable way to import data, i tried several stuff and dint work, but yesterday seens as my deadline approaches i just load all 40 excel files 1 by 1 and just needed to join them together. Thanks a lot again for taking the time to anwer
No PostgreSQL is a "server" and SSMS is not. SSMS is just a text editor (and other fuctions) to connect to Microsoft's SQL Server product. Generally RDBMS (what the database server is technically called) each have their own editor like. RDBMS | Editor ---|--- SQL Server | SQL Server Management Sudio Oracle | SQL Developer MySQL | MySQL Workbench And more. And there's also CLI tools like SQL*Plus but we wont go into that. Personally I haven't used PostgreSQL so I couldn't tell you where your issue is with getting it into viz software, but if you're writing queries then you should be able to
Thanks!
As long as you are willing to move, you can get a job paying in the mid $80K's in most midwestern cities. If you are living in a low cost area with limited job prospects, then that would explain your salary, I've seen it before. I suggest you hit up some BI consulting firms, they will allow you to live anywhere as long as you are willing to travel during the week.
Omg that table makes sense out of a lot of my confusion. Thank you so much! When I tried connecting viz software (particularly Power BI) to it, it keeps showing this error https://imgur.com/a/NlcTt. So somehow I thought, oh maybe because its not on a server...
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/UMQLgzv.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Microsoft SQL Server was named just to be confusing. But big tech companies love to do that. 
That explains a lot of technical words that I saw in others posts but couldnt really grasp. Thank you! How do people learn to know all this T_T googling is quite overwhelming, or is it just me...
Haha thats me, same confused person. And nope I'm still stuck. I actually tried your suggestion to fill in this thing https://imgur.com/a/dxsnz but then it still keeps popping up error. So I guess hostname isnt server name?
In my case? Very gradually, over 25+ years at work.
"as always_true". No need for case or spaces IMO.
In Power BI, when you click on the "Get Data" drop down **do not** click SQL Server. Instead click "More", then "Databases", and from there you can select the PostgreSQL connection. You keep trying to connect PostgreSQL to a SQL Server connection which will not work. You can safely assume that anytime you see SQL Server in Microsoft products, they are talking about their RDBMS called "SQL Server", and not servers that run SQL databases in general.
Never #3.
I didn't know that. Will check it out. Thank you. 
In my opinion, #4 is the only correct one that you listed. String literals as column aliases are deprecated in SQL Server, so that rules out #1 and #3. Also, #2 is an old style of aliasing a column. The issue with #2 is that it can be too easily confused with an assignment. I prefer to always include AS to explicitly state that I'm aliasing a column. 
I hate 2. I generally write cases like this: select a.id , case when blah = 1 and blah = 2 then x when blah = 3 then y else z end as 'ColName'
I should not have answered this so early in the morning. I had this number in my head because I was just looking at my W2s. This is my taxable wages. My actual salary is $51,000. It didn't click until later today. 
General Rule: In the database world, any tool that starts with SS (such as SSMS or SSRS) is related to MS SQL Server.
if you want personal preferences, i like using standard sql identifier delimiters and i like the standard sql AS aliasing method, even for MS SQL also, i hate sticking unnecessary parentheses into my sql, like around your CASE expression
I mean, what exactly is the job description of a BI administrator or BI developer? I would love to see an example resume. 
I am familiar with Brent Ozar, yes. I have seen this survey before, but never knew where exactly to classify myself since everything is title based, and not a list of job activities. I don't think "Business Analyst" really fits me anymore. I don't know what a more accurate title would be.
Depends on your definition of "help" I guess... What are those "things"? Create and maintain packages, build reports?
PM me an email, and I'll anonyimize it. I will help you get to the promised land.
It was too early this morning when I posted and I entered 43 because that was the last number I saw (from my W2). My actual salary is 51. Not that it matters much....but at least it's over 50....
If you have SQL Server 2012+ a window function would let you hit the historical data and group / sum over a monthly period... you can do this is previous versions of SQL Server but it's not always as straight forward. https://www.red-gate.com/simple-talk/sql/t-sql-programming/calculating-values-within-a-rolling-window-in-transact-sql/ See #6 for the windowing function.