Decode can be faster in some cases, but it only does a comparison. decode(status , 'ACTIVE' , 1 , 'INACTIVE' , 0 , -1) http://docs.oracle.com/cd/B28359_01/server.111/b28286/functions042.htm
Indexes are part of being a schema admin, ddl_admin role is the easiest. Otherwise you're in for a [rough ride](http://itknowledgeexchange.techtarget.com/sql-server/giving-users-the-ability-to-create-indexes/).
I'm assuming you are trying to do this in 12c? A new feature in 12c was the IDENTITY column. What you were doing wrong there was trying to insert before your table was created. https://docs.oracle.com/database/121/SQLRF/statements_7002.htm#SQLRF01402 CREATE TABLE DEPARTMENT ( Department_ID NUMBER GENERATED ALWAYS AS IDENTITY, Department_Name VARCHAR2(50) NOT NULL, Main_Office VARCHAR2(50) NOT NULL, Website VARCHAR2(40) NOT NULL ); INSERT INTO DEPARTMENT (Department_Name, Main_Office, Website) VALUES ('department name','main office','website'); COMMIT; -- dont forget to commit 
Error info? You might have to explicitly define the column names youre inserting into in the insert clause, but thats not likely it based on tour primary key definition.
Oh, like "Standard" + "Carry Over" + &lt;some other categories...&gt; per employee? Sure, just add (or exclude what you don't want) as a condition (WHERE/AND): select EmpID, firstname[FirstName], lastname[LastName], 'Total'[AccTxTypeName], sum(amount)[Amount] from table where category in ('Standard','Carry Over', 'whateverelseyouwant',...) Group by EmpID, firstname, lastname 
I"m going to try that right now.. This is ultimately what I am aiming to do. http://i.imgur.com/haSC6D3.jpg
I think I am going to explode if this works :o
 SELECT Accruals.AccProjID, Accruals.AccTxTypeID, Accruals.BenefitYearStart, Accruals.Comment, Employees.EmpCode, Employees.LastName + ', ' + Employees.FirstName[FullName], AccrualTxTypes.AccTxTypeName, Accruals.Amount, 'Total'[AccTxTypeName], sum(amount)[TotalAmount] FROM Accruals INNER JOIN Employees ON Accruals.EmpID = Employees.EmpID INNER JOIN AccrualTxTypes ON Accruals.AccTxTypeID = AccrualTxTypes.AccTxTypeID Where Accruals.AccTxTypeID in ('4', '3', '1') and Accruals.AccProjID = '8' and Year(Accruals.BenefitYearStart) = '2015' Group by Employees.EmpCode, Employees.FirstName, Employees.LastName 
This is the error I got :( http://imgur.com/J2DefCh and Yeah I would totally pay someone for an hour to tutor me through this I am trying to baby step through this, instead of jumping ahead to the dividing values and adding in another hour classification
Yep, you gotta add the other columns (not the ones you're summing &amp; not the ones you're creating (eg, 'Total') to the Group by clause.
oh wow, okay, going to try that now. I took your advice and just booked a tutor for the next two weeks, but really nailing this stuff may put me in line for a promotion so I really appreciate your help on all this
You see my edits?
Now I did.. thank you.. Going to try
I can't wait to try this in the morning, i'm literally about to knock out working on this all evening, but I hope you dont mind replying to me again tomorrow. I really appreciate your help
Alright, I noticed another "group by" query you tried, and there was a "invalid column in select list" error that was returned. That means that you are adding columns to "select", but not adding them to your "group by". You don't necessarily have to add them to group by though, you could also use them inside an aggregate function (such as SUM(value), MAX(value), MIN(value), AVG(value)), but I think in this case you should just add them to your group by. I'll try to give you a better explanation tomorrow
Where can I get solutions of this contest?
Easier just to create a stored procedure with "execute as" which creates indexes? Really, nobody should just be randomly creating indexes.
I apologise for not uploading a schema, I didn't even think it would help. But wow I think you may have just saved my life and I will forever be greatful, when I get home tonight I'm gonna buy you gold. Since I've made a promise of learning SQL fully, I plan to fulfil it and this is a good place to start. So going through the command: We firstly join table B and Table C using the product ID as our join, this join is used to create a table, "WeightedProductCost" with columns: Product and Weightedcost (where weighted cost comes from table C). This is then joined with table A or "Stock" on the ProductID and the Stock.WeightedCost = WeightedProductCost for each product. Just out of interest which part did you create first? With recursive programming I tend to start from the top and work down and then when I find new features I either create seperate functions and call them or work my new feature into the original code. With this did you build the "WeightedProductCost" table first and then work on joining that with the Stock table? Just in terms of the results I had to make a couple of amendments such as help it fit into my tables and join another column and then finally run an update commend on certain values as SUM(quantity) was throwing out NAN. I was going to try and build a case but people started coming into the office so I panicked and got around this by adding +0.001 to SUM(quantity) and then set standard_cost = current_cost where standard_cost = 1. Except that the weighted cost is slightly different that yesterday (it gets updated when stock is booked in rather than on a regular basis) I think it's worked and unless someone has been looking extremely closely (which in a big business they probably do) it should go unnoticed. Thanks again.
There are a few ways, one would be using a cross join. The way I would do it would simply be inner join the tables on ID with a condition in the join that excludes any records with the value "yeye". Hope this helps. 
 SELECT a.pk, a.value, b.value FROM tableA a JOIN tableB b ON ( a.pk=b.fk AND b.value != 'YeYe' );
Doesn't work as still returns 234 B Foo 234 B Bar
My biggest concern is that of performance. My actual table1 has around 250k records. Table2 has over 3.2 million. There are other conditions on table1 that will pair things down considerably, but that doesn't help with evaluating the entirety of table2 in a subquery. Thank you very much for the examples though. I'll probably have to do something like the first one, and just deal with any performance issues.
Ah, you're right. Didn't look at the data set carefully. Exists looks like the best play.
haha 
Do you have a key you can join on, or do you need to compare if a row exists with the same values in 20 columns? In any case, I don't think a cursor is the right approach, for performance reasons. Why not use [MERGE](https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_9016.htm)? It seems it would do the trick for you.
&gt; nobody should just be randomly creating indexes. This is very true, the right index(ices) can fix a large range of problems, but the wrong ones can make them worse and waste resources while they're at it. 
I appreciate this, but unfortunately they're not looking for visualizations.. so they need the raw numbers cranked out for other reporting purposes.. It wouldn't be enough to show some charts, but I appreciate this post, thank you!
I have a unique key in both tables, an employee number. The tables are nearly identical. This is employee/HR data. Only difference between these two tables is the day-to-day changes that an employee might incur, such as a last name change, a rate of pay change, a position change, an employment status change, etc. Without getting into irrelevant detail, there's many reasons why I can't take my "refresh" table (which is a complete extract) and plug it in as a replacement to the production table. But rather I'd like to use if-then logic to check row-per-row, column-per-column, if a data value has changed between the two tables. If it has changed, or it is not found in the production table, use the "daily refresh" value. I was told by a superior to use a cursor and that's why I have to use a cursor. As far as performance, this is a table of maybe 2000 rows maximum, so I could code it really poorly and it would still do what it needs to do in fractions of a second. And this is a once-per-day event, so performance isn't prioritized. I haven't used MERGE but I'll start looking at how that may help. I agree that a cursor should not be necessary but sometimes when you're working for "the man", you have to do what you're told.
Think of it this way, the Employee ID will only represent ONE employee, and that employee will have ONE first name and ONE last name. Now even though there will only be ONE first name and ONE last name for each Employee ID, there may be a lot of employees that share the same name. For example, there may be 34 people with the first name Ben, and 17 people with the last name Hill. However there are TWO people with the name "Ben Hill". If you grouped by first name, SQL would reduce the number of Bens returned to 1, and add up all of the values that you're aggregating (SUM). Since it will only return a single row for each person with the first name Ben, you cannot select the last name because there are 33 different last names associated with people named Ben. So you add last name in the GROUP BY clause. Since there are only TWO people with that last name Hill and first name Ben, it does not include the 32 other Bens in the aggregate function. This is still not what you want, because you want to group by individual people, so if you add Employee ID to the aggregate, it will now treat the first Ben Hill and the second Ben Hill as individual people, and only aggregate data associated with that Employee ID. And you shouldn't add AccTxTypeID to the aggregate function. You shouldn't group by it either. AccTxTypeID Needs to be completely absent from your select statement (except for in you WHERE clause, since you want to SUM(Amount) based on the value of AccTxTypeID). If you want to include AccTxTypeID in the result set, you need to use UNION, which is what I did in my second example. 
Excellent points, all. Of course, I don't know the answers to most of that. :) This is a copy of a client database, that I'm working on outside of their environment, so I'm a little in the dark on server specs. I might have to actually talk to the client. (Damn. I hate that part) Table2 is basically a history table for a number of other tables, and contains foreign keys for each related table in separate columns. I can limit the results from Table2 by looking for rows where the appropriate FK is not null. That strips out about a million records, so it helps a little. Anyway, thanks again.
As is already suggested, use a MERGE. A cursor is entirely unnecessary, don't listen to your supervisor on this one (at least there is no way in hell I would in this case). For columns 1 (col1) to column N (colN) in each table: MERGE INTO production p using daily_refresh d on (p.employee_ID = d.employee_ID) when matched then update set p.col1 = d.col1, p.col2 = d.col2, ..., p.colN = d.colN WHEN NOT MATCHED THEN INSERT (col1,col2,...,colN) VALUES(d.col1, d.col2, ..., d.colN)
What version of sql server is it, and what operating system is it running on?
What you want is called a [Cartesian Product](https://en.wikipedia.org/wiki/Cartesian_product) which can be accomplished with a CROSS JOIN http://sqlfiddle.com/#!9/d039c/1
thank you!! not so unorthodox after all :)
I only have one view in my database where we do this, but that view in turn gets referenced by a lot of other views. In my example we're doing a cross join between the departments table and my dates dimension table. The reason being is that some of our low volume departments may have a gap in transactions and won't appear in every date range. Doing a cross join means when we're subsequently calculating metrics every combination of department and date has an entry for each metric. This in turn becomes important when we're doing a window average function and we want to be certain we're averaging months (1,2,3) and not (1,3,4) because there happened to be no entries for month 2.
You would want to use CONVERT(DATE, &lt;your date time value here&gt;) 
It's explicitly converting your @startdate to a date type, so it really should not be displaying the time as well. Something else might be amiss. You could try using FORMAT but I'm guessing something else is amiss if WHERE convert(date, @startdate) = convert(date, reportdate) is not yielding the proper results.
You want to declare the @startdate as a date (not datetime) variable and set it to '2015-08-07' Then your comparrison would be convert(date, [Field in question]) = @startdate To clarify, the problem you're having is that you're using datetime instead of date, not sure if my comment originally made that clear.
I went against the bosses wishes and used merge. Very few lines of code and it seems to be working okay. Runs quick. My prototype example looks a lot like yours. Thanks! 
Is the server solely for database, or does it serve other applications? (IIS, Exchange, etc...) When this occurs you can right click on the server name, select activity monitor, and the under processes you will see a list of open connections to the SQL server, and there is a column called wait type which if the process is waiting on something will be indicated there. Wait types (tend) to be pretty well documented, and search-able. Alternative you can also look at the resource wait section which can track cumulative time spent waiting based on the wait type. (Though this can also give some false readings since if a process is split into multiple threads of execution, or 'parallelism' as it is called. By nature unless every thread of that process completes at exactly the same time, it will look like a lot of CXPacket waits as those threads are waiting on that last one to complete. Which.... well, may not be good either, but such a wait type is more of a 'amount/context' sort of thing.) What we are trying to identify is, is there another application that is fighting SQL for resources, or is something inside how SQL operates constraining it and causing it to force queries to wait until other SQL operations are complete. 
I think it was my declaration--I've fixed it and all these suggestions are now working. Thank you for assisting 
Yep, I see that now. Didn't realize there was a separate type for date vs datetime--knew it would be something insanely rudimentary. Thanks!
Awesome, glad to help.
Do two filters to be sargable: DECLARE @StartDate DATETIME = CONVERT(DATE, '2015-02-27T00:00:00.000') DECLARE @EndDate DATETIME = CONVERT(DATE, @StartDate + 1) [...] WHERE DateCol &gt;= @StartDate AND DateCol &lt; @EndDate
Glad I could help and thanks for the gold! Now, being considered an SQL genius is never bad, especially for a software developer. It's better though to actually know what you're doing, so take some time and learn it. Proper database knowledge can save your life (not literally heh) and looks great on your resume. I actually love SQL, so don't hesitate to PM me if you have any questions. 
Had some good success today, check it out.. http://i.imgur.com/TxLZ2cD.png now I just want to integrate this second query results I made into the first query result http://i.imgur.com/b20yjdO.pngt
You run a SQL query in Python/R/JavaScript and then do standard pie chart stuff there. SQL stands for Structured Query Language, it only does that. You can pull data and aggregate it but you can't visualize it. 
if you want to show a breakdown by type with total row: SELECT id, firstname, lastname, accttype, SUM(amount) as amount FROM @t1 WHERE accttype in ('Standard', 'Carry-Over') GROUP BY GROUPING SETS ( (id, firstname, lastname, accttype), (id, firstname, lastname) ); if you want just a total row: SELECT id, firstname, lastname, SUM(amount) as amount FROM @t1 WHERE accttype in ('Standard', 'Carry-Over') GROUP BY id, firstname, lastname; to add or remove different types just alter the WHERE clause: WHERE acctttype in ('Standard', 'Carry-Over', 'Some-New-Type')
You won't be able to ensure integrity based on FKs alone I think, at least I don't see a way. What you could do is use triggers. I'm not sure if that's such a great idea though, have a look and decide for yourself. # Approach 1 You have item tables, let's call them Item1 (ItemID, Content) and Item2 (ItemID, Content) and a Note (NoteID, ItemID, Content) table. Note.ItemID would be "FK" to either Item1.ItemID or Item2.ItemID. You can ensure referential integrity from Item1 and Item2 tables to Note table by using a FOR UPDATE and FOR DELETE triggers that would update (if the ID can change) and delete child note records when the item entry is deleted. In this case, you'd need to make sure that ItemIDs in the tables are non-conflicting, because you'd have no way of knowing which Note belongs to which item. You could use UNIQUEIDENTIFIER or BIGINT with IDENTITY seeds big enough for them never to conflict e.g. 1000000001 for Item1 2000000001 for Item2 as the type for NoteID. [Here it is on sqlfiddle](http://sqlfiddle.com/#!3/68e8f/5) So you ensure that whenever an item is deleted (or updated, I didn't write a trigger for that), its child notes are deleted as well. But you can't ensure that when a note is added (or updated), it's added for an item in any of the item tables. Unless you write a convoluted INSERT (and UPDATE if needed) trigger that checks each table, it's not impossible: CREATE TRIGGER [Note_OnInsert] ON [Note] INSTEAD OF INSERT AS BEGIN SET NOCOUNT ON; DECLARE @q NVARCHAR(MAX) CREATE TABLE #validateItemExistence (ItemID BIGINT); SELECT ItemID INTO #inserted FROM inserted; -- Can't use inserted in dynamic SQL, we need to copy to a tmp table -- Generate SQL statements checking Item% tables for existence of items for which notes are inserted SELECT @q = CONCAT(@q, 'INSERT INTO #validateItemExistence SELECT T.ItemID FROM ', t.name + ' T INNER JOIN #inserted ins ON ins.ItemID = T.ItemID;') FROM sys.tables t WHERE t.name LIKE 'Item%'; EXECUTE(@q) -- See if there is at least one row in the ones we want to insert that doesn't have a corresponding item in any of the item tables IF (SELECT COUNT(ins.ItemID) FROM inserted ins LEFT OUTER JOIN #validateItemExistence val ON val.ItemID = ins.ItemID WHERE val.ItemID IS NULL) &gt; 0 BEGIN RAISERROR('No item with that ItemID exists', 16, 10); END ELSE BEGIN INSERT INTO Note (ItemID, Content) SELECT ItemID, Content FROM inserted; END; END; So this would check in any of the tables that match Item% an item with the ItemID that you want to insert a note for exists and if at least one doesn't, the INSERT fails. The code is not bullet proof, but gives you an idea. Of course I'm checking all the tables that have their names like Item% which doesn't make sense, because you're probably going to have some meaningful names. You could come up with a schema or some other sort of shit to identify all the tables that could have notes and should be checked (you can even check sys.columns for existence of ItemID column and query these tables that do, except for Note table). # Approach 2 So this is an extension of Approach 1, but you can actually use an FK on the Notes table. So you have Item1 and Item2 tables like above, but instead of Note table alone, you have two: * ItemNote (ItemNoteID, ItemID) * Note (NoteID, ItemNoteID, Content), FK ItemNoteID -&gt; ItemNote.ItemNoteID So now whenever you insert a new row to Item1 or Item2, the trigger inserts a row into ItemNote table (or updates it with new ID if the item ID can change). This is a bit more complicated, because if you want to add a note to an item, you need to first retrieve the ItemNote ID. Similarly to approach 1, you have an ON DELETE trigger as well on the Item1 and Item2 tables that deletes the record in ItemNote, which then triggers the cascading delete through the FK in Note table. My wife's waiting for me so I can't provide an sqlfiddle at the moment, but it should be easy enough to figure out.
BETWEEN should do it but you're right, multiple join conditions usually require parentheses
Try getting rid of all the time bs. Cast the date field to a float then to an in then to a date. Do that on the join assuming we're not talking about zillions of records. Cast(cast(cast(calDt as float) as int) as datetime) = a2.CalDt Something like that. 
People will always find something to argue about. Stored procedures are not things to be scared of. Just do what works best for you keeping in mind good coding practices. 
Thanks. I agree about ugh, Access, but sadly in this case it's the best tool for the job I have, which is pulling data out of a real database and giving it to someone else (who has some experience but isn't a really database person) to manipulate and report on. It's the one common software we all have, and assuming I can make it work, will do the job. I know about the parentheses in the joins in Access in general and have set them up right -- I tried parentheses on the on conditions, and it didn't work. I also tried using BETWEEN, and got an error that the between needed AND. Well, the statement had an and, it was literally "between lowrange and highrange". Sadly for me, I think what you guys have really answered is my real question, which is, is this me being dumb, or Access being dumb? I'd hate to have to write VBA to get a select-case in there, but I may have to.
because both tables have a column with that name and yes, you do need a join... for which you also need a join condition
There's a pdt# column in both tables and you're not telling SQL which to use.
The way this looks is as if you are trying to learn for some class. Some here will try and make you learn. I am self taught, never went to school, and make good money doing this every day, so I don't really care if I ruin your education. The answer is: SELECT orders.pdt#, product.pdt_name, orders.qty FROM orders INNER JOIN product ON (orders.pdt# = product.pdt#) WHERE (orders.city='Ottawa')
You could also save typing out product and orders so much by assigning them to variables like this: SELECT o.pdt#, p.pdt_name, o.qty FROM orders AS o INNER JOIN product AS p ON o.pdt# = p.pdt# WHERE o.city='Ottawa' You will always need a join when pulling data from more than one table. Each table has to have a common relation to join on. There are mutliple different joins that you could use, depending on what you need to do. I would suggest googling what each join does. 
it might be unnecessary with this tiny example, but parenthesis make code more readable, and are often required when writing complex queries of the sort I am typically creating. I find it best practice to use the same syntax for all queries, rather than write "lazy" for simple queries. It doesn't take any longer.
Gotcha :)
Ha! Never thought to cast it to an int. I usually would: cast(floor(cast(datetimefield as float) as datetime)
Windows? Sounds like they just want you to open up Notepad and create a comma separated values (CSV) file. You could use a different delimiter but the comma is pretty common. CSV files usually have the .CSV extension e.g. ProductData.csv but they're just text files and you could call it ProductData.txt. Most programs that import CSVs really won't care about the file extension, just that the file is formatted correctly. In Notepad, on the first line will be the column names, separated by commas. Each subsequent line is a row of data in the table. e.g. Manufacturer,Name,Barcode,Price Heinz,Baked Beans,0010101,49 Nestle,Coffee,1203002,200 Cadbury,Dairy Milk,125494,100
thanks very much. Ill try this later. If I wanted to make say the manufacturer the primary key would I put Manufacturer primary key, Name, Barcode, Price? 
For this particular task, EF would add a bunch more complexity. I almost always have a stored proc behind my reports. In general, it separates the report formatting from the report logic and makes things easier to maintain.
Small world, I'm trying to do the same thing for Heroes of Newerth!
Yeah. I got total wins, total matches, andd average K/D/A. Heres the code I use. SELECT champions,SUM(result) as wins,COUNT(*) as matches,CONCAT(ROUND(AVG(kills),0),'/',ROUND(AVG(deaths),0),'/',ROUND(AVG(assists),0)) as team_kda from (select matches.match_id,team_id,game_number,role,group_concat(distinct pick_champion ORDER BY FIELD(role,'top lane','jungler','mid lane','ad carry','support') ASC) as champions,IF(team_id=game_winner,1,0) as result ,SUM(kills) as kills,SUM(deaths) as deaths,SUM(assists) as assists from kda_stats INNER JOIN matches ON matches.match_id=kda_stats.match_id INNER JOIN events ON events.event_id=matches.event_id INNER JOIN players ON players.player_id=kda_stats.player_id WHERE subleague=$subleague group by matches.match_id,team_id,game_number)t group by champions ORDER BY matches DESC,champions ASC Subleague is the specific event I group matches by. So like the 2015 world championships is a subleague
BI Positions in my Org: * Data Quality Analyst * System Analyst * Data Architect * System Architect * Functional Analyst * Principle Architect * Developer * Managers for each of those teams. We have ~200+ people in our org, but the Architects aren't considered "senior" to anyone - just a part of the machine. We add "Principle" to the title when the seniority is there (generally it is a position equal to a senior manager reporting straight into a director).
[removed]
How large is the company you work for? 
They have more BI positions than we have developers in our whole company
Try using this to change the year to 9999 DATEADD(year, 9999 - YEAR(@date), @date) For the record, I don't really understand what you want, and I don't really like what I see, I just don't have time to dig into it and suggest a different way.
No problem. Didn't mean to sound rude in my original comment, I just get worried when I see manually setting of date parts and "that should not happen." in the same post Also beware that if your date falls on Feb 29, you will be reducing the day to Feb 28 by running this function. I think this will cause an issue for you, because that's the only way I can reproduce the error in your post. Try running this: SELECT CAST('9999-02-29' AS DATE) Consider using 9996 as your default year instead, as that's a leap year
no, not a good idea 
Actually, our current database isn't even MS. It's a little known database called Unify Dataserver. It is pretty outdated and still required the use of Telnet, so you can see why we are moving to MySQL. It's just interesting to me that you need to make dates formatted as strings in such a popular db platform while an old, outdated platform accepts dates.
I don't see any glaring issues. I mean, VARCHAR(50) seems a bit overkill, assuming you're stripping all non-numeric characters (which you should be). You may want to add columns such as "extension", and if you're expecting to store international numbers, you may want to store a country code along with the number.
it's actually calling the table by the alias you assign to it. For example: SELECT a.column FROM table a Most people use just a single letter that is similar to the table name they use, however the below is just as valid SELECT aasdfasd.column FROM table aasdfasd The main reason for this is if you join 2 tables that both have the same column name, SQL doesn't know which one you want, so you need to tell it by which table it's in. SELECT a.column, b.column FROM tblalpha a JOIN tblbravo b ON a.id = b.id This could also be written as SELECT tblalpha.column, tblbravo.column FROM tblalpha JOIN tblbravo ON tblalpha.id = tblbravo.id Which is obviously alot more typing as most table names are even longer than that. edit* acronym to alias, cause my brain isn't working...
thanks
How do I know what the acronyms are? it's not my db, it's the company db... the example i saw was SELECT distinct a.cdid from disbursements a --join firms b on a.payeename=b.firmname why does a show up at the end of the second line?
Could you be more specific? Why is it not a good idea? Thanks
I see... thank you
I see in the w3schools example it is written out with 'as' SELECT column_name(s) FROM table_name AS alias_name; so that AS isn't necessarily mandatory, i could just put my alias straight away after the table name? 
Correct - see below, not valid code, items between quotes for context only SELECT DISTINCT d.cdid FROM disbursements "I want to call this table" d The thing to remember with SQL is it doesn't necessarily run it from the top down, so you you can put aliases after you actually use them in the column area
Yes 
If you're stripping out none numerics from the phone numbers, why not just make the column type numeric?
thank you!
I know you can do this in MS SQL Server, since that's what I use, I don't know if it's standard in others though!
It depends on your usecase. If you're ever going to have leading zeros in a number, then yes, you have to go varchar. Good point.
lol got it.. thanks
I work for a telecom. ~20k employees including store fronts etc.
Calling them a "bad programmer" for using an unnecessary alias is a bit disingenuous. It could be that their company's coding standards requires all tables to be aliased in alphabetical order or something, and failing to do so would cause a code review to fail. Also, considering the fact that there is a join commented out below, the alias may have been necessary at some point, either for clarity or because cdid is an ambiguous column name between the two tables.
Three columns to accomplish what one column in a normalized table could do? Not good. What would you do if you had to add functionality to allow the user to choose a "primary phone"? Have an INT column with a '1', '2', or '3' to indicate land, cell, or fax? What if they have two or more cell numbers they want stored for the same user? 
I'm not using SQLite so I can't test anything. The docs say you need to set the mode to CSV with the `.mode csv` command; have you tried that?
&gt; coding standards requires all tables to be aliased in alphabetical order This would be an example of a terrible arbitrary coding standard. If you change the ordering of your `JOIN`, or insert or remove a table in the middle, you have to re-alias half the query. Which just introduces unnecessary change, and therefore risk.
It's optional in many RDBMSs. But it doesn't hurt to get in the habit of using it. You can do the same with columns, to rename them. This comes in handy with UDFs and aggregates in the `SELECT` clause, which won't have names in the result set by default.
Yeah, I needed to put .csv at the end. all of the data is just going into my first column though. I get the error "Expected 4 columns but only found 1; filling the rest with null".
I haven't taken any tests even though I've worked for years as a DBA and system analyst. If someone needs reports/queries, that's me. If the database goes down, that's me. Upgrades and migration, that's me. However, I feel like if I took any SQL test I'd probably fail as well. If your problem is the same as mine, it's that you don't memorize every little thing. If you ask me to add a column and add a unique key constraint, I'd have to google the syntax. I've done it 100s of times, but I don't think it's worth it to memorize syntax when I can just google it. That's the problem with these tests. I don't think they prove anything. I had the same challenges with the Microsoft MCSE back in the day. I thought the tests were BS compared to real-world scenarios. Also, like me, you might not be fast test taker. In the real world as an exempt salaried employee, work gets done whether it takes 2 hours or 2 of my weekends. In the test, you have a short finite window of time, forcing you to think faster and that means more wrong answers. And again, what's that prove? 90 days sounds like a good solid number. That's about the same time frame as a college course. If you put in 4-8 hours per week, you should be in good shape. If you haven't actually done anything with an actual database, you should definitely download what you need and practice installing an environment, just like you would if it were your job.
Go look at the Amazon reviews for the book you read - it is known to not be enough to pass the test. The practice tests are also said to be worthless. The book has incorrect info, typos, easy-mode practice tests, etc. You aren't approaching certification correctly. First, there is reading the book and taking condensed notes of course. At the same time you should be watching videos from CBT Nuggets and Pluralsight while trying to keep the videos and what you're reading synchronized. You should also be stopping to practice everything you're learning at every step, preferrably by building your own personal database so that you have some real-world use cases to hone your skills against. Once you finish the videos and book you should begin preparing for the certification. This includes reading and rereading your notes. It involves reading/Googling people's experience with taking the test. Had you done this you would have seen this coming from a mile away (even simply reading the Amazon reviews would be enough to tip you off). To finish prepping for your cert you must take practice tests from as many different sources as possible, and if possible get an app for your phone to quiz yourself with on your lunch break or whatever. Download and review the syllabus as well. The point is to list the areas where you are falling short, and then go back and study those topics with youtube videos and online articles on the topic. Repeat until you are not deficient in any areas and are consistently able to answer most test questions. Then take the test. It sounds as though you tried to burn through an intermediate cert without due diligence, research, or planning. Fortunately for you there is a retake policy that allows you to take it again for free. Treat this as a life lesson and learn from it. 
This is the final test I have to pass for my mcsa, having already passed the other two in the track. This has been the hardest for me to study for. I have a free retake which is definitely nice but I'd like to bang it out in one shot. I'm gonna check out the cbt nuggets and plural sight as others have suggested here. Good luck and let me know if you have any questions or anything.
I'm taking my 461 tomorrow. I've been studying the Microsoft press book for 4 months, watched all the cbt nugget videos in the 461 series. I develop in SQL daily for my job. I've also watched a couple of ms training videos which describe the test and objectives. I've taken two of my 3 transcender tests and while I've failed the, both, my scores have improved. Made 100%s on the dinky test that comes with the press book. I think I'm ready, but we'll see after tomorrow. 
The code is at pastebin.com/Aq8n4gbr I'm using Adobe's Dreamweaver, by the way. Not sure if that helps.
&gt; $conn = NEW PDO("mysql:host=$servername;dbnam=Lab4", $Jake, $****); dbnam
I only used the freely available MS prep videos. Watched them, took some notes, tested a few things. They seemed to nail and explain the tiny gritty parts I was going to encounter. So I only prepped for a couple days, and only watching those videos (though years of sql experience prior) - so I highly recommend them. https://www.microsoft.com/en-us/learning/exam-70-461.aspx
Those are window functions. I'm on my phone but a quick Google of " sql window functions" should explain it much better than me. 
First, the query is in the format of selecting things from multiple tables, and then at the end, instead of a table, it is from another source that is defined as all of the code within the parenthesis and then aliased as "f". So all that jargon within the () can be referred to simply as "f". It's table f, in other words. And "f" is inner-joined to your cp table using id to subscription key. It looks ugly but that's how it is done. Remember, in SQL, you can have things in the format of SELECT FROM table1, table2, (a whole bunch of code that I'm naming "f"), etc. Now within "f", you have some tricky business going on. Those partition over() selections are grabbing the max and min per each partition of subscription_id. So I'm assuming the same subscription_ID appears many times in your table. Let's say I was subscription_ID 123. I could activate, deactiveate, cancel, subscribe, cancel, etc, dozens of times. But you only want my max activation date, not the one from a prior activation. You do that by partitioning records by subscription ID and then apply some sort of aggregate such as max() to get the max date for active, max date for cancellations, etc. Remember, you can't just go grabbing max() date of anything because it would only return the max date of the entire table, not per each subscription_id. That's why windowing functions (the partition over logic) has to be used here. Edit: So you can see the action of what it does, run the select statement within your () by itself so you can see what the windowing functions are doing. You will get a table of subscription_ids and the most recent time they did each activation, cancellation, etc. 
Maybe I'm not following, but based on the example, I don't see a difference between 'semi join' and regular inner join? Also 'anti join' can be accomplished a few ways. One of which being: FROM employee a LEFT JOIN dept b ON ( a.deptname=b.deptname ) WHERE b.deptname IS NULL Or a not in/exists statement. I guess more concise syntax wouldn't be bad (even though it'd be a bit redundant), but I really don't understand the semi join.
Just to add to this, op should change line 7 of the pastebin code to: $conn = NEW PDO("mysql:host=$servername;dbname=$dbname", $username, $password); He's set $username and $password, but isn't using them, and instead has variables which have no values. 
Thanks so much guys! 
Thanks for all of your responses, they've all been very helpful!! &amp;nbsp; I will sign up for CBTNuggets and PluralSight along with creating more real world examples/practicing every day. I would like to retake the exam in two weeks, but I have other projects heating up at work so I'll shoot for November 9th, maybe a little earlier to keep everything fresh. I will reply with my results! &amp;nbsp; Here are some helpful links everyone has suggested (save a second of Google searching) &amp;nbsp; [PluralSight](http://www.pluralsight.com/search/?searchTerm=70-461) &amp;nbsp; [CBTNuggets 70-461](https://www.cbtnuggets.com/it-training/microsoft-sql-server-2012-70-461) &amp;nbsp; [MS Born to Learn Study Guide](https://borntolearn.mslearn.net/certification/database/w/wiki/520.461-querying-microsoft-sql-server-2012?Redirected=true) &amp;nbsp; [Microsoft Virtual Academy Querying with Transact-SQL](https://www.microsoftvirtualacademy.com/en-US/training-courses/querying-with-transact-sql-10530?l=TjT07f87_9804984382) &amp;nbsp; Thanks again, y'all made my day!
have a nice day
There is nothing wrong with descriptive names. Instead of BOM, use BillOfMaterials. Pluralize table names as well, so Parts instead of Part. Also the structure of your BOM table would only allow for you to have one PartID per BOM, is that correct for your application? Also do you want to store the part's ParentID in the BOM table or do you want to store it in the Parts table? For the record, you can have a foreign key that references the table that it's stored in.
Could accomplish the same in a Common Table Expression as that Subselect join. Also, my hackles always go up when I see the word distinct, especially with an aggregate. Makes me get all hulk-smashy.
Hi Farhil, Thanks for the tip, but I don't think I can do that with current access rights either, unfortunately : /. That's why I was going for the general description and hoping for pseudo-code--I know it's hard to help coding issues when generalized, though, so I apologize.
Yeah, better not voice a dissenting opinion! Relax, guy. It's an internet forum. We're here to debate and discuss.
Hi Rex, Can you elaborate on the first solution? I do actually see a view that's stored that seems to store all the coverage and service codes as well as names, so the answer is probably to reference that view which I'm going to try. Can you explain why a view is needed, though, when all of the necessary data in that view exists in the coverage, services, and fee amount tables? Is it simply because I do not have a joinable key relating the services between the fee amount and coverage tables?
Not sure why you interpreted my comment as hostile? I was just pointing out that just because it's possible to do something one way, that doesn't mean functionality should not be implemented to perform the same task a different, more efficient, or more elegant way
You may need to include state and county somewhere on the join?
And if you run select e.* from BOM b join BOM e on b.PartId = e.ParentId where b.ID = 4 Do you get what you'd expect? I honestly have no idea what you're looking for right now
If you are using SQL Server, you'll wan to do [something like this](http://davidduffett.net/post/5334646215/get-a-comma-separated-list-of-values-in-sql-with). 
I do get what I expect - nothing. No biggie, I just wanted opinion on something i've never modeled before. Maybe there's some industry standard way of doing BOMs.
That was a problem initially, but I did correct that (feelocation.state = coverage.state and feelocation.county = feelocation.county in join conditions) and it still didn't seem to rectify the issue. I think Rex above may have been on the right path because I do now see a view that was created that has the service codes and names in it along with the coverage and contractor to which it applies. Going to try to use that instead of the coverage and services table and may have better luck.
Fair enough. I took it as a dismissive since I felt like I presented an argument against it and you said that my attitude was anti-progress. We found 3 options that would give us the same results set. None of which were particular verbose, though, admittedly, using distinct ain't great for performance. But since the article does nothing for performance benchmarking, it's tough to say which would be better/worse from an optimization standpoint. Anyhow, sorry for taking your comment the wrong way. And while I think there may be some merit to an 'anti join,' making 'semi join' a thing still seems silly to me. ;D
Can you show the schema for [WebLogs_Parsed]? Specifically, where does the [Path] column come from? Also, can you explain what your desired outcome is in terms of your schema?
On mobile and about to go to bed, but look up STUFF FOR XML PATH. Get that working as a correlated subquery and that should work. If you need more help in the UK morning, let me know.
Normally you wouldn't. Instead of having a Select Into that you run all the time, you would create a view. So take the select statement, remove the INTO clause and put the following above it: CREATE VIEW WhatYouWantYourViewToBeNamedHere AS (your select statement goes here) This will create a view, which is basically a virtual table. You can reference and use it pretty much exactly like a real table, and it will automatically be updated. I'm assuming here that you don't need to manipulate the data in your table separately to the source data of course. Otherwise you'd need to look at putting a trigger on the source table to update your new table, but that leads to redundant data and more IO overhead any time the source table is written to. If your new table doesn't need to be up to the second synchronized you could also look at scheduling a job to execute your SELECT INTO statement overnight or something.
Took my test today and passed with a 770. Very interesting format. Co-workers that took it about a year ago said they didn't have certain types of questions, and there were 4 of them on my test. Re-iterating that I studied the training kit, cbt nuggets, also watched many videos online about the test material ( most were microsoft slides). Also, took 2 of my 3 transcender tests. Failed both of those. 
What is the command you are using with arguments? Do you have `-h &lt;hostname or IP&gt;` set?
Look up "check constraints", it's the proper way to handle what you're talking about rather than a trigger Edit: never mind I misread, a check constraint won't work here 
&gt; you can give that query any nickname it's an alias, and you must, not just can 
before i tried to configure phpmyadmin and xampp so i can create web pages, i could log in to mysql thorough terminal by executing cd /usr/local/mysql/bin/ ./mysql -u root -h localhost -p
I was trying to keep it simple.
Thanks for updating us!
Worked beautifully. I'll try to remember to shoot you some sweet sweet Gold when I get home tonight.
Yeah I'd need to know more about the tables and the data you're dealing with to accurately help you but you could use #temp tables and loops maybe just based on your description.... so something to the effect of... DECLARE @index int Select * from field into #f While (select count(*) from #F) &gt; 0 BEGIN SELECT TOP 1 @index = index from #F DELETE FROM #F where index = @index SELECT COUNT(*) as Count FROM application a JOIN Field f on f.record = a.id JOIN Lookup.round r on r.id - a.round JOIN lookup.period rp on rp.id = r.period where f.field = 'school_id' and f.index = @index and rp.year = 2015 END This should look through everything in f.index and give you the counts for each its just as easy to dump the counts into another table by adding this above the while loop. CREATE TABLE dbo.indexwithcounts (index int, Count int); then at the bottom of the loop change the count code I stole from you to Insert into dbo.indexwithcounts SELECT @index, COUNT(*) as Count FROM application a JOIN Field f on f.record = a.id JOIN Lookup.round r on r.id - a.round JOIN lookup.period rp on rp.id = r.period where f.field = 'school_id' and f.index = @index and rp.year = 2015 That would eliminate your need for a subquery and you could just use that table to join to your main query. Can also make that a stored proc to run anytime you needed to update the counts.... 
It depends on the manner in which you want to update, and what columns are available on the source table for you to identity what has been updated. If you are able to configure CDC on the source then that is a good way to go, or the little brother of CDC - Change Tracking, which will identify which rows have updates, both of these would be asynchronous to the main update process. Essentially you need to determine a manner in which you can identify which rows have changed and then update/insert those rows, which is easiest using the MERGE statement. If you cannot identity the rows, then you are left with blowing away the destination table data and reloading from source each time. As mentioned by fauxmosexual, you do need to create a source and destination list for the statement, whichever route you take.
or perhaps wrap the 'year' value inside a CASE statement to determine the Month/Day as 2/29 and use the appropriate year when it is true.
Found it, Thank you!
Where do I download this? Where is the source code? Why do we need yet another fucking SQL &lt;-&gt; JSON API? Don't the other 20 work?
You're looking for "UNION" or "UNION ALL." The difference is that UNION will remove duplicates from the result, and UNION ALL will return all records. SELECT SFID , Table1XID --External ID , Name , Address , Table1Info FROM Table1 UNION ALL SELECT SFID , Table2XID --External ID , Name , Address , Table2Info FROM Table2
Thanks, it's very useful. I've already found something, which I don't get at first sight. Maybe someone can give me a hand and explain how and why... It's the [Oracle Text example](https://livesql.oracle.com/apex/livesql/file/content_BPUG62SSVNJG32DH85GFWJZP.html) and: * Why does the **statement 11** work? How does Oracle find a word "queries" when asked for "query"? select * from ot_data where contains( description, '$query' ) &gt; 0 * Why does the **statement 14** return no data found? There are 2 rows, that contain the searching word. What is happening there? select * from ot_data where contains( description, 'This' ) &gt; 0
Yes! This is essentially what I want! The fields that are not shared can be NULL, that is fine. That's an awesome trick with adding in the NULL column! Can I do multiple NULL columns per select statement? That said, I was hoping for something a little more dynamic. I guess this means there is no way for SQL to recognize columns that have the same name and combine those, much the same as it does when I join on a column?
How does the NULL column in the UNION know which column is in each select statement? Ninja edit: Is it just by the order of columns in each statement?
Just by the order in the columns, yes.
You can do multiple NULLs. Since the UNION operates on the order of the columns in the SELECT list, it doesn't really care what the names of the columns are.
Ahhh. OK. That makes sense. Well, I think with my data it may be easier/faster/better to just make a small UNION subquery, and then just start left joining the information after that. Thank you so much for all your help!
Here is the code/query separate from pastebin (looks too large for reddit formatting so wanted to avoid putting it in the OP) BEGIN SET NOCOUNT ON; SELECT DISTINCT ISNULL(T.PRODUCT_ID, 'NULL') AS [Commodity], ISNULL(T.PO_NO, 'NULL') AS [PO NO], ISNULL(T.LINE_NO, 'NULL') AS [LINE NO], QUOTENAME(T.DESCRIPTION, '"') AS [PO Line Description], QUOTENAME(C.DESCRIPTION, '"') AS [Commodity Description], ISNULL(T.FY, 'NULL') AS [Fiscal Year], PH.Vendor_ID AS [Vendor ID], QUOTENAME (V.Vendor_Name, '"') AS [Vendor Name], T.QUANTITY, T.UNIT_COST, T.QUANTITY*T.UNIT_COST AS [Line Amount], PH.Created_Date, (Select CAST(0.00 as numeric(10,2))) AS Sub_Total_Cost INTO ##TmpPOReport FROM dbo.DBVW_FI_REQ_PO_ITEMS T INNER JOIN dbo.FI_VENDOR FV ON T.INST_ID=FV.INST_ID INNER JOIN dbo.FI_REQ_PO_HEADER PH ON T.PO_NO=PH.PO_NO INNER JOIN dbo.FI_VENDOR V ON PH.VENDOR_ID=V.VENDOR_ID INNER JOIN dbo.FI_COMMODITY C ON T.PRODUCT_ID=C.FI_COMMODITY_CODE WHERE T.INST_ID = 'SC00' AND T.FY = '2015' AND V.VENDOR_TYPE = 'V' AND T.PO_NO IS NOT NULL AND (T.PRODUCT_ID &lt;&gt; '' AND T.PRODUCT_ID IS NOT NULL) AND T.QUANTITY*T.UNIT_COST BETWEEN '1000' AND '20000' Order By Commodity DECLARE @PID varchar(15)=00, @QUANTITY int, @UNIT_COST numeric(10,2), @PrevID varchar(15), @CreateDate datetime, @PrevDate datetime = getdate(), @RowAmount numeric(10,2), @SubTotal numeric(10,2) = 0.00 SET NUMERIC_ROUNDABORT OFF; WHILE EXISTS(Select TOP 1 * FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) BEGIN SET @RowAmount = (Select TOP 1 (QUANTITY * UNIT_COST) FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) SELECT TOP 1 @PID = Commodity, @CreateDate = Created_Date FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00 IF(@PID = @PrevID)AND(@CreateDate &lt;&gt; @PrevDate) BEGIN SET @SubTotal += @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND T.Created_Date = @CreateDate SET @PrevID = @PID; SET @PrevDate = @CreateDate END ELSE BEGIN SET @SubTotal = @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND T.Created_Date = @CreateDate SET @PrevID = @PID; SET @PrevDate = @CreateDate END END SET NUMERIC_ROUNDABORT ON; SELECT * FROM ##TmpPOReport WHERE [Line Amount] BETWEEN '1000' AND '20000' DROP TABLE ##TmpPOReport END
Query Separate from Pastebin (too big for reddit formatting): BEGIN SET NOCOUNT ON; SELECT DISTINCT ISNULL(T.PRODUCT_ID, 'NULL') AS [Commodity], ISNULL(T.PO_NO, 'NULL') AS [PO NO], ISNULL(T.LINE_NO, 'NULL') AS [LINE NO], QUOTENAME(T.DESCRIPTION, '"') AS [PO Line Description], QUOTENAME(C.DESCRIPTION, '"') AS [Commodity Description], ISNULL(T.FY, 'NULL') AS [Fiscal Year], PH.Vendor_ID AS [Vendor ID], QUOTENAME (V.Vendor_Name, '"') AS [Vendor Name], T.QUANTITY, T.UNIT_COST, T.QUANTITY*T.UNIT_COST AS [Line Amount], PH.Created_Date, (Select CAST(0.00 as numeric(10,2))) AS Sub_Total_Cost INTO ##TmpPOReport FROM dbo.DBVW_FI_REQ_PO_ITEMS T INNER JOIN dbo.FI_VENDOR FV ON T.INST_ID=FV.INST_ID INNER JOIN dbo.FI_REQ_PO_HEADER PH ON T.PO_NO=PH.PO_NO INNER JOIN dbo.FI_VENDOR V ON PH.VENDOR_ID=V.VENDOR_ID INNER JOIN dbo.FI_COMMODITY C ON T.PRODUCT_ID=C.FI_COMMODITY_CODE WHERE T.INST_ID = 'SC00' AND T.FY = '2015' AND V.VENDOR_TYPE = 'V' AND T.PO_NO IS NOT NULL AND (T.PRODUCT_ID &lt;&gt; '' AND T.PRODUCT_ID IS NOT NULL) AND T.QUANTITY*T.UNIT_COST BETWEEN '1000' AND '20000' Order By Commodity DECLARE @PID varchar(15)=00, @QUANTITY int, @UNIT_COST numeric(10,2), @PrevID varchar(15), @CreateDate datetime, @PrevDate datetime = getdate(), @RowAmount numeric(10,2), @SubTotal numeric(10,2) = 0.00 SET NUMERIC_ROUNDABORT OFF; WHILE EXISTS(Select TOP 1 * FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) BEGIN SET @RowAmount = (Select TOP 1 (QUANTITY * UNIT_COST) FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) SELECT TOP 1 @PID = Commodity, @CreateDate = Created_Date FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00 IF(@PID = @PrevID)AND(@CreateDate &lt;&gt; @PrevDate) BEGIN SET @SubTotal += @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND T.Created_Date = @CreateDate SET @PrevID = @PID; SET @PrevDate = @CreateDate END ELSE BEGIN SET @SubTotal = @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND T.Created_Date = @CreateDate SET @PrevID = @PID; SET @PrevDate = @CreateDate END END SET NUMERIC_ROUNDABORT ON; SELECT * FROM ##TmpPOReport WHERE [Line Amount] BETWEEN '1000' AND '20000' DROP TABLE ##TmpPOReport END
You need a unique identifier for each row. If you can't add an Identity column as a proper primary key, you can always cheat by using ROW_NUMBER to assign each row a numeric value to each record when you create your temp table. Then when you do your update later on, filter for the row number. 
Your code is excessively convoluted. You can easily achieve your subtotalling using ROLLUP and GROUPING SETS, which is available in 2008 R2. 
Waste of time, yet another interface that is dead out of the box. Most DBs like DB2, Oracle and PostgreSQL etc., natively support JSON already.
ziptime is right. In other words, you don't need the loop or temp table at all. It looks like you want a running total? I would use windowed functions. They will typically perform better than group aggregates as well. something like this for the running total column: SUM([Line Amount]) OVER(ORDER BY [Commodity Description] ROWS UNBOUNDED PRECEDING) This will do a running total for the whole set. If you want a running total per commodity try something like this: SUM([Line Amount]) OVER(PARTITION BY [Commodity Description] ORDER BY [Commodity Description] ROWS UNBOUNDED PRECEDING)
Worked is a nebulous term. Didn't take long to realize that I would have to dump the @tempstem table to a permanent table in order to join back at the end of the process to make sense of the concatenation. Oh, and the process is going to take &gt;6 hours to run so I won't see the results until the morning. I also hardcoded an X to be placed in between each ID. Ultimately what I plan to do is trim the strings down to the first n places on the left hand side and then start running the process for specialized cases to see what paths users are taking most frequently (when there's a sale, etc.) My final desire would be to take a table that looks like this: | Path | VisitorCount | |:---|:---| |25x15x19x | 90 | |38x721x4x | 25 | |656x44x972x | 18| And then join the Path back to the Stem to get something like this: | Path | Stem | ID | |:---|:---|:---| |25x15x19x | Index.jsp | 25x | |25x15x19x | Products.html | 15x | |25x15x19x | FindAStore.jsp | 19x | Shouldn't be too difficult to break the strings back down.
Both of these are because of the index created in **Statement 2**. When you create a context index, you're able to use the contains operator, which allows you to use [stemming](http://docs.oracle.com/cd/B28359_01/text.111/b28303/ind.htm#CIHCCCIF) (the $ signifier in front of query). It uses heuristics to determine if the word matches the same root, as long as the word is an indexed word. "This" isn't indexed (maybe due to the commonness of it?), which is why **Statement 14** returns no results. 
Just figured I'd give a quick FYI, views don't actually store anything data-wise (unless you get into indexed views but let's not). It's merely a joining shortcut to make it easier to write more concise code if you have a complicated join. It can also be used to obscure sensitive data too if you do alterations to x coumn in y table you can only get the altered data from the view; meaning the original stays safe if you only allow visibility on the view. There's a lot of info on them if you Google.
twi**c**e Why aren't you selecting anything from workphone (contact_endpoint)? Already tried selecting the following? &gt; worknr = workphone._endpoint
mabye i am misunderstaind you, but i have tried to select 2 times into: MiralixGreenbox.dbo.contact_endpoint, with out luck. I am trying to select it here: LEFT JOIN MiralixGreenbox.dbo.contact_endpoint ON user_account._contact_endpoint_mobilephone_id = contact_endpoint.id LEFT JOIN MiralixGreenbox.dbo.contact_endpoint AS workphone ON user_account._contact_endpoint_workphone_id = workphone.id i use as in the last left join, other wise i will be getting an error.
one more thing contact_endpoint._endpoint contains the phone number, both for workphone and mobile phone. so the id from user_account._contact_endpoint_mobilephone_id and user_account._contact_endpoint_workphone_id contains the id to look up into MiralixGreenbox.dbo.contact_endpoint to get the numbers.
You have two joins: 1) LEFT JOIN MiralixGreenbox.dbo.contact_endpoint -&gt; this one is for the mobile number. This is what you have in your select: &gt; contact_endpoint._endpoint AS mobilenr So, you are using the join to select the mobile number. 2) LEFT JOIN MiralixGreenbox.dbo.contact_endpoint AS workphone -&gt; this is the one you want to use for the work number. But (!), you do not select anything from this join. I don't see you mention anything from the alias "workphone". So, instead of your current select, try the following select... SELECT user_account._user10 AS traftid , user_account._office , user_account.id , user_account._contact_endpoint_sip , user_account._contact_endpoint_mobilephone_id AS mobilid , user_account._name , user_account._forename , user_account._surname , user_account._emailaddress , user_account._atlas_username , user_account._title , company._name AS stab , department._name AS afdnavn , contact_endpoint._endpoint AS mobilenr , workphone._endpoint AS worknr Does it make a difference?
Glad it helped! :). I recommend you to use an alias for all of your joins, btw... select traftid = ua._user10 , office = ua._office , id = ua.id , contact_endpoint_sip = ua._contact_endpoint_sip , mobilid = ua._contact_endpoint_mobilephone_id , name = ua._name , forename = ua._forename , surname = ua._surname , emailaddress = ua._emailaddress , atlas_username = ua._atlas_username , title = ua._title , stab = c._name , afdnavn = d._name , mobilenr = mobile._endpoint , worknr = work._endpoint from MiralixGreenbox.dbo.user_account ua left join MiralixGreenbox.dbo.department d on ua._department_id = d.id left join MiralixGreenbox.dbo.company c on ua._company_id = c.id left join MiralixGreenbox.dbo.contact_endpoint mobile on ua._contact_endpoint_mobilephone_id = mobile.id left join MiralixGreenbox.dbo.contact_endpoint work on ua._contact_endpoint_workphone_id = work.id where ua._atlas_username like '%d1mahsi%' order by ua._name 
A big tnx again! using alias for joins seem to make it a bit easier to keep track of the joins :)
I would question why you're doing so much within a query and not just using SSRS to accomplish this in a much cleaner way. Feed it the detail data using a very simple query and let it do the grouping and subtotaling.
You need to structure the tables so that any one thing that could be redundant is pushed to a reference table. If you need to update an attribute it is reflected for all things that use it as a key. Think of it this way. Every order is unique. It has a time, an item purchased, and a quantity. The order is unique when you give it a primary key but repeating an item description for every order record makes it redundant. So every item also gets a unique identifier that you can refer to in a new table that might have many other attributes of the item. One record in the order table with an ID of 123 on October 15th is related to the item in the order. It contains just the code 456 for the item. If you look to the item table for 456 you find its an apple and cost 1 dollar. 
Thanks for the added info--I'd been able to pick up that a view was usually just a query from a number of tables, but I didn't realize, at least from the way my database is setup, the potential for using them to protect sensitive information. Makes sense, since I'm more than sure you can restrict views to tables, etc., so you could set a person up to allow access to the view but not the table.
Gotcha. I think you will see the biggest performance increase if you are able to eliminate where you group by or join on functions like substring(). When you use substring() or other functions in your predicates, SQL Server will not use your indexes efficiently. Also, I think if you can get all your statistics without using any string manipulation it would be exponentially faster. Doing so would probably mean eliminating that path column lol. Instead of creating that I would use window ranking functions. Essentially you would give each IP an ID and then the stem IDs an order number for building your linear stem history and running all your statistics. Let me know if that is something you would be interested in and I can help with a query. PS. no more gold please. I just find your problem interesting.
The best way I can think of would be to write a user defined function for parsing delimited strings. You are going to have to select all the values from the path into a table and then group and count from there. That being said I think getting rid of that path string would make this so much easier. The string manipulation on [path] is already getting pretty complex. The simpler you make it the easier it will be to maintain in the future. Would you mind entering a few sample rows from [WebLogs_Parsed] into [SQLFiddle](http://sqlfiddle.com/)?
Your current design is the best decision. You can run into a lot of query problems when you have multiple tenants in the same database.
Check out this article -&gt; https://msdn.microsoft.com/en-us/library/aa479086.aspx The last time I did this we used a couple of layers to prevent data loss -&gt; encryption on the fields for their authentication keys as well as their tenant ids. I'd proposed about 3 more layers, but they thought it was overkill. edit: you didn't mention if you'd eventually be federating this or not, but that may tweak your design pattern away from tenant id 
You might be able to search the object table for definitions to replace any varchar(x) with varchar(15) within that set of procedures. That's still a one-off script. The way I'm thinking (REPLACE() ) it'd replace any varchar(10) or (12) with (15). Not just the one you want. Maybe you could come up with a list of the parameter names and limit it that way... 
*right(len(column)-charindex(....)
Thanks for the info! Is a federated database a solution for multi-tenancy? I thought it was more for geo-location. Our main motivation for moving to multi-tenant is to be able to use managed cloud database services like Amazon RDS. They severely limit the number of databases a single instance will support so we need to move to one/fewer large databases instead of client-specific databases. If we can move to RDS then geo-location is easy to implement. [Edit - I want to add that this is an excellent article. It is pretty rare to find an article that covers a broad topic without getting too technical. Exactly what I needed to see. Thanks again!] 
I assumed that we would create a view/stored procedure for each table that requires a tenant ID and returns only the rows for that tenant (would not be possible to return all tenant rows). As long as the application references only those views/SPs (and not the tables directly) there can't be any screw ups. Was considering row-level security or data encryption as an added fail safe. Moving to this really wouldnt even require much of a rewrite of the current application. 
This is an interesting problem, but I think you will have to do it with alter procedure scripts. Now what I'm wondering is if there is a way for you to use dynamic sql to do this so you don't have a huge file. So a process like this: 1) Since you can identify the parameters and procs that need to be changed create a temp table with that information and a row number partitioned by parameter to keep track of the work for each "set" you need to fix. 2) Use that temp table to inner join to a dmv or syscomments in a while loop ONE AT A TIME to grab the stored procedure text and assign it to a variable 3) Now that you have the text assigned to a variable (nvarchar max) replace the parameter size. Use the temp table so it can be done dynamically instead of having to know the different names of the parameter. Also change the create procedure to alter procedure if necessary. 4) Execute the variable, have the loop move onto the next row, and repeat the work until all 93 are done. Print the name of the proc altered and test a LOT. If you do it this way the script would look confusing but not be nearly as long as a file with 93 procs scripted out. Now I'm tempted to play with this approach - but in theory this should work as long as all users have the appropriate permissions.
 ;with person as ( select fullname = 'John, Smith Derrick, Jackson' union all select fullname = 'Re, Dd, It' ) select fullname , [first] = left(fullname,charindex(',',fullname)-1) , [second]= ltrim(rtrim(left(substring(fullname,charindex(',',fullname)+1,len(fullname)),charindex(',',substring(fullname,charindex(',',fullname)+1,len(fullname)))-1))) , [third] = ltrim(rtrim(substring(substring(fullname,charindex(',',fullname)+1,len(fullname)),charindex(',',substring(fullname,charindex(',',fullname)+1,len(fullname)))+1,len(substring(fullname,charindex(',',fullname)+1,len(fullname)))))) from person http://i.imgur.com/ZoWYYhD.png
I'm not saying that you can't be successful with a single database with multiple tenants, but I am saying that experience has taught me this solution poses many operational challenges that most people do not overcome. Of the multi-tenant SQL Server applications I've worked with in the last 5 years, all of them were either using the database-per-tenant approach or were migrating in that direction as fast as they could. The big dangers of the approach you describe come from bad plans and parameter sniffing. Let's say you've got one client with twice as much data as the others. They run a stored procedure first. That procedure is compiled in ways that make sense for them (large memory grants, hash joins, all that fun stuff). When other, smaller tenants run that same procedure they get the plan that assumes a large amount of system resources are being used. This causes all kinds of problems for system level performance. Also, as long as there are humans writing code, there can be screw-ups. Someone will eventually hard code a client value because they copy-pasted from development into production or you'll hit a query generation bug. Or you'll use `OPTION (RECOMPILE)` to get around parameter sniffing which has had a bug on three occasions where you can get someone else's query results. Here's [one example](https://support.microsoft.com/en-us/kb/2965069). Row-level security might be good, but it relies on a developer writing an effective discriminator function based on the user or based on the `CONTEXT_INFO` value (which is a hex value). The RLS functionality requires that you use a multi-statement table-valued function (a user could have multiple roles). Multi-statement table-valued functions also cause some hilarity for the optimizer - it assumes that 100 rows will come back, no matter what. (SQL Server 2012 and earlier only assumes 1 row will come back.) If you mean SQL Server encryption, the other gotcha is around encryption. * TDE is only encryption at rest - doesn't protect you. * Always Encrypted only allows for equality searches - not really useful. And, one last fun one, in many multi-tenant applications, it's nice to migrate customers to a new feature slowly so that you don't have to take everyone offline while you modify a large table. The easiest way to do that is with rolling releases across the many tenant databases. Security is only one concern when you're looking at a multi-tenant system. They're complex beasts. Managing 100 databases may seem like a chore now, but there are many ways to solve that problem and they are well defined solutions. Managing 1 large database with many different use patterns is difficult and requires serious development investment just to keep existing functionality running well, and you'll obviously need more development time to add new features, too.
I'll try to see if I can get SQLFiddle to work, but it wasn't really digging my syntax. I uploaded a sample file. [http://www.filedropper.com/sampleweblogs](http://www.filedropper.com/sampleweblogs)
If you're looking at a DBaaS, Azure SQL Database makes a lot more sense for this than RDS - each client can easily have their own database and you scale the different databases separately. RDS does have a unique set of limitations that makes this a challenge.
What do you exactly mean with: &gt; And I want to parse them down to get the three distinct values on the left such as: ?
Excellent info! I can't thank you enough. What do you think about single-database, multi-schema approaches?
Have you looked into use RegEx expressions? It's an incredibly powerful tool/method, yet can become incredibly complex very quickly.
I would agree but we recently evaluated Azure and they suspended our account on a technicality. Then we found dozens of examples of MS suspending even established accounts for really ridiculous reasons. They shut ours down because they thought we were gaming the free trial. Had a previous free trial account that expired. We never used any free services and they had our payment info. So shutting us down for violations of the free trial terms was BS. I absolutely refuse to work with a company that will do something as severe as suspend an account without multiple levels of communication/checks to avoid that result. We have been with AWS for years without a single hiccup.
|date|time|c-ip|cs-uri-stem| |:---|:---|:---|:---| | 2014-01-01 | 12:00:00 | 42.136.60.58 | /index.html | | 2014-01-01 | 12:00:15 | 42.136.60.58 | /aboutus.asp | | 2014-01-01 | 12:00:45 | 42.136.60.58 | /careers/ | | 2014-01-01 | 12:02:11 | 42.136.60.58 | /quoter.asp | | 2014-01-01 | 12:06:00 | 42.136.60.58 | /products.asp | | 2014-01-01 | 12:06:13 | 42.136.60.58 | /buynow.jsp | | 2014-01-01 | 12:09:45 | 42.136.60.58 | /application.asp | | 2014-01-01 | 12:11:16 | 42.136.60.58 | /purchase.html | | 2014-01-02 | 1:18:04 | 150.48.59.66 | /confirm.asp | | 2014-01-02 | 2:00:00 | 150.48.59.66 | /index.html | | 2014-01-02 | 3:11:15 | 150.48.59.66 | /aboutus.asp | | 2014-01-02 | 4:00:45 | 150.48.59.66 | /careers/ | | 2014-01-02 | 5:02:11 | 150.48.59.66 | /quoter.asp | | 2014-01-02 | 6:06:00 | 150.48.59.66 | /products.asp | | 2014-01-02 | 7:06:13 | 150.48.59.66 | /buynow.jsp | | 2014-01-02 | 8:09:45 | 150.48.59.66 | /application.asp | | 2014-01-02 | 9:11:16 | 150.48.59.66 | /purchase.html | | 2014-01-02 | 10:18:04 | 150.48.59.66 | /confirm.asp | Does that help? 
I briefly researched this, and regex could do something like eliminate a duplicate character when they are adjacent, but in this case of trying to parse patterns of variable length, I don't think regex can do it. However, there is something called "positive lookahead" that looked somewhat promising but I gave up after a few minutes. I feel like the solution is going to be loop-based, iterating for as many "x" is found within the string, and building out a unique string if the pattern is not already in the build out string. 
Say for example we are starting with 479x479x479x479x479x479x1x2x, you would split the string on 'x'... For each 'x', you would store the value in a column (let's call the column TempResult) inside of a table variable. So your table at this point would have 8 rows. TempResult -------------- 1. 479x 2. 479x 3. 479x 4. 479x 5. 479x 6. 479x 7. 1x 8. 2x If you were to SELECT TempResult From @TableVariable GROUP BY TempResult The result would be: TempResult -------------- 1. 479x 2. 1x 3. 2x The next step would be to use the FOR XML CSV method, or unpivot (added bonus, you wouldn't need to dynamically unpivot because you know you only have one column). After that, the result will be 479x1x2x. Without getting too much more into the details, if you used one of the popular delimited split table valued functions, splitting on X, then instead of using the comma for the FOR XML CSV method, have it concatenate on X. I hope that makes more sense. 
Sadly, you're not the first person I've heard this from. They seem to have a crappy algorithm over in Azure that ends up shutting down accounts for ToS violations.
A few tips if you actually want someone to help you: * Format your SQL so it's remotely readable. Nobody wants to parse that just for the benefit of doing you a favor. * Avoid just vomiting SQL here without any context. We have no idea what your table structure looks like and we have no idea what you're actually trying to accomplish. * Provide example table data and provide an example of what your desired output is. Explaining what you're trying to do would go a long way towards allowing someone to aid you with a solution. In short: If you want good answers, learn how to ask good questions.
Excellent questions. One benefit is that the system is pretty lightweight and its use is well defined. There is a wide range of data volumes with the largest clients only having a few million rows. If we go shared DB or shared schema (which you effectively talked me out of) we would also build a robust auditing tool that would allow for individual tenant rollbacks. In the past 3 years we have not had to restore any databases. But there is always a "where did all this data go" question popping up occasionally. The main drive is to be able to use managed DB services (since my budget wont allow for a full-time DBA). With RDS the number of DBs is limited to 30. We already have over 100 client DBs so moving to single DB, multi-schema is desirable from that stand point. From what I read in the article a server should be able to handle more clients/tenants in a single DB, multi-schema configuration than it would with separate DBs. But I don't know what other performance implications that would have. You seem to indicate that one client/tenant could have a larger impact on another's performance in a multi-schema config than they would with multi-DB. Is that the case?
Nope, same result. Thanks anyway though
you running this query inside SSMS or from an Application? (the varchar(50) datatype that /u/evilalive is correct, but if the application is treating it as a date it might be formatting it automatically.) SELECT CONVERT(varchar(50),GETDATE(),110) returns mm-dd-yyyy
I knew it was simple, you made me realised doc entry wasn't unique but LineNum was. Thank you. 
Thx, did it and it wasn't unique. My unique key was actually another field. Wasted too much time on that for some reason.
Oh sorry I forgot to mention that. The procedure appears to execute, but when run in the CMS, it doesn't display the required text.
In your stuff statement from the last thread, change it from: STUFF(Select ID FROM...) To: STUFF(SELECT DISTINCT ',' + ID FROM...) Should be much easier like that. It'll give you a comma separated list of distinct ID values in a column. You may need to convert or cast your ID field to a varchar to make it work.
It's more that you have a single bottleneck in the t-log file and you get a much coarser granularity of disk performance metrics through the various DMVs.
Won't work because of the order by and I'm lining it up on each individual day.
Should still be possible. Change Order by to: Order By ',' + ID For dates,surely you could do; Select Date ,Stuff((Select Distinct ',' + ID FROM table t02 WHERE t01.date = t02.date ORDER BY ',' + ID)) From table t01 Adding the for xml stuff in too of course. Sorry, on mobile so writing real SQL is tricky! 
Its not actually meant to update a table. The procedure is created, then is referred to in a table with an exec command in one field, and a common name assigned to it. That way when I am in the CMS designing the receipt, there is a pull down with fields I can use, which are actually those common names with procedure commands. So I have this procedure, called Procedure1. In the Template table I insert a row that has columns "Code" &amp; "Name". In "Code" it will say "exec Procedure1" and "Name" will be "Address". I then go to the CMS and go to edit the receipt. There is a drop down menu that contains all the "Name" rows from the Template table. When I click the drop down and select "Address" it will add that field to the receipt. Then when an order is placed that command is executed and the "Address" field is shown on the receipt. Does that make sense?
Slow day at work and this felt like a challenge to me. This is probably way too complicated. EDIT: Cleaned up the code a bit and commented it up. ALTER PROCEDURE prc_RedditExercise @stringToParse Varchar(max), @delim varchar(1) AS CREATE TABLE #compressor (cols varchar(max),intID int) DECLARE @i int DECLARE @return varchar(max) DECLARE @delString varchar(max) DECLARE @newString varchar(max) SET @i = 0 SET @newstring=@stringToParse SET @delstring='' SET @return='' --If empty, return an empty string and bail. IF LEN(@stringToParse) = 0 BEGIN SELECT '' RETURN END ELSE --insert the component parts of the string into the #compressor table. WHILE len(@newString) &gt; 0 BEGIN SET @delstring = SUBSTRING(@newString,0,CHARINDEX(@delim ,@newString)) INSERT INTO #compressor VALUES(@delstring, @i) SET @i=@i+1 --if @newstring is null or empty, we're done. IF NOT (@newString IS NULL OR lEN(@newString) = 0) SET @newstring = RIGHT(@newString,LEN(@newstring) - (len(@delstring)+1)) END --Grab the lowest ID values for our unique component parts UPDATE #compressor SET intID = (SELECT MIN(intID) FROM #compressor C2 WHERE C1.COLS = C2.Cols) FROM #compressor C1 --re-assemble the string back in the order we found it, but now only unique values. SET @return = ( STUFF( ( SELECT cols + @delim FROM #compressor GROUP BY cols, intID ORDER BY intID FOR XML PATH('') ), 1,0,'') ) DROP TABLE #compressor --Return! SELECT @return 
Sort of. I suppose I would need to know some more of the flow of the CMS to understand more. The only extra bit I can think of is that maybe you need to use an output variable instead of just a strait select. Examples can be found here https://msdn.microsoft.com/en-us/library/ms188655(v=sql.110).aspx 
Yeah, I'm going to have to do something ugly somewhere. I think I'm going to remove the Left Join and do inline selects for the forecast values. The other alternative seems like it would be to build the hierarchy first then later traverse the hierarchy backward for each User to see if a manager has an Override. 
LOL "regular SQL" as if microsoft's offering was anywhere near "regular"
Yes, if by the first two, the last number is supposed to be 14 and not 1. 
I'm not able to try it, but couldn't you try placing the two pieces of the UNION into their own AS'es and then just unioning the result? I've never tried it, just throwing things at your giant query to try. with A as ( giant wall of code ) ,B as ( first-part-of-union-using-A ) ,C as ( second-part-of-union-also-using-A ) select B UNION ALL select C ?
A But B could be OK, too.
It's not a homework. I got questions that were similar to these in a job interview.
Haha, I don't give a shit. Just sounded a lot like a homework assignment
well. uh. erm. https://en.wikipedia.org/wiki/SQL-92 https://en.wikipedia.org/wiki/SQL:1999 https://en.wikipedia.org/wiki/SQL:2003 https://en.wikipedia.org/wiki/SQL:2008 https://en.wikipedia.org/wiki/SQL:2011
That's not what I was saying. Go try to install "SQL:2011" then. It is a standard, but you are not going to find any database engine that follows it exactly. 
Most DBAs will give you crap about using SELECT * as it can avoid using the indexes on tables. 1. SELECT C.first_name, C.last_name FROM Customers C LEFT JOIN CustomersCar CC ON C.user_id = CC.user_id WHERE CC.car_id IS NULL 2. SELECT CR.car_id, CR.brand FROM Car CR LEFT JOIN CustomersCar CC ON CR.car_id = CC.car_id WHERE CC.user_id IS NULL 3. SELECT C.First_Name, C.last_name, SUM(CR.debt) AS [All Debt] FROM Customers C LEFT JOIN CustomersCar CC ON C.user_id = CC.user_id LEFT JOIN Credit CR ON CR.credit_id = CC.credit_id WHERE CC.car_id IS NOT NULL AND SUM(CR.debt) &gt;= 1000
Postgresql is the program, like Oracle or MS SQL Server. They have their own tweaks, but it's still SQL you use to talk to the RDBMS. 
I think this is the best response in the thread by far. Outer joining where tableB.id is null is much better than using not in/not exists.
Please use joins like a normal person :)
Are you using SQL Server? Check the login properties of the account you're using, if it's set to 'British English' that could be the issue
 I only know of one, but like it. http://sqlzoo.net/
I started here 5 years ago and have come a long way from then. Highly recommend!
Buy the 70-461 training kit and complete the exercises on querying. 
sql-ex.ru
I came here specifically to recommend this resource. 
Have you used SQL Profiler to run a trace to see if the params passed to the proc are what you think they are?
No. It's dependent on the specific database your using but as a general example, "ma" is not &lt;= "m" but is &lt; "n". You really shouldn't be using mathematical comparisons on strings like that due to all the little issues like this. 
The second query is the more correct, as anything not starting with n (e.g. mzzzzzzz) would be included in the correct section. In your first query, mzzzzzzz would be included in the "n" section. Another approach is to make a comparison with the first letter, by using your db's string functions. Rather than comparing a whole string to a letter.
As a best practice you never want to reference column number, so no order by or group by 1. When you say you want to order by something alphabetically why don't you just use order by? If you only want to select schools that start with a set range use: where school_name LIKE '[a-n]%'
Whats the full query and error message? 
Am I doing this right? It just seems to run forever. BEGIN SET NOCOUNT ON; SELECT DISTINCT ROW_NUMBER() over (order by T.Product_ID, T.PO_NO, T.LINE_NO) AS RowID, ISNULL(T.PRODUCT_ID, 'NULL') AS [Commodity], ISNULL(T.PO_NO, 'NULL') AS [PO NO], ISNULL(T.LINE_NO, 'NULL') AS [LINE NO], QUOTENAME(T.DESCRIPTION, '"') AS [PO Line Description], QUOTENAME(C.DESCRIPTION, '"') AS [Commodity Description], ISNULL(T.FY, 'NULL') AS [Fiscal Year], PH.Vendor_ID AS [Vendor ID], QUOTENAME (V.Vendor_Name, '"') AS [Vendor Name], T.QUANTITY, T.UNIT_COST, T.QUANTITY*T.UNIT_COST AS [Line Amount], (Select CAST(0.00 as numeric(10,2))) AS Sub_Total_Cost INTO ##TmpPOReport FROM dbo.DBVW_FI_REQ_PO_ITEMS T INNER JOIN dbo.FI_VENDOR FV ON T.INST_ID=FV.INST_ID INNER JOIN dbo.FI_REQ_PO_HEADER PH ON T.PO_NO=PH.PO_NO INNER JOIN dbo.FI_VENDOR V ON PH.VENDOR_ID=V.VENDOR_ID INNER JOIN dbo.FI_COMMODITY C ON T.PRODUCT_ID=C.FI_COMMODITY_CODE WHERE T.INST_ID = 'SC00' AND T.FY = '2015' AND V.VENDOR_TYPE = 'V' AND T.PO_NO IS NOT NULL AND (T.PRODUCT_ID &lt;&gt; '' AND T.PRODUCT_ID IS NOT NULL) AND T.QUANTITY*T.UNIT_COST BETWEEN '19000' AND '20000' Order By Commodity DECLARE @PID varchar(15)=00, @QUANTITY int, @UNIT_COST numeric(10,2), @PrevID varchar(15), @RowID bigint, @PrevRowID bigint, @RowAmount numeric(10,2), @SubTotal numeric(10,2) = 0.00 SET NUMERIC_ROUNDABORT OFF; WHILE EXISTS(Select TOP 1 * FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) BEGIN SET @RowAmount = (Select TOP 1 (QUANTITY * UNIT_COST) FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00) SELECT TOP 1 @PID = Commodity, @RowID = RowID FROM ##TmpPOReport WHERE Sub_Total_Cost = 0.00 IF(@PID = @PrevID)AND(@RowID &lt;&gt; @PrevRowID) BEGIN SET @SubTotal += @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND RowID = @RowID SET @PrevID = @PID; SET @PrevRowID = @RowID END ELSE BEGIN SET @SubTotal = @RowAmount; UPDATE T SET Sub_Total_Cost = @SubTotal FROM ##TmpPOReport T WHERE T.Commodity = @PID AND RowID = @RowID SET @PrevID = @PID; SET @PrevRowID = @RowID END END SET NUMERIC_ROUNDABORT ON; SELECT * FROM ##TmpPOReport WHERE [Line Amount] BETWEEN '19000' AND '20000' DROP TABLE ##TmpPOReport END
I've been using the same method as you on a relatively small tsql database I manage. I'm sure there's a more efficient way to go about it, but I'm not sure how.
My guess given the information provided is that the transaction has an error but isn't caught/handled correctly. It should look something like: *** DECLARE @errorCount INT = 0; BEGIN TRY BEGIN TRAN INSERT INTO stuff(value1, value2, value3) VALUES(1, 2, 3) -- Do stuff COMMIT TRAN END TRY BEGIN CATCH IF(@@TRANCOUNT &gt; 0) BEGIN SET @errorCount = @errorCount+1 @ErrorMessage NVARCHAR(4000), @ErrorSeverity INT, @ErrorState INT; SELECT @ErrorMessage = ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE(); RAISERROR (@ErrorMessage,@ErrorSeverity,@ErrorState); ROLLBACK TRAN END END CATCH IF(@errorCount &gt; 0) SELECT 'Some transaction(s) did not complete!' *** Edit - As a side note, this definitely sounds like you should consider utilizing a staging database for this on separate hardware. Alternatively if it already is, pull the processes apart and build them better, there sounds like there is a ~~significant~~ **monumental** amount of room for improvement in this process. (*Dropping and recreating tables?* / *Are all the transactions in one block?* / Why is this being ran from a PC?? / *etc*) *Edit2 - Added an error count check.* *Edit3 - To answer the original question as well, it's probably taking locks out on everything which will halt everything else related until it commits.* 
You should cast T2.ID to an INT/Float instead of casting T1.ID to a string. 
I'd make a customer table, a products table, and an orders/sales table. 
Your Order line table would have all of the contents of an order. So you would have your Invoice table, which would have links to customer, addresses, terms, ship to, dates etc, then the Line table would have the items, quantities, and pricing information. This would allow you to facilitate making an invoice for more than one order or something along those lines. It would also allow you to facilitate blanket orders (purchase orders that continually have new lines added, like when you have a negotiated price with a customer)
The first two things I would suspect would first be a trigger on the table, then second, PayeeName might be a calculated column.
When I do that, though, I get either: "Conversion failed when converting the nvarchar value 'ae36e2a70d5576c35eb285c966c67df5' to data type int." OR "Error converting data type nvarchar to float." What can I do with that?
Ran into a few problems using your code today. Does it explicitly have to be ran as a function instead of a stored proc? And if it doesn't matter then I'm definitely doing it wrong. In the other procs I have / use on the same server I'll execute them such as: select dbo.storedproc([column]) from table Which didn't work. I then tried: exec dbo.storedproc(...) But all I could successfully get was for it to return a '.' -- I also read about cursors, but I'm not sure if that's needed? To be clear here I'm expecting to run this on a table with thousands of strings to parse, so it was weird that it would only return a single period even after I defined the variables to pass to it in the exec command. 
Right click on the table in your object viewer and click on design. Find the column in question and click on it. Properties for the column should show up in the bottom pane. I don't recall exactly where, but there will be a property for a calculation or a formula. If there is a formula in that property, then it's a calculated column. Sorry I can't be more specific at the moment, I'm not in front of an SSMS instance right now and I don't use the GUI frequently. 
SSIS : read data from source, massage the format (maybe do some JOINs, etc), save the newly transformed data to a destination SSAS : "process" data aggregates (sum, stddev, avg, etc) into a "cube" database, that you can query across any slices of data (total invoice dollars per second/minute/hour/day/week/month/year) with subsecond response times. SSRS : hard code them Excel files so that they can be "official", saved as PDF, run on schedules, etc.
I actually got it to execute as you described perfectly now that I understand and think about it. The problem being is that now I have to take it a step further and integrate it as a process into a larger set of queries, and for thousands of values at a time. If you give me some direction in how to convert I could explore.
SSAS is really impressive when you first see the response time and the custom slicing and dicing you can do with data. The important point to make about what it's doing and why it can deliver subsecond response times is that it's precomputing and storing the answers to the various slices that are possible in the data.
Either way, without a proper schema/sample I think we've given all the useful information we can. 
You could put in two variables in the select statement ( @dateisnull, @dateisadate), start one with 0 and the other with 1, add 2 to @dateisnull when the date is null, and 2 to @dateisadate when the date is not null, then order on least(@dateisnull,@dateisadate)and take the first 500. Otherwise, the order in which rows are returned is controlled by the engine. so select * from ( SELECT *a.*,b.*, @dateisnull = case when b.date is null then @dateisnull + 2 else @dateisnull end nulldatecount , @dateisadate = case when b.date is not null then @dateisadate + 2 else @dateisadate end datecount FROM A LEFT JOIN B on A.id = b.a_id inner join (select @dateisnull=0 a , @dateisadate=1 b) b on 1=1 WHERE (b.date is Null or b.date &lt; curdate()) order by least(nulldatecount,datecount) ) x1 LIMIT 500 I haven't got MySQL on this box so I can't really test it. But since you'd had no reply for 14 hours, I thought I'd give you an idea on how to attack it.
I absolutely can do that... but I may need someone to explain how to script it out. I'm really not that experienced at SQL - I have had no formal training. I just know how to write the queries..... Sorry. :(
Look up! Space is cool! :)
I'm two years out of undergrad with about 3 years of office experience. I guess just doing research makes me think I can ask for a bit more. they're offering me 16$ an hour now which seems pretty low. 
I would be happy with 20$ an hour. I'm in the U.S. (I should have clarified) with a fairly high cost of living. thanks for he advice!
&gt; run it as a select statement first so they can see what they'd be deleting. Good catch. I and they do this now. My co-workers already know basic select,update and delete statements by nature of the work we do, but having a more formal breakdown of these I think would help, especially considering any new people we hire will need to know this information at some point too. 
Also, think of this as a stepping stone. You will quickly gain new skills that will qualify you for much better-paying positions. 3 years of "office experience"? i would have guessed around $15/hr. 3 years of DBA experience is something else, entirely.
You are a god.
You can make $20 an hour doing excel reports. For real.
Absolutely! Ask for more, try to get more vacation days. This is the most leverage you will ever have when negotiating with this company. Keep your numbers realistic. Mention other offers, if you have them. Make them feel that they are getting a bargain, even at your higher number. Then use your time there to learn and do as much as you can so you can make a big jump with your next job. 
Maybe he works on a database team where everyone is named Stuart. 
Then I hope the coffee mugs in the breakroom have a better PK than `GivenName`
Is this data stored in Access, or are you just querying a real database from within Access? &gt;What makes it extra hard is i stored all the data types as TEXT. So I cant just use a datediff or timediff function 1. I hope you've learned from this. 2. You can concatenate your date &amp; time, then `CAST` the string to a `datetime` datatype, then you will be able to use time functions. 
I've done this with co-workers before, and I used [w3schools](http://www.w3schools.com/sql/) as my syllabus. However, before you dive into the code, it's a good idea to just talk about database theory for a bit. You don't need to get too in-depth, but anyone touching a query window should understand set-based design, primary-foreign key relationships, transaction rollback/commit, and that cursors are almost never the right answer.
Right click on the table in the Object Explorer, Script Table As, Create To
Is there a way to specify primary and foreign keys in SQL? I have 0 experience creating tables. I have only created queries and views, you know? I have posted the script info [here](https://www.reddit.com/r/SQL/comments/3pe0xh/ms_sql_cast_on_join/cw6ibgk) if you wouldn't mind taking a look. I am 100% sure I did something wrong (probably in how I set up the tables). Thank you for your help so far!
Thank you so much for the info! That is really interesting to learn. And honestly, I am kind of learning as I go on this bit. I know how to write queries and views, but creating the tables from scratch is not something I have had any training on. I am kind of winging it. What does ":=" mean? I am totally ok with dropping the table and recreating it with the correct data types. I just need to know what they should be. I have posted the script info [here](https://www.reddit.com/r/SQL/comments/3pe0xh/ms_sql_cast_on_join/cw6ibgk) if you wouldn't mind taking a look. I am 100% sure I did something wrong (probably in how I set up the tables). Thank you for your help so far!
&gt;when people throw the words "OR", "AND" or weird characters like "&amp;" into the field you're joining on sometimes you have to cast as varchar. Could you explain this just a little bit? Why do those words make a difference in the field?
Listagg here is an aggregation routine. It works on a grouping of columns, hence the `group by ID, NAME, CODE` and creates a delimited list. `listagg(to_char(TOTAL), ',') within group (order by DATE) as TOTALS_LIST` says for every combination of the grouped columns, create a list of the TOTAL values for that group, separated by a comma. To `to_char` is because listagg is a string function and TOTAL is a number. The `(order by DATE)` just order the items in the list according to date. The other listagg does the same for the dates and orders them the same way. 
For keys, int. That data normalization page should give you all of the info you'll need. It's daunting right now but it'll click for you once you start using it a bit more.
Yes, that was an amazing auto-correct on that one. Leaving it because it reminded me of an [ancient MadTV Clip](https://www.youtube.com/watch?v=NY1l25OiLBM).
This is a bit of a simplified explanation, but to include the null felds the table City should be your main table. Then 'left join' onto the rest. A left join will show all results from the main table and those that match from the joined table(s). Research table joins, it should help. You can think of them as Venn diagrams. 
You can make $20 an hour typing data into a system all day... Doesn't mean you should though (assuming you can find other positions at a comparable pay grade). Unless you really like data entry of course ;)
Casting/converting to a standard format in a sub-query is probably going to be your best bet.
Hey so i was able to use your advice on dealing with excessive columns. However, with the csv, I followed the advice and created the file, but i don't know how to access it.
based on Internet research it seems like 50 grand a year is the typical salary for this type of position. they are offering me around 33. I understand I don't have much leverage, just thought I'd ask the opinion of sql developers. thanks for your advice.
thanks! I'll let you know how it goes 
I just meant initially 20$ an hour. I figure learning sql on the job makes me more attractive to employers than learning it on my own. it's worth it for me to learn and get paid less for a while.
When I lived in Denver, CO two years ago, I got a job as a consultant writing SQL queries and SSRS reports. I had no benefits and only a very slim working knowledge of SQL (Simple select statements with a couple joins and a few aggregate columns). I was only 2 years out of college and the hourly rate was $25. I have moved jobs twice since then, and each move has come with at least a 30% pay increase... So stick with it, and learn SQL as best you can. The next job you get you can be more picky with your rate.
Yup, still in it. You'll most likely get pay bumps from switching jobs, unfortunately. I just started a new one as a data architect, now. Mostly focusing on ETL for software conversions for clients. 
Were you doing the same thing (queries and SSRS) each time you moved? I ask because I'm doing the same thing
&gt; So I have a table where the dates are formatted as such: 15-Jan-13 two questions: which database? i'm guessing microsoft sql server, but you gotta tell us explicitly (please see sidebar) which datatype?
Still making everything red as shown here: http://puu.sh/kRqEp/4f122ab1ee.png If I don't have the ROWS UNBOUNDED PRECEDING, none of the red lines occur. However, in both instances, there is an "Incorrect syntax near 'order'" message. What am I doing wrong? Much appreciated.
You have some extra parentheses in there. You can remove the one before SUM(), the one just before ROWS UNBOUNDED PRECEDING and the one after ROWS UNBOUNDED PRECEDING. The PARTITION, ORDER BY, and ROWS UNBOUNDED clauses all need to be within the OVER(). Try copying the text exactly from above.
Is the column stored as a varchar2 or is it actually a timestamp or date? (it actually looks like a timestamp with timezone) Describe the table and look at the column type. If it is stored as a varchar2, then your DB design is flawed. Dates and timestamps should be stored as those types. Your query can be written as : select * from MY_TABLE where MY_TIMESTAMP between to_date('01/01/2014', 'DD/MM/YYYY') and sysdate If MY_TIMESTAMP is a timestamp you can use as is, if it is a varchar2, you can convert than format to a timestamp with time zone using the following.... `TO_TIMESTAMP_TZ('15-JAN-13 02.23.27.000000000 PM -05:00', 'DD-MON-RR HH.MI.SS.FF9 PM TZH:TZM')`
I did exactly this in the UK, earning under $20/hour doing mostly Excel, transitioned to SQL work and learned on the job. Now on three times as much about five years on. Just don't be afraid to move companies once you've got some experience unless you're getting pay rises to match the new skills.
thanks. they did mention that they stay competitive once they finish training us. 
sounds really cool. thank you for typing that out. 
At the last place I worked we had a database that was used by a call center. A person would call in and the phone rep would make a new account and put the account # of the original account into a varchar(100) field. The goal was that we could cleanly join from that field to account# to get the original record. The reps would sometimes put "Accounts 12345 AND 456789" into that field or "123 AND 456" You can't cast either of those as any type of numeric. So when I had to join the original accounts to the account created by the rep I used a case statement and made a list of records that wouldn't join for the project manager to fix. I could have cast the original account number as a varchar and joined that way, but I didn't want to deal with the fact that sometimes there were (differing amounts of) leading zeros. eg "0123" &lt;&gt; "123" &lt;&gt; "0000000123".
It was from a csv imported to access. Does cast work on access? 
It's the same function, just pass DATEDIFF a different first parameter "s".
We added a row_number partition on the mugs.
Some people get paid $0 to be taught sql. They are not low balling you
good point
VARBINARY just stores the binary data of an object. It's "safe" to use, but there are drawbacks and benefits to using VARBINARY vs using FILESTREAM. I can't get into the details here but it should be pretty easy to google
It should be a comma between fieldname and format mask, not a semi colon. To_Date ([Actuele Personen].[Verblijfsplaats historie].[Datum Adreshouding Nr.], 'yyyymmdd') &gt;= ?P_Datum vanaf?
So, no security concerns in using VARBINARY?
Define concerns.
The 70-461 training kit covers SQL server 2012. If you are using an older version of SQL at work you should still be fine. Most of what's in the book can still be applied. I bought the developer edition of SQL server 2012 for 60 dollars on my personal computer. I'd recommend the same. Again, I can't recommend buying this book any more. It will jumpstart your career. I created a spreadsheet that's helps you study for certifications. It has KPIs such as percent read, days before exam and a confidence metric that increases as you complete tasks. If you are interested I'd be happy to set it up for you. 
I'll take your word for that. Then what about performance? Is it better to store pictures in database or disk?
I've been told (And seen it work) that I have to use the semicolon. It's bloody strange but I gotta. I've managed to work around the fault in the meantime. Still thanks for the tip!
You could try use Format before the CDate, also added a TRIM to ensure that you didn't import a bunch of spaces that could be causing problems. *** SELECT Format(TRIM(dateCol) &amp; " " &amp; TRIM(timeCol), "mm/dd/yyyy hh:nn:ss AM/PM"), IsDate(Format(TRIM(dateCol) &amp; " " &amp; TRIM(timeCol), "mm/dd/yyyy hh:nn:ss AM/PM")) AS IsValidDate ***
Cant i just use cDate instead of Format?
Remember that you have to log that binary file. In experiments saving files in SQL Server, I've found that any file over about 8000 bytes sees serious overhead in the transaction log. The worst case I measured was a 10MB file that required 33MB to log the file insert. When you write to a file system, you're writing once. When you use an RDBMS, you're writing twice, at a minimum - once to the log file and once to the data file. The other consideration is memory - that file you're saving as VARBINARY will get pulled through the database's memory once you read it from disk. That will likely evict other data from memory, so you'll eventually have to read it back in. In general, storing binary data in a database is a bad idea. I blogged about it a while back in [Getting BLOBs out of the database](http://www.brentozar.com/archive/2015/03/no-more-blobs/).
Why not just store binaries someplace more appropriate and keep a record of that location in the database?
I don't know, its just what I thought. I remember at one point there was a patched vulnerability in JPG files where it crashes any software reads it. So I figured something similar might happen.
Yeah, seems like it'll be too expensive to store files in database
Your os is more likely to "read" the file than the database. (Think Windows indexing, thumbnail generation, etc)
You have to make an oracle account but their brand new live sql site is amazing for this. https://livesql.oracle.com 1. Create an oracle account and login. 1. Click on the *Schema* tab 1. On the right hand side change the *Schema* select list to HR 1. Observe the different objects that exist in this schema. 1. Click on the *SQL Worksheet* tab 1. Query any of those objects, for instance: select * from hr.employees 1. You can also execute any of your own schema scripts and play around here.
Am I the only one who doesn't know what OP wants?!
Yeah, we really need to know what you want before we can offer help. That said, in this query, you're grouping by columns that you aren't selecting. Either you'll end up with duplicate rows of selected fields, or the extra columns are dependent on the selected fields, and should be removed from the group by clause.
Sorry updated with more information at bottom.
Sorry updated with more information at bottom.
Sorry updated with more information at bottom.
Think I figured it out. I was selecting "Create a file for each database" and now I'm sending the backup to a predefined file (Is that considered a meda "set?" and choosing to overwrite if a backup exists). Hope that works. 
Codeacademys new tutorial is good.
so you said what you want output to look like. But you are using inner joins. Inner join means only return data if the value matches in both tables. Is that what you want or do you want to pull information even if it doesn't exist in one of the tables? 
You could always just run: *** BACKUP DATABASE yourDatabase TO DISK = '\\otherServer\sharedFolder\yourDatabase.Bak' WITH COPY_ONLY; *** Edit - Put that in a job and ensure the SQL Job Agent has permissions to that share/folder.
That depends how you do it. IF you do have to store them, make a table that only has a key and the VARBINARY information then have another table with a foreign key to the it that has the file information. It also lets you store the files on a different partition, which means you could use slower/less expensive storage for the binary component. 
Download SQL Server Express and install an instance on your own machine. You may already have a development license considering you have SSMS installed on your machine. You may just need the media (SQL server install disk). You can download adventureworks or start creating your own databases. You can though write very advanced queries with read only access. You can use CTEs and Table variables to build smaller subsets of data rather than (and probably should be) writing temp tables.
If I'm not wrong, this doesn't happen when the file is still stored in the database, does it?
Yeah, no fun at all!
from the 3 tables what information do you want output? also i'm confused by your example. There isn't any normalization... 
Anyway you can get a backup of the Prod dB to play against? This way you are learning what could be useful for the company while not really messing anything up.
Correct, it does not. Sql stores it just as a "blob", it doesn't try to understand it, it's just a string of bytes.
been a while since I used php but there are several pieces of optimization you could look at in your php.ini. Specifically look for something along the lines of prefetch etc... as far as DB design. if the rows are unique and you are only selecting one column. Add an index. WAMP is not going to be your friend when it comes to optimization and fastest speeds. 
Fuck this guy. Test in production. Be a man. Then hit the bar.... 
 SELECT DISTINCT COLUMN2, COUNT(*) FROM TABLE GROUP BY COLUMN2 SELECT DISTINCT COLUMN1, COLUMN2, SUM(COLUMN4) FROM TABLE GROUP BY COLUMN1, COLUMN2
I just want the names and IDs of the people that have the specified types, with the specified codes, at the specified rank but are missing one specified type with a specified code at that same rank Does that make sense?
Unless this is something like a single core machine, 40k rows is nothing. I've dealt with millions that return results in seconds. My suggestion would be to add timer checks within PHP and see how long it's taking to do certain tasks. I would do something like `file_put_contents('timer.txt', date('H:i:s') . ' before .. function', FILE_APPEND);` in a lot of locations throughout the PHP script and see where the timer is taken up. I usually put them all over the place, before/after function calls, when using PDO i'll go before/after prepares, executes, and fetches just to see what is taking a long time.
Select Name, ID FROM MyTable WHERE ID IN (Select ID From MyTable Where Types='meat' AND Ranks&gt;3 AND Code=110 AND Code &lt;&gt; 125 etc)
Your join is incorrect on the above query. :-) A join is used when the value in a field in the TEAM table matches a value in the TEAM-PLAYER table. Right now you're trying to get the values 100, 101, 102... to equal the values East, West, Central.
I changed both to TeamID but I still got the same output.
Your question in post doesn't match the question in the image.
But going forward a safer Where would be Where TP.EndDate IS NULL OR TP.EndDate &lt; Getdate()
nvm Thanks it helped 
Ok, I'm finally back on a desktop, instead of mobile, so let me go into a bit more detail. Firstly, to qualify, I am a developer at a place that has a 2.5TB MSSQL 2008 database, which is entirely binary data. Is it bad? Yes, I still believe it's not the best idea, but it has it's merits. Pros: * Acid compliance to a transaction level. * multi-source accessibility. Applications, Web Servers and other clients/sources can fetch/manipulate the binary data, and needs only a SQL connection to do so. * permissions can be managed via SQL permissions * relatively easy to consume, once you've got the groundwork in place Cons: * Backups. The DB is huge, so backups are huge. I guess you get this everywhere, but I find backing up 2.5TB on a file server easier than 2.5TB in a single SQL DB * Memory usage. SQL caches objects in memory, and binary data gets included in a number of aspects. If you use a single DB for both your normal data and your binary, you end up wasting a lot of cache memory (and thus page life expectancy) on stuff that probably isn't as frequently used. (if your use-case would benefit from this level of caching of binary data, maybe its a pro) -- We worked around this by moving all binary storage to a separate VM on a different host, thus meaning our main DB can have its full allocation of memory/cache for use for row data. Your security angle really isn't an angle at all. Whether in the file system or database, binary data really doesn't pose a massive threat, since they're non executable file formats that typically aren't accessed. (don't let windows index your data storage area, etc.) Look into SQL Server's File Stream option if you've got a new enough version of SQL. This is a hybrid approach that gives you the best of both worlds. We're migrating our 2.5TB to File Stream if I can get my head around SSIS any time soon!
I think they kind of go hand in hand. I don't think window/windowing functions are nearly as handy without.
Honestly, don't learn it online. Set up a local database on your PC and start working through some exercises - but that won't really teach you shit. Use this to get comfortable running queries, learning things like syntax, etc. When you've gotten that far it's time to start *making* a database. Do it with something you're interested in, for example basketball stats. Scour the web for some data files, maybe create some custom param tables on your own, and then start using it to answer questions such as, "Who scored the most points between 1985 and 1988 in the Eastern conference but who never played a career game for a Western conference team." Or, "What player born west of the Mississippi scored the most points playing for a team located east of the Mississippi. Then start coming to forums like this and asking for help until you've managed to answer that question. Then repeat that process. Learn how to graph the data, export to Excel, etc. That will get you ready for a job. The goal isn't to "learn" SQL, it's to learn how to use SQL to accomplish a task once you understand how data moves from one table to another table, how the tables are structured &amp; indexed, what params you might need to create in order to accomplish it, and then finally (and importantly) how to optimize your queries in such a way as to provide efficient answers.
I don't agree. You don't need CTEs to leverage window functions, some RDBMS implemented the latter before the former. An equivalent to CTEs (query refactoring) can be achieved through materialized views, views, sub queries, temp tables, whereas there is no easy functional equivalent to window functions.
When you use GROUP BY, it segregates the rows by the distinct values in that column, then performs the aggregate function. In this case COUNT() and SUM(). For example: C1 C2 1 1 1 2 1 5 2 4 2 7 3 1 3 5 GROUP BY C1, would break it up: C1 C2 1 1 1 2 1 5 -- 2 4 2 7 -- 3 1 3 5 Then performing a SUM() would work within those groups. C1 SUM(C2) 1 8 -- 2 11 -- 3 6 The DISTINCT is redundant in this case since it already happens as part of the GROUP BY. 
Honestly, Oracle has pretty decent documentation: http://docs.oracle.com/database/121/index.html I recently had to get back up to speed with Oracle for a project and I found the documentation, overall, to be excellent and thorough. Tom Kyte's "Expert Oracle Architecture" is hard to go wrong with. The OReilly PL/SQL one is pretty thorough too. 
Try [schemaverse](http://www.schemaverse.com/), this is an online space sim game that runs entirely inside a database using sql commands.Practice with a live database is infinitely more useful than just completing online tutorials. 
You should consider a career in Data Science. I spent some time doing data work during an internship as a software engineer (also a possible path) at a top tech company and better knowledge of SQL/Data engineering in general could have taken me very far.
What you are looking for is called a [Composite Key](http://www.orafaq.com/wiki/Composite_key).
Haha yeah that was the problem thanks so much 
Great! Thanks!
He hangs out in here sometimes too :-) /u/tkyte 
This looks entirely too complicated if I understand what you are trying to do. A sub-query with a SUM should be all you need. *** -- Obviously there is some joins and filters for the table placeholders I'm using, -- but this should be the general gist of it SELECT * FROM all_your_tables AS ayt INNER JOIN (SELECT t.Commodity, SUM(t.Unit_Cost) AS CommoditySum FROM tables_needed_to_get_values AS t) AS cs ON ayt.Commodity = cs.Commodity WHERE cs.CommoditySum BETWEEN @Param2 and @Param3 *** &gt; he said we can't move to 100 mode from 90 mode because a lot of things will break I'd be curious as to what would break when changing this. Outside a few [specific things](https://msdn.microsoft.com/en-us/library/bb510680%28v=sql.105%29.aspx) it's generally pretty safe / advisable to run your compatibility at your SQL Server version equivalent. The only time I'd encountered anything breaking was a SQL 2000 database upgraded to SQL 2012 and a few stored procedures used *(NOLOCK)* instead of *WITH (NOLOCK)*.
I have an BA in English. Ended up getting an MS in Information Technology and Management with a track in BI. So schooling definitely helped me out. I more than doubled my salary in 3 years. I haven't done much learning outside of work nowadays, it gives me plenty of opportunities to learn there. Degree really helped me out.
Tell us a little about your change! Is there a reason you moved from open source to the MS stack? How have you enjoyed the move?
there doesn't seem to be anything here
It looks too complicated or I get it wrong, but what about this simple query with HAVING clause: select company, count(hobby) from table group by company having count(hobby) &gt; 1
Are m, ra, and ra2 variables? Is that how you declare them?
You know what's weird? Microsoft SQL Server Manager suddenly started spitting out my results in what looks like TypeWriter font, even the headers are not normal Windows headers that can be adjusted, etc.. The headers are just part of the query results now
/u/Vardy provided this query from my post in /r/postgresql, i was able to manipulate it to achieve what I was looking for, thanks for your option as well, I will play with it and see if I can get it to do the same as his. SELECT count(*) FROM ( SELECT q.company FROM ( SELECT company FROM testies GROUP BY company, hobby ) q GROUP BY q.company HAVING COUNT(q.company) &gt; 1 ) s ; 
Well those queries that I posted won't help you then. So the way to do it would be to use pivot, which I find difficult. The way I've done this in the past is a series of left joins. It is not efficient and you'd be better off using a pivot, but I suck at pivots and they are much harder IMO. This is good enough for a small number of results such as employees because I doubt you have 1m+ employees. Example Name Sequence Info Mike 1 Manager Mike 2 New York Sally 1 Analyst Sally 2 New York Sally 3 Mike . select a.name, a.info, b.info, c.info from employee a left join employee b on a.name = b.name and b.sequence = 2 left join employee c on a.name = c.name and c.sequence = 3 This would return . Mike Manager New York Sally Analyst New York Mike
You must have hit ctrl+d Under query &gt; results to Change this from text to grid.
Will be in 9.5
&gt;Removing Group by or adding Group By makes no difference. Adding GROUP BY &lt;insert all columns&gt; is the same as saying SELECT DISTINCT If there are no duplicates, there wont be a different in your query output. If you do SELECT GENDER, COUNT(*) FROM dbo.employees GROUP BY GENDER ----- MALE | 12 FEMALE | 14 You are doing an aggregate and your grouping your results by gender. You have 26 employees. 12 with MALE as gender and 14 with FEMALE as gender. SELECT GENDER, Name, COUNT(*) FROM dbo.employees GROUP BY GENDER,Name You'll get 26 lines, and your count of male/female will always be one unless you have multiple people with identical names. You are asking "give me a count of genders by name". If you omit name from the group by you get an error as you can't include name and continue grouping by only gender. How exactly is it supposed to display name when you are grouping only by gender? It's far more common to use DISTINCT over GROUP BY to remove duplicate results. GROUP BY is used with aggregates all though it performs the same function as DISTINCT, it just takes longer to write.
Thanks... I think it's because i'm out of memory i have like 20 queries open
they are not variables. They are aliases for the tables. It's shorter to type ra instead of rating etc... 
Right, they are almost the same, but I'm not sure if you should group by "hobby" while using this having clause. It looks like the query returns all companies with at least two employees sharing the same hobby.
yall were both right, i was stupid and forgot to add my WHERE clauses to yours. congratulations!
You're writing and modifying SQL. Do you touch views? procedures? Edit a few reports? What size databases do you work on? How many users do they support? Talk it up on your resume and switch jobs. I got pigeonholed into writing glorified mail-merges in a proprietary language, but had enough SQL experience on my resume I was able to get an Associate DBA spot on a team. I was honest with them, but expressed a great desire to learn and be mentored. They understand I'm not going to be a Rockstar right away. You're verifying accuracy. Working within an SDLC. Working on a team with Users, Analysts, DBA's, Managers, and other Stakeholders. Think hard about what objects you touch, and who you interact with. It all counts. Don't LIE, but you can make selects and updates sound impressive to the right people.
I think the vehicle(s) these plates were attached to is the real story.
Real low too... Reeeeeeal low
Data science has a lot of options and possibilities, but it's definitely a higher level position/career path. In a recent survey conducted by O'Reilly, only &lt; ~20% of data scientist had less than a master's. 
Sorry man, for some reason I didn't see this comment until now. It's really simply actually -- when initially entering your Data Source as a Stored Proc type and entering the SP name, SSRS scans for any needed variables and adds them into the object explorer above Data Sources. You can right click them to modify how they react for the end user running the report. This means that you can give the user the ability to input what they want or specify the inputs that are available to them, depending on what the purpose is and who the users are. Works great for me in situations where users want to see how pricing works with __ % markup. Can default the commonly used values and re-run the report on the spot without having to type anything in -- it's just a simple drop down for them if I want. Can also use bit or int variables to handle more complicated logic -- I use int variables and then on the SSRS report display things like "Yes", "No" when in actuality the report is passing in 1 or 0. You're only limited by your imagination.
Does Table 2 have any other columns that you can use to 'match' your data across the tables? If not, you can do a Cross join to put all of the records on table 2 against all of the records on table 1. SELECT * FROM Table1 CROSS JOIN Table2. You'll end up with the following. Without any relationship between the two tables, I'm not sure how you want to reduce this further. I don't know how tables work in Reddit :\ |Col1 |Col2 |Expr1 :--|:--|:-- |a |abc1 |abc3 |b |abc2 |abc3 |a |abc1 |abc4 |b |abc2 |abc4 
Ah that won't do. I didn't validate my output on this one. 
Also it doesn't have to match in MySQL. (welcome funny bugs). You'll get random value for column which is not in GROUP BY. In Postgres it is possible to use primary key in GROUP BY and skip other columns from table.
It's ambitious, I know but at least something OP should check out and consider. More school could be a possibility for him
Ok, show me an example of partitioned running total over a window of the three previous records ordered by several fields, using your approach. It's not impossible, but it's far from easy.
Well, theyd need to modify the table by adding a key, but not by adding a new identifier column.
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/327#Explanation) **Stats:** This comic has been referenced 917 times, representing 1.0733% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cw9znyv)
Where (roughly) in FL are you? FL has a terrific SQL Server community, including two PASS chapters in the Orlando area. You just missed SQL Saturday Orlando a couple weeks ago (or maybe you went? I hope?) but get involved with your local chapter &amp; keep your eyes &amp; ears open for another SQL Saturday near you (Tampa in February, Pensacola in June - there's usually one in South Florida too, but not yet scheduled for 2016).
Do your Google search as TSQL instead of MSSQL. Then you will not usually get MySQL results. I usually put TSQL as the first item in the search as Google sometimes ranks based on search order.
Thanks, that's very helpful.. Do you think it's important to learn MS SQL flavor more so than others? Which is most prevalent/useful?
thanks, this makes perfect sense! :)
That was some great advice. Thank you! What school did you go to? Any recommendations?
I was in Dallas, so I went to University of Texas at Dallas. Finished paying my student loans this year! I would just find a program that allows applicants that come from various disciplinary backgrounds. I should also mention many of my co-workers did not go to school for SQL and merely learned on the job. However, their career progression was much, much slower than mine.
Your original query has production minutes with an aggregate function (sum) and in your group by. This is probably not what you intended and likely the source of the problem.
I started at $55,000/yr in California as a DBA/DBMS Developer. No industry experience, but strong academic record and coursework projects.
SQL is platform independent. Your brand of computer and OS really don't matter, especially for learning purposes. I wouldn't bother with VMs or setting up particular versions of Windows. Check out this sub's [wiki](https://www.reddit.com/r/SQL/wiki/index) for learning resources. 
&gt;I work in major hospitals which employ PCs. Would it be advisable to use VMware and some version of Windows to learn so as to keep a consistent them? You might want to look into what database management system(s) they use and make a decision based on that. If it's SQL Server then that necessitates Windows.
Yeah, I mean I'm really at baseline zero here... my goal is just to learn as much as possible. My concern is that, considering the different types/levels of management systems that ultimately I'll be shooting myself in the foot if I get used to something like MySQL or the Oracle OSX port and then learn to adapt to something else entirely... I'd like to imagine that the changes would be the most painless part, and that just learning the basics is most paramount. Needless to say I'm a bit overwhelmed by all the options and terminology about what's best in getting off the blocks.
I'd probably recommend SQL Server tbh. I think you would be more likely to run into that or Oracle than anything else, and SQL Server is easier to get up and running in my opinion. Not sure whether Oracle can run on a Mac.
It's important to keep track of what is part of the ANSI SQL standard and what is a vendor-specific syntax or function. The standard includes all the basic syntactical elements, aggregate functions, coalesce, other random stuff Functions to manipulate dates and strings (including concatenation) are all vendor-specific. You can learn a lot without getting into any of that though.
Ugh I fucking hate anyone who uses floats as ids
www.codeacademy.com
Express should be fine for where you are. It's easy enough to install additional instances.
Being in the healthcare industry does your company place a large emphasis on on-boarding and standardizing new data sources? Do you utilize MDM and DQS?
There is a site that has a SQL console, so you can create database objects and write queries directly from your browser. Basically a SQL sandbox. It even has most of the different flavors of SQL. Maybe someone here can help with a link?
indexing is likely the issue......where the queries are searching on unindexed fields
OK, if you're at baseline zero, forget all of the advice here. Learn the basics of SQL first. select, from, and where are the same no matter if it's Windows, OSX, MSSQL, etc. You'll be getting sidetracked with VMs and trying to install Windows, and configuring sql server software. You're putting the cart before the horse. Search the web for online learning opportunities. There are lots of them. Codeacademy, W3, etc. You can do it over a browser with any computer or OS. Get this down first, then move on to setting up your own RMBS.
This teaches MySQL, although there's not a whole lot of vendor-specific functionality to chase after with the scope they cover.
I've taught a handful of people to use SQLite for data analysis bigger than Excel but smaller than requires a robust RDBS architecture. Literally the definition of disposable databases.
Check some job descriptions of jobs you'd likely apply for. They'll likely list a particular system. You can then pick the majority holder. My guess for hospitals would be SQL Server.
I'm not 100% sure about access, but in MSSQL you'd need a case statement. Something like... select case when month &gt; 6 then year + 1 else year end as FinancialYear from MyTable group by case when month &gt; 6 then year + 1 else year end Put the same case statement in your group by clause. To get the "2012-2013" column you can probably fake it with dateadd(year, Date, -1) + '-' + year To concatenate the years you'll need to convert them as varchars. You'll need to figure out the exact syntax you need. That should be enough to get you in the right direction.
Use a case statement to calculate the fiscal year from the days, then group by this new column. You may have to update the month specifics. CASE WHEN DATEPART (MONTH,date) &gt;=7 THEN CONVERT (VARCHAR,DATEPART (YEAR,date))+'-'+CONVERT (VARCHAR,DATEPART (YEAR,date)+1) ELSE CONVERT (VARCHAR,DATEPART (YEAR,date)-1)+'-'+CONVERT (VARCHAR,DATEPART (YEAR,date)) END
Yeah, I was looking for an If or Switch, which do exist. I think I'll just have to play with the formatting and syntax. Maybe a good nights sleep will help, been working way too long. Thanks!
Int
This [course](https://www.khanacademy.org/computing/computer-programming/sql) from [Khan Academy](https://khanacademy.org/) might give you some ideas.
Ah, got it. Thanks
Yeah this and sqlzoo were actually my inspiration for data :D 
I did get this to work using pretty much the same logic. I ended up using an IIF statement like /u/core_dumpd suggested. Thanks for helping!
Make your own: https://www.mockaroo.com/ and the Northwind DB from Microsoft
Keep it simple. Just add six months to the income date to get the financial year end. You can play with the formatting later if you really require it. SELECT year(dateadd(month, [Date], 6)) as FinancialYearEnding, SUM(TotalIncome) FROM myTable WHERE (([date] Between #1/1/2007# And #31/12/2015#) AND (project='aproject')) GROUP BY year(dateadd(month, [Date], 6));
Oh good! I learned something about access too! Edit: I kinda like /u/tikketyboo's suggestion to use dateadd() instead of the case. They'll both work but his is a little simpler. I usually try to limit logic in queries to keep them simple.
If this works for SSMS and provide similar capabilities to red gate tool belt for less than 500 dineros, I will pick up a personal copy
Lehman database with all MLB baseball statistics. Google it to find the download. 
It's written in Java and really ugly. Try it! 
Select InvoiceID ,SUM(Cost) ,(Select PostageFee from tblPostage t02 Where t01.InvoiceID =t02.InvoiceID) FROM tblProducts t01 GROUP BY t01.InvoiceID If you want a grand total, add another field adding sum of cost to the result of the correlated sub query.
I spend 90% of my day in SSMS, but when I need to do a quick CSV file import or run something against another DBMS, I just use Navicat's tools. They're not as good as SSMS, of course, but the cross-DB option and import tools are handy to have around. Link: http://www.navicat.com/products/navicat-premium I love Jetbrain's PHPStorm IDE, but I don't know that I'll be drinking the cool-aid with their SQL IDE. And yeah, the fact that it's written in Java does kinda suck.
You're welcome, glad to gave helped. :)
I don't find the fact that it is written in Java has much of any impact - it performs just as well, if not better, than Visual Studio based tools. And at least they bundle Java for the windows client, so I don't have to have it installed on my systems. 
You will need to use some cases
The same data but in a different order? You may be approaching the problem wrong. What are you trying to achieve with this? 
Yep, you're doing it wrong. You need three tables: 1. Users 2. Skills 3. Users mapped to skills Table #3 is only 2 columns - User ID and Skill ID. Then you can join Users and Skills to the mapping table to get the skill names mapped to the user names. This will also (via foreign key constraints) prevent you from creating a user with a new skill that doesn't exist (maybe you fat-finger a skill name and end up with "databases" and "datbases"). Read up on [normalization](https://en.wikipedia.org/wiki/Database_normalization). No trigger required.
You can use the "GROUP BY" clause to get a separate value for each UserReference.
The first one does, but the second one indeed does not given the nature of the problem. Good idea, I've added some emphasis.
Yup, this is the way I was doing it. Still no luck, I think this may be why, something to do with IE version https://social.msdn.microsoft.com/Forums/vstudio/en-US/f87fb3fc-f291-4e2b-bafd-4a254be3047c/design-view-error-was-not-displayed-properly-in-visual-studio-2005-professional-edition-on-vista?forum=vssetup
Make a temp table with all the 'specified' codes/ranks, then join it to the data and perform a count/row number. Where the count is one less than you expect you have your answer. For example, if you have six items specified, you'd look for the max row number or count of five. 
Install the appropriate version of BIDS (Business Intelligence Development Studio) or SQL Server Data Tools (SSDT) that corresponds to the version of SQL Server/SSRS you're running. Then open your RDL file (or SSRS project file) after starting that separately. VS 2005 doesn't know what to do with RDL files AFAIK. &gt;Our company has to use Visual Studio 2005 because it's the only version compatible with the accounting software we use You can have multiple versions of Visual Studio installed on a given computer. In fact, it's pretty much required if you do anything but strictly one type of development.
I think I found an effective basic; https://www.youtube.com/watch?v=vGeRSrZDJ_A thoughts?
I installed this (and did full install on BIDS) [SQL Express 2005 Toolkit SP3](https://www.microsoft.com/en-us/download/details.aspx?id=8531) 
Could you try changing it to concat the values inline instead of passing them. Try and find out why the value is changing. Off the top of my head I can't think of an easy way without changing the code. *If the same thing is running repeatedly with different values, why is there just not a Stored Procedure with two parameters that does it?* Alternatively, if you've exhausted other means. Try [forcing the query plan](https://technet.microsoft.com/en-us/library/cc917694.aspx) to one that you know runs well. Not sure how it will behave without knowing more about how the application calls it. Edit - At an absolutely minimum, you should strongly consider enabling [Optimize for Ad Hoc Workloads](https://msdn.microsoft.com/en-us/library/cc645587.aspx) if you haven't already. It will prevent SQL Server from storing full plans (uses a stub instead) for one-off queries, it will help a lot with cache bloat in, what sounds like is, your situation. Edit2 - Turning it on (note: that this only effects new plans, and you will have to clear out the cache): *** sp_CONFIGURE 'show advanced options',1 RECONFIGURE GO sp_CONFIGURE optimize for ad hoc workloads,1 RECONFIGURE GO ***
not enough information. We don't know what type of content you are updating. Adding more data . removing data etc... edit: Also, coming from an oracle background. That update syntax looks messed up. you are updating every single row with a cartesian join because your update doesn't actually have a where clause attached to it. I may be reading it wrong... what is the *t* alias pointing to? 
"t" points to "data". It doesn't need a "where", they're updating every row. Though, you're right about that weird join. Where A = A? We must be missing some code.
It's a rough estimate, chill. I know machine specs matter. I know processing threads matter. I know indexing matters. I'm just trying to help OP out given that I have done something at least remotely similar.
&gt; I'm just trying to help OP out You're not doing them any favors if your estimate is incorrect.
What I'm trying to do is add an AccountCreated column which adds a date. The date is found from the earliest tDate associated with that UserRef. So if UserRef appears in 3 rows, with tDates 01/01/15, 01/02/15 and 01/03/15 then I want to populate the AccountCreated column for those 3 rows with MIN(tDate) which is 01/01/15. I tested the code against a subset of the data (50,000) rows and it worked. 
I'm not very familiar with the sqlite query optimizer, but it may be executing the subselect for every record - not optimal. To prevent this from happening you have several options. One would be Update t Set... = u.minrecord From tabletoupdate Join (select userid, min(date) from dates group by userid) u On u.userid = t.userid You could also use a common table expression. (on mobile, sorry for kurt texts) 
You can create a plan guide of type TEMPLATE for the parameterized form of a query using [sp_create_plan_guide](https://technet.microsoft.com/en-us/library/ms179880\(v=sql.110\).aspx). So even if your parameter name changes it should use the plan guide. However, your vendor shouldn't be changing parameter names, simple as. It's a bug in their code and is the simplest source to a solution.
 How can you claim a time when you don't know how big the dataset is? For all you know his average row length may be 1MB?
OR provides that functionality - think of it as evaluating two WHERE statements. You will run into instances where that isn't sufficient. In those cases, use IN: SELECT &lt;whatever&gt; FROM &lt;Any Table&gt; WHERE &lt;whatever&gt; IN (Value1, ... , ValueN)
There's also the question of I/O and the total size of the tables being used. I don't know how SQLite handles caching, etc. but if you're working with slow storage the I/O hits (if it's constantly going back to disk) will ruin you here. 8GB RAM means nothing if 7GB are consumed by other apps running at the same time.
It's a correlated subquery. For each record in `data`, it's looking for the earliest `tDate` for that record's `UserRef`.
You could accomplish this with case statements. SELECT CASE table1.mseq WHEN 42351 THEN table1.data1 END as Score. (Obligatory "your schema is bad and you should feel bad.")
Maybe this one can help you! You can export your queries through a very flexible interface. https://github.com/stevenbarragan/julia_builder
 SELECT table1.data1 AS score , table1.data1 AS points , table2.name , table2.code FROM table1 INNER JOIN table2 ON table2.seq = table1.seq WHERE table1.mseq IN ('42351','353452345') 
The table 'cafe' doesn't have an 'ID' column? As far as I can see, you can't link/combine/join the two tables, because you are missing a 'shared' column (like cafe_id).
sorry i've updated the schema in the first post
Depends on DB platform, but my advice, don't use them. They prevent people understanding SQL. 
If TSQL you can store the lat/long as a Geography type (in another column?). Then use the STDistance function to calculate the difference between your given point and the Lat / long of the cafe. Then join with the cafe table on the id to also show the name / desc. Order by STDistance and select the top 10. If you're not using TSQL I am not personally aware of inbuilt distance calculations for other flavors of SQL, so you would need to write your own trig function to calculate the difference between two points. Not actually that difficult and I'm sure there are examples out there, but not trivial either.
Thanks very much for your time again that's great. Although I want that statement to work in a pie chart therefore i would want it to count the placed and unplaced students, is this as simple as adding the count function in? Sorry i am still learning sql. i really appreciate your help so far but don't worry if your busy. 
Your logic seems sound to me but fn_FileExists(@pathAndFile) is obviously not returning 1 in your case. Now you have mentioned it worked for a while and then stopped, I'd focus on that. Have your function write @pathAndFile as an output to screen, and then attempt to manually select to see if it matches in examples where it's failing. 
I'm not sure what you mean here. If you're selecting table1.data1 twice, you'll always get the same value for each, no matter what you name the column. What you're doing is akin to: SELECT USR.UserName ,USR.FirstName AS FirstName ,USR.FirstName AS LastName FROM User USR WHERE USR.UserName IN ('muchargh','smellyarmadillo') In this instance you'll never select our last names, because you're only requesting the first name. I will mirror the sentiment of /u/OMGTehAwsome (even if his solution is specious): either your data model (or your understanding of it) is totally screwed or you're not providing enough information for us to actually answer your question.
Using one and understanding SQL are not mutually exclusive. It's simply much faster to use a designer for some queries. SQL Developer and Flyspeed are decent. 
 WHERE (X = 1 AND Y = 1) OR (X = 1 AND Y = 2) OR (X = 2 AND Y = 1) OR (X = 2 AND Y = 3) ninja edit formating
Maybe the optimizer in your platform doesn't get to look into the view and simplify and has to scan the entire result set of the query before filtering?
Yes, it did work until about two weeks ago. How would I get it to output to screen? thanks for suggestions
So what you are trying to do is a relational division right? Give me all inputs that has a (x,y) combination in this subset (x1,y1...) Unfortunately for you this is not implemented in any sql languages that I use or have heard of as a standard operator. Nor is there a simple catch all workaround that is quick and works every time. This means you need to either make an intersect which scales poorly or use what /u/Mamertine wrote which makes your logic hard to parameterize. Btw an intersect might look like this (T-SQL): declare @t table(id int, x int, y int, moreInfo varchar(60)) insert into @t values (1, 0, 1, 'stuff regarding first post') , (2, 1, 2, 'more stuff') , (3, 1, 1, '&lt;this space intentionally left blank&gt;') , (4 ,1, 2, 'This is totally not the second record again') , (5, 2, 1, 'Hi mom') , (6, 2, 3, 'Oh no how did this get here, I''m not very good with computer') , (7, 1, 3, 'This is not supposed to be returned in the query') select * FROM @t WHERE EXISTS ( select x,y INTERSECT select x,y from (values (1,1),(1,2),(2,1),(2,3)) as filterset(x,y) )
&gt; I'm not super sure if this is what you mean, so let me make sure: you have a table [table1] that has (among others) two columns, [data1] and [mseq]. [data1] is sometimes score and sometimes points, based on whether [mseq] is '42351' or '353452345'. yes this is correct, this statement means that the score and points will be populated for each name?
its definitely my understanding of it.
How come it would be asinine to add the AccountCreated to every row - once I have run the query then I don't need to calculate it again? I'm using SQLite so I don't think I can change the primary key now that I have created the table?
For short lists of filter criteria I would most definitely go with /u/Mamertine 's solution of doing it in the where clause with OR's, but for completeness sake here's another solution: In MSSQL you can also use the table value constructor VALUES to build a table out of the input and then join it to the original table. Dependent on the size of the input for the filter it may be more concise. At some point it would probably be easier to create a temp table even to hold the data. SELECT tb.ID, tb.X, tb.Y FROM table AS tb INNER JOIN ( VALUES (1,1),(1,2),(2,1),(2,3) ) AS va(x,y) ON tb.X = va.X AND tb.Y = va.Y
You've piqued my interest. Why INTERSECT and not INNER JOIN to a different @/#table? Also, in hindsight I'm guessing this is a homework question that I've answered for OP.
I question the need for this in the first place. You're storing data which can be calculated from other data already existing on the table. I would recommend not storing the leave remaining *at all* and simply calculate it when you query the system to get it - either in the query or in the calling application. If you *really* want it on the table itself, create a [computed column](https://msdn.microsoft.com/en-us/library/ms188300.aspx). This is a "virtual" column which you can query directly, and is always up-to-date without any additional work on your part. Just be aware of the [performance implications](http://www.scarydba.com/2015/05/05/effects-of-persisted-columns-on-performance/)
This sounds like a *unnecessary* replacement for a [Computed Column](https://msdn.microsoft.com/en-ca/library/ms188300.aspx). *** ALTER TABLE dbo.leaveValues DROP COLUMN leaveRemaining; GO ALTER TABLE dbo.leaveValues ADD leaveRemaining AS (totalLeave - leaveTaken); GO ***
Mostly because then I would have to write down all the columns twice more. If at a later point I add more columns then I must not forget to add it again in the filter. If I use Exists(intersect)) then that is my filter and I'm done. If I forget columns somewhere the query I wrote will not run.
I don't know why there are so many ALMOST correct answers to this simple requirement. SELECT t2.name ,t2.code ,SUM(CASE WHEN t1.mseq = '42351' THEN t1.data1 ELSE 0 END) score ,SUM(CASE WHEN t1.mseq = '353452345' THEN t1.data1 ELSE 0 END) points FROM table1 t1 JOIN table2 t2 ON t1.seq = t2.seq GROUP BY t2.name, t2.code
I ended up using: &gt; WHERE (x,y) in ((1,1),.....) Thanks all for giving your input. 
Thanks, i needed it actually for bigger list. Made it smaller to show the problem more easilly
I definitely won't argue with you on that one. 
Thank you! the problem with that is that they are not "duplicates" in the normal sense. They are being sourced from different databases and departments. I do not have any control over that data, so I can't clean them. :( Edit: I replied without reading the article. This is really nice. This may be the solution I need. I will have to toy with it a bit. I did not know these functions existed! Thank you!
It's the performance effects I'm concerned about. I don't want to do this from the webapp since it opens up the possibility of a user attempting to mangle the value of leaveRemaining, at least in theory. Obviously I can try to defend against this, but doing it within the DB itself renders it pretty much inaccessible to an end user. There's going to be at least 20k employees per country, not including contractors, registered with this application at any one time, and the region covers Europe (including the UK), the Middle East and Africa. This makes for a good few hundred thousand potential registrants, it would be logical to assume around 10% will be using the application in some way at any given point. Considering these figures, I feel it would be sensible to do this as a single calculation whenever leave is approved - a much smaller workload. FYI the need for this column is to set an upper limit for a MaxValue-like value in the corresponding webapp.
&gt; I don't want to do this from the webapp since it opens up the possibility of a user attempting to mangle the value of leaveRemaining, at least in theory. Not if you do the calculation server-side, after you execute the query and before you send anything to the user. &gt;I feel it would be sensible to do this as a single calculation whenever leave is approved - a much smaller workload. I think you grossly over-estimate the execution time to perform a simple arithmetic operation in the `select` clause relative to the rest of your operations. It's going to be far lest costly than performing a secondary update after each time a user makes an entry. &gt;FYI the need for this column is to set an upper limit for a MaxValue-like value in the corresponding webapp This sounds like it should be a configuration option, and that value checked at runtime when the user enters their leave (so that you can pop up a message saying "this entry will exceed the allowed leave."
Your approach should depend on the quantity and quality of your data sources. But you also should consider that creating a complicated JOIN using EXCEPT and INTERSECT is going to be difficult to debug. I almost always use a permanent staging table to hold all of the incoming data. This is nice because when someone asks you why the output is wrong, you can easily research using the staging table. At a minimum, you should store the PK from each data source in the staging table. I usually add a Created datetime column and an ErrorMsg column as well. Pseudo code like this: 1. Clear staging table. 2. Add rows from data source #1 3. Using data source #2, add new rows to staging and update rows that already exist. 4. Repeat #3 for each data source. If you do this, it is also straightforward to generate and send emails detailing how many rows were inserted from each data source.
you could make a temp table with a row_number(), then left join the temp table on itself (use different aliases) on row_number() + 1. then for your last column do temp1.column - temp2.column. something like this maybe: select row_number() over (order by id) as rownum, someValue into #temp from someTable select t.rownum, t.someValue, t1.someValue - t2.someValue as valueChange from #temp as t left join #temp as t2 on t1.rownum = t2.rownum + 1 You'll probably want an order by, not sure how your table is set up. But this should work for as many rows as you want. Your first row valueChange will be null, you could wrap that in an isnull(), or, if you don't want to include the first row, put a 'where t2.rownum is not null'
You should be able to just set it in the select. SELECT @leaveRemaining = totalLeave - leaveTaken FROM HolidayBooking.dbo.leaveValues WHERE employeeID = @empID
MSSQL 2012+ allows for this with the LAG() and LEAD() windowing functions See: https://msdn.microsoft.com/en-us/library/hh231256.aspx SELECT t.ID, t.value, t.value - LAG(t.value, 1) OVER (ORDER BY ID) AS DiffFromPrevious FROM table AS t
You are correct, BIDS 2005 is the way to go with this.
thanks Aznflipfoo. Been trying to figure this one out. Unless there's a SQL query that would help make it easy.
Youd want something like an item table, an ingredient table, and a recipe_item table. A row in the recipe_item table would have foreign key matching the target item and a foreign key matching one of the target ingredients. Then to get the recipe for item 42, you use SELECT * FROM recipe_item WHERE item_id = 42; And then you can iterate through the returned rows to assemble the full recipe.
That's a good idea. I mean, I kind of have that in the views I created, right? They are based directly off the original tables. But I guess a table would be a good idea, too!
I see what you're saying to an extent. It's still not clear. How do you structure the tables.. `Items` (Item, Damage, Critical) sword 1d8 *2 `Item_Materials` (Item, Ingredient, Count of Ingredients needed.) sword Steele 1 sword gem 2 User chooses to craft a sword. They place the ingredients on the table. Now SELECT ingredient, and count from table, check if user has right amount of ingredients? 
Thanks, eye opening read :)
Going through a conformance exercise on the dimensions seems like overkill, but... * Pick a source that is the "master" * Collapse your dimension on the natural keys for the master file * Add back in the dimensional natural keys that aren't in your "master" * Union away with your conformed dimension
Well, you should be able to Select where Ingredient = SteeleAND Count = 1. That will get you everything that requires exactly 1 Steele. So you don't have to do much comparison after retrieving the rows. SELECT DISTINCT item FROM Item_Materials WHERE (ingredient = 'Steele' AND count = 1) OR (ingredient = 'gem' AND count = 2); Should get you a list of all items that include one steele and two gems in the recipe. If the number of returned rows is one, then you have a target item. Then you'll want something like SLECT inredient FROM Item_Materials WHERE item = 'Sword' AND ingredient &lt;&gt; 'Steele' AND ingredient &lt;&gt; 'gem'; If there are any more returned rows, those would represent any further ingredients that need to be added before the item can be constructed.
Every vendor is different - always contact the storage vendor and consult your product documentation for best practices concerning offset. Most modern platforms don't even recommend worrying about it these days, as it's handled by software in the device.
Thanks folks :) 
wait a second, you actually said this -- &gt; adapt this to not group by programme just to show placed to unplaced active students. but now you want to count them again? okay, go back to the original query, remove programme.name from the SELECT clause, and remove the entire GROUP BY clause 
SELECT DISTINCT item FROM Item_Materials WHERE (ingredient = 'Steele' AND count = 1) OR (ingredient = 'gem' AND count = 2); Just wondering if you could help explain how this works with OR. `Item_Materials` `Item``Ingredient` `Count` Sword Steele 1 Sword Gem 2 Since the statement is to select item where ingredient equals 1 steels OR 2 gems.. wouldn't it select any item, and not just the sword? Sword Steele 1 Sword Gem 2 GemSword Gem 2 Wouldn't you get back GemSword also because you used OR and not AND?
It's a heinous normalization error. However, we don't normalize for normalization's sake, so here's the most practical reason: Let's say you make a mistake and need to import records with dates prior to your supposed "AccountCreated" date. Now you'll need to run a fairly obnoxious UPDATE statement to reflect the change - if you notice the problem at all. For SQLite I think you'll need to create a new table, copy the data over, drop the old table, and then rename. Which adds to the list of reasons I'll never use it. 
Ok, it might help if you fill us in a little on what the data is, a little more about its structure, and exactly what you want the end result to be.
You want: ISNULL(B.FirstName+' '+B.LastName,'N/A') AS ManagerName 
Perfect! Thank you so much. That was probably the only permutation I didn't try. 
Wow, thank you so much for the response. I will need to wait until tomorrow to try this out but you have given me hope and some reading material.
Sure thing, let us know how it goes.
Thank you, I will give this a shot!
Your TSQL syntax is correct, unfortunately [It looks like TaraData doesn't support multiple-row inserts](http://forums.teradata.com/forum/database/multi-row-inserts). It looks like their solution is to stack the inserts, which doesn't seem like much of a solution.
You can achieve this using the [Tabibitosan method](https://community.oracle.com/thread/1007478?tstart=0). Here's an example, where I've chosen 30 days as the threshold. I've broken the sections in CTE blocks for comprehension and commented it for you, but you can use subqueries if you wish. This gives your groups of visits within a threshold a sequence number and the count of them within it. with qry as ( select 'A' as PatientID, cast('02/01/2014' as Date) VisitDate union all select 'A', '02/10/2014' union all select 'A', '03/02/2014' union all select 'A', '03/08/2014' union all select 'A', '03/30/2014' union all select 'A', '04/16/2014' union all select 'A', '05/01/2014' union all select 'A', '11/07/2015' union all select 'A', '11/29/2015' union all select 'A', '12/14/2015' union all select 'A', '02/10/2012' union all select 'B', '01/29/2015' union all select 'B', '02/14/2015' union all select 'B', '02/27/2015' union all select 'B', '03/04/2015'), qryPatientData as ( select q.*, row_number() over (partition by PatientID order by VisitDate desc) RNum -- Patient data running order from qry q), qryDateDiff as ( select c.*, n.VisitDate NextVisitDate, DateDiff(Day, c.VisitDate, n.VisitDate) as DaysBetween -- join patient data to itself shifted by one to get the next visit from qryPatientData c left join qryPatientData n on n.PatientID = c.PatientID and n.RNum + 1 = c.RNum), qryThreshold as ( select d.*, case when DaysBetween &lt; 30 or DaysBetween is null then RNum else RNum + 1 end RNumAdj -- Leave a gap in the running order if threshold broken from qryDateDiff d ), qrySeq as ( select d.*, RNumAdj - dense_rank() over (Partition by PatientID order by RNumAdj) Seq -- Tabibitosan method to create groups based on sequences of values from qryThreshold d ) select s.*, count(*) over (partition by PatientID, Seq) SeqCount -- Count the number of items in sequence from qrySeq s order by PatientID, VisitDate desc | PatientID | VisitDate | RNum | NextVisitDate | DaysBetween | RNumAdj | Seq | SeqCount | |-----------|------------|------|---------------|-------------|---------|-----|----------| | A | 2015-12-14 | 1 | (null) | (null) | 1 | 0 | 3 | | A | 2015-11-29 | 2 | 2015-12-14 | 15 | 2 | 0 | 3 | | A | 2015-11-07 | 3 | 2015-11-29 | 22 | 3 | 0 | 3 | | A | 2014-05-01 | 4 | 2015-11-07 | 555 | 5 | 1 | 7 | | A | 2014-04-16 | 5 | 2014-05-01 | 15 | 5 | 1 | 7 | | A | 2014-03-30 | 6 | 2014-04-16 | 17 | 6 | 1 | 7 | | A | 2014-03-08 | 7 | 2014-03-30 | 22 | 7 | 1 | 7 | | A | 2014-03-02 | 8 | 2014-03-08 | 6 | 8 | 1 | 7 | | A | 2014-02-10 | 9 | 2014-03-02 | 20 | 9 | 1 | 7 | | A | 2014-02-01 | 10 | 2014-02-10 | 9 | 10 | 1 | 7 | | A | 2012-02-10 | 11 | 2014-02-01 | 722 | 12 | 2 | 1 | | B | 2015-03-04 | 1 | (null) | (null) | 1 | 0 | 4 | | B | 2015-02-27 | 2 | 2015-03-04 | 5 | 2 | 0 | 4 | | B | 2015-02-14 | 3 | 2015-02-27 | 13 | 3 | 0 | 4 | | B | 2015-01-29 | 4 | 2015-02-14 | 16 | 4 | 0 | 4 |
i feel ive used this feature before on other tables, but cant re-do it. but now im even more confused haha
you might be right... it was not taradata that i used it, it was in a different dev server, i assume it allows for multiple insert into statements i hadnt even though about it being on a different server
Thank you! 
That's the downside to a lot of these alternative environments. A lot of them don't fully integrate the TSQL DML. They pick and choose things they think are important. Sometimes that leaves the devs without useful methods of handling data.
i dont mind writting the statements over and over haha, but it was just killing me not figuring out why..
Try using an AND statement in your join, rather than a WHERE statement after the join. So instead of this: SELECT a.ID, b.Value FROM Table1 a INNER JOIN Table2 b ON a.ID = b.ID WHERE b.Criteria = 'blah blah blah' Try this: SELECT a.ID, b.Value FROM Table1 a INNER JOIN Table2 b ON a.ID = b.ID AND b.Criteria = 'blah blah blah' BEWARE! Depending on the complexity of your query, this does NOT always result in the same result sets and can give differing result sets. Test carefully before deploying. 
Sorry for the confusion cheers i will give that a go
cool, didn't know you could do this!
If you have Teradata SQL Assistant you can type, `insert into tablename values (?, ?)` and import those from a text file. You have to adjust your import settings in options and toggle import mode to make it work. There's also FastLoad, which I've never used but from what I heard is true to its name.
This is a decent start, but you'll need tables like `Item`, `Recipe`, `Recipe_Material`, `Inventory`, maybe `Item_Attribute` for the stats depending on how you want to organize things and how complicated the stats on items can be. Should equippable items be stored in the same table as other items? You basically just need to think through the logic of it. I'd be happy to help you normalize, but it's difficult to propose a table structure when you don't fully understand the problem it's meant to solve.
The nice part of having a permanent table is that you can easily add indexes to make it perform better. This is important if you are handling millions of rows. And much easier debugging. But I already said that. :-)
Thanks /u/treebeard901 :)
You were close with the lead source, but you want that to be in the WHERE clause. You want to find artists that have no entries in the Titles table, so anything you pull from that table SHOULD be empty/blank. That said, INNER JOINing to that table is going to get rid of any artists that don't have an entry inside of it. This is the opposite of what you want. To find out whether or not they have any recorded titles, you could do a couple of things. You could do: WHERE a.ArtistID NOT IN (SELECT DISTINCT ArtistID FROM Album) You could also do LEFT OUTER JOIN Titles t ON a.ArtistID = t.ArtistID WHERE t.ArtistID IS NULL A left outer join is going to return all nulls for title columns where there is no match, so then you just look for where t.Column = NULL. Does that make sense?
Something like the following? SELECT a.ArtistName FROM Artists a LEFT JOIN Titles t ON a.ArtistID = t.ArtistID WHERE a.LeadSource = 'Directmail' AND t.Title IS NULL You need a left join because not all artists will have titles. Sorry of the syntax isn't correct I just typed it out quickly
thank you , i think ive figured it out using left join instead!
i don't think so. i have them set to use the default server settings and under server properties &gt; Database Settings &gt; Compress backup is unchecked.
Do you just need the rows that have at least one of the ten? Why not use an OR? SELECT PatronOwn.*, Ebrary.ebrary_DocID, Ebrary.Trigger_Date, Ebrary.Purchase_Total FROM Ebrary JOIN PatronOwn ON Ebrary.[ISBN_Electronic] = PatronOwn.ISBN OR Ebrary.[ISBN_Electronic] = PatronOwn.ISBN2 OR...
Maybe the transaction log had never been backed up, the non-active transactions would have been held there Have a look in the backup history report to check
A bunch of open transactions holding VLFs as active in the log file? https://www.mssqltips.com/sqlservertip/1225/how-to-determine-sql-server-database-transaction-log-usage/
I'm on mobile right now, but look up "markdown syntax daringfireball". Anyways, to format blocks of code, insert 4 spaces in front of all the lines. Even better, post it on a pastebin like site, so you get syntax highlighting and line numbers. 
Not sure what constitutes "Reddit friendly" but check out Poorsql.com
I'm mobile so I can't type it out properly but... Why not make a cte with the max score in it and the total count. Left join to it where it is null to eliminate all rows with the max score if there's a tie. Then just take the average of what's left dividing the sum of scores by the count from the cte. If you can't use ctes in your database make a table instead. I don't know what that database is you're using.
This doesn't work if there is a tie for the max score. I believe using dense rank instead of row number will fix this. Your current query will just pick one at random when there's a tie based on score, so then the max is still in the calculation.
Even better put that sub queries result in a temp table then index it and join to that
I mentioned that. Did you read where I stated "Word of caution, this code has a problem if the two rows share the max score" and then suggested "In that case you may want to use rank instead of row_number"? And then I proceeded to say that rank would drop the repeating max score entirely while row_number will keep one max score, depending on if he wants to do that? Reading, do you even? 
Nope I only looked at your ugly incorrect query. Sub queries as columns, Jesus. Then a group by at the end. Fuck are you doing? You're over complicating it and making it inefficient. Why are you using a group by and a row number
I grouped by your mom last night nigga 
Lol
lmao awesome response. Also "with (forceseek)"!
It works until it doesn't.
Yea I would go with a simple CASE statement SELECT Accounts , Cost , Year , CASE WHEN Accounts BETWEEN 10 AND 100 THEN 'Administrative Costs' WHEN Accounts BETWEEN 101 AND 500 THEN 'Salary and Bonuses' WHEN Accounts BETWEEN 501 AND 600 THEN 'Rent and Maintenance' ELSE NULL END AS Grouped FROM Expenses.
That's literally company policy where I work. Every table and join must have with (nolock)
Do you have lots of concurrency?
That worked beautifully, thank you so much!
Huh, that's interesting. Thank you, I'll look into it!
&lt;3 &lt;3 worked perfectly plus i learned something new, thank you
Now what if I want to group by the aliases I applied to each range of code? So all Accounts in the category of *Salary and Bonuses* would be combined onto a line *'Salary and Bonuses'* I got an error **Invalid column name 'Grouped'** When I Threw 'grouped' into the 'group by' section
Why don't you declare tempTimeStamp as a datetime? Make data the correct datatype! 
Not sure I understand. If you just want to add 2 more seconds, just change the 1 to a 2. SELECT @tempTimeStamp 'a', DATEADD(ss, 2, tempTimeStamp) 'b'; If you just want to update the variable, then you need to declare it as a DATETIME and then set it DECLARE @timeVar DateTime DECLARE @tempTimeStamp AS varchar(25) SET @tempTimeStamp = '980316' SELECT @timeVar = CONVERT(DATETIME, @tempTimeStamp); SELECT @timeVar = DATEADD(ss, 1, @timeVar) SELECT @tempTimeStamp 'a', @timeVar 'b'; SELECT @timeVar = DATEADD(ss, 1, @timeVar) SELECT @tempTimeStamp 'a', @timeVar 'b'; ...and repeat 
You have to put the whole case statement in the GROUP BY without the 'AS Grouped' GROUP by Accounts, Cost, Year, CASE WHEN Accounts BETWEEN 10 AND 100 THEN 'Administrative Costs' WHEN Accounts BETWEEN 101 AND 500 THEN 'Salary and Bonuses' WHEN Accounts BETWEEN 501 AND 600 THEN 'Rent and Maintenance' ELSE NULL END
Ok, I think I see the difference. SELECT CONVERT(DATETIME, @tempTimeStamp); and then I was also trying SET @TimeStamp = CONVERT(DATETIME, @tempTimeStamp); when I really wanted this... SELECT @TimeStamp = CONVERT(DATETIME, @tempTimeStamp); Thanks for your help sir.
Fair point and valuable input for me, the price is not set in stone. I aim for a price that's makes it worth my while to develop the plugin while also being well worth it for the user. If the price sends too many people away I'll correct it. 
Http://sqlfiddle.com
http://poorsql.com/ is my favorite.
This works in MSSQL. KR_SCORE has a single int column. SELECT ( ( SELECT SUM(SCORE) FROM KR_SCORE WHERE SCORE &lt; (SELECT MAX(SCORE) FROM KR_SCORE) ) / ((COUNT(SCORE))*1.0 ) ) FROM KR_SCORE
amazing, thank you so much
By deduction, SQLEXPRESS is the default name of sql-express servers. You're seeing what you should be seeing. If Named Pies is enabled and you're still having problems, disable the firewall (or open the SQL ports) on the server to confirm that's the problem--95% of the time that's why I can't connect to a database. [Default port is 1433](https://msdn.microsoft.com/en-us/library/cc646023.aspx)
 where size like '% - 14%' or size like '% - 10%' or size like '%-12%' &gt;Is there a way I can make the "where size like" part shorter? It seems a little too long to me and almost like a lazy way for some reason. Yea like this: where size like '% - 1[024]%'
&gt;If you know the data you need, build your where statement around that, don't just throw in wildcards unless you need wildcards. I can't stress enoug what /u/Konraden says here. Just use size = '10-12' the like operator costs more *especially* if you have a leading wildcard
u/robertnpmk colleague and BA here, working in NZ. He told me about this yesterday before posting his comment. Really interested in this as a product. Technically, I would be interested in a 'commercial' licence, but it wouldn't be paid for by me. I use a number of tools, that I pay for licences myself, because whilst I can't get an employer to pay for them, they really fill out my toolbox. E.g. i have licences for evernote premium, and Lucidchart, and Xmind. So I can totally see the value in this product as a data modelling demonstration tool. I'd love to be able to take it with me to client sites, or even just have it available to try ideas out, etc. That's the value proposition for me. At the price you are offering, it's a hard sell though. It's enough that I have to question whether or not I simply install SQL express on my machine, and do it that way. I can't honestly say whether I'm an edge case user though - most of the BA's i know, despite their BI involvement, are in corporates where installing an instance of SQL server express on their machines is not an option... I'm fortunate in that regard. This might be an excellent product for them (not having seen your install process). I'm super keen /u/Anakic, but it's simply a few more steps for me to have a local SQL Express install to do that kind of thing. That being the case, the value proposition is not quite in your favour. As a personal toolbox item, used for commercial purposes, I think I would consider it a sale at US$35. I wish you all the best - it's a super idea, and an excellent product. As a demonstration and possibly even a teaching aid, I can really see a bright future, and if i have 90 bucks lying around, I'd be flinging it your way simply to support your endeavor. Good luck!
Can you paste the error? Are you connecting to the SQL server from the same machine it's hosted on? If you can query it the database server, "SELECT @@Servicename" will return the name of the instance.
YES YES YES YES. Not necessarily for you, but good where statements are sargable. I had to do a lot of reading on [sargability](https://en.wikipedia.org/wiki/Sargable) last year to help with some performance problems a customer of mine was having. Learned a lot about SQL that ~~day~~ week.
A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error 40 - Could not open a connection to SQL Server)
is bimonthly twice a month, or once every two months? :p
JUST SOLVED IT Changed the connection string from using the Data Source parameter and instead used Server = MACHINENAME\INSTANCENAME. Worked like a charm! Thanks for your input, Konraden!
Check out GROUP BY WITH ROLLUP or GROUPING SETS for subtotals. https://technet.microsoft.com/en-us/library/bb522495(v=sql.105).aspx
yes
&gt;Should say that I want the 1 to exist such as WHERE MIN(LeadId) = LeadId and the 0 to exist for all other values But "MIN(LeadId) = LeadId" evaluates true for both rows you've given us. However, let's just do this: SELECT ApplicationID, LeadID, ROW_NUMBER() OVER (PARTITION BY ApplicationID, LeadID, SubmitCount, SettleCount ORDER BY LeadId DESC) - 1 AS LeadCount, SubmitCount, SettleCount, Production FROM Table Apologies if that doesn't compile, I wrote that in notepad. Also, if there's three or more rows, it will go over 1. 
I'll play with it. I've started encountering more ROW_NUMBER() OVER (PARTITION... but don't feel fully comfortable with it. The table is actually a fair bit larger than shown here but I imagine expanding it should be easy enough. Thanks!
cool, exactly what i had in mind - and yeah there is a trust in place, should be pretty smooth.
No prob. Window functions (ROW_NUMBER, LAG, LEAD, RANK, etc..) are pretty awesome and surprisingly simple, I use them all the time.
True. I should start giving bad answers to homework questions. 
List out the fields instead of using the *. Select ColA, ColB, ColC From Table1 Union All Select ColA, ColB, ColC From Table2 
The ORDER BY is not redundant. The purpose of the above sub query is to perform something like a MySQL GROUP_CONCAT. In other words, aggregate string results with a semicolon separator. This implies that it is expected to get multiple results from the subquery and the ORDER BY will order the results in the resulting string. 
I f you want to detect differences between the two tables, use the EXCEPT operator (MINUS in Oracle).
Cheers. What's the syntax? Sorry, super green with sql. I get the idea just having trouble with syntax. 
MS SQL uses % as wildcards in strings. * is only a wildcard when referencing columns (Like SELECT * FROM [table]) So, '%10-12%' will match 110-121 as well as 10-12. Using IN is a cheap way to do a bunch of ORs, but I use it a lot to referecing data in anotehr table. SELECT DocName, DocNumber, CreationDate, DocOwner FROM Documents WHERE DocID IN (Select fkDocID From DocumentChanges where ChangeDate &gt; GETDATE() - 7) This grabs all of the documents in my table that were changed in the last week. As /u/SemiNormal mentioned, you can use: WHERE size IN ('8-10','10-12','12-14','14-16') And this is the same as saying: WHERE Size = '8-10' OR Size = '10-12' OR Size = '12-14' OR Size = '14-16' 
I agree, the Order By is meaningful and not redundant. I use it all the time to concat a selection of rows (typically names) into a single string to place in a column for a SearchView. That single column is full of names--the Order By helps me keep the names alphabetical when they might otherwise not be.
A response! That's the push I needed to find out why it's not working. I'm used to this method in SQL 2k5+, and I live in scorn whenever I run across SQL 2000. Turns out FOR XML doesn't work in subqueries and can only be used in the outer most select. I need to find a way to concat an unknown number of rows into a single string in SQL 2000. Any tips?
That's the problem, I don't think you can save the FOR XML result anywhere, string or table, maybe capturing the output of a dynamic sql statement somehow? Otherwise, you may need to find a different way to generate the output required, perhaps using a cursor to go row by row and concatenate the strings together to get your result.
Yeah true. Hard-coding for exact matches. Thanks for the suggestions :)
To be honest it's all pretty confusing to me too. I've set up a google doc here that people can look at an comment. Hopefully someone will edit and put in their two cents :) I believe the easiest method suggested was to get back items needed to make a specific item. This would mean you would need to know the item you're going to craft ahead of time. You could have blueprints, or a scroll, or something, that has the item name attached to it. When the user clicks that they want to craft that item, you would use sql to get the ingredients from the table, and see if they had the right ingredients in their inventory, or selected on their "tray" or "anvil" or whatever you're using... https://docs.google.com/document/d/1pjzKKOBR5c84-GJ3FxTm86LbKOw81FkX3zpDS7lnhmg/edit?usp=sharing
It's not ordering by the concatenated values in the string. If it was like the following, the ORDER BY would be meaningful.: *** SELECT DISTINCT aggregate_store.user_id ,( SELECT a.store_number + ';' AS [text()] FROM user_responsibility a WHERE a.user_id = aggregate_store.user_id ORDER BY a.store_number FOR XML PATH('') ) AS stores FROM user_responsibility aggregate_store *** 
that's clever.. but the aliases in brackets [ ] can only appear after the closed parantheses?
&gt; No. Setting aside the lack of time, PL/SQL can't do this, which is why it was done in another language. PL/SQL is a Turing complete language, so it can do pretty much anything. What exactly do your VB procedures do? 
Oh, I don't remember my professor saying % is a wildcard I may have overheard her. Thanks though :)
&gt; PL/SQL is a Turing complete language, Do you know what makes PL/SQL Turing complete, while SQL isn't? It is the scripts extensions that allow it to do things in embedded languages. This allows programs do things the core language cannot. So if you encountered a situation where the SQL language could not do a thing, you would write it in another language (say, VB) and use the drivers to pull this external procedure in.
I am aware of what a view is. Again, as I said, a temp table is a less efficient solution here. More to the point, bickering about what structure to use is pretty pointless until there is a way to call the external procedures and bring in the information. Do you have any solutions on how to do that?
I am assuming your database solution is windows, correct me if I am wrong. If it is windows, bring your VBNET application on the database host. You can use a java stored procedure to call a host OS command to execute the vbnet command. I do not know when you want to call this external process, but you can do so perhaps using a database trigger. The java code and examples can be viewed [here](https://oracle-base.com/articles/8i/shell-commands-from-plsql) or copy and paste the below code: CREATE OR REPLACE AND COMPILE JAVA SOURCE NAMED "Host" AS import java.io.*; public class Host { public static void executeCommand(String command) { try { String[] finalCommand; if (isWindows()) { finalCommand = new String[4]; // Use the appropriate path for your windows version. //finalCommand[0] = "C:\\winnt\\system32\\cmd.exe"; // Windows NT/2000 finalCommand[0] = "C:\\windows\\system32\\cmd.exe"; // Windows XP/2003 //finalCommand[0] = "C:\\windows\\syswow64\\cmd.exe"; // Windows 64-bit finalCommand[1] = "/y"; finalCommand[2] = "/c"; finalCommand[3] = command; } else { finalCommand = new String[3]; finalCommand[0] = "/bin/sh"; finalCommand[1] = "-c"; finalCommand[2] = command; } final Process pr = Runtime.getRuntime().exec(finalCommand); pr.waitFor(); new Thread(new Runnable(){ public void run() { BufferedReader br_in = null; try { br_in = new BufferedReader(new InputStreamReader(pr.getInputStream())); String buff = null; while ((buff = br_in.readLine()) != null) { System.out.println("Process out :" + buff); try {Thread.sleep(100); } catch(Exception e) {} } br_in.close(); } catch (IOException ioe) { System.out.println("Exception caught printing process output."); ioe.printStackTrace(); } finally { try { br_in.close(); } catch (Exception ex) {} } } }).start(); new Thread(new Runnable(){ public void run() { BufferedReader br_err = null; try { br_err = new BufferedReader(new InputStreamReader(pr.getErrorStream())); String buff = null; while ((buff = br_err.readLine()) != null) { System.out.println("Process err :" + buff); try {Thread.sleep(100); } catch(Exception e) {} } br_err.close(); } catch (IOException ioe) { System.out.println("Exception caught printing process error."); ioe.printStackTrace(); } finally { try { br_err.close(); } catch (Exception ex) {} } } }).start(); } catch (Exception ex) { System.out.println(ex.getLocalizedMessage()); } } public static boolean isWindows() { if (System.getProperty("os.name").toLowerCase().indexOf("windows") != -1) return true; else return false; } }; / Reference using something like Create or replace procedure call_os_command (p_command in varchar2) as language java name 'Host.executeCommand(java.lang.String)'; Privileges Required: DECLARE l_schema VARCHAR2(30) := 'YOUR_SCHEMA'; -- Adjust as required. BEGIN DBMS_JAVA.grant_permission(l_schema, 'java.io.FilePermission', '&lt;&lt;ALL FILES&gt;&gt;', 'read ,write, execute, delete'); DBMS_JAVA.grant_permission(l_schema, 'SYS:java.lang.RuntimePermission', 'writeFileDescriptor', ''); DBMS_JAVA.grant_permission(l_schema, 'SYS:java.lang.RuntimePermission', 'readFileDescriptor', ''); END; / I assume you are not using Oracle XE, which you cannot use Java stored code. If you are using XE, I would create a small, easy, third VBNET application that has a file watcher, and use UTL_FILE to create a file in a constant directory to force the kick off of your original VBNET application, but using Java stored code would be my first option. 
Select ColA, ColB, ColC From Table1 EXCEPT Select ColA, ColB, ColC From Table2 You have to switch tables 1 and 2 and run it again to get a complete check. In short it does this: take all these rows in T1 and subtract the same rows from T2 and show me the result. Then you switch T1 and T2 to check the reverse.
If I understand what you want, you basically want to offset each additional by another second? If that is correct, instead of using a fixed values in the DATEADD, use a [Sequence](https://msdn.microsoft.com/en-us/library/ff878091.aspx) instead. *** CREATE SEQUENCE dbo.yourSeq START WITH 1 INCREMENT BY 1; GO -- Before you start, you will need to reset it ALTER SEQUENCE dbo.yourSeq RESTART WITH 1 SELECT @tempTimeStamp AS a, DATEADD(ss, NEXT VALUE FOR dbo.yourSeq, @tempTimeStamp) AS b *** 
Thanks for the reply. Too bad. On the other side it isnt required neither in the paper nor for my specific topic. Would have been a nice gimmick though.
I have a vendor I deal with who *insists* that `nolock` is necessary "in case other people are using the system while this query runs". No, `nolock` is dangerous **because** other people are using the system while this query runs!
I like this version better: UPDATE fucks SET given = 1 (0 row(s) affected) 
No, he's kinda right, and at the very least more right than you are. Initial SQL was not Turing complete. Later additions made it so. Cyclic tags didn't get added to Postgre until 2009 iirc, and its the procedural extensions that let PLSQL and TSQL be turing complete, including the ability to go outside to other languages. It is more than just the scripts extensions that make it turing complete (eg conditions, loops, variables, etc) but to say that SQL is turing complete without any extensions is just false.
What do you mean? Is this a question or are you pointing out an issue? In TSQL the brackets indicate that the text between them is the literal object name, which allows you to put characters in them that normally wouldn't compile (such as symbols whitespace). You can put the brackets around any alias or database object's name at any place in the query and it should work. I'd be surprised if any other DBMS would treat them differently
That's just not true. There is a way, see previous comment.
&gt;but to say that SQL is turing complete... Where did I say SQL was Turing Complete? Please don't misquote because I didn't say that at all. I said *"PL/SQL is a Turing complete language...*" because it can - without external calls, implement recursion, cyclic tag, conditions, loops, storage, Lambda calculus, closures and simulate a Universal Turing Machine using its standard syntax set. &gt; and its the procedural extensions that let PLSQL and TSQL be turing complete You have the wrong interpretation of Extensions - these are calls to external libraries outside of PL/SQL not language constructs. PL/SQL and TSQL are by their nature procedural, so to say its the procedural extensions that made them so is a complete non sequitur. Aside from PL/SQL, Oracle's own SQL has been proved to be Turing complete since the introduction of both the Model clause and recursive CTE. A demonstration of Turing Completeness is using Oracle SQL alone in calculating [*Fast Fourier Transforms*](http://www.adellera.it/investigations/nocoug_challenge/index.html) So you are mistaken, not me.
Why is that? I've heard conflicting opinions but no evidence that supports either one. 
If you use `nolock` in a query against a table that someone else is updating at the same time, you may get data that doesn't really exist, or exists in a different form when both queries are complete. http://sqlperformance.com/2015/04/t-sql-queries/the-read-uncommitted-isolation-level People who care about the integrity of their data do not sprinkle magic `nolock` fairy dust unconditionally on every query.
No problem I'll clarify.. thanks for offering to help...do you mind if i reply in a couple of hours, I will be in the office a little later
Cool. I dumped to a temp table and did this and just fed unmatched records to that. Thanks. 
Hi SPARTAAAAA, Please see the answer at: http://imgur.com/85COKdA create table Artists ( artistId integer Primary Key Identity, artistName varchar(100) not null, leadSource varchar(40) not null ) create table Titles ( titleId integer Primary Key Identity, artistId integer Foreign key References Artists(artistId), title varchar(200) not null ) Screenshots created using sqllocus (http://www.sqllocus.com) thanks Martin
Weird, the first one and second version bring up different results, i think the first one might have been right though. Posting the whole damn thing for giggles... -- Why the fuck is this Drop Table never triggering? IF OBJECT_ID('#FullMonth') IS NOT NULL DROP TABLE #FullMonth Create Table #FullMonth( Store smallint NULL ,SalesMonth Varchar(2) NULL ,SalesDaysTY smallint NULL ,SalesDaysLY smallint NULL ,ExpectedDays smallint NULL ) IF OBJECT_ID('#TempSalesComp') IS NOT NULL DROP TABLE #TempSalesComp Create Table #TempSalesComp( Store smallint NULL ,SalesMonth varchar(3) NULL ,SalesMonthYear Varchar(10) NULL ,Department Varchar(50) NULL ,TYNetSales float NULL ,LYNetSales float NULL ) Declare @StartDate date Declare @EndDate date -- Temporarily Hard coded values because reasons. Set @StartDate = '10-01-2014' Set @EndDate = '09-30-2015' /* This isn't used Right now, just gets Month names In case I need them for something. SELECT DATENAME(MONTH, DATEADD(MONTH, x.number, @StartDate)) AS MonthName FROM master.dbo.spt_values x WHERE x.type = 'P' AND x.number &lt;= DATEDIFF(MONTH, @StartDate, @EndDate); */ -- Get a summary of all Sales, By Store, By Date, By Department -- For the Period Between @StartDate and @EndDate -- As well as for the Period a year before. -- Doing this Here because stores enter each cash register separately -- And I need to eliminate the cash registers from the equation Select DS.Store ,DS.SalesDate ,RIGHT('00' + CONVERT(VARCHAR(2),(DS.Department)),2) + '-' + SD.Description AS Department ,SUM(netsales) as Netsales INTO #TempNetDepSum from tblDailySalesByDepartment as DS INNER JOIN tblStoreList as SL on DS.Store = SL.Store INNER JOIN tblSalesDepartments as SD on DS.Department = SD.Department WHERE SL.Opened = 'True' and SL.Store &lt;&gt; 1 AND (DS.SalesDate between @StartDate and @EndDate OR DS.SalesDate between DateAdd(yy, -1, @StartDate) AND DateAdd(yy, -1, @EndDate)) group by DS.Store ,DS.SalesDate ,RIGHT('00' + CONVERT(VARCHAR(2),(DS.Department)),2) + '-' + SD.Description -- Summarize the Department summary By Store, By Date -- This is going to be used to determine how many days were entered -- For the purpose of knowing if a store was either opened, renovated or closed -- During the selected Periods Select Store ,SalesDate ,SUM(netsales) as Netsales INTO #TempDaylySum From #TempNetDepSum GROUP BY Store ,SalesDate -- Magically determine; -- * How many days worth of sales -- * How many days are in a month without reciting Thirty days Hath September... -- For the selected Period. INSERT INTO #FullMonth SELECT Store ,RIGHT('00' + CONVERT(VARCHAR(2),MONTH(SalesDate)),2) ,COUNT(SalesDate) ,0 ,DAY(DATEADD(DD,-1,DATEADD(MM,DATEDIFF(MM,-1,SalesDate),0))) FROM #TempDaylySum WHERE SalesDate between @StartDate and @EndDate GROUP BY Store ,RIGHT('00' + CONVERT(VARCHAR(2),MONTH(SalesDate)),2) ,DAY(DATEADD(DD,-1,DATEADD(MM,DATEDIFF(MM,-1,SalesDate),0))) -- And for my next trick! -- Do it again but for the year before the selected period. -- I know, I know, you should never do the same trick twice. UPDATE #FullMonth SET SalesDaysLY = LY.SalesDateCount FROM ( SELECT Store ,RIGHT('00' + CONVERT(VARCHAR(2),MONTH(SalesDate)),2) as SalesMonth ,COUNT(SalesDate) as SalesDateCount FROM #TempDaylySum WHERE #TempDaylySum.SalesDate between DateAdd(yy, -1, @StartDate) AND DateAdd(yy, -1, @EndDate) GROUP BY #TempDaylySum.Store ,RIGHT('00' + CONVERT(VARCHAR(2),MONTH(SalesDate)),2) --,convert(char(3), Salesdate, 0) ) as LY WHERE #FullMonth.Store = LY.Store and LY.SalesMonth = #FullMonth.SalesMonth -- Second Method of determining if a store has gone from ECR to POS -- Within the selected period and it's previous year. -- Kindly Provided by Http://www.reddit.com/u/Coldchaos -- Tweaked a tiny bit because it threw errors that it couldn't find -- Store, Depcode and DepName ;WITH posCte AS ( SELECT Store, SalesDate, LEFT(Department,CHARINDEX('-',Department)-1) AS DepCode, RIGHT(Department, LEN(Department) - CHARINDEX('-',Department)) AS DepName FROM #TempNetDepSum ), minMaxCte AS ( SELECT Store ,DepCode ,DepName ,MAX(SalesDate) AS FirstSalesDate ,MIN(SalesDate) AS LastSalesDate FROM posCte GROUP BY Store, DepCode, DepName ) SELECT c1.Store, c1.DepName, MIN(c1.FirstSalesDate ) AS NewPosUsedDate, MAX(c2.LastSalesDate) AS OldPosLastUsed FROM minMaxCte AS c1 INNER JOIN minMaxCte AS c2 ON c1.Store = c2.Store AND c1.DepName = c2.DepName AND c1.FirstSalesDate &gt; c2.LastSalesDate WHERE c1.DepCode &lt;&gt; c2.DepCode GROUP BY c1.Store, c1.DepName Order by c1.store, c1.DepName DROP table #TempNetDepSum DROP table #TempDaylySum Result Sample (Just Store 201) Store DepName NewPosUsedDate OldPosLastUsed 201 Baby 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 CARDS 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 CRAFTS 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 HARDWARE 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 HEALTH &amp; BEAUTY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Housewares 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Party 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 PET 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 STATIONERY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Wrap 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 Total number of records: 69 I would have assumed it would return 20 entries since there are 20 departments... Also Note the NewPosUsedDate is 2015-03-27 and OldPosLastUsed is 2015-03-07... Weird Gap -- First Method of determining if a store has gone from ECR to POS -- Within the selected period and it's previous year. -- Kindly Provided by Http://www.reddit.com/u/Coldchaos -- Tweaked a tiny bit because it threw errors that it couldn't find -- Store, SalesDate and DepName ;WITH posCte AS ( SELECT Store, SalesDate, LEFT(Department,CHARINDEX('-',Department)-1) AS DepCode, RIGHT(Department, LEN(Department) - CHARINDEX('-',Department)) AS DepName From #TempNetDepSum ) SELECT c1.Store, c1.SalesDate, c1.DepName, MIN(c1.SalesDate) AS NewPosUsedDate, MAX(c2.SalesDate) AS OldPosLastUsed FROM posCte AS c1 INNER JOIN posCte AS c2 ON c1.Store = c2.Store AND c1.DepName = c2.DepName AND c1.SalesDate &gt; c2.SalesDate WHERE c1.DepCode &lt;&gt; c2.DepCode GROUP BY c1.Store, c1.SalesDate, c1.DepName Result Sample (Just Store 201) Store SalesDate DepName NewPosUsedDate OldPosLastUsed 201 2015-03-07 00:00:00.000 HARDWARE 2015-03-07 00:00:00.000 2015-03-06 00:00:00.000 201 2015-03-07 00:00:00.000 PARTY 2015-03-07 00:00:00.000 2015-03-06 00:00:00.000 201 2015-03-07 00:00:00.000 PET 2015-03-07 00:00:00.000 2015-03-06 00:00:00.000 201 2015-03-07 00:00:00.000 STATIONERY 2015-03-07 00:00:00.000 2015-03-06 00:00:00.000 Total number of records: 21230; 2065 Entries for 201 alone. 3-4 entries per day since the day of the swap, no weird gap between NewPosUsedDate and OldPosLastUsed. Again I would have assumed it would return 20 entries since there are 20 departments... Then again the fact there's only 3-4 per day doesn't matter since I could use this to whittle it down to just the months involved. -- This Space for rent -- To be used by a nice, clean SQL statement that will -- Summarize by Store, Sales Date, Department -- As long as they are not part of the Months found by -- * The Full Month Section of the code above -- * The Changed POS Section of the Code above -- * Our Lord and Savior the Flying Spaghetti Monster -- Probably will use a delete from #TempNetDepSum where Store = and Month = -- Don't Even need to care about the year since it needs to be gone from current and Previous year. -- Followed by the same Code I use to get All Store Sales regardless of status. 
Unfortunately, I'm required to have a consistent, readable value for the amount of leave remaining per employee. I did put the idea across of on-the-fly calculation from within the admin side of the webapp, but the person(s) who specify the requirements either don't quite understand that there's no *real* difference, or have other undisclosed needs for the column. I'll use the last example, depending on whether or not a computed column is a good idea - I don't have a whole lot of machine power to play around with.
If the computed column is too demanding(though I doubt it will be) Then SELECT totalLeave - leaveTaken as leaveRemaining FROM HolidayBooking.dbo.leaveValues WHERE employeeID = @empID should be fine for when your web app is requesting the information for display such as when an employee wants to see how much leave they have or when a manager is looking at his list of vacation requests for approval. When the manager does the approval then you can simply do UPDATE HolidayBooking.dbo.leaveValues SET leaveRemaining = (totalLeave - leaveTaken) - LeaveRequested (I'm assuming when someone requests a holiday it gets stored somewhere) WHERE employeeID = @empID This is a simple calculation so it won't be heavy on the server unless there's a ridiculous amount of people using it at the same time. (Talking hundreds of thousands here)
Why do SQL developers never get asked to help move house? They always drop the table.
It really wasn't called out in the example, but if the original value is X, I need to make all subsequent rows = 0. 
The second one should be the last date the old POS was used. 
That's the problem. This app will grow to be company-wide, ergo world-wide. Assuming 10% of the workforce will access it every ten minutes or so, I'm looking at at least 100k unique visits. Out of those, perhaps a thousand will perform that individual query, but I also have to think about what other queries will be being performed at that same time. This is the first time I've done something on quite this large a scale, I'd like to get it as close to right as possible. Replication across a few distributed servers isn't an option at first, either. That might possibly come six months down the road as we decommission the sharepoint servers that this app is replacing, but balance is key here. A procedure that only runs when leaveTaken is changed would, I think, be appropriate here - bearing in ming that I'm not storing leave requested, that's a volatile item until definitively booked as most employees need the approval of several different managers. Trying to keep track of potential changes that could happen several times is just too much pressure, imo. Instead, I store a single object that has a start date, end date and total time taken. Only when this is finalised does it become a permanent leave object. It's kludgey, but easier to manage. I might make a maximum number of "pending" items too, try and save a little space.
1) If you this is going to have at least 100k unique visits a day, you need to not be running on shit hardware, I know its likely not your call but your bosses need to understand that hardware needs to be able to keep up with demand. 2) If you don't store Requested Leave, how does the leave request get to the managers? Email? Smoke Signals? Having the last Requested leave stored means the manager could see it in the app, change it if necessary with a Changedby, ChangeDate field so other managers and the employee could see the change Possibly as a separate table you can join to so that multiple changes can be tracked until approval. 3) Save a little space... how many records do you think are going to be in this thing and how much space do you think each record will take that saving space is a requirement? I think someone needs to sit down and look at your table design and the plan for the app because there's some weirdness here I can't understand...
There's definitely something off with both versions. Unless I am reading the results wrong It's close but not quite... This is the whole resultset for version 2 Store DepName NewPosUsedDate OldPosLastUsed 201 BABY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 CARDS 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Crafts 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 HARDWARE 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 HEALTH &amp; BEAUTY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Housewares 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 PARTY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Pet 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 STATIONERY 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 201 Wrap 2015-03-27 00:00:00.000 2015-03-07 00:00:00.000 235 BABY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 CARDS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 CRAFTS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 HARDWARE 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 HEALTH &amp; BEAUTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 HOUSEWARES 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 PARTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 PET 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 STATIONERY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 235 WRAP 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 BABY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 CARDS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 CRAFTS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 HARDWARE 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 HEALTH &amp; BEAUTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 HOUSEWARES 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 PARTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 PET 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 STATIONERY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 384 WRAP 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 512 Baby 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 CARDS 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Crafts 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Hardware 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Health &amp; Beauty 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Housewares 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Party 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Pet 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Stationery 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 512 Wrap 2015-05-21 00:00:00.000 2015-02-03 00:00:00.000 526 BABY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 CARDS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 CRAFTS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 HARDWARE 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 HEALTH &amp; BEAUTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 HOUSEWARES 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 PARTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 PET 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 STATIONERY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 526 WRAP 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 BABY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 CARDS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 CRAFTS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 HARDWARE 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 HEALTH &amp; BEAUTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 HOUSEWARES 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 PARTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 PET 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 STATIONERY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 578 WRAP 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 BABY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 CARDS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 CRAFTS 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 HARDWARE 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 HEALTH &amp; BEAUTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 HOUSEWARES 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 PARTY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 PET 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 STATIONERY 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 583 WRAP 2015-09-30 00:00:00.000 2013-10-01 00:00:00.000 See the ones who say New 2015-09-30 - Old 2013-10-01? The gap shouldn't be that huge between when the ECR was last used and POS was first used. Another thing of Note. 583 Closed as an ECR Store on 2015-01-30 for 3 weeks and Reopened as a POS Store on 2015-02-26 So the values there are off too. For the first version of your code I did ;WITH posCte AS ( SELECT Store, SalesDate, LEFT(Department,CHARINDEX('-',Department)-1) AS DepCode, RIGHT(Department, LEN(Department) - CHARINDEX('-',Department)) AS DepName From #TempNetDepSum ) SELECT Distinct c1.Store ,Month(c1.SalesDate) --,c1.DepName --,MIN(c1.SalesDate) AS NewPosUsedDate --,MAX(c2.SalesDate) AS OldPosLastUsed FROM posCte AS c1 INNER JOIN posCte AS c2 ON c1.Store = c2.Store AND c1.DepName = c2.DepName AND c1.SalesDate &gt; c2.SalesDate WHERE c1.DepCode &lt;&gt; c2.DepCode GROUP BY c1.Store, c1.SalesDate, c1.DepName And for a couple of stores including 583 it threw out all 12 months... Because we are looking at; October-2014 to September-2015 VS October 2013 to September 2014 If I do the Math in my head it should have kept; * January-2015 Vs January-2014 * December-2014 Vs December-2013 * November-2014 Vs November-2013 * October-2014 Vs October-2013 I think it's close but there's a piece we're missing. Btw In case you didn't see it I put in my entire code in my previous reply to you.
I'd usually agree with you on the power of the machine behind it, but bear in mind this machine *also has to run the IIS server* as well as all the proprietary shite that we mandate - including our own top-heavy update installer, encryption refresher, &lt;company&gt;service (120k, how?!?), reporting tools etc. I'm thinking it isn't going to be the most responsive webapp by a long shot, but it should trundle along. I'm thinking more something along the lines of this: * User creates leave request * Request URL is generated (dates,employee ID) and sent to line managers inviting them to approve or contest * upon contest an email to user is generated - line managers don't necessarily have the power to deny leave, but employees are expected to inform them and work with them * Upon complete approval (each employee has a chain-of-command column with all line managers/side managers) a final mail is sent to the employee's overall manager who has the decision power. If the holiday is approved, an email is generated with an appointment for the employee to accept (I think it's automatically added to outlook by default, have to double check that again). The leave tracking app also has a calendar that hooks into EWS, more for managers to view their team calendars tbh. Also, note that the idea behind the email message is to basically make a giant banner shouting "Click here to accept" or "Click here to contest" when it comes to John Smith's next excursion to Disneyland. Hopefully I can implement a two-click policy for this, one to load the event and one to approve or contest it. The latter is done within the application, like you say. Might be worth creating an object base for it after all... I had it all planned out and it suddenly makes more sense. Also, it might sound risky to use URL formation to create approvals or contests, and you'd be right to think so, but I can *kind* of get around that by disallowing anyone not in that employee's command chain from interfering with their profile, and I'm lucky enough that we have logservers that are entirely separate - we can only write to them in certain ways, plus there's live monitoring. The answer here, I think, is to log *everything*, paper trail everything, order it by hour and date, and hopefully that should suffice. 
Sounds like you need to open the solution. Otherwise you'll only see the XML behind the rdl. Try to find the solution file the rdl is in and double click to open the project. 
Pre*cise*ly
If your drop table isn't triggered it might be because your sql is in one batch, add a go statement after the drop, also I'd drop statement inside an exec so the compiler doesn't get confused. If object_id ('#table_name') is not null Exec ("drop #table_name") Go 
http://charlesleifer.com/blog/techniques-for-querying-lists-of-objects-and-determining-the-top-related-item/ The above shows a few approaches with benchmarks
Just do a GROUP BY: SELECT C.customer_num, SUM(I.total_price) FROM customer C INNER JOIN orders O ON O.customer_num = C.customer_num INNER JOIN items I ON I.order_num = O.order_num GROUP BY C.customer_num
You cant assume someone with the same first and last name is the same customer... Given that there can be only one address for a given customer, there is no way to know if a customer is a duplicate. You could total by address, but that would simply be modifying the group by in the above query and might not take i to account multi family households. Unless your customers are making new account with the exact same info of course...
I've tried saying this at work to the people porting data from our legacy system to a new system, but nobody seems to care. At this point I'm just staying as far away from that project as possible. Even so, I cry a little.
Hey, thanks for your concern! The original table is set up in german, so it shouldnt be a problem, but it is good to know for feature work.
Hey. Thank you very much! Actually this was the only question I was struggling with, but now I can check out all of my solutions. I definetly should and will check bot of it out. The various forms of SQL are a bit confusing, but I'm sure I will handle it somehow.
I'm willing to help! But I need to know a little more so I can fit the solution better. What is the preferred SQL Server? PostgreSQL? M$ SQL Server? Something Else?
I'm using MS SQL Server. 
&gt; get my database like up to a server You'll need a server, some skills in ssh and ftp/sftp and your scripts. &gt; maybe connect it to a website Are you looking at wordpress or hand-rolling your own? You'll need some skills in php probably for both. If you just want to connect to a remote database and run queries then for MySQL there is [phpadmin](https://www.phpmyadmin.net) Edit: there are other languages out there Edit2: Just noticed that you want to use SQL Server
What is the actual problem you're trying to solve here? &gt;Also I read in a forum that you shouldnt use calculateable fields in a table, but instead "have them in a query in runtime". I'm new to access/SQL and dont know what this means nor why calculatable fields are a problem. In a transactional system, you don't want to store data that can be calculated from other data maintained in the database because it's redundant and reducing redundancy is [one of the cornerstones of normalization](https://en.wikipedia.org/wiki/Database_normalization). Also, if part of your process fails (depending on how you've implemented it), your calculated value may not match the source data it's calculated from. Or maybe you have someone decide "I can update the database myself, I don't have to use the website to update inventory." When that happens, what's the correct value? Data integrity is king.
can i use node.js instead of php? i've heard that you can do some server stuff with it, but i don't know the details. 
Thanks for the reply and the link. My problem is, that I still want the field to be calculated and updated. I want to calculate how many exemplars of a book are left in the library (Book.Available), based on the number of books that are in the library's property (Books.MaxNumber) based on the number of loans of this book. But to count the number of times a book is loaned I have to write something like this SELECT Count(Loan.LoanID) AS times_loaned, Loan.ISBN FROM Loan GROUP BY Loan.ISBN; This query gives out the number of times each book is loaned. But now I cant use this query in my update query, since access doesnt allow me to do so. I can probably can solves this by writing a query with DCount and use that one in the update query, but I dont know how the DCount lets me Count groups. I hope you understand what I'm trying to do and also that it isn't stupid :\
The SQL engine creates a file for the database and stores the data in there. Think of a database as a bucket, tables (y axis) are shelves, columns (x axis) are the little cubby-holes on each shelve. A row in a table will be the third dimension on the Z axis. That's how I visualize databases.
The storage of data in a database is done using files but this way too much detail for what I think you are looking to do. The database sits on a server waiting for requests, you send it a query and then it goes off and looks for the data in those files for you. If you are using JS then you can reference the database in those scripts [link](http://stackoverflow.com/questions/857670/how-to-connect-to-sql-server-database-from-javascript/857688#857688) and send the query to the database server that way.
I think it depends on the environment and scope of the query. I tend to use `nolock` often precisely because I know tables are in a constant state of update (OLTP extract) and don't want to cause other jobs to fail downstream. However, this practice tends to be for stored procedures that run at set intervals and update a results table. Inaccuracies are bound to happen, but also bound to be corrected, and over the scope of hundreds of thousands of rows not likely to have any significant impact on totals for reporting purposes.
I've always thought of them as stacks of excel sheets!
I really don't know what kind of project would suffice. I have gone through a few free online courses and I have read through some of SamesTeachYourself SQL textbook. Do you have any good ideas? I really want to step up my programming game, because right now I only know VBA, HTML, CSS, JavaScript... I REALLY WANT TO MOVE INTO MORE DATA WORK!!
Making a portfolio with SQL seems difficult, but I see there are some guys under the "daily programming" tag, who solve problems using PL/SQL or pgSQL. Maybe it could be a way to show someone your skills while lacking experience? 
Sorry, how do I find them?
[Here](https://www.reddit.com/r/dailyprogrammer/) is the tag where you can find a lot of programming tasks. And [here](https://www.reddit.com/r/dailyprogrammer/comments/3q9vpn/20151026_challenge_238_easy_consonants_and_vowels/) you find one guy, who solved a problem using PL/pgSQL. ('Ctrl+f sql' and you will find the code). I don't know them, but you can meet people using SQL under the tag from time to time. Just watch the tag and maybe try something yourself.
There is high demand for SQL developers. If you interview well (can articulate answers), and know the basics (how to JOIN tables) you'll land a contract. Your background will help, but the first two are likely more important.
Interesting, that's more or less how I ended up as a developer too. 
&gt; I still want the field to be calculated and updated You still haven't explained *why* though. &gt;I want to calculate how many exemplars of a book are left in the library (Book.Available), based on the number of books that are in the library's property (Books.MaxNumber) based on the number of loans of this book This is a pretty standard thing to calculate. You know how many of the item you have. You know how many active loans you have. Therefore, you can calculate the number that are not on loan at **any** given moment. As I said above, there is **no need** to maintain this 3rd piece of information as a static value in the database because you already store all the information required to derive it. If you continue down this path, you are setting yourself up for data integrity problems - your calculated number of available books may not match the number owned minus the number on loan. If the integrity of the data in your database can't be trusted, you have nothing.
Your first query does not need the WHEN so: SELECT pnumber, SUM(w.hours) FROM company.project p JOIN company.works_on w ON p.pnumber = w.pno GROUP BY p.pnumber ORDER BY SUM(w.hours) DESC LIMIT 3 Use that query to create a derived table: SELECT e.fname, e.lastname, x.pnumber FROM company.employee e JOIN company.works_on wo ON e.ssn = wo.essn JOIN ( SELECT pnumber, SUM(w.hours) FROM company.project p JOIN company.works_on w ON p.pnumber = w.pno GROUP BY p.pnumber ORDER BY SUM(w.hours) DESC LIMIT 3 )x ON wo.pno = x.pnumber 
Good answer - I just have one pedantic comment :) &gt; Your transaction log backups are a chain (!not cumulative!) since your latest FULL backup; instead of the latest DIFFERENTIAL. They should be backed up/kept at least as long as your latest full, preferable as long as you keep all your backups. Technically, the transaction log is a chain from the first full backup, and subsequent full backups will not break the chain. If you take a log backup at 1pm, a full backup at 1:30pm, and another log backup at 2pm, the 2pm log will contain all the log records for the full hour, from 1pm through to 2pm. If you restore the full backup, then restore the 2pm log, SQL Server is smart enough to start restoring the log from the point in time that the full backup finished - i.e., it will start from 1:30pm, assuming the backup finished almost immediately. The full backup will contain both the database backup, plus all log records that were active at the start of a backup, through to all log records that occurred until the time the backup completed. I'm off on a tangent now, but go along with it :) If the last active transaction started at 1:20pm, the backup started at 1:30pm, and completed at 1:40pm, the full backup would contain log records from 1:20pm through to 1:40pm. Restoring this would get you to a consistent database as it was at 1:40pm. If you then apply the 2pm log, the log records from 1:40pm -&gt; 2pm would be applied (the earlier log records in that log backup are ignored as the database is already past that point in time.
Always worth noting "edge" cases, trying to be concise before I drifted off :) Was attempting to enforce that the transaction log is a linked list. If any intermediary is lost, it is devalued past that point or until the next database backup occurs; it won't be able to construct an LSN chain. *Edit - But if you keep them for long periods(with relevant database backups), it is possible to point-in-time restore up until the last successful one was taken, even if it spans multiple full backups and database backups were lost in between.* 
&gt; And since we are limited to 3 tables I cant see prob with integrity You don't buy database storage by the table. A 3-table limit is completely arbitrary, but you can solve this without exceeding that anyway. That said, **any** time you have redundant data like you're describing here, there is a risk to the integrity **not** because of the number of tables, but because of the number of steps required to keep all of your data in the correct state. &gt; I want to take the calculateing thing off the user, so he will get all the information automatically. That's why you do the calculation in the user interface, or in the queries that the user interface executes. &gt;At least can you tell how you count in groups by using Dcount? No, because I avoid MS Access like the plague that it is. `DCOUNT` is proprietary to Access and is not applicable to any other database. It's just a shortcut to writing a standard query with an aggregate. You should be speaking with your TA or instructor first, not getting help from the internet. If the solution (which, since you haven't provided the full schema, can't be figured out from your post) is just handed to you, you won't learn.
 Incorrect syntax near '!'.
Further to my previous point, I hope the output of the VBNET can be a comma delimited or text file, after which point I would use an external table to read the data. 
On the sidebar. **Note /r/SQL does not allow links to basic tutorials to be posted here. [Please see this discussion.](http://www.reddit.com/r/SQL/comments/2jcw2y/what_do_we_think_about_tutorials_especially_basic/) You should post these to /r/learnsql instead.**
Approved
Ok so in essence you created a table, inserted data and now the table is gone? It sounds like you may have created a temporary table, but no one can answer your question without knowing what happening. What commands are you using to create the table?
How would that prevent invalid data from making it into a table?
Yah I figured it was a VM issue
I would say they want OP to create a script that can be run to Generate the tables &amp; insert the data. Then separate scripts/queries to extract the required information. This was the type of assignments I was given in school and clearing the data on login ensures students will have to create the tables using a script or painfully type it out for each login. 
Run a transaction. If you're using access, you can error check your values in the module that's processing your data. If you have a convert function that spits out a defunct isbn then you can cancel the transaction. 
It should work in sqlLight. I probably just messed up some formatting using mobile. Check out the following link. The selected answer is what you are looking for. http://stackoverflow.com/questions/2334712/update-from-select-using-sql-server
It's not a VM issue; /u/PeacefullyFighting has it correct. The school has setup the VM in sandbox so anything you do is gone once you are signed out. This is to make sure students don't reuse other peoples tables and queries. If you want something static to work with install sqlserver compact at home, it's free and allows for a database of up to 2gigs, which you won't reach in a million years with school projects. But your teacher is looking for two answers in these projects, he wants you to show how you get the data in and how you get the data out. Just make sure you save your queries in a local file outside the VM. I use Snippet Manager to do that personally but text files work just as well.
The Database itself is an MDF file The tables inside do not have file extensions as they are just organized data within the MDF. When you make code inside SQL management studio there's no "link" necessary. Once you start moving code to outside applications then you'll need a connection string that references what server you are trying to access as well as which database on that server and the username and password. You will not reference the file itself.
What kind of questions?
Sure, I'm on lunch break for the next hour, send me a PM
Can send me a PM as well
I didn't expect all the quick responses, I will send the question in an hour after this class, thank you all for responding 
PM me, I'll take a crack at it.
Sure, PM me as well.
This is exactly it. Thank you for your input!
Just post the questions in a thread; you can get multiple answers.
This script is going to save me a headache of trying to figure out how to delete 40 million records from a single table audit log that's in constant use in production. 
Ask away man! You can even get multiple perspectives here :).
That's nice of you not to pull a Bad Santa
Use C#/VBA to automatically generate reports and manipulate the data daily, rather than pulling information from SQL, copying it into Excel, then manipulating it. 
C# is for webapps if I remember correctly? I was contemplating making some form of user self service reporting tool to cut down on my ad hoc reporting time. 
Either your field's data has trailing / leading whitespace, or a field is CHAR rather than a VARCHAR. CHARs are trailing padded to the length of the field. Trim where appropriate.
Are you looking for a DBA, or a programmer that utilizes SQL? These are two very different things.
Looks like nulls aren't possible. Try len(INV_DATE) &gt; 0.
Try /r/datasets
Each column must have a unique name, and you have a few that are repeated (CUSTID, DISTRICTID, REGIONID, etc.). If the column names are the same but the data is different, alias one or other so that they're unique in the view; otherwise, just take one of them out, no need to duplicate the data.
When you have all the answers, can you consolidate them and publish them(any private info removed)?
You could program the script to check the server name, so it only runs on 'testserver'
Have a python script running on cron ask you, if you answer yes it runs the SQL script.
Thank you for answering my questions.
Get the 70-461 training kit. You will learn A LOT! I passed the exam with this material and I am working on 70-463 now. In depth explanations as well as exercises. My skill set has increased dramatically. By focusing on these exams you can transition from Analyst to developer. 
Step 1; get rid of access. It is very limited in what you are allowed to do and does not follow regular SQL conventions which is why you are hitting a brick wall. Step 2; Setup a small machine with either MS SQL Express or MySQL (don't know if Oracle has a free, limited used version) Step 3; move whatever APP you are building for this library to either a webpage (PHP, ASP) or a windows form app in VB or C#.
It's not the CONCAT function doing this, your data has the spaces. Use TRIM to solve this. 
There is a difference in the concepts of "nothing" in regards to a string in Databases, this can be a null string or an empty string. However, they do differ! Null =&gt; Absence of data. Empty String ('') =&gt; Field has data. It has been set and explicitly marked as empty. In your case it looks like you want to check if the field is null or empty. There are several ways : `(INV_DATE is not null) and (INV_DATE &lt;&gt; '')` or `not (INV_DATE is null or INV_DATE = '')` or `IsNull(INV_DATE, '') &lt;&gt; ''` or `len(INV_DATE) &gt; 0` As a footnote, Oracle added VARCHAR2 precisely to work around these problems. VARCHAR2 fields treat '' and null as the same.
I've done this in the past. It works, but you need to make sure the servers are named correctly. My company's dev and prod servers have the same name...so it would obviously fail in that situation.
Do they have the same IP address? how about "select CONNECTIONPROPERTY('local_net_address') AS local_net_address"
In OP's case there are no nulls allowed in the field definition though so he's never going to encounter a situation where INV_DATE is not null will NOT trigger as it will never be null. I'm more concerned about someone putting in dates as Varchar(9) when Date or DateTime is a perfectly valid field. 
Yes, appreciated, NOT NULL constraint aside, I was just highlighting the differences and something a surprising number of DB developers don't realise. Date representations should indeed be date types, I never understand a systems which don't use the corresponding correct datatypes.
If it is at all possible I would suggest you consider converting your Inv_Date to a Date or DateTime field. It might mean having to tweak whatever software inserts into the DB but using the right datatypes makes quite a bit of difference. Plus at the same time you could convert Inv_Date to allow nulls which would allow you to use /u/Ziptimes suggestions of looking for null and empty
wow I just looked at the training kit and it looks great. I'm taking a database design and admin class in night school right now and this looks like a really comprehensive and applied track! cheers. 
Thank you so much. I can't believe I got stuck on this .... 
What experience level are you looking for? I've done 1 year as a SSRS / SQL dev and im in my 2nd as SQL / SSRS / SSIS currently if you're looking for someone on the younger end of the career.
/u/SQL_Green made a bit of a booboo, I think. His first where condition only specifies that the description must not equal Facturation. It must say that it cannot be Facturation OR FacturePayable (see my post above). If you were to run this code, all FacturePayable types would be returned because the first WHERE clause would always return true. Does that make sense? Say a row for Description = 'FacturePayable' and Type='Banana' comes up. (mv.MSV_Description1 != 'Facturation') FacturePayable != 'Facturation'---so this would return true and this row would be returned, even though you only want to see Type = IC.
You're right, I was thinking that if I included FacturePayable in the first statement that it would override wanting some of the rows later but the or covers that possibility.
It's weird though... I would have expected mv.MSV_Description1 &lt;&gt; 'Facture Payable' with a OR would have overwritten (mv.MSV_Description1 = 'Facture Payable' AND glt.glaccounttype = 'IC') 
Alright I got a lot of good responses, thank you to everyone who responded.
If you are on SQL Server, you can do something like: IF @@SERVERNAME = 'Name of your test server' BEGIN END it will do nothing if run on your production server.
Yep. Decades old application with layer after layer of less-than-best practices.
Can't you just do something like: *** sp_MSforeachtable @command1="SELECT TOP 3 * FROM ?" ***
DNF = A sum of products (OR of ANDs). CNF = A product of sums (AND of ORs). OR is considered a summing, products are because they are the logical AND of a set of variables. These concepts are dual because of their complementary-symmetry relationship as expressed by De Morgan's laws.
&gt; The question is: how come this returns the way it does instead of returning the car colors in order, and then the amount of total different colors each time? That is, how does it know to apply COUNT to each individual color rather than the full set of colors at large, which is what COUNT(*) otherwise returns. Queries with aggregation functions like COUNT are based on the grouping of rows. If GROUP BY is not specified in the query, there is only one group for the whole table. This group contains all rows of the table and thus a COUNT-query simply returns the number of rows in the table. If GROUP BY is specified, there can be more than one group of rows in the table. Each row belongs to exactly one group. The group of a row is determined by looking at the value(s) of the attribute(s) specified in the GROUP BY clause. In case of attribute "color", this means that all rows with the value "red" will belong to the first group, while all rows with the value "blue" belong to the second group, and so on. Now, the aggregation functions of the query are calculated for each single group and we get one output value for each group of the table. The query with COUNT now returns the number of rows in each group of the table, instead of the number of rows of the whole table.
I could perhaps, I don't think it would be better performing though? Additionally, that means I can't limit to only one schema as I currently do.
&gt; If you don't specify an ORDER BY, SQL will just give you the rows in which order it got them from the table in the quickest way. There is no order in SQL, and you shouldn't expect an order in the result unless you specify this. If you get it sorted the same time every time, it's because of an index or the way the table is now. If you need (business) data return sorted always use an ORDER BY. Even though you are correct, your statement may sound a little confusing/paradox for other readers. How can there be "no order in SQL" if there is an explicit "ORDER BY" command to create an ordering? A more detailed explanation is that the relational database model is based on the mathematical theory of sets. Mathematical sets do not have an order and since relations (tables) are seen as sets of tuples (rows) there exists *no natural ordering* of the tuples of a relation. SQL is based on the relational database model but does not always confirm fully to it's mathemtical definition. Therefore, SQL allows us to create an *explicit ordering* for relations (tables) by looking at the values of its tuples (rows). Nonetheless, your main point is absolutely correct and still stands: &gt; you shouldn't expect an order in the result unless you specify this
As an additional note, the nature of what you are doing is dynamic, so at some level the query will probably have reflect that as well. As for performance, you are taking a very small sample from each table. Unless this needs to run several times a second (*why?*) it shouldn't produce any noticeable performance impact. 
Run the query: *** BACKUP DATABASE yourDatabaseName TO "C:\backups\mydatabase.bak" WITH COPY_ONLY *** Copy the file to the new server. *** RESTORE DATABASE yourDatabaseName FROM DISK = 'C:\backups\mydatabase.bak' -- Note: You will need the following before the database will be online WITH RECOVERY *** Check to see what database users don't have a login *** -- On the "new" development side USE yourDatabaseName EXEC sp_change_users_login 'Report' ***
Backup and restores are *by-far* the most reliable/consistent method to migrate a database. If you don't trust backups you may need to re-evaluate what you know / practice utilizing them; because when it comes to a recovery scenario, copy database or getting the data/log files very likely will not be an option. *Edit - Taking down a production environment unless absolutely required is just silly*. 
We use a backup server that allows for granular restores for recovery scenarios. Every attempt I've made of doing a backup/restore from MS SQL itself using the commands you outlined in your own post. has resulted in the DB getting stuck in a dirty state (don't remember the exact term MS SQL used as it has been a while) and unusable no matter what commands google fu brought up.
It's not a 'dirty state', can almost guarantee that it was in NO RECOVERY (AKA waiting for another backup file/transaction log/etc). Personally seen this dozens of time where people thought the backup/restore was broken. *** Running the following would most likely fix the issue: *** -- Do the queries in my other post RESTORE DATABASE yourDatabse WITH RECOVERY *** Also adjusted my other post to have it as well for clarity. 
What about 70-462? What units are necessary for MCSA? I presume MCSE is the next step after. What BI tool are you intending on learning? I have used Cognos TM1 previously and we currently use Crystal Reports at work (although I don't really like crystal so stick to access where I can!)
Import wizard 
I am using the import wizard
I can't even import as all text. Half of my columns get screwed up and throw quotation marks and combine column values into and leaves others blank. It works for about the first 100 rows then randomly there are quite a few that are just completely off. There is a column in my csv that is all ints no
Thanks!
&gt;there will be quotation marks thrown into some of my columns You are using the wrong codepage then. You have to select the correct codepage of the CSV. &gt;some of the column values get combined into one. That sounds a bit fishy, are you sure / can you go into more detail? If you have issues with the field terminators, you should get that as errors on the import. If some of the rows don't match up to your column definition, that shouldn't even import. It would be good if you could give examples of rows that are not parsed correctly (not the ? thingy, the field terminator thingy). 
UTF-8 or 16? If data space isn't a concern, use nvarchar. I'm wondering if you've got janky characters causing problems. 
The quotes are text qualifiers. To prevent commas in text to mess with your delimiters. It sounds like you have a row where the text qualifier or comma are not corrext. Open it in excel and use a filter to try to find offending row. I believe it's a file format issue. Seems like you are doing it right.
Alright this actually helped the most because it helped me realize two of the DB's were not syncing to the replica. Which I should have noticed when I received space alerts on one node but not the other but I digress. I ended up fixing that. I shrank the DB's to a smaller size (1 file was already at 16GB) I did some more searching on stackexchange since thanks you guys I knew what words to use for my problem and ran this for each database, which as I understand it will set limits on log growth. USE [master]; GO ALTER DATABASE Test1 MODIFY FILE (NAME = yourdb_log, SIZE = 200MB, FILEGROWTH = 50MB); GO And finally setup transaction log shipping every 15 minutes. Do you guys thing this will solve my problem? Anything I should do or shouldn't have done? 
What flavor of SQL? Regardless, its possible to order the results you get based on a WHERE IN () clause by saying ORDER BY FIELD("fieldname",ID,ID,ID,ID) where each ID matches an iten in your WHERE IN clause. thing is though you generally need to know what ID's you are looking for. The question is built strangely and it's not really a good way to go about things. The only reason you would ever want to use ORDER BY FIELD is if you absolutely want suff to show up in an exact order like; ORDER BY FIELD(ID, 4, 6, 8, 2, 1, 5, 7, 3) Are you sure that tip isn't supposed to be "use an IN expression **and** an ORDER BY"? 
did you check the permissions on featherj@'localhost'? can you post your. $servername = ; $username = ; $password = ; $dbname = ; config lines (blank out the password though) and is the server localhost or is it a remote server?
Yeah checked it over and over, i just uninstalled xampp and installed it again and now it works :s no idea haha, thanks for the reply though.
I think I got it: SELECT name FROM customer WHERE (SELECT REGEXP_REPLACE(phone,'[^0-9]','','g')) = '01234567890' edit: got the regex right now too, good to know postgresql wants that 'g'...
Yes. We use a couple cubes at my employer for sales data. They give the ability to the business user to quickly crunch numbers in many different ways. A single well designed cube could be put in the place of 50+ reports depending on how it is built. 50+ reports that I now then would not have to write. However, a reporting tool is only useful if it is used. The problem is you have to have business users that know how the cubes are put together to use them efficiently. That is the caveat to having them. Sometimes they get too confusing and then they don't get used at all. 
Yeah, without it would only find first occurrence, /.../g didn't work, some digging later found out it will want that 'g'
 INSERT INTO AdTable ( Column1 ,Column2 ,... Etc ) VALUES ( Value1 ,Value2 ,...Etc ) WHERE Name NOT IN (SELECT Name FROM AdTable) Since Test1 and Test2 will be found in the Select they will not be added. This is just a one insert example since you didn't provide your instert code, but the logic will be the same for a bulk insert too. Try Catch is mostly only for errors and an IF/Else needs a query into the database for every row you try to insert. Since your script tries to enter every user every time it runs that would make it cumbersome. 
Right, forgot insert can't have a where unless an insert from select that'll teach me to try and do work AND help people at the same time... gimme a sec. here you go DECLARE @ComputerName nvarchar(20) = 'test' INSERT Stale_AD_All (Stale_AD_All_ComputerName) SELECT @ComputerName WHERE NOT EXISTS ( SELECT @ComputerName FROM Stale_AD_All ); Sorry about that.
Your major doesn't factor into the equation much unless its data entry analysis for a company related to your field, your best bet is to look at job postings for entry level data analyst positions and see if your qualifications apply.
Came here to post just this. I am a math major with a physics minor. Find something entry level (once you're familiar with SQL and how it works) and you can go about as far as you want. The key is to apply for all the jobs, not just the ones you're qualified for.
No Problem, I appreciate the assistance. **Edit** And it does exactly what I needed. Thank you very much.
Yep. My co-worker (at a large financial industry company) has a physics degree. He's a senior-level data analyst. As far as I understand, he's self-taught. We use mostly SQL, python, and problem-solving skills in our work. I have a degree in neuroscience, so it really doesn't matter your background. I didn't know anything about the industry before I got an internship at the company, but they liked my work-ethic and my interest in learning.
Thanks for the advice, guys, I'll keep that in mind. This is very reassuring.
No, we use tesseracts now
You're welcome.
 SELECT cup, cnt_colors, cnt_drinks, cnt_ids, case when cnt_colors = cnt_drinks and cnt_drinks = cnt_ids then 'Counts OK' else 'Counts Fail' end result FROM ( SELECT cup, COUNT (DISTINCT colors) cnt_colors, COUNT (DISTINCT drinks) cnt_drinks, COUNT (DISTINCT Ids) cnt_ids FROM cup_table GROUP BY cup )
As long as you dont have a soft degree you are pretty much good to go.
DBMS? UPDATE MyTable SET MyField = REPLACE ( MyField1, "very dark", "navy blue") WHERE SUBSTR( myField1, 1, 8) = 'the coat'
Can you show what error you're getting? Also just based on your syntax you have depot_location at your insert line, and then depotlocation for your select. Not sure if those are supposed to be the same thing. Actually I'm confused why you have a query right after your insert line. I'm somewhat of a beginner but this seems like something I can figure out if you explain what's going on. 
Spot on
Assuming you have the correct code page, the file itself could be botched by having data that contains your text qualifier perhaps
Have a look into the merge command syntax if you're on a compatible version of sql server. The syntax can be tricky to get your head around sometimes, and the queries tend to look long/large, but they optimise and scale very well, and future expansion/growth is often much easier on a base merge than a select where/insert type of arrangement.
Sure.. that would make sense.
Is it possible you downloaded a different version of the AW db? There are several different versions each year and they all have different sets of tables and columns. It's one of the most frustrating parts about working with the AW dataset. 
I don't thunk so, the database was already added by the professor. I'll ask, but I am correct that those should match up though, right? 
This should help: http://www.w3schools.com/sql/sql_primarykey.asp
The reason is because of your insert query. You need to just have the total for the current year instead of summing up the total for both years in the same column. The most likely culprit is Cost, Cost from depot in the original view/query. it looks like you are aggregating for multiple years. insert into profitablecounty (depot_location, journey_cost, journey_dates, time_weekStart, time_weekEnd, year) select DepotLocation, Cost, journeyDate, WeekStart, WeekEnd, year 
You aren't grouping on what you are selecting. Keep first and last name separate in select and group, then select from that and build the [Customer] name up.
What do you mean? Use something like intersect?
This is just a copy of the [Tabibitosan method](https://community.oracle.com/thread/1007478?tstart=0) which was posted 5 years ago.
So if the select statement is Select X + Y The group statement should be: Group by x+y
Yes. select has to match the group by. Personally, I like to group by columns, not column operations, so I would've built the customer name in a select on the results.
Since OP mentioned SQL Server is the RDBMS, it's also more efficiently solved using [2012's Windowed Function](http://sqlmag.com/sql-server-2012/solving-gaps-and-islands-enhanced-window-functions) if available.
This is correct. You want a HAVING clause, since you're searching for something that happens after your SUM. Anytime you want to perform a search after an aggregation occurs, it's in the HAVING clause. WHERE always occurs before the aggregation is performed. 
Remove the price from the group by. You are aggregating on price, so don't group on it.
um, no -- this is totally inaccurate it is perfectly okay to have `SELECT a+b AS c` together with `GROUP BY a,b`
remove `oi.price` from your GROUP BY and vwalah!
Tabibitosan is based on windowed analytic functions. The article was written in an Oracle forum, but equally works in SQL Server.
Change your WHERE to an AND 
 select o.firstName + o.lastName as [Customer], o.total_price from ( select p.firstName, p.lastName, sum(oi.price) as total_price from orders o join order_item oi on oi.orderid = o.orderid join people p on p.peopleid = o.customerid where oi.price &gt; 50 group by p.firstName, p.lastName ) o order by o.total_price desc EDIT: Added table alias
 MERGE Target AS t USING Source AS s ON (t.KeyID = s.KeyID) -- insert WHEN NOT MATCHED BY Target THEN INSERT(KeyID, Field1, Field2) VALUES(s.KeyID, s.Field1, s.Field2) -- update WHEN MATCHED THEN UPDATE SET t.Field1 = s.Field1, t.Field2 = s.Field2 https://technet.microsoft.com/en-us/library/bb522522(v=sql.105).aspx *** edit: You should also look at table constraints. It puts more rigid controls on the table. http://www.w3schools.com/sql/sql_constraints.asp
This seems to give me what I want. Is it as good as it can be? SELECT row_number() over ()::int4 AS id, coords[1]::float8 AS x, coords[2]::float8 AS y FROM (SELECT string_to_array(subcoords, ',') AS coords FROM unnest(string_to_array('1,2 3,4 5,6 5,1', ' ')) AS subcoords) AS coords; And how can I replace the select query in the function with that?
Thank you sir.
You need to change your JOIN statement so that you're comparing the same key in each table. It should be this: ON BusinessEntity.BusinessEntityId = Employee.BusinessEntityId. Additionally, you can't often JOIN on different data types can't be compared (such as an integer or decimal against a text based type like char or varchar). That's why the int to nvarchar error came up. You can sometimes get around this with CAST or CONVERT but it's better to avoid that if possible. 
Are you sure you haven't misread the question, and it's asking you to only *display* the job title, or maybe filter (i.e., where clause) for it? Or is there also a job title field in the BusinessEntity table?
I think so lol. It's late, I think I'm going to take a power nap before I submit this stuff. Still a few hours till 12! Thank you sir.
Thanks, new to impending database stuff. I found out the answer
So, cross join?
When I add the HAVING SUM(oi.price) &gt; 50 it won't let me order the results.
Nope still does not show Martha and still shows Martin with $80
In SQL Server, [`TIMESTAMP` is an alias for `ROWVERSION`](https://msdn.microsoft.com/en-us/library/ms182776.aspx). While it may have some correllation to a clock, it isn't correct for storing actual dates &amp; times. [`DATETIME`](https://msdn.microsoft.com/en-us/library/ms187819.aspx) represents an actual date &amp; time. But I'd advise using `DATETIMEOFFSET` instead, unless you don't care about timezone issues.
 Mine: select p.firstName + p.lastName as [Customer] , sum(oi.price) as [Total Price] from orders o join order_item oi on o.orderid = oi.orderid join people p on p.peopleid = o.customerid group by p.firstName + p.lastName having oi.price &gt; 50 order by [Total Price] asc Msg 8121, Level 16, State 1, Line 21 Column 'order_item.price' is invalid in the HAVING clause because it is not contained in either an aggregate function or the GROUP BY clause. Yours: select p.firstName + p.lastName as [Customer], total_price from ( select p.firstName, p.lastName, sum(oi.price) as total_price from orders o join order_item oi on oi.orderid = o.orderid join people p on p.peopleid = o.customerid and oi.price&gt; 50 group by p.firstName, p.lastName ) o order by total_price desc
No, I changed the query, please use the changed version. Show me what's in your tables, not your SQL.
You mean the result?
There is no Data only Zuul!
The data in orders, order_item, people.
Orders: OrderID, OrderDate, CustomerID, EmployeeID Order_Item: OrderID, ItemID, price People: PeopleID, FirstName, LastName, Location, Phone, EMail, PeopleType
Dude, your data is inconsistent. Your ORDERS dates are mixed formats, some are mm-dd-yyyy, yyyy-mm-dd. Create the tables in http://sqlfiddle.com/ with data and send me the link, I haven't got time to fix your data format.
Off the top of my head: * Execution plans: There is *A LOT* to these; they are probably one of the most (if not the most) versatile tool a DBA has. There are third-party tools, such as [SQL Sentry](https://www.sqlsentry.com/) that can analyze them even better than SSMS. *As a side note, the plans are stored as XML and can be queried to tell you what you want to know. Second note: Beware the observer effect, showing execution plans will cause the queries to be slower.* * Template Browser: Has dozens of common SQL samples that you can click on without ever leaving SSMS. Add it by going hitting *CTRL + ALT + T*. * Statistics IO: When you are optimizing a query, this can be a huge hint about something being amiss; excellent compliment with the Exection Plans. Turn it on by opening *Query Options* -&gt; *Execution* -&gt; *Advanced* -&gt; check *SET STATISTICS IO*, once again, be wary of the observer effect. * Extended Events: These can be done without SSMS (so can everything on this list), but they are much easier within it. Generally, they are very light weight metrics that fire on the engine level. They are much more efficient for capturing any number of monitoring requirements than the alternatives. 
Don't cross the (file)streams.
No, the plan is generated on query call by the engine. Pulling/displaying the relevant information takes more time. It's just something to be aware of; if you are trying to replace a 3 second query with a &lt;1 second one. The execution plan can make that difference. 
The plan content is the important part there, not the execution time difference due to pulling the plan (which should be minimal, except for **very** large queries). Or if you really have to, skip running the query with execution plan option and just pull the XML out of the plan cache, like you implied.
Guess I could have clarified it somewhat. Having the execution plan turned on increases the *perceived* time to return results, not the actual time the results themselves take. Similar to turning on 'Discard results after execution' will more accurately report the duration. &gt; except for very large queries That really depends on the content of the query. I can write a ten liner that would need A1 paper to display the execution plan legibly, but others that display in a small area inside SSMS. 
Would you mind if I did the same for the dashboard?
&gt;ISO date looks like text but it's actually a valid date format and can be cast to DATE if you so choose. Appearance (visual formatting) is a presentation-layer detail. An ISO8601-formatted date, when stored as a `DATETIME` or `DATETIMEOFFSET`, is the same bytes on the table as `MM/DD/YYYY 12:00:00` (again, when stored as the appropriate `DATETIME` type). And it'll sort by the same rules. &gt; Also the ISDATE() function works against it to weed out any crap data. Unless you're using that in a check constraint, it won't prevent crap data from getting into the table. Rather than have to code extra defense against bad data being in the table (because you're storing a date as text), keep the bad data out in the first place (by defining the column as a date).
I use the Profiler and DB Tuning Advisor extensively which are part of the non-express toolset. I have a host of registered servers I access every day--if you're managing more than one server, that helps a ton. I suspect those three features are commonly used. Mostly, I've used SSMS to upgrade my position in the company from support to software developer. I think that's a pretty neat trick for it to do.
&gt;Unless you're using that in a check constraint, it won't prevent crap data from getting into the table. I agree that's why I'd use it while querying the data instead. I'm not certain why, but an application that I frequently have to query data from has chosen to store the date in ISO format in many places and they use the presentation layer to format it properly. Even so there are some places where they've failed to implement the "input mask" and crap data makes its way into the tables. Thankfully they're phasing out the usage of ISO date and moving to datetime to prevent these issues.
Check out [ssmsboost.com](http://www.ssmsboost.com/) for more things you can do in SSMS. It's a free add-on that does several very handy things.
I think the issue lies with the interpretation of the question. The opening statement, "Write a query that identifies those ward departments that are not earning money because they are charging for provided services (also known as treatments) at below the service charge rate." just gives you a start. It tells you that the only way a ward can lose money is by not charging at least the service charge for a treatment. You need 3 columns in your result: a ward name, their *total* profit/loss, and the count of treatments they've performed. Your joins look good, but I'd do them in a different order. I'd start the from with ward_dept (because your results stem from the ward name), then build from there. This is, however, personal preference. It can be useful to have a set framework to build from, especially when you start using outer joins. I believe you'll only have one group by field. It doesn't make sense to group by service_charge and actual_charge because these should be part of an aggregate function, just like count. The reason you're getting the wrong answer for the count (and more than one row for each ward) is because you're not aggregating the charges and you have the charges in the group by. I hope that helps you out some.
I was a music major. I just left a position with Home Depot as a Senior Data Analyst in order to step into a position as a Senior Business Intelligence Engineer. I worked for IBM about 2 years ago. Our Senior Data Architect / team lead was a former high school English teacher. Honestly, most employers don't care too much about where you went to school or what you majored in. We were literally just having a conversation about that today at work.
This is great stuff - thanks!
Looking strictly at the descriptions of the units, I'd consider it beginner. Unfortunately they won't let you see the full content unless you progress through the whole course linearly, so it's hard to say for certain.
Can the knowledge of it's material lead to an entry level job with SQL?
How can I become a DBA?
You have a few options here, I'll give you a rundown on what I've settled on in a very similar situation: -Full backup on Sunday morning (midnight Saturday). -Log backups every 15 minutes into the SAME file as Full. -Diff backup every day at midnight (new file). -Log backups every 15 minutes into SAME file as Diff. This gives you 1 file per day. Any restore operation will require the differential base taken on Sunday morning, and the file containing the day you want to restore to (Diff + logs). You do not need every file in between. You can optionally separate out the full backup into it's own file, no harm in doing that, but it's not really going to make things any safer. The only advantage would be when you're restoring, you don't have a base file with the 24 hours worth of logs from Sunday. Your call on that. I find it much easier to manage a single file per day. I've had a lot of experience doing exactly what you've described with the copying inside SQL, and the easiest and most effective way (IMHO) is to do it with an embedded PS script...That being said, I no longer bother. I do my backups directly to the SAN device. I am not certain if a Maintenance Plan wizard will allow you to specify a UNC path to back up to, but you most definitely can do it. You might need to edit the job after it's created and just change that parameter. Good luck!
Where (Name like '%joe%' Or Name like '%mark%')
I generally try to have SQL Server handle the compression if possible. * You'll generally see less network traffic, especially in a restore scenario * Moving files between different units doesn't require the files to be decompressed and compressed again. * SQL Server usually does a better (or no worse) job of compression than most SAN controllers. * Most SQL Server instances aren't normally CPU bottle-necked during full backup maintenance windows, where-as SAN units often can be in the same window when they have compression turned on. 
select * where name like '%Joe%' or Name like '%mike%' and Name NOT like '%Marcos%' response is; Validation Result : unexpected token: FIELD NOT
There is no FROM. Also, you should bracket the OR so it doesn't get confused: select * FROM db where (name like '%Joe%' or Name like '%mike%') and Name NOT like '%Marcos%'
whenever you mix ANDs and ORs in your WHERE clause, you either 1) have to know what you're doing, or 2) use parentheses
If the primary columns have all the same information, why not just GROUP BY all of them? 
They don't have the same info all the time, but most of the time they do. From an example standpoint I know it seems kind of dumb to want to do this but I have a reason for it. But I don't want to bore everyone with details. 
*** DECLARE @totalAnswers FLOAT(7,2) = (SELECT COUNT(answer) * 1.0 FROM FinalAnswerLogs) SELECT answer, ((1.0 * COUNT(answer)) / @totalAnswers) * 100.0 AS percentOfTotal FROM FinalAnswerLogs GROUP BY answer ORDER BY answer ***
I completed it last week attempting to learn some SQL myself. It's definitely beginner. I think I got through everything in about 2 hours.
There isn't any guarantee that the row numbers will be sequentially entered, so I don't believe that would work. If you used ROW NUMBER or RANK PARTITIONED BY EmpNo, that would be viable with an additional CASE expression to segregate the first and second entries.
I mean, OP said there are two entries for every employee. No more, no less. If he grouped by EmpNo (his primary key?), gave the rows numbers, then limited results by a mod.. should be golden.
It is the beginning level of beginner. Like, the very easiest part. It would be like learning the alphabet in order to learn English.
how long do you keep stuff in this format? like what happens in week 2? do you just keep 7 days at a time? i like the plan (might separate the T-Logs tho)
Maybe CHARINDEX isn't supported? I'm not as familiar with Oracle as SQL, but maybe try this: select * where (name like '%Joe%' or Name like '%mike%') and INSTR(Name,'Marcos')=0 
Something like this perhaps SELECT f.Answer, (CAST(f.SelectedAnswers as numeric(5,2))/ CAST(f.TotalAnswers as numeric(5,2)))*100 as AnswerPercentage FROM (SELECT Answer , count(*) over () as TotalAnswers , count(*) over (partition by answer) as SelectedAnswers FROM #FinalAnswerLogs ) f GROUP BY f.answer, f.SelectedAnswers, f.TotalAnswers
Look at that beautiful code. This works perfectly. When I first tried to tackle this problem, the LISTAGG function came to mind but that's not really what I'm doing. However, I didn't know about the PIVOT function and that was the missing link. You are an asset to this subreddit and you've helped me out several times before. And I've seen your solutions to help other people out as well. I don't know what you do for work but you sure do know all the nooks and crannies of SQL! 
Yes, you can have multiple headers with the same name, so your real key is (message_id, header_key, header_value). That exceeds the clustered index size limit, so you're pretty much out of luck and need to choose another approach in order to improve performance.
Yes, you may have guessed I work with DBs. I never really had anyone to help me when I was learning SQL, I wish I had, so I had to learn the hard way. So I suppose I'm just doing what I wish I had access to, it's nice to help people. Plus sometimes the problems are interesting challenges, it keeps my skills sharpened.
I read the sidebar a little too late, sorry. * [MySQL]
I like /u/ziptime's pivot solution, but what you're suggesting also works. Oracle tables have a pseudocolumn `ROWID`* that is a unique identifier suitable for this purpose: select * from QRY t1 join QRY t2 on t1.EmpNo = t2.EmpNo and t1.ROWID &lt; t2.ROWID ; [\(sqlfiddle\)](http://sqlfiddle.com/#!4/1a618/2) *ROWID identifies a record's position within a data block/file, so it can change if a record is deleted/reinsert or through other means. It should never be used as a PK or for long term identification of a row. 
Really hard to know what's going on here without seeing your code and table schema(s). If you're trying to join a CHAR column to an INT column, you're going to have to transform target INT to a CHAR first (or do it in the join if you're feeling particularly badass). 
As you sure it's a mismatch and your starting LSN isn't [incorrectly 0x00000000000000000000](http://stackoverflow.com/questions/16298167/scalar-function-fn-cdc-get-min-lsn-constantly-returns-0x00000000000000000000)? 
Nope. Whenever I run the sys.fn_cdc_get_min_lsn() with capture value as the parameter, I am getting a valid LSN value.
You're joining a varchar column to a data type INT column. The database engine converts varchar to INT to do the compare. Usually it's in the JOIN ON part, it could be one of many places including WHERE clause or in a CASE statement. You can deal with that by casting all columns as varchar(10), or start commenting out part of your code and find the problem and just cast that column as varchar(10).
getting the distance isn't the problem the problem is; for each of the deal, i need to find its shortest distance
[Oracle Data Modeler](http://www.oracle.com/technetwork/developer-tools/datamodeler/overview/index.html). It's free, works for all DB platforms (not just Oracle) and is fully featured.
I love Oracle Data Modeler, however, I don't see it supporting export to PostgreSql.
I could be wrong, but I thought it supported any DB that you could connect JDBC/ODBC to?
Just as a general note you almost never see/use right joins in the real world. Stick to left.
Yup. I am passing three paramters. StartLSN, EndLSN and one more parameter specifying the type of pull.
http://www.modelsphere.com/org/ http://wiki.postgresql.org/wiki/GUI_Database_Design_Tools 
This isn't really an SQL question, but to give you an answer; Unless you have a background in software dev, which I doubt since you are asking for "beginners software to write a program" I would suggest hiring someone to build this for you. If not you are going to be sinking in a lot of time learning a programming language, and then trying to figure out exactly what it is you want to do, how to do it and what to do when someone inevitably breaks your program by doing something you didn't think was possible because stupid... just like life... finds a way.
God, I admire you...
This is one way to do this. I found the key part of this on StackOverflow, which is a good place to find useful code snippets: http://stackoverflow.com/questions/19269151/sql-server-how-do-i-get-the-date-of-previous-saturday If it's run on a Saturday, it'll give you the previous Saturday. This expression will always give you the previous Saturday at 7pm, (A/K/A "19:00 hours") I am not sure if you care about what time during Saturday, but you can adjust the "19:00:00" to be whatever time you want. I am not sure if your columns are of type DATE or of type DATETIME. My example uses DATETIME because that's what most people seem to wind up with. Given what I think I read in your paragraph, I think that this will calculate the dates you want: declare @LastSaturday datetime select @LastSaturday = DATEADD(week,DATEDIFF(week,'19000101',CURRENT_TIMESTAMP),'1899-12-30T19:00:00') declare @FirstDayOfYear datetime select @FirstDayOfYear = '1/1/' + DATENAME(year, CURRENT_TIMESTAMP) select @FirstDayOfYear, @LastSaturday 
Thank you so much for your help. Model Sphere is a nice design tool but it can't export ddl after I played with it for a while. After frustrating two days of trying to find right tools for PostgreSql. I decided to switching over my project over to MySql. Maybe in few years once Postgresql's tool is more mature I can return. 
Where has Access come from? If you wish to have something the customer/user interacts with you are better off writing a program or a Access Program but don't overcomplicate things with Access if you dont need to. I've wrote an overview below that might point you in the right direction however a lot of assumptions are made so take it with a grain of salt and try and fit it to your data. Lets assume you have 2 tables, stock and stockmovement. |Stock| Product | Physical Quantity | Free Stock* | Lead Time(Weeks) **| Price | | ---- | ---- | ----| ---- | ---- | ---- | | - | Bananas | 20 | 20 | 1 | 0.20 | **The free stock will probably come from other fields such as Physical Stock - Allocated Stock + On Order Stock* ***The lead time is a measure of how long it takes to receive goods from an order* |Stockmovement| Product | Quantity Sold | Date | Price | Total Price | | ---- | ---- | ----| ---- | ---- | ---- | | - | Bananas | 3 |01/01/15 | 0.20 | 0.60 | | - | Bananas | 2 |02/01/15 | 0.20 | 0.40 | | - | Bananas | 4 |03/01/15 | 0.20 | 0.80 | | - | Bananas | 6 |04/01/15 | 0.20 | 1.20 | Lets assume the date now is Monday 08/01/15 and we are looking to place an order to come in on the 15/01/15 and on the 15th we'll place another order. What I'd firstly do is find out how many we're likely to sell over the next 2 weeks. Looking at the quantity sold table we'll assume sales are consistent for the next 2 weeks. So sales W/C 8th are 15, W/C 15th are also 15. Using SQL we'd find that by using the following query. SELECT Product, AVG(Quanity) as AverageSales FROM table1 (SELECT product, SUM(Quantity_sold) as Quantity, DATEDIFF(Week, date, GETDATE()) AS WeekNumber FROM stockmovement GROUP BY Product, WeekNumber) table1* **Apologies if the SQL is slightly wrong, I'm still learning and all my resources are in work* Id expect to see a table: |Table| Product | Average Sales | | ---- | ---- | ----| | - | Bananas | 15 | Therefore I know that for the next week I require 15, the week after I require 15 and I currently have 20 in stock. Logically I'd need to order 10 bananas. However, as you probably know calculating sales is never that easy, we now that sales can go from 1 in a week to 30 in a week to 1 in a week quite quickly. That means we could order 10 but then we could stock out over the next 2 weeks. To overcome this many businesses use something called "Safety Stock." This can be measured in multiple ways, but the standard seems to be: Safety Stock = SQRT(lead time)*InvNormal(Service Level*)*STDDEV(Range(Previous Sales) **Service Level is a percentage that decides how often you stock out, this is calculated from the profit margin and quantity of sales/popularity and wastage and is a different ball game entirely. A 50% service level represents 0 safety stock, 99% means you will likely stock out once every 2 years based on current demand)* We'll assume here this has been calculated and is stored in the stock table as I'm not sure whether SQL will handle these functions. Lets say the Safety stock is set to 5 items for bananas. My next query would tell me how many of each item I need to order. SELECT stock.Product, (Free_Stock - (Average_Sales*2) - Safety_Stock) as Quantity_Needed FROM stock INNER JOIN Table ON stock.product = table.product I would expect to see the below table. |Table| Product | Quantity Required | | ---- | ---- | ----| | - | Bananas | 15 | As we can see we currently have 20 bananas,this week we will sell 15 bananas, the week after we will sell 15 bananas and we should always aim to have 5 bananas in stock, therefore we need 15 bananas to cover demand. Disclaimer: I will be honest, this wont be any good with stock that is increasing sales week on week or even high variability. This will only work on consistently moving stock. If you wish for your program to analyse these things, you will need to write formula that analyse the movement for example regression, moving averages, exponential smoothing etc. I hope this helps though, I'm sure the SQL command can be combined so this works for one query, however, someone else might be better off advising there. 
Sigh... I know, PostgreSQL is superior. OK, you convinced me to battle on. I will continue to bang my head against wall until I find a better solution.
http://pgmodeler.com.br/ This looks like just the job! It's about $8 or compile it yourself from the source code.
Yes - this. DECLARE @StartDate DateTime, @EndDate DateTime SELECT @EndDate = Getdate(), @StartDate = DateAdd(DD,-7,GetDate()) That would set @StartDate to 7 days ago.
&gt;For some God awful reason my customer set up their sql server with a \ in the name (IE: LC2\SERVERNAME) Are you sure that's the *Windows* server name and not a named instance? Windows server names can't contain a backslash AFAIK.
The windows server name is LC2, however, in SSMS it looks like this: LC2\KOVA(SQL SERVER 12.0.2269 - LOVELANDCOMM\Kova1)
Try `END IF` instead of `END`. https://dev.mysql.com/doc/refman/5.7/en/if.html edit: or rather in addition to.
Both my CHAR and VARCHAR2 fields are defined as *size* byte. Which has nothing to do in order to explain why my CHAR variables have doubled in size between Monday and Tuesday with no table/function/Oracle updates. i.e. in a function with a parameter defined as CHAR (5 byte) by the fact it's defined as *tablename*.*fieldname*%type I call that function from PowerBuilder passing it a string padded out to length of 5 spaces. In that function I write that parameter out to a log file and it shows the parameter contains 10 spaces. A second parameter which is VARCHAR2( 11 bytes) I call that function from PowerBuilder passing it a string padded out to length of 11 spaces. In that function I write that parameter out to a log file and it shows the parameter contains 11 spaces. *No increase in size for VARCHAR2, double size for CHAR* 
Try something like this. So if TRUE it will count, but if false it will not matter. WHERE something = somethingelse and ((key = ??) OR (@parameter = 'FALSE'))
to make it a proc create proc dbo.ParamaterTesting ( @Parameter bit ) as declare @sqlWhere varchar(50) .... etc and then of course, switch out the print for an exec (@sql)
oooh this sounds great, i'm going to try
Thank you so much for taking the time to give such an informative answer. I am still not 100% but i have a better idea of what i have to do now. Like you said previously i think my insert statement is not well formed and thus i cannot get the required data easily if at all. Thank you once again!
I recommend you the following web site. You can do exercises online and progress at you own pace. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Before going down the SSIS rabbit hole I suggest reading these: [Link1](https://www.reddit.com/r/SQLServer/comments/2yfuoh/does_anyone_else_hate_ssis/) [Link2](http://ayende.com/wiki/I+Hate+SSIS.ashx?AspxAutoDetectCookieSupport=1)
Why not just use if else statement?
I feel like I'm in the same boat. The query usually comes by intuition and it's usually correct, but sometimes there are false starts for more complex queries. They work, but I'm never sure if there is some better way to do what I just did. Sometimes I have to do things which feel really clumsy, such as using lots of outer applies, or doing group by and then rejoining to the original table, or have three of four EXISTS/NOT EXISTS correlated subqueries in the where clause. 
I'm in a similar situation myself, and recently looked into it. There's loads of companies that do certifications, but it's $1k per course and the ones I saw were basically two-week intensives.
You probably don't need a certification. Most of them are worthless and don't truly evaluate whether you can do the job. Go to PluralSight and work through [these courses](http://www.pluralsight.com/search?searchTerm=70-461). It'll put you *way* ahead of most of the other database users in your company. And if you decide you still need a certification - 70-461 **is** the first tier on Microsoft's SQL Server certification. If you want a SQL Server certification, accept no substitutes.
I could easily be using LIMIT incorrectly as I just learned it but this is the error I receive when using a zero offset: Incorrect syntax near '0'. When just limiting for 1 I receive: Incorrect syntax near '0'. Thanks a lot for the reply though. I hadn't tried using offsets before so now I'm looking up how to use that properly.
I keep 14 days at a time, and as others have said, keeping the transaction log files separate is A-Okay. I keep them the way I do just to ease the management of the files (7 days, 7 files.). As the next week starts, I keep a rolling 14 day window by deleting the deltas from 14 days prior as I create a new diff for the current week (Keeping the full backup (diff base) until the last delta is removed. 
Wholeheartedly agree. I only mention it because I've seen situations in the past where SAN level compression was a business rule that couldn't be changed.
Also, If you're going to create a PS script to delete expired backups...for the love of god be careful! PS is ridiculously powerful and can do hella damage to a server with the smallest oversight. I say this from experience ;)
No problem. These are the troubleshooting steps I'd take on the original tables: * select * from depot; select * from journeys; select * from time; - you should see that time is just the time dimension and you don't need it for this as you can bind it in later off of date. This leaves us with depot and journeys * figure out how these two tables bind together on depot. there should be common depot values between the 2 tables in one field. * get a count of the journeys -&gt; select count(1) on journeys; * select count(1) from journeys inner join depot on journeys.[common field] = depot.[common field]; this should equal the number from the previous unless there is a null value in the field (if there is we have other issues ;-) ) * if we are all good, update your insert statement to be only the fields from these 2 tables + the time value that you want. then bind in the time table later. the group by queries should work for you then.
why are you using GROUP BY if you're not actually aggregating anything? i'm going to guess you want to put SUM() around your SubTotal, and remove SubTotal from your GROUP BY
That was when I was using it quickly to see what it would do with Splith suggested it. So, 1 would be the real number in there and the error states incorrect syntax near 1.
It's frustrating to me that this isn't common sense for developers. That said, I'm glad articles like this exist.
With the INNER JOIN it is going to go through every single record and return a result for everything that matches the join condition, THEN the WHERE condition is applied. So that where condition will have to look at all the dates where the account matches to find the ones that are the same. In the other scenario it runs the subquery coming back with a handful of dates and will exclude afsInner rows immediatly. So if the engine is smart enough it can easily exclude records that don't have viable dates, and then only join records that are needed. So depending on how many unique dates there are in 'prnt' and how many records will match those dates in afsInner, the first method will run faster. The only difference from logical perspective is that in the first example, you could have a prnt record with the right account number, but the wrong date, and have another prnt record with the correct date as to keep an afsInner record alive that doesn't have a corresponding Acct/Date combination. 
&gt; Dynamic SQL. If by "dynamic SQL" you mean queries with user input, then yes, it's the case. &gt; I don't believe MSSQL will make any changes to CHAR input types Not using CHAR input types, not now. 
When I said 'Dyanmic SQL' I meant that the SQL statement is built up using a string, versus users supplying variables. For example if you have a stored proc that takes two dates, and finds the records between them, injection isn't possible. If you take a (N)(VAR)CHAR type, and build up a new SQL statement using that text, injection is possible.
&gt; Ah the paper doesn't prove skill, but it gets you the job over the guy who knows everything and doesn't have the paper. Only if the people you're hiring don't know how to properly evaluate skill/knowledge/talent. The really good places to work will hire you based on proven skills for the job, not the ability to pass the test.
select top 500 o.Name as 'Object Name' , p.Name as 'Property Name' from my_object o left join my_obj_prop_relation r on o.ID = r.ID left join my_property p on r.ID = p.ID where p.Name in ( select p.Name from my_object o left join my_obj_prop_relation r on o.ID = r.ID left join my_property p on r.ID = p.ID where o.ID = 1 ) 
First, make sure your junction table has the right primary key, in this case (property_id, object_id). Otherwise you're just going to confuse the query optimizer. You also will need the right foreign key constraints, but that goes without saying. SELECT rel1.object_id ,rel2.object_id as related_object_id ,COUNT(1) AS shared_properties FROM my_obj_prop_relation rel1 INNER JOIN my_obj_prop_relation rel2 ON rel1.property_id = rel2.property_id AND rel1.object_id &lt;&gt; rel2.object_id --The object will always match itself WHERE rel1.object_id = 1 GROUP BY rel1.object_id ,rel2.object_id ORDER BY shared_properties DESC LIMIT 500 --Since you're using Postgres
Thanks! This seems to run faster than doing a sub-select, as far as I understand the EXPLAIN results in Postgresql. 
**Thanks** again for everyone's sound advice! I passed with a score of 866. &amp;nbsp; In hopes of helping others, here's what worked for me (take with a grain of salt): &amp;nbsp; **1.** Practice all of the example code/queries. Make your own using your real world data until you are familiar with not only the syntax, but when to use them/some of their constraints. &amp;nbsp; **2.** Don't take exams too often (I think I did), use them as a guide on what to study. Having said that I highly recommend the official practice test MeasureUp &amp; Transcender. &amp;nbsp; **3.** Videos, whatever the source are awesome, again, practice what they are showing you. &amp;nbsp; During the exam: &amp;nbsp; **1.** If you are stuck on a question, mark it for review and go back to it. &amp;nbsp; **2.** If you begin to panic, stop, close your eyes and take a deep breath until you are relaxed. The first 5 questions of my exam made me panic/I thought I was not going to pass and I lost my confidence. I marked certain questions, relaxed, then carried on. After answering other questions correctly, I regained my confidence and momentum. &amp;nbsp; **3.** After reviewing all the questions you had marked, review the rest of them. You may catch something the second time. I know your first answer is usually the correct one, but now you can walk through the scenario/look at each question more in depth.
Edx.org has very basic into to sql its free and u get an honor certificate unless you pay for the actual class. 
Cool!
in a testing environment, typing * is a hundred times faster than a list of 30-40 columns with 10+ letter names. the increase in the query time is minimal.
You need the sql for the tables?
Ideally, you should try to write the SQL statement first. When it doesn't work or you get stuck, you post what you have so far and what your desired results should look like. If you haven't got a query, then you may need to describe how you'd get your results if you had to do it by hand. Posting sample data is very useful and posting table schema is often useful. If possible, you should set it up in SQL Fiddle and link to that. Remember: We don't even know what your data means. Looking at what you've posted, I have no clue how those two tables are supposed to be related, since none of the obvious key fields don't exist in both tables. Is there a third table that's missing? How would I know? The only field that appears to be shared at all is Location, but that doesn't mean that that's how the tables should be joined because I don't know what "Location" means. 
Also 30 - 40 columns on one table!?!? Don't get me started on normalization
I don't think you have enough info? The only logical thing to join on looks like location, but that doesn't seem right.
The Create Table is part of the code I cannot edit. The only line that's editable is the *name varchar(50)* line. HOWEVER, I think I've figured it out..the first lesson is free but the second lesson (where this problem is presented) is charged. I was able to watch the instructional video for free but not take the "challenges" until I subscribed (pay). I like the coursework so I'll likely be subscribing.
It literally will not let me edit anything but the varchar line. The create table is the framework for the question asking me to modify the code to add in the constraints. I think the issue is with the subscription to the class.
That's the only thing that makes sense given what you're seeing. You would either have to drop the existing table and recreate it or alter the existing table. 
If you are working in mssql, what you have is correct for declaring the field as not null able in the create table command. You can do it like you have it, or as an alter table command after the table is created as other ppl have suggested. It is likely the course will not let you submit your answer until you pay.
Can you try this...? CREATE TABLE Actors ( name varchar(50) NOT NULL, country varchar(50), constraint pk_name primary key (name) );
I guess I'd always been ingrained against su queried but this explanation makes a lot of sense
Yes that makes perfect sense and this will be my first improvement I make when I rewrite the code. I guess because as this expands in will get progressively slower as it has to go through 100 case statements to get to the correct nested statememt potentially. I was seeing if there was a cool technique that joined on a case statement. So column A = 1 so then I join the needed Case statements that are applicable for 1. Does that make sense? Sorry i struggle explaining what I am thinking sometimes
I thought it some... I think it might be something to consider in the future for sure, but I need to step into the database realm first. I've had very basic exposure to web development, but nothing recent. I think the class was in C#. I was good in the class til we started working on using classes, for some reason the concept just didn't click for me at the time. I could conceptually understand that it's written code that gets referenced for use in other parts of the application, but I could not figure out how to call or use the class. Recently, I was trying to have a look in the application is still sometimes support... Part of the application is in ASP, other parts are in .NET, but the app is a mess of spaghetti. I can hardly discern what is calling what... But I typically only look to figure out if a page is calling a stored procedure or if the query and calculation is in the web code - if it is, I just get lost.
[removed]
Thanks for the suggestion! I don't have much exposure to other platforms outside SQL Server, would this be a big hindrance? I thought that most of the platforms follow the same basic language, and maybe have a few utilities and tools unique to the platform.
It wasn't me, it was the course..I wasn't signed up all the way and it prevented me from progressing in the lesson. Thanks for your help!
It wasn't me, it was the course..I wasn't signed up all the way and it prevented me from progressing in the lesson. Thanks for your help!
It wasn't me, it was the course..I wasn't signed up all the way and it prevented me from progressing in the lesson. Thanks for your help!
Database thinking is quite different from code thinking. It is difficult. That is what makes those with skills in both valuable. If you can climb the hurdle, you will be valuable, and I love my job doing both. It's more fun than doing one or the other exclusively. Consider the fact that you will understand how a database functions in the real world a lot better if you understand how it is actually used beyond itself. How it is used in real-life and how the programs using it will interact with it, and what demands will be placed upon it. If you want to take your database skills to the next level I'd recommend you to start coding.
Do you work as a contractor/consultant to utilize both skillsets? As far as my limited exposure has seen, I see most developers being front end or back end, but never really working on both simultaneously. We have had some issues that we have needed to troubleshoot where the developers would discuss where they felt the root of the problem was - but it would typically end up with "This needs to be fixed here..." and the particular developer would pick up the work. I think working with both is definitely something I would want to look in to, but I'd like to take it one step at a time and first learn how to make my database work more efficient before trying to think of how it would be used beyond that work.
Which DB Platform are you using (MySQL, Postgres, etc.)?
For the most part, as long as you use ANSI standard SQL, the major differences between platforms will be functions and how they deal with dates. There are gotchas specific to each, but you learn those when necessary. SQL Server (T-SQL) is fine. It is what I've used most in my career. But I've interviewed Oracle guys (PL/SQL) and the good ones do just fine on our SQL Assessment (that is in T-SQL). And again, if it is a junior level position, they shouldn't care that much. I know I don't. If I can find anyone with SQL experience only a few years out of college with a desire to increase their tech skills related to database/query development, that is a win. 
MS SQL
Anything in the log?
I know it's not always possible, but the best case statement is a helper table.
Luckily in Teradata this is integrated into the SYNTAX. it will only process the first CASE statement and then if FALSE goes onto the next without processing the resulting branch of that statement
I agree. We use helper/cross reference tables all the time to simplify complex logic. It can take some doing to get the table right, but the performance and logic is much better. 
Most likely - x is close to s and actually makes sense!
My attempt at number 6 in medium questions: http://sqlzoo.net/wiki/AdventureWorks_medium_questions any input again would be greatly appreciated. select * from ( select SalesOrderID, UnitPrice, count(OrderQty) as TotalOrder from SalesOrderDetail group by SalesOrderID ) as sub where TotalOrder = 1
Do you think it would be worth pursuing a certification? I realize certification does not necessarily mean I can do that job, but it can help with recognizing experience on paper. Otherwise, do you have any advice as far as what I should cater to on a resume?
By the 70-461 training kit and do the exercises and practice tests. 
Maybe [that's](http://stackoverflow.com/questions/999200/is-it-possible-to-execute-a-string-in-mysql) what you need...
Agreed again. It's too bad not everyone will have that option/luck, especially immediately entering a job market. I know it's about drive and getting your foot in the door where you personally want to work. 
Gotta make sure "auto-update statistics" is turned on for the database. While I see this turned on for the majority of databases, every now and then I run across a database that has it disabled.
If you really want to see how good your skills are, answer questions on stack overflow. They are real world problems in situations you might not encountered other wise. 
I will for sure look up into creating another MySQL instance to pull from the SQL Server. That seems like a decent solution actually as the data is always going to be Read-Only (update information for a web app) 
auto-update statistics is on.
If you can't fathom a situation where a data table has 30-40 columns --- well I guess you haven't done very many projects. In this case, it's an ETL project, the 30-40 column table is a transaction/ fact type table that is derived from a 3rd party application's flagship software worth $2 billion, aka quite mainstream. Why you would assume such a table is not normalized, I don't know. Some tables truly have many disparate dimensions. This table in particular is mostly comprised of IDs pointing to dimensional tables in any case. So it already is normalized -- and I'm dumping into our data warehouse. And to be fair, once you've worked with databases for a while, you understand the advantages of normalization and want to normalize everything. This is probably the best for the source/ lowest layer database. However, after a while, you begin to understand the occasions where denormalization has its advantages, in particular and especially concerning Business Intelligence applications. Not relevant especially here, though, the table was already normalized.
Shrink tasks, whether they're on your database or logs, are usually limited to situations where something has gone wrong and the files are unexpectedly much larger than they should be and larger than they will ever need to be. Say your database should never be more than 3 GB, and suddenly you find that it's 30 GB because data was not being purged correctly. Then you may want to consider a shrink back to 3-4 GB.
Group by on First-name and Last name and MIN of Year and Month?
In a separate note, I would only keep one column for date and get the year month number etc by calculated columns. Also may be master table for sales person for Last name and first and the transaction table to have UserID. 
Is your saved query safe? Is there any way for a malicious user to get a nasty query snuck into that table for you to execute unwittingly? It doesn't even have to be a `DROP TABLE` or exfiltrating data - it could be something as simple as a cartesian product which consumes every CPU cycle, every IOP, and every byte of RAM available.
the firstmonth is to say that this is this person's first month of selling
There should be index rebuilds at a minimum. The number of times I've seen maintenance jobs that have... * Full Backup * Rebuild Indexes * Blanket Update Statistics ...in order is staggering; giant waste of time/resources. [Ola Hallengren Scripts](https://ola.hallengren.com/) should probably be used where possible. 
The problem is you don't have a separate users table, you have it mixed with your sales table. A rolling logic that checks month to month is going to produce ridiculously long query times for no reason and even then it might not produce good results. I don't know if you have control over the software that uses this table but I would consider shifting it so that you have; A Users table UniqueEmployeeID ,FirstName ,LastName ,SalesRegion ,DateHired ,Any other field that might be necessary. A Sales Table UniqueTransactionID (in case you want a separate table that has a recored of every item in the sale) ,SalesDate ,EmployeeID ,SalesAmount ,other fields like taxes, fees, etc. That way you can clearly see the month your employee started to sell from when he was added to the DB. You can even query the first sale they did by joining the two tables together. Then shift the software that uses those tables to adjust tot he new structure. should only be a couple of changes in the Select, Update and Insert queries.
Hm, i'll go try on another sub I guess then, do you think /r/sqlserver would be one since it's related to Microsoft Sql report server? 
That'd be a better shot than this sub for sure, since they deal with the whole MSQL stack
Sounds good, thanks for the advice, sorry it was an unrelated question. 
From Orders a This line creates an alias for the table Orders called a. You can use a instead of Orders at any other point in the query, for example in the select; SELECT a.OrderNumber Or WHERE; WHERE a.OrderNumber = 'A0123' What is important here is that it is used instead of, not alongside, the table name. It's a much easier way of working with large table names. If and when you start using more complex queries with table names, it helps to use aliases that relate to the the table name as it makes the SQL easier to read, ie Orders Ord, OrderLines OrdL, Customers Cus etc...
Does not work unfortunately, it won't bring up the specific data I want to pull up. I cant create a shortcut for users for them to just double click and open the relevant information. That /reports/ just has the main report on it, it won't allow custom data attached to the end of the URL. 
That fixed it, Thanks!
For SQL Server, you could have a row number: Select Row_number() over (partition by firstname+lastname order by year asc, monthnum asc) as firstmonth The 1's will be your first month. This will not perform well.
If you can get how the url is configured, that would be helpful so I could try, whenever. I tried to do the same command, for instance: The problematic page: /ReportServer/Pages/ReportViewer.aspx?/Production/DispatchList&amp;rs:Command=Render&amp;ResourceID=1002-EMG&amp;ResourceID=0801-EMG I tried /Reports/Pages/Report.aspx?/Production/DispatchList&amp;rs:Command=Render&amp;ResourceID=1002-EMG&amp;ResourceID=0801-EMG but none of them will bring up the data under /reports/ When I select the data, I thought it would show the URL with the data extension, but it does not.. Instead, the working URL is /Reports/Pages/Report.aspx?ItemPath=%2fProduction%2fInboundToResource I tried to add &amp;rs:command or &amp;resourceID but neither worked. 
 *** SELECT firstname, lastname, middleinit, COUNT(*) AS countr FROM yourTable GROUP BY firstname, lastname, middleinit ORDER BY countr DESC *** 
Good work! I'll post how my links work when I'm at work if for nothing other than reference.
Have you tried using union all and not just union? I very well could be wrong but this may be a solution. DB2 isn't that great with column aliases I've had similar issues trying to use the alias in group by's that I could do in mysql Edited: this works on an iSeries which is different than DB2 z/OS or luw. Just curious what version of DB2 are you running? 
The first query in the union governs the column names. You don't use square brackets.
I agree with this distinction, my 'Dynamic SQL' statement was a little general in thinking of purely Server side code, and not allowing for Ad Hoc queries. My sentiment remains the same though, both Dynamic SQL and Ad Hoc queries are vulnerable points for SQL injection.
Thumbs up for this great point. When calling an SQL statment, if you want to include variables don't include them directly into the string... "SELECT * FROM " + tableName Instead use a parameter and supply the value "SELECT * FROM @tableName"
I tried each query by itself without a union, but still got no love. 
I'm just trying to execute SQL commands that sanitize data. I've seen this done so many different ways now, and this looks like one too. They all tend to work sometimes and not work other times. I'm frustratingly foggy on their usages and tradeoffs. Ways I think I'm seeing this done so far are: Way 1: def self.create_table sql = &lt;&lt;-SQL CREATE TABLE IF NOT EXISTS table ( id INTEGER PRIMARY KEY, name TEXT, other_column TEXT ) SQL DB.execute(sql) end Way 2: DB.execute("INSERT INTO table (column_1, column_2, column_3) VALUES (?,?,?)", thing_1, thing_2, thing_3) Way 3: DB.prepare("SELECT column_1, column_2 FROM table WHERE location = ?") DB.execute(thing_1, thing_2) Way 4: sql = "DROP TABLE IF EXISTS table" DB.execute(sql) I think I've seen maybe one or two more. Not sure about OS scripting. Every time I try to do this, the way I used the last time doesn't work for some reason and I have to find another. I'm dying for the definitive answer on why/when these are used as sanitation techniques. Combining reddit, stack overflow, and various docs has been pretty brutal in terms of arriving at helpful conclusions. 
I can,tomorrow, once I'm at work. 
Do you have to use Microsoft SQL Server, or are you allowed to use any RDBMS and are trying to use PostgreSQL because you're on a Mac? Your question is very unclear.
1) I was kind of hinting at that, but I guess I should have said it explicitly. 2) Haven't done it myself, but have had to "advise" a user or two when they had questions about their query trying to spit back 400M records over 5 minutes before giving up on it.
MS SQL Server does not run on Macs. You'll need a VM with Windows. Postgres works fine, but that is not the same thing. 
Like this guy says, use Windows and follow the instructions you have. Later, when you understand what you're doing more you can deviate from whatever instructions you have, but for now it sounds like you'd be wasting a lot of time trying to improvise. edit: On the other hand, if you have a *lot* of time, feel free. You might learn a hell of a lot.
Sql server runs only on Windows. Postgres runs on both. Also I have no idea what you're asking 
Well, there is a bit of a distinction. If you are 100% "back-end", then you are probably actually creating and developing (think data architect) the schemas, tables, keys, etc. And if the only purpose of this is to be a data warehouse, then things stop there. But, if you are building the database to support an application, there is development that takes place to support the app. Typically stored procedures or triggers that the app needs in order to interact with the database. For example, when the user clicks this save button on the web page, I need a record inserted into the customer table with this information they typed in. Someone has to create the stored procedure that takes the inputs as parameters and makes them part of the INSERT into the table. So, you only have to worry about a front-end if you are supporting applications. If not, you don't have to worry. In my previous role, we built web apps. In my current role, we just do BI stuff with the data, so all we have to worry about is creating datasets for Tableau, SSRS, etc. 
Simplest way to get PostgreSQL running on a Mac, [Postgres.app](http://postgresapp.com/). As others have said, you'll need a virtual machine to run SQL Server.
 select ACTN_TYPE_ID as Type_ID , ACTN_TYPE_DESC as PLAN_DESC from OpenQuery(dmfrc191,' select ACTN_TYPE_ID as TYPE_ID , ACTN_TYPE_DESC From BIYOM87.T_ACTN_TYPE_LOOKUP where ACTN_Type_DESC in (''Appeal Sent to HPU'', -- criteria ''Appeal Process Complete'', ''Appeal Expired'' ) ') ERROR: Invalid column name 'ACTN_TYPE_ID'. if I use ACTN_TYPE_ID = TYPE_ID then I get: OLE DB provider "IBMDADB2.DB2COPY1" for linked server "dmfrc1" returned message "[DB2/AIX64] SQL0104N An unexpected token "=" was found following "select ACTN_TYPE_ID". Expected tokens may include: "&lt;space&gt;". SQLSTATE=42601 ". Msg 7350, Level 16, State 2, Line 1 Cannot get the column information from OLE DB provider "IBMDADB2.DB2COPY1" for linked server 
Try something like this: http://stackoverflow.com/questions/5650830/returning-month-name-in-sql-server-query A little Google will go a long way for things like this.
You are correct in doing select ACTN_TYPE_ID as Type_ID, and not ACTN_TYPE_ID = TYPE_ID that will not work. I mean this may sound dumb but have you tried doing just select * and see if you can return any of the tables, If you do select * see what the column name comes back as. I would try these two queries select * from OpenQuery(dmfrc191,' select ACTN_TYPE_ID as TYPE_ID , ACTN_TYPE_DESC From BIYOM87.T_ACTN_TYPE_LOOKUP where ACTN_Type_DESC in (''Appeal Sent to HPU'', -- criteria ''Appeal Process Complete'', ''Appeal Expired'' ) select * from OpenQuery(dmfrc191,' select * From BIYOM87.T_ACTN_TYPE_LOOKU edit: Also DB2 does not use double quotes if you can get the second query to run and not the first one I posted replace the double quotes with single. It's my understanding that part is ran by the DB2 query engine and not MS so it will expect the syntax to be DB2 specific. 
I swear I tried that yesterday, and got the same result. But, as I tried it today so I could post the exact error, it worked. Thanks for chiming in, I really appreciate it. 
Ah, so it does use /reportserver/ as the main URL for the parameters, not /reports/ I thank you for replying back and showing how it's setup. 
I usually start all my searches with 'T-SQL...' Tends to cut away articles about SQL concepts and gets straight to code examples. Although sometimes the concepts are important too.
Good point, sometimes I'll do "SQL 2008" for that same reason
LEFT JOIN and ISNULL() should get you where you want to be.
I would combine /u/mikeyd85's and /u/krankie's ideas. I think it is important to include the owner in case it changes. I also think it is important to be able describe the list without it having an owner. 
Thank you. I owe you a beer. This is stupidly simple, as I suspected. 
Where's the question? You've (more or less) described how a `JOIN` between those two tables works. The result set is the combination of the two tables' data, with the columns that you selected. The only thing wrong with your syntax is the comma in the `JOIN` - it should be `FROM table_1 JOIN table_2`. You *don't* want to "literally create" a table here, because that would result in redundant data and breaking normalization. Unless you were looking for a temp table for further processing. That said, you *should* (but it's not a hard requirement, and isn't possible/practical/available in all cases) have a foreign key relationship between these tables if their data is related in some way.
You aren't linking your outer query to the exists, it should look something like: *** SELECT * FROM tblGstPstByProvTemp AS Temp WHERE EXISTS ( SELECT * FROM tblGstPstByProv as TaxT WHERE Temp.Supplier = TaxT.Supplier AND Temp.SupplierProduct = TaxT.SupplierProduct AND Temp.EventCode = TaxT.EventCode ) ***
I knew it was something simple.... stupid brain getting frazzled after a couple thousand lines of code. -_- Thanks for the assist. This sub really needs a solved CSS Tag.
I've been doing this for 10 years and I rarely use plain vanilla JOIN. I mostly use LEFT JOIN (also called LEFT OUTER JOIN) for the simple reason that's easy to visualize. Imagine a standing side view of human spinal column that would be facing your right (I'm an epidemiologist so bear with me). Now imagine someone attaching the ribs to the spine one by one from a box of ribs. The spine is your main table, the one where you want to see all the rows. The vertebrae represent every row in the spine table. The ribs are rows from the second table that are being joined to the first. Pretend there are more ribs in the box waiting to be joined but there is not a spot on the spine. These are the rows in the ribs table that get left off the skeleton. The spine table goes on the left side of the equals sign "=" and the ribs table goes on the right side. SELECT S.*, R.* FROM SPINE S LEFT JOIN RIBS R ON S.ID = R.ID You will get all the values in columns and rows in SPINE and only the columns and rows from RIBS when the IDs match. In the rows where there is is no match you will still get all the values in the columns from SPINE but only NULL values in the RIBS columns. I like the spine/rib metaphor because in human anatomy the spine is always longer that the thorax. https://en.wikipedia.org/wiki/Join_(SQL)
This is called a [correlated subquery](https://en.m.wikipedia.org/wiki/Correlated_subquery) btw Also, your join code could be optimized. Im on mobile so cant do it, but you basically dont need all of the where conditions as they are part of the join itself.
Don't really need the join anymore thought.
Glad to help.
That is possibly the most colorful but complex analogy to an SQL select that I have ever read. Its brilliant!! 
&gt; You need to join on two related columns for the join to work. Can you explain that a bit more in depth? As I said I have just started MySQL. So what is common knowledge to you is totally new and alien to me. I'm a designer *trying* to cross over into the developer world if that helps.
Try just removing the word as: SELECT columnname alias, columnname alias, .... 
I believe this is quite possible. You create a stored procedure to SELECT the query into a string variable. Then you use BINDING to dynamically EXECUTE the string against your database (still within the procedure). From a security perspective this seems like a not so good idea.
It's just your IDE. look for something like "output to grid" instead of "output to text" in preferences or in the icons above the results area.
I'm not really sure what you're trying to say. Can you correct my syntax in its entirety? Like I said before I am *just* starting to get into MySQL so your simple suggestion causes me to ask more questions without seeing the whole thing laid out.
I won't tell them you asked Reddit! As /u/alinroc mentioned, going for a certification might not be the best path for you. That'd be learning a LOT of SQL. I code to SQL every day and use maybe 5% what you'd learn in a certification course. That said, if the company will pay for it, and it interests you, go for a course - it won't hurt anything!
Full DBA would be most useful for the company.. though I'm not sure that they need app development.. maybe Powershell somewhere down the line, but i'm not sure how that ties into all this
If you were unsure, try finding a 70-461 and 70-462 training course(s) and have them pay for the certification exams. The books/resources are very reasonably priced and the knowledge is widely applicable. 
If you're looking for pure DBA training, check out [SQLSkills' Immersion Events](https://www.sqlskills.com/sql-server-training/) and (yes, I know they don't want you to go this route) [Brent Ozar's training packages](https://learnfrom.brentozar.com/) Brent Ozar Unlimited also has a [free 6-month training plan by email](http://www.brentozar.com/archive/2013/07/announcing-our-free-accidental-dba-6-month-training-plan/). There's literally no downside to you signing up for this thing. Go do it. Now. Why haven't you done it yet? C'mon! Find your local [PASS](http://sqlpass.org/) chapter, and start attending meetings. Also, find [SQL Saturdays](http://sqlsaturday.com/) near you &amp; go to those. If you can swing it, DC is coming up in a couple weeks - hop on the train!
In my experience it has more to do with employer's firewalls preventing access to the sites because of their address.
I was more responding to their statement than the idea that some employers filter the word "porn".
I am just commenting on what I perceive as a massive overreaction to a word. Also, the idea that "pornography" in an of itself is only "quite vulgar" if one views nudity and sexual content as "vulgar" is rather antiquated. You are welcome to your opinions of course, I just find them rather outdated and out of touch with our society today. Consumption of sexual content should not be viewed with such denigration, IMO - such views contribute to the fetishization of sexuality, which IMO, results in a net negative for our younger generations. Young adults should be comfortable with their sexuality, understand what it is all about, and have healthy avenues for expressing their energy. I personally cannot grasp the viewpoint that would rather supress and contain such information. Nonetheless, this has gone far off topic. tl;dr: I just feel like your visceral reaction was quite over the top! 
I don't give a shit about the "porn" moniker being tasteless, it's just not funny or descriptive. It's a tacky joke that reddit has mashed into the ground like so many other jokes. I like the concept of the sub though.
Here a good tutorial with autocorrection exercises. Free and for beginners mainly. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
You can measure your level by doing exercises on that web site. And you can compare the right answer with the one you submitted. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Seconded. SQL Server Express is unquestionably the superior tool of the two. Access is definitely easier for someone with no experience with databases, but SQL Server is the way to go here.
This is something that should be on /r/iamverysmart
Learning Tree is what our SQL developers use. They offer online and in person training. [http://www.learningtree.com/training-directory/SQL-Server-Training-8.htm](http://www.learningtree.com/training-directory/SQL-Server-Training-8.htm)
What sort of feedback did they give you about learningTree? pros and cons?
Certain commands are expecting your parameters/syntax in a specific way. For example, the Insert Into command, is expecting: Insert Into [Table_Name] (list_of_columns) If you try to use: INSERT INTO table (table.name) then SQL is looking for columns named table.name and it won't find that. 
I get the feeling i'll pick up quick, i've managed to teach myself a decent amount of SQL, though i don't know everything by heart and have to google a lot sometimes to jog my memory, i can read through code and figure things out, but I obviously want a level of competency that I can put on my resume..
When is that syntax used? I know I was using that before and it was working. I thought with SELECT and JOIN. But it doesn't seem to work with SELECT either. Also, what is it called?
I think that is what I was trying to do. Why has qualifying broke the code in this case though?
Other posts are right. SQL all the way. What you really need, though, is SQL Server Management Studio (or some other sql mgmt tool). It's a separate and distinct thing from sql express. You need both to play, but management studio is where you will do the learning. Your database (SQL Server Express) runs like a headless computer with no monitor (management studio) and can be accessed by many things including management studio, command line, or code running on your server like PHP/VB/Java/C/Py/Node/about a thousand other things. Management studio is how you will access it at first to learn and build. Access attracts bad databases in part because the setup is simpler. For example it's bundled with the 'management tools' by default.
/r/truecynicporn
Seriously? It might be immature to use the word 'porn' in this manner but vulgar? You can keep puritanical views and put them in the closet with your sexuality. 
What do you do professionally?
. 
I really don't have experience with DB2, but it looks like it could be because of your locale. I found some info [on here](https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.sql.ref.doc/doc/r0007110.html). In table 2. Format elements for the VARCHAR_FORMAT function there is following information Format element| Description ---|--- 9| Each 9 represents a significant digit. Leading zeros in a number are displayed as blanks. 
We sent most of our personnel to interfaceTT for sql training. Unless you are truley an absolute novice, get intermediate courses.
awesome, ill look into what TDS is and get cracking. i appreciate the guidance!
you need api drivers correctly installed and configured usually
Are all of your date between columns set as date/time data types? Which column is the value 'EE3142E2-DF2B-43A0-87D1-6FC3C8943536' in? 
What data type is the column?
varchar value 'EE3142E2-DF2B-43A0-87D1-6FC3C8943536' tells me your working with a guid. Can you cast the varchar as an int? Occasionally Ill see messed up records that cause headaches. Is it possible you have accidentally set one of the user ids to a guid in the past?
Change the AND to WHERE for a start, so it reads Where nc.sent_when between @start_dt And @end_dt 
That's from nc.sender_id. The column contains 2-4 digit integers, and also GUIDs. This is one of those GUIDs. The nc.sent_when column is datetime. 
OK, so when you add that filter to the JOIN clause, it isn't going to change the results you see. &amp;nbsp; SELECT * FROM dbo.table1 t1 INNER JOIN dbo.table2 t2 ON t1.ID = t2.ID AND t1.Date between @Date_Start and @Date_End &amp;nbsp; Will return the same as: SELECT * FROM dbo.table1 t1 INNER JOIN dbo.table2 t2 ON t1.ID = t2.ID WHERE t1.Date between @Date_Start and @Date_End &amp;nbsp; If you can, always keep your WHERE clauses together, it makes for much better reading. There are times when putting additional filters in to the join (ie, joining to a reference table multiple times in one query), but I'd keep that to a minimum if you can.
Maybe adding a where clause with LEN() could filter out the Guids?
Varchar I believe 
Or WHERE Column NOT LIKE '%[^0-9]%'
These are great suggestions. I'll check back in when I've had a chance to try one or both.
I had tried integer but not float. This seems to work - thanks!
The third party program you speak of, where does that run? On the same website? What is it coded in? 
If you need to construct a hierarchy the ideal solution is recursive SQL, which isn't supported by MySQL. If /u/aburger's solution solves your problem then that doesn't matter.
INSERT IGNORE INTO ta(col1, col2, ....) VALUES(a, b, ...) you need to define a key in the table ta like this UNIQUE KEY unique_row (col1, col2)
Use TO_CHAR rather than cast, then you can specify precise formatting.
I've come to the conclusion that to use triggers inside the stored procedures may be the best thing for now.
This is definitely possible, and is something we use at my company when jobs fall and need attention. SQL Server Agent jobs have Alert settings to send emails and such. I think you could create a job step to determine the success of the procedure, checking for an empty table. The rowcount check might be more difficult - maybe you could create a temp table to store the pre-sproc rowcount, compare that came against the post-sproc one, and determine success or failure accordingly. You can change the error message by specifying a custom one use RAISERROR. This should display in both the email and the job history. I'm relying on my phone so I don't have access to my with environment, but Google SQL Agent alerts and you should find a configuration guide. Good luck!
You say there's three columns with the data, then you give a numbered list with many values; do your 1 thru 3 represent the data in the columns for a single row? I'll assume that. I think you'd need to separate them out into their own table/list to get a distinct list of items from each column. Jeff Moden did a post on this and it's the most efficient way I've found: http://www.sqlservercentral.com/articles/Tally+Table/72993/ Edit: Crap, you said MySQL... Er, anyone else have a way to do it there?
Yep, I know. I'd love to solve it in php or python, but that will only add to the procedure since I get new data all the time. The data is indeed unclean, and I want to learn how to solve it in the query directly if it's even possible :) Thanks for the help!
one final question, how many total (max) countries will there be across all three columns?
Then you'll want to solution I linked. Convert C001, C002 and C003 to tables and UNION them together, then add the ordering. Then use a null or comma node XML to put them into a single string again. Like I said, there is no easy way to handle this since it's completely dynamic. 
I wish I could give you an answer to that, but since those fields are created and managed by humans, I'd say as many as there are in the world. 