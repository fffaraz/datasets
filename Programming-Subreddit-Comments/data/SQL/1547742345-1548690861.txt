thanks for that, hearing him call out the behavior as goofy makes me feel a little better about missing it
I wrote this on another post about [Certification Camps](https://www.certificationcamps.com/) in Florida, just copying and pasting here for you: I did my MCSA with them, it was a great experience, learned a lot in the class, they take you out to lunch every day with the whole class so you end up making some friends/connections. That said as someone else mentioned, the test prep is literally a brain dump, they give you the exact(ish) questions that are going to be on the test and you memorize them. This worked well for me because I already knew a lot of the material as a DBA but if you're just getting into this stuff it's not the best way to learn. Also if you're going to do this, make sure you do the recommended amount of studying, you're not going to have any sort of "vacation time" while in Florida for 10 days, the people who goofed off and went out every night instead of studying failed all of the tests. This is a 6 month course condensed into 10 days, it's intense.
I haven't heard of a "boot camp" per se, but check out SQLSkills' [Immersion Events](https://www.sqlskills.com/sql-server-training/). High-intensity, very focused, excellent instructors. Brent Ozar has excellent live classes but they're not in-person, they're live (or recorded) &amp; remote. https://www.brentozar.com/training/
No. I personally like to capitalize certain things to make my queries easier to read. e.g. SELECT field1, field2 FROM table1 WHERE a=b AND x=y
[1KeyData](https://www.1keydata.com/sql/sql.html) 
Thanks for the response, just looked into them as i saw your notification come across. I usually work pretty good with the dumping of info as described. Fresh out of college, with about 6 months of experience, I've done or been apart of some day to day DBA stuff, but would by no means declare myself as such. They offer the [MCSE Data Mgmt &amp; Analytics (MCSA SQL 2016 Admin + MCSA SQL 2016 Developer) - 11 Days](https://www.certificationcamps.com/bootcamps/mcse-data-mgmt-analytics-mcsa-sql-2016-admin-mcsa-sql-2016-developer/) which is very close to the outline of what my boss suggested. I'm into having the certification but it really is not a necessity in my current role. In your opinion is the teaching/lecture side good, or was it just a dump with the hope that you remember questions. Im down to study, but i like the hands on labs, and thats what my boss will like to hear from me. 
So I implemented the transaction, and also implemented some logic in my assignmentService to retry if I get the error "could not serialize access due to concurrent update". However when I'm stress testing my system (e.g. sending 100 requests at a time) the service has trouble giving enough assignments without setting the retries number to be very high (like 20-30). However, if I use any isolation level higher the SERIALIZABLE, my tests fail due to users getting the same assignment. Is this isolation level able to handle this many requests?
Further I want to present result in Python visualization so basically I want way to find whether new entry in Oracle table satisfies conditions mentioned in Python program. Thanks
Double quotes are defined (required) by the SQL standard to make identifiers case-sensitive. 
Sorry, duh, I thought you meant double single's such as `''` and not `"`. I've never used double quotes for anything before
Triggers work in context/as part of original transaction that inserted record. Program that is inserting record will be unecesary slowed down. And also all readers of the table.
Holy shit and these things are cheap either. That's a lot to blow for a course only to party every night bomb the tests.
Dude, that's exactly what I did on our morning meeting today. I'm serious. Great minds think alike! I busted out the RPO/RTO worksheet right from the First Responder Kit (I had only prepared notes) with my team and I was like "Ok if the accounting team needs zero minutes of recovery time for this data then I'm going to need about $250k-$1m for a budget." My VP laughed out loud and 10 minutes later he sent an email saying we weren't going to be keeping the app server online. 
I guess your results are subject to "start\_date", right? It's more complicated that your result subject to both "start\_date" and "end\_date"..
&gt;thus are factually less memory intensive That's utter nonsense
 it's just for that day only 
I use the One True Keyboard™ which thankfully has no caps lock and puts control in its rightful place. I type app SQL in lowercase and snake case table names.
I just had an interview at a merketing agency where they have a php team and a coldfusion team. First I ever heard of it but yeah its a middleware simikar to PHP owned by Adobe. 
Also I was wondering why if you remove DISTINCT from id I get an error. Aren't the id all different from the first place?
&gt; if you remove DISTINCT from id I get an error. unpossible &gt; Aren't the id all different from the first place? what if a single client has more than one service? 
your HAVING should be WHERE
I would recommend Python for data analysis. Check out Pandas/Numpy/Matplotlib for your analysis needs. Also, look at Anaconda and Jupyter notebook to set up your coding environment.
I’m currently re writing and COMMENTING Some of my code because I realized if this goes into production it will not go well for any sort of customization. 
Yes. You read my mind. Definitely all in the pipeline. Thank you. 
totally overlooked that, again thanks for your help
Do you really need this to run instantly after an insert? What if you suddenly get 10000 inserts, and need to spawn 10000 python environments concurrently? Perhaps consider just tagging the records with a status field, and then have a job where python can run and update them in batches My advice would be to recode your python in SQL if you can, so you have a native trigger. You are likely heading into horrible performance problems otherwise
&gt; Am I missing any obvious, baked in solutions? Yes. You seem to be crawling over broken glass just to avoid using joins. If you really think it may be a performance problem, then benchmark it. Don't just go by gut reaction or what sounds better, because often it's misguided. &gt; I will go to pretty great lengths to minimize non-numeric data in my databases, even when it can make my life a little harder in the short term. I mean, its good to have efficiency in mind when designing your database, but don't prematurely optimise. You will often end up with a horribly complex codebase which you cant maintain, and possibly for no real benefit. The worst consequence is that you never get to ship your product, in which case none of the optimisations you've spent so much time on even matters 
[removed]
Obviously joins will be the most computationally efficient, straightforward solution, and sometimes the 'best' queries are long and ugly. I just want to minimize the amount of repetitive, generic junk in my queries. If I have to join my data tables with my lookup tables 99% of the time I select from one of the data tables, that join becomes something of a liability. Those join operations (or rather, the architecture that requires those joins) increase the complexity and quantity of code that I have to write to accomplish simple tasks. It's not really a big deal. I'm happy enough writing out these join statements, but if there's a way to make them happen automatically it would reduce the window for typos to return bad / unexpected results. This is a side project, so I see it as an opportunity to indulge my OCD tendencies and experiment with interesting, if not totally sensible, solutions.
If you are joining the same lookup tables every single time, you could just create a view which does all the joins for you. Then you just can just select from the view, instead of repeating the join criteria every time 
&gt;Our CRM stores user staff IDs but they're mostly the old IDs. Some of our stored procedures check staff tables for new IDs, which has been fun. &gt; &gt;I've been thinking I should put some sort of cut over layer somewhere that combines data for people using both old and new IDs, potentially adding columns to tables with new IDs etc. Anyone have suggestions? If there's no risk of overlap between the two sets of IDs (new and legacy) then combine both sets of IDs as a single key, with the expectation that some individuals may be represented twice, but with different IDs.
If it’s really a performance problem, then this is what caching is for. Store a dictionary in your app with the contents of the table if it’s small. If not, memoize the frequently used keys. But I think you are caught in the premature optimization trap and have psyched yourself out of the simple solution, which is to let the database do its job and return results from a real query.
Given an ID, do you have a way of identifying whether it's an old one or a new one? Do all staff have a new ID? Maybe you could create a view which uses the new id, and then query that instead of the base table. This could is also a good way to review your codebase to ensure no queries are hitting the underlying table directly (ie, looking up staff using old id) 
What the fuck!!!!!!!!
&gt;I know I could define a function which takes a table name, a foreign key, and a field name as parameters and returns the value of the specified column in that table for that key. Over large datasets that's actually going to reduce performance. You'll quite likely miss out on the full benefit of indexes.
Great info guys! Thank you. 
It really depends on what you are trying to accomplish. What do you mean by "dealing"?
I think you are probably overly thinking this. The query optimiser on pretty much all decent databases deal with this pretty well, and if you have added an index or histogram to the relevant columns off effected databases then you should be normally fine. However, it does depend on your application. If you are running reports and analysing vast quantities of data for particular time periods, then you might want to consider setting up a dimensional data structure. This has a driving table with dimension tables on the driving table. Unlike an OLTP database, something using dimensional databases doesn’t care so much about redundant data, and can add such fiends as precalculated results (e.g. total sales for product) into the driving table. In general you load these tables in batches. In fact, this sort of database works best with numeric data, so you might want to consider it. Alternatively, there are such things as materialised updatable views. These are views that contain data - once the underlying data is updated, then it updates the data in the view. This is great for fields that don’t change that often, or the join complexity is such that an optimiser finds it hard to choose the best execution path. You want to be careful in a write heavy database as it could degrade insert/update/delete performance though. It’s ultimately a trade off. The best tip is to know you application data requirements well, keep an eye on performance issues as they appear, consider data distributions and changes to these distributions and understand usage patterns. You also n ed to really understand the strengths of the database you are using also - so do t get too hung up on join performance as normally it won’t be your biggest issue.
Interestingly, what the function is doing is virtually making a roll your own join. Given that there are many ways databases optimise for join performance, I agree it could actually hurt performance more than it improves it.
&gt; If I have to join my data tables with my lookup tables 99% of the time I select from one of the data tables, that join becomes something of a liability. Does your database support creating views? If so, you can create a single view that has the "fully constructed" form of your data, including your joins, then use that view as a source in subsequent queries. Your "bunch of lookup tables" is essentially a way of normalizing your data (lookup "2nd and 3rd normal forms"), and using a view takes that normalized structure and gives you back a more rectangular data set that can be used for other purposes without explicitly putting the joins and lookups in every query.
SELECT [name] FROM sys.Tables 
In MySQL you'll have to select database first using `use databasename;`, then use `show tables;`, which gives you the output you want. However, this command might be different in other db's.
DRY principle really doesn't apply to SQL at all. It's the nature of the language.
Followup question: id=trip id, and member= 0(non-member), (1) member id|start_Date |end_date |member :--|:--:|:--:|--: 1 |2017-01-01 00:00:00 |2017-01-01 00:20:00 |1 2 |2016-01-20 00:00:00 |2016-01-20 00:17:15 |0 The question asks to find the fraction of the trips done by members for the year 2017 by month. I have the results for both the queries i need but the only solution I could figure out was to create views for each query and then divide them to get the answer. **SELECT COUNT**(*) AS number_of_trips, MONTHNAME (start_date) AS 'Month' **FROM** trips **YEAR**(start_date) = '2017' **AND** is_member= '1' **GROUP BY** MONTHNAME(start_date) **ORDER BY FIELD**(MONTH, 'January','February','March','April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December')) which gives the number of trips by month for members, and then I have this query: **SELECT COUNT**(*) AS Number_of_trips, MONTHNAME(start_date) AS 'Month' **FROM** trips **WHERE** YEAR(start_date) = '2017' **GROUP BY** MONTHNAME(start_date) **ORDER BY FIELD**(MONTH, 'January','February','March','April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December') would appreciate any help/hints 
your FIELD function is specific to MySQL -- it's wiser, in my opinion, not to use proprietary and non-portable extensions to SQL SELECT MONTH(start_date) AS month_number , MONTHNAME(start_date) AS month_name , COUNT(CASE WHEN is_member= 1 THEN 'ok' ELSE NULL END) / COUNT(*) AS fraction FROM trips WHERE YEAR(start_date) = 2017 GROUP BY MONTH(start_date) , MONTHNAME(start_date) ORDER BY themonth 
here ya go, only one column, the table name, as requested works in most databases SELECT table_name FROM information_schema.tables WHERE table_schema = 'databasename'
that looks much better than mine, do you mind explaining the case function 'ok' I tried to google it but couldnt find anything regarding it
the THEN portion of a CASE expression provides a value for the expression whenever the WHEN condition is met so in this case, the value provided is 'ok', which is not null note in particular that the ELSE value, when is_member is not equal to 1, is NULL not come out to the enclosing COUNT function -- as you must be aware, aggregate functions disregard NULLs, so the COUNT function returns the count of all the 'ok' values, and only the 'ok' values 'ok' could really have been any non-null value
I believe both Red Gate's SQL Prompt and ApexSQL's comparable product will do that. I uninstalled Apex because it made SSMS crashy. If you have common queries you need access to frequently, think about putting them in an SSMS project and always keeping that project open. Then they'll be readily available in the Solution Explorer pane.
Thank you. I'm still new to SQL and didn't even know about projects. I have Redgate, but I cant find the option for that in SQL prompt. You wouldn't happen to know what its labeled as, would you?
If you have a set of queries you run all the time why not open SQL by just clicking on the .SQL files, or just opening SQL and dragging them over to the window? An easier way might be to create a stored procedure that runs them for you? Or an SSRS report? Are you running the queries just to get the data and paste it to Excel or email to someone? Or are they doing something on the database and you aren't interested in copying data to another place?
They are to check and modify run dates on a server for my own test use. 
I use the Snippets feature of Red Gate's SQL Prompt for something similar to the OP question.
Not really what I'm asking... Are you running a query and that's it, you're done. Or are you running it and then taking data from the results window and doing something with that data?
Run and done
https://www.red-gate.com/hub/product-learning/sql-prompt/quick-sql-prompt-tip-tab-history
&gt; It really depends on what you are trying to accomplish. No disruption in reporting. &gt; What do you mean by "dealing"? Managing the change.
Just make it a stored procedure and run that such as `exec sprocname`, or set it to auto-run in the scheduler every x minutes. Voila.
Use not exists, for example. Or do the row_number by priority.
 SELECT itemid, location, attribute1, attribute2 FROM items AS a WHERE location = 'A' UNION ALL SELECT itemid, location, attribute1, attribute2 FROM items AS b WHERE location = 'B' AND NOT EXISTS ( SELECT itemid FROM items WHERE itemid = b.itemid AND location = 'A' ) UNION ALL SELECT itemid, location, attribute1, attribute2 FROM items AS c WHERE location = 'C' AND NOT EXISTS ( SELECT itemid FROM items WHERE itemid = c.itemid AND location IN ( 'A' , 'B' ) ) 
I would maybe do it like : Group by item id and letter, and do an over on this with a top 1 So you’ll get the ‘topest’ (yes ffs) where it exists else the next for each id 
Kind of tough to visualize without the data set.
the possibilities are endless because all it says it your creating the database. So the tables can be as simple or as complex as they want.
Off the top of my head, it appears that you really only need the neighborhood and some student info. Am I crazy for simplifying it? started at: 7:34 Table: Students (studentid,studentincome,neighborhoodid) Table: neighborhood (neighborhoodid, neighborhoodname) tables done at 7:37 &amp;#x200B; To pull the information you just join the tables, SUM the income, using HAVING and COUNT. &amp;#x200B;
But WHERE can not be used in Group statement..
You just need one CTE (or a subquery) and `ROW_NUMBER()` ;WITH cte AS ( SELECT * , ROW_NUMBER() OVER (PARTITION BY itemid -- you can change the priority calculation however you want ORDER BY CASE location WHEN 'A' THEN 1 WHEN 'B' THEN 2 WHEN 'C' THEN 3 END) AS prio FROM items ) SELECT * FROM cte WHERE prio = 1 I'd normally use `APPLY` to calculate the priority because I like to keep my more complicated calculations outside of the select list, but you failed to mention what RDBMS you're using, so I'm sticking to fairly standard stuff.
Select neighbourhood from city Where students&gt;5000 Group by neighbourhood Having avg(income) &lt; 50000 This is how i see it. It would help to have the database. Hope its what you need. I.m still new at sql
This is exactly what I was looking for, I just couldn't quite wrap my head around how to do it. This is in MS SQL 2012. If you would use Apply for MS SQL, could you show that too? I am interested to see it. Thank you for this!
I think thats mostly it, although you could argue that income per family isn't the same as 'studentincome'. Could be interpreted either way really, but you could make the case for a 'household' table as well. Student joins to household, household joins to neighborhood.
Syntactically everything looks correct. If your query is going against SQL then everything should work as you've shown in your example. Can you try highlighting the first part in SSMS and making sure it runs on it's own and there isn't another hidden "@" symbol somewhere in your code? You can also consider concatenating the result to get 'PAYMENTS\_nnnnnnn' if that's what you need to ultimately query against: &gt;DECLARE @batch\_id\_table AS VARCHAR(15); &gt; &gt; SET @batch\_id\_table = 'PAYMENTS\_' + CAST((SELECT MAX(job\_id) job\_id FROM syjob WHERE program\_id = 'PAYMENTS') AS NVARCHAR(15)) &gt; &gt; SELECT \* FROM @batch\_id\_table &amp;#x200B;
Don't know why you got downvoted. Easily the easiest solution. 
Here it is with priority calculation with `CROSS APPLY`: ;WITH cte AS ( SELECT * , ROW_NUMBER() OVER (PARTITION BY itemid ORDER BY itemPriority ASC) AS prio FROM items CROSS APPLY (SELECT CASE location WHEN 'A' THEN 1 WHEN 'B' THEN 2 WHEN 'C' THEN 3 END AS itemPriority ) calc ) SELECT * FROM cte WHERE prio = 1 It's functionally exactly the same, but in my opinion much easier to read, especially as complex calculations pile up. `APPLY` also makes it easier when you want to both select and group by a calculation - normally you'd have to repeat the entire formula in both `SELECT` and `GROUP BY`, with an `APPLY` you just use the calculated column name. 
Totally. Probably better to do that! He could be a student later on in life and be in a completely different household! Nice one.
&gt; But WHERE can not be used in Group statement.. wut??? i think you'll find that's wrong
There is no reason to store it as varchar. What if you need to get the month, day of month, etc? Are you going to cast to date/datetime every time, or roll your own functions? I don't know what RDMS you're using, but on SQL Server DATETIME2(0) is six bytes, while you need eight bytes to store just the date as VARCHAR (I assume you meant to write '20190101').
Well you need to design a data model firstly with an appropriate number of tables. I would say the following tables would be a good start: * Citizens Citizen ID (PK), Name (split into Title, First, Middle, Last names?), Date of Birth, Address ID (FK), Neighbourhood ID (FK), Occupation ID (FK) * Addresses Address ID (PK), further address fields * Neighbourhoods Neighbourhood ID (PK), further neighbourhood fields * Occupations Occupation ID (PK), Occupation Name, Income, further occupation fields Think that is probably all you need for this, it might be overkill.
neighborhood table with id and name columns neighborhood_stats table with neighborhood id, total students, and avg family income columns joined on neighborhood_id = id select n.name as neighborhood, s.total_students, s.avg_family_income from neighborhood_stats s inner join neighborhood n on n.id = s.neighborhood_id where n.avg_family_income &lt; 50000 and s.total_students &gt; 5000 I think given the information provided anything else is making assumptions you have more data
This is fine SQL syntax, but bad data modelling!
Right. It doesn't mention the granularity of the data. This could technically be done with a single neighborhood table and simple SELECT ... WHERE. But, I'm assuming that they're implying that students are a separate table.
OK, thanks for the feedback. The reason this came up is, on another table I have of similar size, the dates are stored in that varchar format, and I can return results there much more efficiently, so I was thinking perhaps it had something to do with the data type. For example, in my experience doing string-related searches are much less efficient than looking at 1's or 0's
If you think there is potential advantage to storing it as a string, like the easy of writing queries like WHERE Date LIKE '2019%' Since this is a data warehouse, you can have both columns and use whichever one you find easier/faster.
Got it, I got them both to work. Funny, I actually think the first way is a bit easier to read, but I see what you mean about the advantages of using Apply. Thanks a lot for the help!
CREATE TABLE Neighborhoods ( ID INT IDENTITY(1,1) , Name VARCHAR(100)) &amp;#x200B; CREATE TABLE Addresses( ID INT IDENTITY(1,1) , ADDRESS VARCHAR(200) , NeighborhoodID INT) &amp;#x200B; CREATE TABLE People ( ID INT IDENTITY (1,1) , NAME VARCHAR(50) , Income INT , IsStudent BIT , AddressID INT ) &amp;#x200B; ; WITH CTE as ( Select NeighborhoodID, SUM(Income) IncomeSum , SUM(IsStudent) StudentSum FROM People JOIN Addresses ON [Addresses.ID](https://Addresses.ID) = People.AddressID GROUP BY NeighborhoodID ) &amp;#x200B; SELECT [Neighborhoods.ID](https://Neighborhoods.ID), [Neighborhoods.Name](https://Neighborhoods.Name) FROM CTE JOIN Neighborhoods ON [Neighborhoods.ID](https://Neighborhoods.ID) = NeighborhoodID WHERE CTE.IncomeSum &lt; 50000 AND CTE.StudentSum &gt; 5000 &amp;#x200B;
Gonna go out on a limb here and say this is a question intended to assess, not penalize for incorrectness, so the teacher can understand where you are coming from.
Agreed. Use the date/time datatypes. That's why they're there. It's generally best to just type your columns as accurately as possible from the get-go and to rely on the RDBMS to know the best way to deal with it (assuming it's a respected RDBMS, of course). I wouldn't seriously suggest that you "go against the grain" with your datatyping unless you **REALLY** know what you're doing. Times and dates are a big enough pain in the ass to work with as it is. No need to further complicate it.
SELECT * FROM dbo.Neighborhoods WHERE NumStudents &gt; 5000 AND AvgFamilyIncome &lt; 50000
Building on this point, in the warehouse you need to think about how you will be reporting the data to understand the most ideal way to arrange it. As in, do you expect it would be common to run reports monthly/quarterly/yearly, or is it more abstract and arbitrary date ranges that are picked on the fly? Adding an extra a column for the common dateparts you use might make it easier to do query things like year = 2019 and quarter = 1 instead of datetime between "2019-01-01 00:00:00" and "2019-03-31 23:59:59"
This doesn't really fix my problem. I'm just trying to make it so I dont have to rejoin them everytime I open ssms. I dont want to run them automatically and I dont want to run them together. When I'm building a test, I'll run one to check the dates on the server, and then run the other to modify the dates. The problem is that every day I have to search through a dozen scripts I had open yesterday to find the two I want pinned at the top. 
An alternative to adding a column that I've used in the past is to just have a lookup table for each date from 1899 to 2199, with the dateparts broken out (day of month, day of year, quarter of year, is month end 1 or 0, etc), and join that to the date field whenever need so no need to parse everything out
Yeah using a date dimension table is a good point, ultimately you just want to have the common parts broken out somewhere in the DB ahead of time so you can try to cut down on the number of functions you have to run every time you execute your report.
Without looking at the other answers first this is what I came up with. CREATE TABLE [dbo].[Neigborhood] (NeigborhoodID INT IDENTITY (1,1), NeigborhoodName VARCHAR(100)) CREATE TABLE [dbo].[Family] (FamilyID INT IDENTITY (1,1), FamilyName VARCHAR(100), FamilyNeigborhood INT NOT NULL FOREIGN KEY REFERENCES dbo.Neigborhood) CREATE TABLE [dbo].[Person] (PersonID INT IDENTITY (1,1), PersonName VARCHAR(100), IsStudent BIT NOT NULL, Income DECIMAL(10,2), PersonFamily INT NOT NULL FOREIGN KEY REFERENCES dbo.Family) GO SELECT n.NeigborhoodName ,SUM(CAST IsStudent as INT) AS NumberOfStudents ,AVG(SUM(p.income)) AS AverageFamilyIncome FROM Neigborhood AS n INNER JOIN Family AS f ON n.NeighborhoodID = f.FamilyNeigborhood INNER JOIN Person AS p ON f.FamilyID = p.PersonFamily GROUP BY n.NeigborhoodName HAVING SUM(CAST IsStudent as INT) &gt;= 5000 AND AVG(SUM(p.Income)) &gt;= 50000; I didn't want to make it too simple and discard the notion that families might include one or more students who may or may not have an income. 
I submitted this type of query. Fingers crossed its correct
We can only hope. It's a fast paced class. 
Thank you! I am going to re work the question this weekend
Hmmm I am going to try this question again. Thanks!
Wow I think I over thought the question. Thanks!
.... sum( salesytd) - sum( salesquota) as uhoh
I swear to god I tried that EXACT same thing and it kept kicking it out and now I tried it again and it worked....unreal. Thank you for your help friend.
So you run query to see what the date is, then another query to modify it to another date? How do you know when to change it, and how do you know what to change it to?
There are no automatic criteria for that. It's based on my needs. I have another script that does update based on certain criteria, but not every instance calls for it.
In this case it might be easier, but as the queries grow and calculations become bigger, they clutter up the select list and it becomes unclear what you're actually trying to select in that query. I'm talking about queries spanning dozens of lines in a single CTE, with several CTEs, where your wrap a single selected column across five lines because there's so much stuff going on in there. Add comments and other references and it gets hard to read real fast. Fortunately, this is by no means required, so you choose whatever works for you. Glad I could help, cheers!
What book are you using for the class? I am learning too and would love any reference materials! Thanks!
[removed]
Easiest question ever Select neighborhood from City where students &lt;5000 and income &lt; 50000
Why not try it and see....
You have to also SELECT customer if that is what you would like to GROUP BY. SELECT MAX(OrderNumber), Customer FROM tableName GROUP BY Customer
&gt; can I use this:? sure you can, but you'll have a hard time figuring out which number you get in the results of the query belong to which customer unless you also include `Customer` in the SELECT clause 
I'm gonna make a guess that those numbers in SQL are actually string and you something like 1, 10, 11, 2, 20, 21, 22 when you try sorting them. That is correct text sorting. Either format your Excel cells to be text as well or change text to numbers in SQL. If I didn't guess correct, then you need to give some working examples of what is happening and what you want to happen.
So what I'm having to do is a WHERE in ('#', '#', '....#') because there are 25k of them.
I don't get it.
Soft parse because it's happened before. Deja Vu. 
Okay I'm feeling dumb now because I'd never heard of this. For anyone else like me: http://www.dba-oracle.com/t_hard_vs_soft_parse_parsing.htm
I suggest that u create some views to change all columns that represent the same thing to the have the same name and work the select with them. Here's the best i could come with on my phone, hope that helps: Select sum(case when m.type = 'classic' and m.active = 'Y' then 1 else 0 end) as classic_active , sum(case when m.type = 'classic' and m.active = 'N' then 1 else 0 end) as classic_inactive , sum(case when m.type = 'basic' and m.active = 'Y' then 1 else 0 end) as basic_active , sum(case when m.type = 'basic' and m.active = 'N' then 1 else 0 end) as basic_inactive , sum(case when m.rico = 'Y' and m.active = 'Y' then 1 else 0 end) as RICO_active from ( Select m.membershiptype as type , case s.subInActive when 1 then 'Y' else 'F' end as active , case when r.xmi_membership_id is not null then 'Y' else 'N' end as rico From Reporting.dbo.membershipStartsAndDrops m Inner join XM2.dbo.tbl_Subscription s on s.subSubscription_ID = m.MembershipID Left outer join XM2.dbo.Import_RicoMembers r on r.xmi_membership_id = m.MembershipID ) as m (type, active);
SQL does integer division by default. For postgres, making the digit a float/numeric/etc will allow decimal places: select 1/2 = 0 select 1.0 / 2 = 0.5000000 select 1::float / 2 = 0.5 In a business setting you may want to consider casting as ::numeric. See this link for more information https://stackoverflow.com/questions/13113096/how-to-round-an-average-to-2-decimal-places-in-postgresql/13113623
It isn't preferred and I personally use all lowercase at work.
or you can type the same queries in lowercase for free.
Where in/not in is much simpler and more straightforward in my experience. If you're not using an old and shitty db, the performance difference is negligible https://stackoverflow.com/questions/50800120/where-exists-vs-in-in-amazon-redshift https://stackoverflow.com/questions/50800120/where-exists-vs-in-in-amazon-redshift
So create a stored procedure that consumes a @variable (e.g. a date), etc. 
Why are you using that index hint? I'm a SQL Server person, but I would think MySQL would choose the other index in some of those cases, making your queries run faster. But you are forcing the non-selective index. You said specifically that a only has 3 values. Which means the index only turns your 100,000,000 rows into 90,000,000 rows, right?
That's the "safe, sensible" plan. I've had some time to contemplate the issue that I'm trying to solve. I'm probably going to end up using views like any reasonable person would. But, since I'll have some free time tomorrow morning I'm going to write a generic function to automatically join to a lookup table tomorrow, run some tests, and follow up with the results.
Best response I've gotten yet.
You're probably better off pasting the results into a separate temp sheet, sort it there, copying the data in the temp sheet, and pasting into your target sheet. It's a separate hop. But then you wont need to be concerned with two different sort methods. 
https://www.geeksforgeeks.org/database-normalization-normal-forms/
Switch to vscode or winvim
I should have said `a` has a handful of values. Some people read few as exactly three. I'll edit the post. I use the index hint because sometimes the query planner picks the wrong one. InnoDB doesn't have exact statistics to make a perfect plan every time. My actual indexes have a few more columns in them, and sometimes MySQL will try to scan a smaller index (fewer columns) but end up doing a filesort due to the column order. I also left the hints in the example queries to make it clear which index I'm intending to use. In actuality, index `a` has about 12 or so different values. About 70,000,000 rows have a single value, another 20,000,000 have a second value, and a few values appear only a dozen times in the entire table. But I often want to query for those values that only appear a dozen times. If a value has high cardinality, then you usually do want that column to come first. If a value has low cardinality, then having the column first is doesn't really help, except to make the index a covering index. InnoDB (the primary storage engine in modern MySQL) stores its indexes in a B+tree. It will rearrange the equality predicates in the query to match the order of the columns in the B+tree. Having the columns in the index in the suboptimal order for cardinality really doesn't affect B+tree performance that much. If I cared only about a single value for `a` and a single value for `b`, then I'll end up with the same sequential chunks of rows, in order, regardless of the column order. Having both before `date_created` makes perfect sense so I get a nice, sorted list to apply a limit to. The problem arises when I want rows matching one of a list of values for either `a` or `b`, then sorted by `date_created`. Because I'm selecting multiple values, the candidate rows will come from different branches of the B+tree, and won't be in `date_created` order, and so MySQL fetches all the matching rows, then sorts them, before applying the limit. So if the values for `a` has 70,000,000 rows, and the values for `b` has 10,000,000 rows give an intersection of say 5,000,000 rows, it's much too slow to sort them. There are two solutions I'm aware of to get around this limitation, which are the index and query combinations I used above. First is to do an index scan. Put `date_created` first, then scan the index checking every row to see if it satisfies the predicates (basically depth-first walk the B+tree). This works exceedingly well if the predicates for `a` and `b` are common values as not many records from the index need to be fetched to reach the limit. But if either `a` or `b` is checked for a rare value, the entire index may end up being scanned trying to find enough rows to reach the limit. This can be excessively slow (in my actual query, about 70 seconds). The second solution is to split the query into a bunch of unions, which can be retrieved in order, so a sort isn't necessary before applying the limit. It would be amazing if MySQL were smart enough to do this automatically (this is my biggest complaint about MySQL). This approach works very well when `a` or `b` only matches a few rows and works acceptably well when there are lots of matching rows, except for the memory explosion in MySQL 5.7.
Ok, this account is two months old, so I'll give you the benefit of the doubt and lend a hand. The first place you should visit for any new subreddit of interest is the sidebar for that sub. That includes this one. Or Google...
Fucking lol, this is a joke right?
Uh no, not at all. This question seems completely noob. It's the 4th day of class. Unless I'm missing something. Of course, I didn't do the first part
Interesting comment...to be fair (I’m new to) I didn’t know about the side bar because I only open reddit through mobile
I’m on the phone where’s the sidebar.
How much SQL do you already know? What's your confidence level?
0.2%
Oh boy. 
Can I be your padawon
Depends on if you're on an app or the mobile website. Android or iPhone?
iPhone homie
Alright obi wan. 
Unfortunately I've never held an iPhone for longer than to take a photo. If you're using the Reddit app when you're in a subreddit it should be under "Community info" under the "..." In the upper right. On the website, or another app and you're going to have to do some digging. 
The question is obviously asking that each entity be separate and connected via keys, hence and ERD. Such a database wouldn’t have columns like “students” or “avg_income” on the City table, that would be more akin to a data warehouse. It would be multiple tables, such as City, Student, and Family. Student and Family would reference city, and then the query would filter Cities based on aggregates of the student and family tables. As pseudocode since I’m on mobile, it may look like this ‘SELECT city FROM City JOIN Student JOIN Family GROUP BY City HAVING COUNT(Student) &gt;= 5000 AND AVG(Family.HouseholdIncome) &gt;= 50000’. Still simple. Not *that* simple
If you’re on the mobile website (via Safari), there’s an “About this community” link to the right above the first post listed in the r/SQL main page. If you’re on the Reddit app, at the very top right of the r/SQL page, click the “...” and go to “Community info” to get to the sidebar. https://www.reddit.com/r/sql/about
What do you mean by data modelling? How i arrange the code in the page?
Just install some kind of SQL (e.g. MS), and import the file into a table. 
Three options are: * [W3Schools.com](https://www.w3schools.com/sql/default.asp) * [SQLtutorial.org](https://www.sqltutorial.org/) * [CodeAcademy.com](https://www.codecademy.com/learn/learn-sql)
As already mentioned, best option is to just use some flavor of SQL. However, if you just want something quick, Notepad++ with the CSV query plugin is nice for quick queries.
Ok
SQLite is great for this and has syntax to import CSVs.
1 - Install Postgresql on any machine you have available (preferably Linux using a VM) or SQL Server is a free download for development use (if you're dead set on using Windows / MS SQL) 2 - Take the 'Learn SQL' course on Codecademy and then do the 'SQL Table Transformation' courses. You can literally run through them in an hour or so if I recall correctly and that will give you the basics but there's a lot missing. &amp;#x200B; Good luck and have fun. SQL is great.
Csvkit is the best tool Parses csv correctly. Can write to db or even create db for you https://csvkit.readthedocs.io/en/1.0.3/ 
Column "# of students &gt; 5k and avg income people family &lt; 50k" (bit, not null) Then rows are the neighborhood names SELECT * from table WHERE # of students... = 1
Sql Server has import tools in SSMS. I use them all the time to pull stuff in.
Python to clean the data using pandas, then import into Sql. I use PostgreSQL 
I learned a bit from https://www.w3schools.com/sql/default.asp because I was learning asp.net core and ssrs at one point. 
As an SQL newbie, why a Linux VM?
I used W3Schools.com to start. Then moved on from there. Been in it 6 years now and all self taught.
Could you please elaborate, I also use postgresql
&gt; best option is to just use some flavor of SQL That's not a thing. Use some flavor of "database". SQL is the query language for manipulating data within a database.
Apologies. Should have been more clear. &gt; some flavor of relational database server software, such as SQL Server, Postgres SQL, MySQL, etc. I try find a balance between being overly pedantic and getting the point across. Especially when I'm suggesting an alternative to actually using said products. For my suggestion, the CSV Query plugin utilizes a SQLLite backend which lets you use SQL as a querying language against text files with minimal steps.
Change your ORDER BY to do a CAST(column AS INT).
Linux generally runs on a toaster so you can carve out a small vm and learn. You also develop Linux skill as well as familiarizing yourself with vms. Windows requires a much larger footprint and in my 20 year experience, you're much more likely to work on Linux based systems or a mixed bag environment so why not say, hey - ive seen that error before on Ubuntu or Centos package has that bug". GL
Same here, W3Schools is an excellent starting point. Simple and understandable presentation of core basic functionality to get you going.
Udemy.com
This is the book everyone who is just starting out should read. SQL in 10 Minutes, Sams Teach Yourself (4th Edition) https://www.amazon.com/dp/0672336073/ref=cm_sw_r_cp_apa_i_ZQ2qCb2BE7PRK
Will give this a try. Thank you. 
https://pgexercises.com/ 
In a similar situation as you both /u/RandyInMpls &amp; /u/easy_wins I have an accounting degree (undergrad/masters) and always been interested in programming after trying to learn C++ and python a few years back I fell off the track due to interest. Going to be hitting 30 soon and I've realized I want to switch over to Financial Analysis. What do you recommend in terms of the best way to Learn SQL to help me in this journey ? I'm using W3schools at the moment to get the basics down. I don't have the money or time to go back to school but I think learning SQL will make me much more appealing in the market. I can't see myself doing accounting much longer but the mortgage has to get paid.
https://www.w3schools.com/sql/
Patindex - to find strings Charindex - to find position in string Substring - extract part of string
“SQL Server Administration in a Month if Lunches” is decent. Just installing SQL Server Express or Developer edition on your workstation and playing around with the Adventureworks databases is also helpful.
&gt;SQL Server Administration in a Month if Lunches Whoa that looks great! Starting to read it now. 
SSMS is just the client for the SQL Server, but SQL Server itself isn't exactly hard to set up, you just download the installation media and click Next, Next, Next for a basic setup that should actually cover your needs just fine. You can tweak that installation later on if and when needed. [Microsoft's own docs should be enough to get you going.](https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server-from-the-installation-wizard-setup?view=sql-server-2017). You should be fine with the defaults I think. About the only thing in there that you may want to think about is the security mode: either Windows Authentication (Local Machine / Active Directory based), or Mixed mode (allows both Windows auth and setting up dedicated accounts with login/password, which you will probably need if you want to connect to the server from outside of your company where there's no AD access. Security in general is something worth thinking about, you don't normally expose database instances for the whole world to see, I'd strongly recommend not doing that. Set up a firewall with a whitelist of addresses or ranges you can connect from instead. Before you install though, there's one very important question - what about the license? SQL Server isn't exactly cheap and licensing terms aren't exactly easy. [Here's an overview of SQL Server editions](https://www.microsoft.com/en-us/sql-server/sql-server-2017-editions), there's one that's free (Express) but it has some serious limitations to memory usage and most importantly database size (up to 10 GB, usually not nearly enough for a data warehouse). It also doesn't have SQL Server Integration Services, but I don't know if you'll be using that if you're going with CData's sync solution. It's pretty common in BI / ETL solutions, so it's worth considering if you need it or not. 
https://sqlzoo.net/ is recommended for absolute beginners.
i think there are ways you can use SQL on flat files, e.g. CSV files that are tab- or comma-delimited needs some kind of driver software, and i wouldn't recommend it, especially since installing a dbms is trivial 
&gt; Is SQL outside of a DBMS kind of like a letter without a font? Kind of. SQL is used specifically to interact with databases - hence it being a "domain-specific" language. You can, however, practice SQL without installing a database with such things as [SQL Fiddle](http://www.sqlfiddle.com/#!3/16fd1/1) but it's probably going to be simpler installing a particular database and interacting with it that way. Now, ANSI or ISO SQL is the SQL standard that's database-agnostic, and any good reference manual for a particular database that's teaching SQL should give notice of when something is considered "Standard SQL" compliant, and when it is specifically implemented for that particular database. There's a portion of the SQL language that will basically be standard regardless of the database you work in. And then, each RDBMS has their own flavor/implementation of non-standard SQL. 
Thank you!
Is there a reason you don't want to, or cant install an SQL server? I feel like this may be the real question
Theoretically there's ANSI standard SQL, but because like any interpreted language SQL needs an interpreter to run so you end up being stuck with whatever syntax quirks the chosen RDBMS brings (and lord knows they bring them!), so in a sense, the answer is no. If your concern is about choosing the 'wrong' flavour of SQL to learn don't worry. Once you've got the hang of it, the concepts are portable and syntax is Google-able. If you want to be really good, the biggest thing to learn isn't the language - it's thinking in sets and modeling data in your head. My advice to people starting out is to find some data you know/understand/care about and ask real questions of it. 
If you look online there are databases setup just for this (adventure works for example).
[Csvkit](https://csvkit.readthedocs.io/en/1.0.3/) can do that, via csvsql and some other tools. I think it's recommended only as part of a workflow for [Data Science at the Command Line](https://www.datascienceatthecommandline.com) type approaches, not as replacements for a DBMS.
Awesome! The standard edition looks to be what I will need for long-term, although can possibly get by with express to start since I see that it straightforward to upgrade from it (not sure if I will hit the size limitation just yet). I have actually not used SSIS/ETL yet although has been on my list to get an understanding of how this software can enhance my ability to work with data.
Install Postgres, google for "free datasets". Download as CSV files, import to your DB, profit.
&gt;Is there a reason you can't, or don't want to install an SQL server? I can. I was just trying to figure out if there's some "platonic" SQL I should learn instead of a particular DBMS. From what I've read, it doesn't really matter because SQL is basically the same everywhere, small differences between different DBMSs not withstanding.
Don't be absurd! Lower case is for peasants. /s
SQL Server is straight forward enough to setup, not far off installing any app on a PC and there's good documentation from Microsoft on it so I'd point you in that direction. You probably do need to buy Standard, if you're taking about an important business system Express may not be enough. Plus, you'd probably really benefit from Integration Services for ETL. &gt;I want to start the server on the current machine and be able to connect to it via SSMS from anywhere in the world You do not want a publicly available SQL Server, that would likely be a disaster unless its security environment is meticulously maintained. Could you setup a VPN in maybe? Why would you want it to be wide open to the world? 
I also need a list of questions that will be pertinent to the database. Where do I get that?
I agree with your points - note that I am very much a novice on networking! VPN route definitely seems the way to go.
Most platforms have some oddities or different syntax of their own, but theres various SQL standards they all basically try to adhere to. MySQL or Postgres would both be good choices to learn with. If you're on Windows, MS also produce 'SQL Express' which will get you up and running with a feature limited version of SQL Server. Personally I would avoid non-server platforms like SQL-Lite, MS Access etc. They are file-based, and 'kinda' support SQL syntax, but not always in a way that would be useful for learning
For the most part, SQL isn't used outside of database querying. In some languages, either at the language level or a library, people have started to use SQL to perform operations on their arrays of objects. An example is here: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/ 
There's quite a lot of 'open data' data sets out there (use that term when you search) from government departments, etc. My suggestion is to find some data that you know, understand or care about and work with that. Import it into something like MySQL or SQL Server Express. If you ask questions of data that holds meaning for you, you'll have a driver to help you learn the techniques you need to get the answers. 
You pay for a SQL course. People aren't in the habit of creating a curriculum for their data: they're being generous enough by offering their data for free.
If your issue is having to install a server you could use [SQLite](https://www.sqlite.org/). It's a database inside a file without server which supports standard SQL. Look for a Graphical User Interface that works with SQLite and your OS. 
Look for a decent SOHO router that has VPN capabilities built in (or check if you already have one). I've not been in that market for a while but names I've heard are good include Netgate and Billion. 
I am more than ready to pay if I only someone else can vouch that they got their money's worth.
Dude just do your research. There are countless free online tutorials. I only say that because when I first started learning programming and SQL specifically (please don’t yell at me I know SQL is technically not a programming language) I spent countless hours doing research, reading and looking and doing tutorials. Use YouTube and Google. If you really want it you will find a way. Your question poses a sense of almost wanting things a little too easy. That said, I am more than happy to help you learn the basics - for free. If youre interested lmk we can set something up.
https://sqlite.org/ might be what sort of what you're thinking of, it doesn't have the big multi-user network server management system. It is intended to be a SQL compliant data storage system for single applications and as that page says it is used everywhere, there's probably a dozen instances of it in use around you in browsers, your phone, your smart TV, etc. It does have its own programs that do make up a kind of DBMS or libraries to integrate with a programming language and program. It may make sense to start with the simplicity of SQLite before moving on to a more capable DBMS because you don't have to mess with users, permissions, network ports, etc. Of course, any serious use of SQL does need to address those things but SQLite may provide the foundation of thinking in sets and the basics of relational data without the distractions of additional technology. 
You generally don't want to run other stuff on a domain controller. Mixing a DC and SQL Server is definitely an unusual scenario. You could create an account to run SQL Server as, instead of the local service account (which you should really be doing anyway). You'll just need to make sure you give the account the necessary permissions - which shouldn't be too tough to Google.
Great insight, thanks!
Install sql developer version Download adventure works database and attach Browse w3school and practice writing SQL queries
"MySQL tutorial" was helpful for me in providing a sample database/questions 
Code academy has a pretty good online free intro course. After that, go to Kaggle.com and mess around with some of there stuff
Try /r/datasets But most database platforms usually come with some sort of test database you can practice with. Really, theres no point paying for a course if you haven't even dug into the basics.
This gets asked daily - go to codecademy and do the sql courses. You can knock them out in an hour each and essentially grasp the fundamentals.
You could buy the Oracle 1Z0-061(?) SQL Fundamentals Exam prep book and follow it through. https://smile.amazon.com/Oracle-Database-Fundamentals-Guide-1Z0-061/dp/0071820280/ 
You could just get started with Google's BigQuery. They host a bunch of public datasets. You can query them with SQL right from the web browser. https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui#query_a_public_dataset
I learned on Ms-Access too many years ago. 
Thank you everyone! I will let you know when I can move things with my mind after studying all this.
Sqlfiddle Modeanalytics Stanfards course on RDBs
Sqlzoo is great for beginners 
Stackoverflow has their database available for download. It’s an SQL Server backup. Run a copy of SQL Server in docker and play with that data to your hearts content. You can even play with their database online if you don’t want to setup a machine https://stackoverflow.blog/2010/06/13/introducing-stack-exchange-data-explorer/ https://data.stackexchange.com/stackoverflow/query/new
To practice? Download MSSQL Server Developer Edition and start learning! https://www.microsoft.com/en-us/sql-server/sql-server-downloads 
From a book. SQL Queries for Mere Mortals has the datasets, and the questions for you to try and query/answer from.
Tell my thousands of stored procedures and functions that sql isn’t a programming language. 
I kno what u mean trust me I got hundreds of em but “technically” it isn’t. I just didn’t want to get yelled it still did lol 
You can try [RBQL](https://rbql.org/) which is SQL-like dialect. It is also built-in into Rainbow CSV plugins which are available for many text editors. I am the author, BTW =) Also there are: [q - Text as Data](http://harelba.github.io/q/) \- uses sqlite as backend [TextQL](https://github.com/dinedal/textql) \- also uses sqlite [bigbash](https://github.com/Borisvl/bigbash) \- converts *select* statements to Bash one-liners 
There are FOSS solutions such as mysql, postgres, sqlite.
Not 100% sure I follow, but could you create a new column such as Select * , case when likecolumn like ‘%name%’ then ‘join’ end as [Join_OnThis] from tableA Then do the same for tableB Finally... Select cols1, cols2...Join_OnThis from tableA Except Select cols1, cols2...Join_OnThis from tableB This of course assumes the tables have the same columns. Just make sure you don’t select likecolumn If this doesn’t exactly solve your problem, hopefully it sparks up something that will.
Maybe context will allow for better clarification. &amp;#x200B; I have two tables that I want to compare. &amp;#x200B; TableA = Work Completed TableB = Work To be completed &amp;#x200B; I am basically trying to run a comparison between what exists in table B and doesn't exist in Table A and visa versa. So I have a column named the same in both tables called 'Work\_Name'. I want to pull data from both columns and do a comparison for tableA.work\_name like tableB.work\_name and see what exists between each of the tables. The naming convention of the cells should be consistent between the two, but I can sort that out with DBAs on Monday if that proves to be combative. &amp;#x200B; I am a bit intoxicated and having a creative spike with work, so I apologize if I am being incoherent and less concise than I normally would. Hope it makes sense. i will certainly try out your previous instruction right now. &amp;#x200B; Thanks!
I am not aware of your skill level, but here is how I learned: &amp;#x200B; I created a database with the intent of organizing something in my life. For me, it was media. I created movie tables, book tables, etc. As long as you have the files existing in a directory on your computer you can easily export those directories to a CSV. Then you can import them into a database, by right clicking on the db you create and clicking on 'import flat file'. From there, you can tailor your db and create tasks for yourself based on the things you want to learn. I have found that learning SQL is applying it to something that will be meaningful to you, or you family, or your friends. &amp;#x200B; Since then, I have started created web apps to catalog requests for new movie and books purchases by learning PHP. Find a reason to learn that you care about, the rest will come naturally. One thing I will tell you, SQL is fucking awesome. I have been teaching myself for the past 6-ish months and I am constantly getting nerd boners over it. 
 I am not aware of your skill level, but here is how I learned: I created a database with the intent of organizing something in my life. For me, it was media. I created movie tables, book tables, etc. As long as you have the files existing in a directory on your computer you can easily export those directories to a CSV. Then you can import them into a database, by right clicking on the db you create and clicking on 'import flat file'. From there, you can tailor your db and create tasks for yourself based on the things you want to learn. I have found that learning SQL is applying it to something that will be meaningful to you, or you family, or your friends. Since then, I have started created web apps to catalog requests for new movie and books purchases by learning PHP. Find a reason to learn that you care about, the rest will come naturally. One thing I will tell you, SQL is fucking awesome. I have been teaching myself for the past 6-ish months and I am constantly getting nerd boners over it.
In case you go with express first, but still like to try out some ETL, there's a free open source suite called [Pentaho Data Integration](https://community.hitachivantara.com/docs/DOC-1009931-downloads). You can play with it and see if it suits any of your needs. It's not as straightforward to use as SSIS (at least it used to be a bit more complicated), but is very powerful.
First I'd say u/quentech is right, be very careful as you don't want to risk damaging your existing services running. If at all possible, test first and/or run on a different box but I know that can be tough in a small business. The solution is as described, create an account and give it privileges to run applications as a service, write to the file system wether needed, I can't remember all of the details but this might help as a starter: https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-windows-service-accounts-and-permissions?view=sql-server-2017 
Firstly, you don't need a PK or a FK for a join. Minus is a set operation between result sets. You can certainly add the "where" clause to your individual statements. select * from tableA where tableA.likecolumn like '%name%' minus select * from tableB where tableB.likecolumn like '%name%' if you are comparing just the column values themselves (not the whole row), just replace * with 'distinct column_name'
I haven't tried it myself, but you could try [SoundEx](https://en.m.wikipedia.org/wiki/Soundex)
Desktop link: https://en.wikipedia.org/wiki/Soundex *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^232964
**Soundex** Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, SQLite, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for "phonetic algorithm". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Why not clean in sql? You really need nothing but Postgresql.
The perl DBI module DBD::CSV is another such tool.
I use SOUNDEX() pretty regularly. It works nicely for fuzzy matching if you know how to use it. 
It is definitely possible, and I have done it. Depends on what you want to do it for. For example... would you want to use a fuzzy algorithm to match patients on their way to surgery? Suddenly Dennis Johnson is on his way to have Debra's mastectomy. Not good. Very useful though for other things, and you can assign some kind of match % to isolate cases where it's too low, and then mentally intervene as necessary. For attribution modeling it's a very powerful tool.
He'll need to do more than that to incorporate things like address, zip code, phone number, etc. I had a process that took each row of data (such as name, address, phone, etc.) and then assigned a primary key to each individual piece of data, so: | PK | Variable | | :--- | :--- | | 1 | Debra Smith | | 1 | 123 Walnut St. | | 1 | New York | | 1 | NY | | 1 | 5555555551 | | 1 | 5555555552 | | 1 | 5555555553 | Takes some cleansing to make sure you remove extra spaces, abbreviations for ST are consistent with a period, etc. From there you can join the variables where the PK doesn't match to string together possible duplicates and run a match rate algorithm that will compare each character in the cell with the characters in another cell. Can be slow but can do a lot with it.
Well, yeah, no one said it was going to be easy lol. Just SOUNDEX and DIFFERENCE might help along the journey. 
The Levenstein distance algorithm is a good method for doing fuzzy matches in SQL Server. It requires a CLR proc. Some information to get you started here: http://www.harshabopuri.com/sql-server-fuzzy-search-levenshtein-algorithm https://stackoverflow.com/a/9731894/83144
Could you not just say select * From tbA Join tbB On X=y Where TbA.clmA =tbB.clmB Not sure what type of server you’re on either syntax may be slightly different. 
Full text queries?
I could just farm out some of my tasks to you how's that sound lol
I did fuzzy matching in SQL Server extensively a few years ago, and still do sometimes. Part of my master thesis was about it... I answered it more generally on a thread about ["What is something cool you've done in SQL Server?"](https://www.reddit.com/r/SQLServer/comments/5diee4/what_is_something_cool_youve_done_in_sql_server/da4wlx6/), to give you a general idea what I'm talking about. As /u/mattmc3 already mentioned, SOUNDEX is not very good for more advanced matching scenarios. You will need CLR. I would recommend setting up scalar CLR functions using the algorithms of [SimMetrics.Net](https://github.com/StefH/SimMetrics.Net) (it's a port for .NET from the original [SimMetrics-Library](https://github.com/Simmetrics/simmetrics)). In my experience [Jaro-Winkler](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) gives very good results for names (like person names and streetnames). Also, try [Damerau-Levenshtein](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance), Monge-Elkan and [NGrams](https://en.wikipedia.org/wiki/N-gram) as well as some similarity-metrics used in bioinformatics like [Smith-Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm) or [Needleman-Wunsch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm), which can have surprisingly good results on some data. Disclaimer: it totally depends on your data wheter or not a specific algorithm yields good results. Fuzzy-matching is always a fair bit of **trial-and-error**. Once you've set up the functions, a query could look like this: -- simple firstname/lastname matching SELECT * FROM person p1 JOIN person p2 ON dbo.JaroWinkler(p1.Firstname, p2.Firstname) &gt; 0.9 AND dbo.JaroWinkler(p1.Lastname, p2. Lastname) &gt; 0.9 -- streets (depending on the country, streetnames are often multiple words. -- you will have to trial and error with the monge-elkan tokeniser). SELECT * FROM addresses a1 JOIN addresses a2 ON dbo.MongeElkanJaroWinkler(a1.Streetname, a2.Streetname) &gt; 0.85 AND dbo.MongeElkanJaroWinkler(REVERSE(a1.Streetname), REVERSE(a2.Streetname)) &gt; 0.8 WHERE a1.Zip = a2.Zip Obviously, you will have to play with the threshold-values here. From my experience, a few tips and tricks: * The better you prepare your data, the better the results will be. For example, you could (should!) separate housenumber and streetname. Maybe even geocode them and run geographic distance calculations on matching-candidates. **Generally, the better your data is normalized, the better the results.** * There will **never** be a 100% perfect dataset where you can match everything automatically. You will either have to accept false-matches or set a very high threshold and match the rest of the data manually. * Like I showed in the second query, REVERSE() is your friend. Some String-distance algorithms weigh the similarity in the beginning of the string more than in the end. This is good, but as you are in trial-and-error territory here (depends of the quality of your data), reversing a string can also give you good results * Performance: Your first reaction to the sample queries above is probably: "oh god, performance nightmare" (and it should be!) If your dataset is big, you will have to use some sort of **blocking** before matching. Blocking means, that you prepare your data in blocks of likely matches and only compare within a block. Good blocking fields are usually Gender, Area-codes, geo-coordinates, age ranges, ... If your data is blocked, you don't have to run string-distance functions over the cross-product of all records. For example, without blocking a 50k records dataset can lead to 2.5 billion(!) similarity comparisons. This can take hours or days per query, NOT what you want. Of course, you want try different blocking strategies to minimise false-negatives, so don't rely solely on Gender if you suspect that your dataset could have data-entry-errors (like Gender could be entered wrongly). * Another little neat trick: try to normalize the strings themselves before comparing. For example, get rid of all spaces, dashes and dots before feeding them to the sim-functions. For example: *Franklin-D.-Roosevelt-Boulevard* should become *FRANKLINDROOSEVELTBOULEVARD*. This can bump up edit-distance scores because it doesn't have to factor in 'non-descriptive' characters like spaces. Unfortunately there is no straight-forward step-by-step recipe for fuzzy matching, at least not in my experience. Have fun, it is a very cool topic to sink your teeth into :) (sorry for the english, not my native language) 
Isn't there a built in ssis transformation for something similar to this?
If your SQL Server license included SSIS, it has a Fuzzy Matching merge join between two datasets that uses a modified Levenstein process under the covers. It's cool because it spits out similarity scores. If you have SQL Server 2017 or later, you can also run Python or R scripts directly against your SQL data and there are a number of similarity algorithms available in those languages. 
Yes.
The term you're looking for is Record Linkage https://github.com/larsga/Duke is a java command line tool that can talk to MS SQL and do this for you
You can use SQL to query (continuous) Streams of data in [Apache Flink](https://flink.apache.org/) or Batch data in ETL tools like [Apache Spark](https://spark.apache.org/), or search for Complex Events in tools like [Esper](http://www.espertech.com/esper/) 
&gt;Select Right(field name, len(fieldname) - 1) I finally had a chance to try it, and the changes don't get committed to the DB, it only shows results that remove the first character &amp;#x200B; Also, I need the query to remove the first character only if matches the letter Z ) &amp;#x200B; I"m on MS SQL btw
I finally had a chance to try it, and the changes don't get committed to the DB, it only shows results that remove the first character &amp;#x200B; Also, I need the query to remove the first character only if matches the letter Z ) &amp;#x200B; I"m on MS SQL btw
I finally had a chance to try it, and the changes don't get committed to the DB, it only shows results that remove the first character &amp;#x200B; Also, I need the query to remove the first character only if matches the letter Z ) &amp;#x200B; I"m on MS SQL btw
Learning SSAS is at the top of my list
Sounds good!
I would definitely give that a shot.
I'm the opposite of this subject. I know TSQL, SPs, Reporting, Complex Queries, I'd like to learn the admin side of things. Good video and good points though. All very important to know. 
Thanks! I have been an admin before. I transitioned to the dev few years ago. Now running my own company. 
I need a joint after this post.
sure, of course you can why do you ask? and what's wrong with PKs and FKs?
Absolutely yes you can A foreign key means "make sure a value in this column exists in another column"
Yes, you just join on the columns that you want, no keys of any sort are necessary. Otherwise you wouldn't be able to join subqueries for example.
haha I need one too now
Just curious...im new to this and learning at the moment
Got it...thank you 
Thank you
You can join on about anything... but you can run into performance bottlenecks. So on larger tables, it's a good idea to use some value with a key/index
Got it. Thank you 
Yes you can. One thing to note is that when the data of field being ‘join’ed are unique enough, it will be helpful to put an index on that field, performance wise. 
Yeap. Levenschtein distance.
U can roll a joint without any key but do need some chronic 
I see. If you're trying to update the actual underlying data, you'd probably need an update query of some sort. It'd look something like: UPDATE [your_table] SET [your_column] = RIGHT([your_column], LEN([your_column]) - 1) 
You can join on any column as long as the data types are the same (with the exception of BLOB, Text)
Once had to code the Levenshtein distance algorithm to deal with keystroke errors... Would not recommend...
If you can't my whole system will crash..... None of my tables (ok some do) have pks. Sobs in stupid.
Foreign keys are just a constraint on the data that can be entered into the table, it is not a prerequisite for joining tables. You can join on whatever you choose.
Seriouslyhaha
Why? You can create a sql function to di levenstein?
Not a fast enough one.
Depending on the task maybe, but ive done it a few times for some of my own projects no problem
You can join on just about anything if performance isn't an issue. That's what makes joins so powerful. Even then, you can add an index to tables for a funky or unusual join to make it more performant like a function based index (or an index on a virtual column depending on the SQL dialect). That being said, join on keys when at all possible. If you can't then find the next most obvious joins that make sense and then index them if that is gonna be a high traffic query (one that'll be used a lot).
If you know how to join tables and use aggregating functions like SUM(), COUNT() put it down. You're an intern not an experienced developer, they won't expect you to have mastery of it.
as a cowboy`s fan you maybe limited 😀
If you have access to CASS software, which standardizes addresses, and all of your records have addresses, then this becomes easier. Valid addresses can be “exact matched”. 
thanks, can this be made conditional so only if the first character is for example a letter Z, then remove it, otherwise leave the field intact?
Awesome, thanks for the quick reply.
It's a very useful skill and you will use it at basically every job, learn it :) 
It will tie in to other software you use, you'll see haha
[removed]
Not even software per se, just an understanding of manipulating data!
The "with the exception of" part depends on the DBMS product being used. In Postgres, you can absolutely use equality joins on BLOB (=bytea) columns and text columns. Note that "SQL" (as in "the SQL standard") doesn't have a type "text" to begin with.
Nah that would be formatting :) Try and imagine the city table you have there. What fields would it need in order to run that query? Is there a consistent granularity?
It would be easier to understand what i'm working with if i had the database. But i understand where you're coming from. also thanks for the feedback.
Use `AND`. OMG, this is SQL 101.
&gt; None of my tables (ok some do) have pks wat bro
Which one? Or all of those data sets? Your post doesn't really describe the problem you're facing.
What have you tried? SSMS lets you import a csv file or you can do it with BULK INSERT (SQL Server 2017).
It might be a little much since you're new to boolean operators, but using cursors in SQL is a really bad idea, especially as your data set grows. Check out [Doug's talk](https://www.brentozar.com/training/t-sql-level/8-replacing-cursors-part-1-6m/) and it may have some useful info to help start thinking about getting data in sets.
Use CSV Kit (python command line). Does proper CSV parsing, and makes it easy to insert to different databases
&gt; these don't work though sorry, not familiar with that particular error message by the way, if you pass in the value of the `_id`, what's the point of only selecting it back out? is this supposed to be a "does this thing even exist" query?
Well that's the point, you're meant to design the database too! Data modelling is basically database design
Firstly, +1 for "burgerland." Since you called me out for taking 4 paragraphs to answer your question, here's two words to answer the question in the title of the OP; 6 months. Now onto the real meat and potatoes of it; there isn't really such a thing as an "entry level" DBA position, as a DBA simply wields too much power and responsibility to be left in the hands of someone with literally zero real-world work experience. There are certainly entry level SQL positions, like as a developer or data analyst or the like, but not DBA. I didn't take the time to write out those 4 paragraphs for my health. I'm speaking from experience as a seasoned developer and DBA. I'm trying to set you up for success and the path you are trying to get on is a recipe for disaster. I'd strongly recommend you re-read the 4 paragraphs and consider the advice within them. But you do you bro. 
I'll give ssms a go thank you!
To be specific, the cast/crew csv and the movie meta data. when i try to import them to SQL Developer, there are a ton of errors appearing and preventing me from creating a new table with the data
Thank you, i'll give it a go &amp;#x200B;
You can use that or you can use a free version of visual studio to create an SSIS import package. A package should take maybe a few mins to make once you’re set up and use SSMS to run the import task. 
What errors are you getting? Usually they tell you what the problem is. I've been using DBeaver for all my database connections lately asd it lets you connect to multiple sources pretty easily. You can even connect to CSV files and query them with SQL which you can use to import into another database.
Maybe set the isolation level within the sproc? 
As long as you are in Washington, Oregon, California, Colorado... I think Michigan now too. Maybe a couple other states? Or Canada.
First off that code works if I make it work on one of my servers (missing catch and commit): SET TRANSACTION ISOLATION LEVEL REPEATABLE READ GO --exec [dbo].[AddOrderv32] 123 ALTER PROCEDURE [dbo].[AddOrderv32] @eventid int AS SET NOCOUNT ON declare @OrderIdTable TABLE ( orderid int ) declare @Randomvariable TABLE ( Randomno int ) BEGIN TRANSACTION BEGIN TRY WITH CTEmobileno(Value,Expiry) as ( SELECT Top(1)INTCOL,DATCOL FROM SOMETABLE WITH (ROWLOCK,XLOCK,READPAST) --WHERE DATEDIFF(MINUTE,GETUTCDATE(),DateAdded) &lt; 0 ) update CTEmobileno Set Expiry = DATEADD(MI,16,GETUTCDATE()) output inserted.Value into @Randomvariable select * from @Randomvariable END TRY BEGIN CATCH END CATCH COMMIT TRANSACTION Secondly - use a temp table over a table variable if this is intended to use anything more than a small handful of records. Thirdly - why are you updating a CTE to output a variable? This is a very strange way to do things. Would SELECT @Variable = SOME QUERY HERE not be sufficient? Fourthly - what reasons are you using this transaction isolation level and the table hints (ROWLOCK,XLOCK,READPAST)? Could you explain whats caused you to go down this route so we can better advise? I'm not getting the same error so clearly something is different with your set-up/code.
I don't get what this articles purpose is? Proving that entity framework is concurrent? I keep well clear of it for many reasons relating to how many issues it causes but even I would surprised if it was not concurrent. Proving that SQL server is concurrent? Thats like the most obvious thing to ever be 'proved' that has ever reached this sub. Clearly I am missing something as if my understanding was correct I would expect articles such as 'water wetness checks' from the author.
Try these: Select * from table1 JOIN table2 on 1=1 Select * from table1 JOIN table2 on 1=0 select * from table1 JOIN table2 ON table1.DateColumn BETWEEN table2.OtherDateColumn and table2.ThirdDateColumn select * from table1 join table2 on COALESCE(table1.column1,table1.column2) = REVERSE(table2.column) select * from table1 join table2 on table1.column != table2.column
Three must read reasons why I did not get past the first minute or two of this video: 1) It's an advert for the companies services and clearly is intended to bring customers rather than inform. 2) Pure clickbait title. 3) The music is much louder than the guy speaking. 
I have make everyone happy. Thanks for not watching!
Google
A few issues: Get rid of the [] brackets, that is MSSQL/MSAccess syntax. Your parentheses error seems to be right after your from. Switch it to: FROM DB1 LEFT JOIN DB2 ON DB1.LCD = DB2.LCD LEFT JOIN DB3 ON DB1.GCD = DB3.G# Also, your final IIF should be a case statement, and PL/SQL doesn’t use double quotes for strings, so use Single Quotes. CASE DB1.Y WHEN ‘Y’ THEN ‘Test’ ELSE ‘Test2’ END AS CaseTesting You can use NVL and NVL2 as a substitute for ISNULL + IIF, just make sure you look the syntax up. Especially if you’re trying to nest them, which is tricky. My advice is to make sure they can operate well as their own columns first, then combine them by nesting. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions105.htm https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions106.htm
SQL Developer is for Oracle Database. SSMS is for MS SQL Server. I feel that reading a CSV file can be both solved and can cause some headaches in both environments. If you can't get the built in functionality to work out of the box it may be an idea to parse the file line by line with your programming language of choice and just running INSERT statements in a loop. 
Well I guess if your target audience is the same as buzzfeeds then it might bring some views. However I would have thought a technical topic would attract a more mature audience than what would be inspired to click on clickbait. Suggest you keep your advertising off this subreddit though as its for discussions of SQL, not for generating views or new customers.
Thank you for all of your feedback and insight! I am evaluating the possibility of going directly to SQL Standard as a high probability. Regarding SSIS / Pentaho, would either / both of these tools potentially be able to help me organize all of the various data flows that are presently taking place between QuickBooks and various third parties? Or are they more suited towards midmarket+ enterprise needs where the internal IT dept has a lot of control as opposed to relying on third parties? I have seen an ETL setup in play before and really like the organization aspect of it, however am too unfamiliar with how it actually works to answer this question... will be taking a certification / web course to get more exposure if it pans out to be a viable option.
Decided to move towards a dedicated workstation. Thank you very much for all of your assistance and support!
Decided to move towards a dedicated workstation. Thank you very much for all of your assistance and support!
I don't understand. I import .CSV files to SQL Developer all day, every day with no issues. Go to your connections, right click on Tables and select import data.
I'm not really familiar with QuickBooks, but you could definitely utilize some of the features of an ETL suite from what you've described. You can make these ETLs as simple as one to one mappings between source and your data warehouse or as complicated as you want. I've seen them used with great success in all kinds of environments, small businesses using weird software that's basically impossible to interface with maybe apart from exporting to a CSV, or driving hundreds of gigabytes of data in an enterprise; using workflows set up In a couple of hours by a basically non-IT person, as well as entire solutions that took months to develop by an army of expert developers. If you're going with a Standard license I'd recommend SSIS because you get it as part of the package and it's tightly integrated with SQL Server. Pentaho isn't worse but I'd say it's not as user friendly and straightforward to use, may take longer to grasp all the concepts. If you have a thing for IT, just playing around with the software and watching a couple of tutorials should be enough to give you an overview and enough skills to set some really useful stuff up. Don't be discouraged by lack of knowledge, try stuff for yourself, if time permits of course. It's not like you're going to break something, given you don't have a productive environment yet (you can even use the 180 day trial to set it up right now and just plug your license in later).
I’d try to find an analyst job and worry about the cert later. 
5+ years of experience ain't nothin' but a number. &amp;#x200B; I've filled roles that have asked for more and the number of years of experience really don't mean much until it's like 10+ years.
In my experience the certs don't demonstrate any real degree of knowledge, since they are not terribly difficult to pass (multiple choice, lots of websites providing answers, same sort of questions every year etc). What it does show is interest, so it certainly wont hurt your resume if you are applying for these positions. What sort of jobs are you applying for? It's unlikely anyone will hire a DBA without proven experience, but you could perhaps apply for data analyst roles or junior database admin roles (You could also consider a small business which needs an SQL developer and is not in the position to hire a full fledged DBA, and express an interest in taking on that responsibility in your interview). 
The ordering of your tables and criteria in the `INNER JOIN` doesn't matter. You can join on anything. Whether or not it's correct depends on your data model. When there's a trusted FK constraint, some optimizers may make better query plan decisions, but it's not required.
Agreed with this, the only hesitation I have in FK to FK join is multiple records in one table may share the same FK value, and if both tables experience this you will be returning too many records. As explained above, depends on your data model....
Usually when you're doing a many to one join, it should be intentional though. Similar to joining PK against the many to one FK, you'd still need to be aware of it. IMO that's not a "problem" so much as the standard operating procedure there, ya know? I definitely agree that in OPs question, order doesn't matter and I've never heard of someone nitpicking over it for any "best practices" or "optimizations" reasons. I'd say you should choose to write it the way that's easiest to read. For example, it might make it easier for future readers to understand if you're doing A inner join B inner join C on a=b and a=c VS A inner join B inner join C on b=a and b=c At that point it's literally just symantics, but if you think it'll help the next person understand what you're trying to do, then you should go for it.
Thank you, I am strongly leaning in that direction. We definitely have some of that "weird software" here! Not quite at hundreds of gigabytes of data yet but there is a very aggressive acquisition plan and I want to get ahead of the game on our backend and put a datamart + trained staff into place rather than getting my active time swallowed by trying to keep up with dissecting data moving into and out of many different QuickBooks files...! 
&gt; the only hesitation I have in FK to FK join is multiple records in one table may share the same FK value, and if both tables experience this you will be returning too many records If the relationship is one-to-many then this is the expected behavior. 
You can join on anything, not just PK/FK. Technically you don't even need an equals sign since you are free to use other comparison constructs like &gt; or LIKE. But when you're doing a simple INNER JOIN like you are here, the order doesn't matter. 
Depends on your planned career path. I know a few guys in BI development roles who started out in finance who progressed in skills mainly in Excel before getting into SQL/BI work. If you're looking to make the jump from finance into BI then getting a BI MCSA is going to make the transition into your first BI role easier. Not many job ads for entry-level BI roles will actually list something like the MCSA as a requirement, but finance experience plus a BI MCSA is definitely going to make you an attractive candidate. Database admin is a whole other thing, pretty much nobody starts their IT career as a DBA. If that's where you're looking to end up the path is going to be a lot longer and I don't think a DBA MCSA is going to be right for you.
This is just a set operation. The UNION, INTERSECT, and EXCEPT keywords will probably work. 
Data science positions are a world apart from DBA. If a data scientist gets things wrong, they'll present factually incorrect data to their stakeholders. If a DBA gets things wrong, privacy can be breached, disaster recovery fails, data can be lost, literally a company can go out of business because of a bad DBA. Yes, there are lots of entry level SQL jobs, but very few if any will be relevant to the DBA certification you're asking about.
Would `COUNT(ifnull(loans.loanstatus, 0))` work?
Will `COUNT(IFNULL(loans.loanstatus, ''))` work?
Just tried that and it doesn't seem to have done the trick. 
"I've tried nothing and I'm all out of ideas." 
Try putting... loans.loanstatus = "Default" On your join condition instead of in your where clause.
Oh, I see. You have filtered anything except `Default` in the WHERE clause. I have loaded your table and data into my test db. Give me a few minutes and I will come back.
Do you mean like this?: SELECT applications.actid, COUNT(loans.loanstatus = "Default") AS "defaulted loans" FROM applications LEFT OUTER JOIN loans ON applications.appid = loans.appid GROUP BY applications.actid; If so, the result comes out like this: |actid|defaulted loans| |:-|:-| |1|3| |2|3| |3|4| this query seems to count all loans, regardless of status.
You can use the import wizard in management studio. Right click the destination database, choose tasks, import data, flat file. Follow the steps. You can save the import steps as a dtsx package for future use.
Was thinking... SELECT applications.actid, COUNT(loans.loanstatus) AS "defaulted loans" FROM applications LEFT OUTER JOIN loans ON applications.appid = loans.appid GROUP BY applications.actid;
Wouldn't that also give me the count of all loans, regardless of whether or not they are set to "Default" or not?
I ninja edited it after copying the wrong thing, take a look now.
True. I've applied for many roles requiring more experience than I have. I won't lie and say I've gotten all of them, but something it's more about your overall experience, capability to learn/adapt, and how you carried yourself in the interview that lands you the job
IIF = Inclusive IF, (mentioned in your code) IFF = If and only IF (mentioned in your comment) After learning IFF in math class in University in the 80's, I still always want to use IFF instead of IIF. Glad to see I am not the only one who is prone to that mistake.
 SELECT applications.actid ,SUM(CASE WHEN loans.loanstatus = 'Default' then 1 else 0 end) as "defaulted loans" FROM applications LEFT OUTER JOIN loans ON applications.appid = loans.appid GROUP BY applications.actid;
Google slowly changing dimensions type 3 and type 4. Type 3 keeps limited history and type 4 keeps all history. If you use the MERGE statement you should be able to detect all changes. Suggest having the following fields. ID/NAME/START_DATE/END_DATE/CURRENT_RECORD MERGE based on name and id. If not matched on name and id it must be new combination. Update record for that name END_DATE = GETDATE()-1 and CURRENT_RECORD = 0 INSERT new record with START_DATE = GETDATE() and CURRENT_RECORD = 1. You should then be able to use WHERE CURRENT_RECORD = 1 to find current ID and the START_DATE and END_DATE to see who had which IDs and when. Apologies typed on phone.
&gt;SELECT applications.actid, COUNT(loans.loanstatus) AS 'defaulted loans' FROM applications LEFT OUTER JOIN loans ON applications.appid = loans.appid AND loans.loanstatus = 'Default' GROUP BY applications.actid; &amp;#x200B; That did the trick! Thanks so much!
` SELECT DISTINCT applications.actid, IFNULL(a.defaultloans,0) FROM applications LEFT OUTER JOIN( SELECT applications.actid, COUNT(*) as defaultloans FROM applications LEFT OUTER JOIN loans ON applications.appid = loans.appid WHERE loans.loanstatus = "Default" GROUP BY applications.actid ) AS a ON applications.actid=a.actid `
Beautiful!
Depending on collation SELECT ID, MIN(STATUS) FROM table GROUP BY ID This should return ACTIVE rather than ELIGIBLE where both exist because alphabetically ACTIVE is MIN.
this
no it isn't beautiful the correct solution is the proper use of the LEFT JOIN SELECT applications.actid , COUNT(loans.loanstatus) AS "defaulted loans" FROM applications LEFT OUTER JOIN loans ON loans.appid = applications.appid AND loans.loanstatus = 'Default' GROUP BY applications.actid what's the difference between this and your instinct? changed the WHERE to AND, so that the out join looks only for defaulted loans, not all of them 
Thanks. I learned something from you. Yours is beautiful, too.
Janky but if you use a variable i.e. @Record Select @record = @record + LEFT(@str,@offset-1) + ' union all' Set @record = left(@record,Len(@record)-10) --to remove additionally union all Exec @record 
just to elaborate, when you use a WHERE condition on a column of the right table in a left outer join like you did, three things happen -- 1/ the left outer join returns all rows from the right table (instead of just the ones you want), and then the WHERE condition throws away the ones you don't want -- quite inefficient 2/ you know that left outer joins return rows from the left table even if they have no matches in the right table, right? but those rows (applications with no loans at all) are discarded!! because in a left outer join, the columns which would've come from the right table are all set to NULL, but your WHERE condition requires them to actually have a value 3/ you got `COUNT(loans.loanstatus)` right, because on unmatched rows, and in the correct version the only matches are for applications with default loans, the unmatched rows have `loans.loanstatus` NULL, and aggregate functions like COUNT ignore nulls
Typically, you can join any two columns together from any two tables / views / table valued functions, as long as the data types match. Keys do not define that. However, typically you **want** to be joining tables on their primary / foreign keys. "Primary Keys" are typically two things bundled together in relational databases: 1. A unique constraint enforced on a column (no two rows can have the same value on that column) 2. An index on that column for better performance Similarly, "Foriegn keys" provide two things: 1. A foreign key constraint that ensures a non-null value in the column matches at least one value in the referenced column 2. They can also be indexed like primary keys for better performance Because of the indexes typically included in both foreign and primary key constraints, using primary and foreign keys in your JOINs will give you better performance than columns without keys, because the database will be able to use the index to quickly find matching rows for the join.
To add a little bit here as we've all fixed your query but haven't explained why your original one didn't work... When you put the criteria into the where clause, you're essentially turning your left join into an inner join and eliminating all rows from both tables where the criteria is met. When you put the criteria into the ON of your join, it's evaluated at join time without excluding the rows from the left table, as expected.
In access you need to delete and relink the table. Access will ask to which columns to include in the key (it doesn't matter if you have one in SQL) just choose the columns in Access that make the record unique (it may be more than one required). You can then run action queries against the SQL table. The problem is Access not SQL.
[I've written about this before if you are interested.](https://jonshaulis.com/index.php/2018/07/25/why-and-who-should-become-certified/) &amp;#x200B; &gt; Is this cert even worth it for me? &amp;#x200B; Spoiler about my article, you are the only person who can answer this question. &amp;#x200B; I think certs are a fantastic method to help get your foot in the door, the problem you are experiencing is that most jobs with SQL are not entry based jobs. You may need to start into IT or begin shadowing the IT folk at your job site to get into things. &amp;#x200B; I would begin looking at analyst type jobs, many of them require SQL and can be a stepping stone up into DBA or Engineer or Architect. Since you are in finance, you offer a unique perspective regarding data and information. Look for financial institutions who need someone to help report or base numbers off of. Also don't be afraid to apply for jobs you think are bigger than you. &amp;#x200B; I don't always fit the criteria when I apply for a job, so it's important to address this and say why you are the person they need in your cover letter. Also, begin working with recruiters and see what they can do to help place you somewhere. (Remember, interviewing is a two-way street. You are interviewing to see if that's a place you want to work just as much as they are interviewing you to see if they want to work with you.)
Replace all consecutive spaces with one space. Select replace(replace(replace('string',' ','*£'),'£*,''),'*£',' ') If you replace the last part ' ') with '|') this will pipe delimit the string instead of having spaces. 
George Squillace (he's got SQL in his name)
PIVOT
&gt; Clearly I am missing something Yeah, the article (which is simple to the point of not needing to exist at all) is talking about https://en.wikipedia.org/wiki/Optimistic_concurrency_control Not thread level concurrency of EF or SQL Server.
Wow, 3%. And for some reason I was thinking that MCSA certificates were very valuable. 
Agreed. Honestly certs are all but useless. Knowledge dumps make them a couple hours of studying and that's it.
What role are you in? DBA, developer, or analyst? If you are just running SQL statements/Views then there isn't much difference. Different function names, but that's an easy translation. As a developer the transition is a bit trickier. Postgres is much more strict on return types and they don't really have the SQL Server equivalent of a stored procedure. As a DBA you will be using a completely different tool set. 
Developer /admin
How was the test? Most of it multiple choice? Did they ask a ton of XML or JSON questions?
I never found a good SQL Server to Postgres conversion guide. Rather I'd recommend reading Postgres tutorials and guides. Your existing database knowledge should make them quick reads.
LIKE IN AND 
Those being the logical operators? So how would I structure it to use 3 queries?
 select title, category from books where category = 'CHILDREN' or category = 'COOKING';
&gt;Create three different queries to accomplish this task You do three completely separate queries, each of which accomplishes the task. Not three interdependant queries.
Ohhhh. That makes a lot more sense lol. Thanks!
= is the logical operator like is the search operator in is the "other" operator
Yeah, I had though she wanted me to do all of this in ONE line to get the output, not 3 separate ones. Thanks! 
Lazy question, but what takes the role of stored procedures in Postgres?
Functions. Conceptually a blend of SQL Server Functions and SQL Server procedures. With more strict syntax and compile time return types.
Why are they wanting to go to postgres?
Thank you all for the answers. Really helped me to understand. And if you ever hit NJ joints on me :)
Please share experience about the 70-761 exam. how to pass 70-761 exam? 
Python?
You have plenty of options to choose from! Bulk Insert is a great option, but a simpler way is to encapsulate the values in the excel sheet in parentheses, select those values into a temp table, then either join on that table, or set up a cursor function to run your select for you. Depends on your query.
Define a temp table using those values. Use an excel formula to put the excel data into the correct syntax. 
Thank you ! If I insert them in to temp table, and then if I join them on my other table, will it return all results on the join? As in, the column in the excel sheet contains single unique values but the SQL table contains many of the same entry.. of that makes sense. 
We really need more detail about what the query is and returns. Are you looking to get out 500 different datasets? A dataset with 500 items? Is it like a query with 500 items in the where clause? There's so many ways of skinning this cat.
Ok so the table in SQL contains time recording data, with many columns. One of those columns is called “ matter_number “. The same matter number can be present in multiple rows. In basic SQL I want to run: select * from TABLE A where matter_number = ‘01000’, of which it will return many items. I have an excel sheet with one column in it called matter_number and 500 rows. I want to query my TABLE 500 times with the above matter numbers from the excel sheet and output the results. So row one on the excel sheet might have 20 results on the table. 
Hey everyone, Been working on this quite heavily recently and I think it's in a semi-usable state for others to try. I'd love to get the opinion (and bugs!) of the SQL'ers that might be on this sub. Cheers!
Im in the same boat as you. Passed the 70-761 last year, taking the 70-762 sometime this year. I'm between jobs now (travelling Asia) but will get a new gig when I get back to the US. My last job was basically doing remote support for 200+ production environments. So stressful (why I quit) but I learned waaaaaaay more doing that than studying for the cert. The only reason I took the exam was to hey promoted, and while they gave me more $$ I still had the same or more stress levels so I quit. It wasn't worth it. Bottom line, if I had to choose between my experience and what I learned from the test, I would take experience hands down any day of the week and I'm sure most employers would also. My reccomendation if you really want to learn is download SQL Server developer edition (its free) and set up some test environments of your own. Use vms if you have to, and play woth stuff. Find data sets and make things (break 'em too lol). Pluralsight has a lot of great videos also. I think being able to talk intellegently about things you've actually done IRL carry more weight than beging able to describe the outer apply operator etc. That's my two cents...
It's the splitting an outputting into separate sheets that's going to be the tricky part. Because I am lazy, I would consider whether my users would be happy with just a single Excel workbook where they can apply their own filter. In ye olde days I would also look at a VBA solution: link excel via ODBC to the data source, set up a pivot table, add a slicer for Matter_number and then create some VBA that iterates through the slicer and pastes results into new work sheets. Assuming that's a non-starter, then you'd be best off looking into actual reporting tools. If your data is in MS SQL you most likely have access to reporting services or integration services, either of those could create instanced spreadsheets. And, just maybe, it might actually be easier time-wise (but much more damaging to your ego) to do 500 copy/pastes. Say your matter_number is in cell A2 on downwards. You could prepare your 500 statements quite easily with something like = "SELECT * FROM tablea WHERE matter_number = '"&amp;A2&amp;"';" Paste it down the sheet to get your five hundred statements, run them all at once, and in most environments you'll get 500 separate result sets (usually tabs). When you've finished your pasting try not to feel like you've failed as an IT worker even though you have. tbh this sort of tomfoolery is why god invented interns. Slight alternate version of this: use ="'"&amp;A2&amp;"'," to turn your excel column into a delimited list and paste it into the 'IN' clause, such as SELECT * from tablea where matter_number in ( '01000', '01001', '01002') etc etc. This will return all relevant records, including the matter_number field. You dump that in a spreadsheet and send it back to your user so they can do the monkey work of splitting it into separate sheets. A last option would be to use your data analysis language of choice to manipulate the results and export into separate spreadsheets but I guess if you could do that you wouldn't have posted the question. 
Yes that is correct. If you put it in a temporary table and do an inner join, you'll get every record.
another one (perhaps not as robust, i dunno, i can't see your other categories) -- where category LIKE 'C%I%N%'
Since Postgres 11, there are real procedures as well &amp;#x200B;
Thanks that's my thoughts as well
Costs mostly, Dev pressure as well
Actually I had zero questions on XML, JSON and Temporal tables. Weird. 
It’s not as hard as people make it out to be. Make sure you fully understand the difference between Views, functions and stored procedures. Make sure you understand transactions and error handling. Make sure you know how to use set operators properly. Make sure you know how to use cross apply and outer apply. And most importantly, take the test by eliminating the wrong answers first, so you can narrow the question down to the possible correct answers. 
cross-join to 3 rows (step1..3)
 select mt.* from myTable mt inner join tempTable tt on mt.someCol = tt.desiredCol;
I’m on my phone so formatting ability is limited. You want to use datediff() to find the days between the dates and row_number() over() to find the sequence. Here is some I formatted pseudo code to get you started (I’m on my phone, sorry): SELECT site, DATEDIFF(DAYS, date1, date2) AS interval, ROW_NUMBER() OVER (PARTITION BY site ORDER BY date1 Desc) AS sequence FROM your_table ORDER BY site, date1 That should work with any ANSI SQL including Oracle. 
Hahaha, yeah, I tend to do that a lot. First place I look at if I am using IIF and am returning an error :)
Check out pivot/unpivot functions in oracle. You could query the Deltas between the dates and organize them in rows as the sample data is set up then unpivot it
ahah chill putting a few days on that
&gt; And for some reason I was thinking that MCSA certificates were very valuable. I think the tests are quite valuable, but it really depends on how you define valuable and how you temper your expectations. From a career perspective, it can help you check a box to move past HR, be a deciding factor between a tie in an interview process, or show that you have knowledge in the SQL realm to get your foot in the door. It can also help provide confidence and can definitely help increase your knowledge if done correctly. Most of those points I just listed won't affect someone 5+ years into their career as much as someone wanting to start out. The confidence and knowledge boost at that point are the 2/3 factors that help with your career, the last 1/3 factor is showing who you are. If you have a certificate and that's it, it's not very impressive. If you have a portfolio and the certificate is a piece of that, it is now significantly more valuable. I would also say that replacing the certificate with something else equivalent is just as good. It's showing you are the kind of person who goes the extra mile consistently and puts genuine effort into your career outside of the workplace. Having a single certificate does not illustrate this unless you are just beginning that journey. I searched Database Administrator / DBA and also Database Administrator MCSA / DBA MCSA and MCSE on Indeed, I would say the results match up with what I've been discussing. &amp;#x200B; |Certificate Type|Number of Job Postings for Database Administrator|Number of Job Postings for DBA| |:-|:-|:-| |None|14227|15223| |MCSA|264|63| |MCSE|292|66| Only 685 job postings mentioned MCSA or MCSE, that's basically 2% of the job postings. The other 98% care more about the individual and their experiences (probably), the certificate would be included in that deciding factor. At the end of the day though, people want to work with someone they like who can also get the job done. Certs are valuable, you just have to decide whom it is valuable for and whether it is worth it to you. I think it is, but they are not the only method to improve your career. 
Check out [SkipTheDrive](https://www.skipthedrive.com/jobs/?search=sql&amp;homefindjobs=Search). I'm the owner. let me know if you have any questions.
You need all records from one set and matching records from another set. This seems like an outer join operation. Have you tried this? What was your query?
That sounds like a foreign key.
Yeah, just create a foreign key constraint and it’ll give a constraint error if it’s missing. 
Thanks, will do!
how many complete weeks are there between the zero date and today? remember, if today isn't a monday, there are a few extra days, but we're interested only in the number of complete weeks add that number of weeks back to the zero date and you get a monday
So a datastore essentially? I always start with with a dimension table for dates. Then I can easily grab a day of week, week of year, etc. Then build your fact table. Basically I build it like a datawarehouse. What particular facts do you want to analyze? Sales? So you'll need the customerid, name, maybe zipcode or area code, and then the sales date and data. Flatten out the fact table where it makes sense. The beauty of doing it this way is that the analysts can now connect with SSRS, Microstrategy or other reporting tool without having to do a bunch of joins. If your analysis involves geography or demographics build dimension tables on zipcode. What cities are in this zip? for example. On the demographics dimension (some make this a fact table, the truth is it doesn't really matter in my opinion what you call it) I use census website data. Now I have income information tied to a zipcode, race, education etc. The big question I guess is why not just use the data warehouse? I've done the same thing, and the reason I did a datastore, as opposed to the data warehouse is so the users had less data to navigate, just curious as what your reasons were.
You want to break it down by parameter. SELECT GETDATE() This is selecting the current date. SELECT DATEDIFF(week, 0, GETDATE()) This is returning the number of weeks from 0 (1900-01-01 for SQL server) to the current date. SELECT DATEADD(week, DATEDIFF(week, 0, GETDATE()), 0) This is then adding 6212 weeks (from the second portion) to 0 (1900-01-01). This gets you to the first day of week 6212. I believe your @@DATEFIRST setting will tie into this as well, to determine the 'first day' of the week.
Lets break it apart; SELECT DATEDIFF(WEEK,0, GETDATE()) * datediff(week,0,getdate()) gives you the current week number counting from week 0 (1900-01-01) to today. This week is week number 6212 So in essence your statement is saying SELECT DATEADD(WEEK,6212,0) * From Week 0 give me the date of week 6212, which based on SQL settings begins on a Monday unless you've changed the settings on how your week begins. Therefore it gives 1900-01-21
Damn, that was easy. I think I just had Sunday as the first day of the week in my mind, but Monday is 1 and Sunday is 7. Thanks!
Simple sledgehammer method: Union three queries that each pull one of the intervals. 
I'd also put in some code to determine the server setting for DATEFIRST. If somebody set it to 3, it would return a Wed, instead of a Mon for your code. You are relying on Mon being set as the first day of the week - which it is by default, but some companies might use Sunday or something.
Have you tried 101 instead of '101'?
&gt; From Week 0 give me the date of week 0 + 6212, The command is designed to give a single date, not a rang This is what got it for me. Thanks. Don't know why I stressed over this so much. I think it's sometimes you just write code on auto-pilot and don't think about how it works.
I still don't get results where 101 is absent, the item is there, usually, but not that specific reference. T2.ATSP# in my example below. Here is my query. SELECT T1.PRDNO,T1.PTYP1,T1.PTYP2,T2.ATSP#,T2.ATSIN FROM COLFILES#/MSPMP100 as T1 FULL OUTER JOIN ATMDBCOL/ATSIQ300 as T2 ON T1.PRDNO=T2.PRDNO WHERE T2.ATSP# = '2229' 
Thanks for this. Don’t have time to dive into whether or not you have any other job types besides SQL devs but commenting so I can review later. 
I’m in the same position, got few side gigs using upwork but It requires a bit of luck.
We had a similar discussion about this just a couple days ago - read through this thread and see if it solves your issue. https://www.reddit.com/r/SQL/comments/aier9g/help_joining_properly/
Same results, that's just my habit as many fields are strings in my data.
I've tried to simulate it, but I don't understand what is your logic for the output. Your output: * 1-P-101-Y (it's an inner join, but a previous one is missing 1-P-100-X) * 2-P-null-null (bottom table has item 2, so this isn't inner join) * 3-U-101-M (again inner join and again, there are multiple items 3 in the bottom table) * 4-NULL-101-Y (again, multiple items 4 in the bottom table) I thought maybe you want to join with only the first item, but 1-101-Y is not the first item. Can you explain your output more? 
Actually you are not wrong, check your DATEFIRST settings. SELECT @@DATEFIRST More info here: https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql?view=sql-server-2017
SELECT T1.PRDNO,T1.PTYP1,T1.PTYP2,T2.ATSP#,T2.ATSIN FROM COLFILES#/MSPMP100 as T1 FULL OUTER JOIN ATMDBCOL/ATSIQ300 as T2 ON T1.PRDNO=T2.PRDNO AND T2.ATSP# = '2229' Try that?
understanding how something works makes you able to scale it to your needs.
I know it already has been answered, but try to think like this: I want to optionally (left) join to loans but also loans.loanstatus = "Default" is mandatory (it's in a WHERE condition). So now the LEFT join is really normal INNER JOIN. 
Point by Point, I'd like it to return all cases. *I don't want to return 100, may dataset has hundreds of references per item, only want 101 in this example. *Has item, 2, but not reference 101. I just want it to return blank in that case. This is my current problem. *Yes. Multiple items are part of the problem, there are sometimes hundreds of multiples in my data *Yes, this is as expected. Answer above solved, moving my where criteria into on 
For MS SQL (TSQL) it's the same as the reply from /u/oarabbus. I usually take a hint from data precedence: If SQL is working with two different datatypes, it first converts to the highest common datatype. so if I'm working with float (10th in precedence) and an int (16th in precedence). It has to be converted to the highest one which is a float and so the result will also be a float. Data precedence table is here: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-precedence-transact-sql?view=sql-server-2017
This worked. Thanks so much! Basically moving my where statements to ON with AND
So you want something like this? (Works from SQL server 2008) ;with t1 as ( SELECT * FROM (Values (1, 'P') , (2, 'P') , (3, 'U') , (4, NULL) ) table1(item,value) ) , t2 as ( SELECT * FROM (Values (1, 100, 'X') , (1, 101, 'Y') , (1, 102, 'Z') , (2, 100, 'A') , (2, 102, 'B') , (2, 105, 'C') , (3, 101, 'M') , (3, 107, 'N') , (4, 100, 'X') , (4, 101, 'Y') , (4, 102, 'Z') ) table2(item,ref, value) ) -- Ignore above, just simulating your values SELECT t1.value , t2.ref , t2.value from t1 LEFT join t2 on t1.item = t2.item and t2.ref != 100 
How does commenting save the post somehow? Every time I comment something it gets lost in the hundreds of comments I made 
Certainly DATETIME and not Varchar. If you need extra precision you can go for DATETIME2, but it's also larger. You've also mentioned you have DW but no indexes. While you should be cautious about the number of indexes in OLTP (live database), you can go wild in OLAP. Good indexes make reading data faster, but slow down modification (Insert, delete, update). Since DW won't be modified that much, you can put many indexes in place.
I was originally going to suggest UNPIVOT but that requires us to know all of the distinct date values and that is not exactly what we are looking for. So instead I have a writeup of how you can use a cross join to replicate the records. It assumes there will always be 4 dates. You should be able to filter out records based on null but I will leave that part for you. https://livesql.oracle.com/apex/livesql/s/hu7mxtkyuzjslorr9gxhp89tg 
&gt; 200+ production environments On daily basis? That's just crazy. I mean you worked on the maintenance or development? Also did you know the business side of it? In lots of projects, there is this "inner workings" that you get familiar with if you spend enough time in the project. 
Overcomplicating, I think. Possible values are infinite, I just want to return whatever is there from T1, T2 only on a specific secondary reference, but also records from T1 if there is no match in T2 at all. Moving where criteria to ON worked perfectly.
It really depends on the way you assign the users, how are you using transaction and what is your isolation level. If you check worker_id in one statement and update in another, you need serializable (the highest isolation level), but your concurrency might go to hell due to all the locking. If your UPDATE statement includes the check for empty worker_id, then DB default (either READ COMMITTED or READ COMMITTED SNAPSHOT) is enough. UPDATE dbo.assignment SET worker_id = @currentWorker WHERE worker_ID IS NULL
Yes, this is exactly it. Thanks!
Great, glad I could help
Welp. I can look back in comments. I spelled it out that I commented for this particular reason. So there’s that way. Not the best way. But one way. 
So you are looking for DATEADD(YEAR,1,timeColumn). I would also change your WHERE condition condition to something more robust. &gt;= StartDate AND &lt;= EndDate. Or at least DATEPART(YEAR, timeColumn) = 2018
You can also just click "Save". 
No problem. FYI, there's a bot within reddit called RemindMe. You can use it to remind you whenever you'd like (i.e. 2 hours, 1 month, etc.). More info in it can be found [here](https://www.reddit.com/r/RemindMeBot/comments/2862bd/remindmebot_date_options/).
No problem :)
Wilcards don't work in a SET statement. So the db thinks you want to use the value "%19-%" as a date. Which it obviously isn't. 
My first question was 'why not simply do json/xml'... and then it became a circular reference.
Sorry I didn't see this sooner. I was in a company that had an IT department, with multiple empty chairs in the DBA group. So learning for me was 'jump in the fire'. Coming at it cold, without fellow dba's around, I'd suggest what you're doing, plus perhaps look at Udemy. They always have a 'special' 90% off deal, but it does look pretty solid for 10 or 12 bucks a course. https://www.udemy.com/topic/sql/ 
This is probably a great use case for CLR... pass the string to C# managed code and return the parsed results... https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/introduction-to-sql-server-clr-integration
i'm not sure what the problem is. Does your varchar(max) contain a valid xml? What is stopping you from converting it to xml if it is not valid? If it is valid or can be made valid with certain modifications, can you write an xquery to get the data that you need?
You would need to show us the cursor to help you decide what else we can to speed it up. Cursors in SQL Server are not optimized when you go with the default options. On a side note, potentially changing your temp table to a physical table and changing the cursor to a loop or changing the cursor settings may help you here. (Or maybe not.) I would suggest to see if you can use [pastetheplan.com](https://pastetheplan.com) to share the plan with us.
Oh man I missed the other columns. If you can unpivot your data my method will work for you. Otherwise the solutions using CROSS JOIN are probably what you need. 
Cast as XML, use XML functions (.value(), .query(), .nodes()). And don't use the cursor for the set based operations if you can. Simple select, CAST and Cross/Outer apply column.nodes() should do the trick. Also, investigate if there is some locking and blocking going on. 12 hours is too long for mililons of rows, let alone thousands. 
Sometimes the query is not the issue. If you dont have a cluster key (never cluster the primary key or an identity) in your table, and you dont have multiple files/filegroups, and you are not indexing for the where clause.... you're gon a have a bad time. Always use declare - cursor local fast_forward if you dont have joins in the FOR and you are not updating the table you are cursoring. Else. Local static read_only forward_only. Make sure your physical model and indexes are separated and optimized before you optimize the query.
Wut. Cursors over joins? That's sadistic
I thought like that for years. But when you realize (or were taught) that a join is just a cursor at its lowest level... you change your mind.
You guys should check out the UpWork app or freelancer. They have a variety of different roles on there, almost all can be done remotely👌
Which gives me another idea.. maybe don't use the xpath/xquery in the FOR. Do it in the while. Maybe you're getting a delay as it's trying to temp the data set, work with the data as a single row. You're in a cursor. Null your @s, rip the XML apart, do what you want with it.
[This will probably explain](https://www.periscopedata.com/blog/sql-query-order-of-operations) everything you need to know. 
I could but I use the save function for very specific items. I have to do that because there are no organizational capabilities provided. 
When I'm troubleshooting a query for efficiency I generally start by finding out where time is being spent. How you do this depends on the query. You can run the process and include the execution plan and view it after. You can place print timestamp type code within the script, after each operation and review it after it runs. You might be able to use SQL profiler, personally I've never used it, but I believe it provides the same or better utility. I'd assume the issue lies with the cursor(s) as every stored procedure/ job with runtime issues has always been a result of cursor use (or abuse). I've taken 3 hour runtime stored procedures and knocked them down to minutes just by eliminating cursors. Now that's not to say that the cursors couldn't have been written more efficiently but you need a proper use case to justify them.
Yes thank you. I wouldn’t use that functionality for this purpose because I want to have a decent amount of time to spend on it and I don’t know when that would be. 
A cursor and a loop are basically the same thing, with cursors being used for alpha-numeric values, and loops for integers, right? I never use cursors and just populate an auto incrementing ID column to use for the @LOOP. Are you saying that one process that is a cursor, and one process is a loop, where both processes produce the same data set... one could outperform the other? I imagine this would be the case, but curious. 
So you get a file with (1) column that has 6000 rows in it, and each value in that column is thousands of characters long and includes a number of pieces of information, and you want to extract 5 of those into new columns such as: | ATM | Transfers | transtype3 | transtype4 | transtype5 | | :--- | :--- | :--- | :--- | :--- | | 150 | 500 | null | null | null | How can you tell whose account that line relates to? Beyond that is this an accurate description?
www.flexjobs.com
All of my previous statement and my next statement are related to SQL Server specific. A cursor builds a construct under the hood in SQL Server, loops actually do this too but to a lesser degree. You can use cursors or loops interchangeably until you can't. To continue my explanation, it needs to be stated that set based SQL is almost always the best practice solution unless the needs or performance dictates otherwise. Most people use loops and cursors when they shouldn't, but if they did, they would probably be able to do a loop or cursor interchangeably. The cursor gives you significantly more control and power, but this is also the problem with cursors. If you don't specify and change the settings and just use the default options, it's usually going to perform poorly. If you modify and tweak the settings, they could out perform the loop. It just all really depends on the scenario. Here's a few good reading materials on this subject: https://sqlperformance.com/2012/09/t-sql-queries/cursor-options https://sqlundercover.com/2017/11/16/sql-smackdown-cursors-vs-loops/ https://stackoverflow.com/questions/3022965/which-is-faster-in-sql-while-loop-recursive-stored-proc-or-cursor https://www.techrepublic.com/blog/the-enterprise-cloud/comparing-cursor-vs-while-loop-performance-in-sql-server-2008/ 
Curious. So you adopt the 6nf model at design, does your opinion on cursor efficiency change if you're thrown an existing production database and redesign isn't an option due to the app tiers, of maybe it's a vendor/third party application? This is honestly the first time I've seen normalization talked about so specifically, so for all I know 6nf could be standard since 2003 (did some sloppy Google Fu). I've designed relational databases before but did not know these forms were a thing. I figured reducing data redundancy was just a common thing.
Yes. Legacy mentality = surgical edits. Best not to redesign and create bugs. Optimize the physical layer because nobody does. New development on legacy systems is where I start to implement master data management concepts. From the Beginning - Master Data Management in SQL: https://www.youtube.com/playlist?list=PLPI9hmrj2Vd8m_w3By7pI7xlkXMRzNYzS
Thank you for putting this together. 
My pleasure
Maintenance mostly. Yea it was insane, but I stuck with it because I needed the experience. The company worked for did automation/material handling software and was a small company at first that blew up fast due to current demand for the tech. It was like the wild west. People could call 24/7 and we had to suppport their systems. Think like fortune 500 distrobution centers. I was literally the only guy on my shift supporting software for huge warehouses. Couldn't get developers after 5pm. The past few years I've seen anything and everything that could go wrong. Very little documentation and old custom environments, so a lot of times I had to dig into source code by myself to find out the business logic of systems over a decade old. Mostly c# and c++. Basically reverse engineer systems I had never seen before
My friend had a crazy project. Used to work 12-14 yrs. Won't recommend that ever. Most of people I know who worked that many hrs, had health issues. I usually do certifications to keep up with current stuff. Rarely do I get to do the cool stuff on my job. 
IMO certification do tell you the best practises. And a candidate who has knowledge along with certification will look better than one without it. When I say certified I mean someone who knows the in and outs of the particular subject. A lot of times, working on projects you lose touch with new stuff. And some times you work on crappy projects. Certification is way of clearing those things. 
It's not easy to get a junior developer position without a degree or working experience. Your history is likely what is hurting you, 3-4 months self study really isn't enough to jump into a job *unless* you can list the projects / link to your GitHub /link to your online portfolio to show things you've worked on. It's all about work experience or a degree. I've only had a programmer title for a year and a half, prior to that I worked 4 years in application support which gave me the work experience on top of schooling. I started doing call center tech support at that job before promoting to operations and app support. It's much easier to move internally within a company, might be better to get a foot in the door as support and move your way into a developer role, won't be easy but it's possible.
I would recommend to post a resume example of what you have been sending out and cover letter example too, but with personal information redacted. This can help us critique what you have. &gt; i feel like im good enough to work as a data analyst or junior developer You should look up imposter syndrome, it is very real. My first job in IT was a jack of all trades database application administrator type position. No SQL experience prior, but I did work at a call center where they would provide experience over pay. (This worked out well for me since I had no degree and two years previously I was a manager at McDonald's.) So my first IT position made a whopping $34,500 a year but quickly moved up the ranks afterward. &gt; i have two positions. One as a computer technician for 5 months and a McDonald manager for 8. What can i put on my resume to make it more appealing to hr? Regarding McDonald's, I'd highlight some key areas related to the job. * Consistency * Reliability * Leadership * Ownership Highlight stories that illustrate those traits but also show success and validation of your job. Here's an example: &gt;I led team building exercises on site which helped improve morale of our employees, creating a 10% increase in employee retention saving the company $25,000 annually. With your technician piece, I'd try to tailor the stories more directly to the job posting and see what is relatable. Something to keep in mind is that you will want to be looking for an entry level position and there are not many entry level jobs with SQL. Most folks will start through help desk positions and climb their way up. Analysts and junior development roles can be entry (even though they may say requires 1-3 years experience) and may work. It really just depends on whether you have the skill-set that matches their expectations and you are someone they want to work with. If you aren't getting interviews, it's how you are applying for jobs / your cover letter / your resume. If you are getting interviews but you aren't getting offers, I would continue to invest in education and experience as possible. I would also recommend to begin networking at local meetups and recruiters, that is going to help you significantly. (Even if not now, in the future.)
Would you mind if I were to pm you my resume instead?
I never thought about it but I could build a GitHub portfolio. Thanks for your input!
I understand you "shouldn't" use loops/cursors, but realistically there are plenty of examples where they must be used. When you say that they can use them interchangeably until you cant do you mean in terms of performance, not functionality? Thanks for the links. I'm a little lost by the Smack Down one because it's example of a loop looks a little strange to me, here is a sample of one I use: DECLARE @ClientList TABLE ( ID int identity(1,1) , Client nvarchar(255)) INSERT INTO @ClientList SELECT DISTINCT Client FROM dbo.mpTable DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @ClientList) BEGIN DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop) INSERT INTO dbo.tabTable SELECT * FROM dbo.dataTable A WHERE A.Client = @Client SET @Loop = @Loop + 1 END 
No problem. Yeah, somewhere you can show off your personal projects and school projects. Then make sure you're well prepared to demonstrate SQL on a white board if it comes to an interview. You may also want to consider reaching out to recruiting agencies, they can help sell you and get you the interview at least until your resume is validated by experience.
&gt; I understand you "shouldn't" use loops/cursors, but realistically there are plenty of examples where they must be used. I agree, every tool has its purpose. I just wanted to clarify that best practice is to not use cursors or loops under most circumstances if a set based approach exists, but that's why we always say "it depends" right? I just needed to state that because most people who do use cursors or loops when appropriate, they may not know which one to use or why they should use one over the other which is not cut and dry. This is different from using a cursor or loop as a hammer and treating every problem as a nail, at that point it really doesn't matter whether you picked a cursor over a loop because nothing is written to best practice regardless. &gt; When you say that they can use them interchangeably until you cant do you mean in terms of performance, not functionality? I mean both, cursors have a HUGE array of options and tuning tweaks that you don't get with loops. Here's a few other really good articles I found too, especially the Bertrand one. [https://sqlblog.org/2012/01/26/bad-habits-to-kick-thinking-a-while-loop-isnt-a-cursor](https://sqlblog.org/2012/01/26/bad-habits-to-kick-thinking-a-while-loop-isnt-a-cursor) [https://www.brentozar.com/archive/2018/11/the-curse-of-cursor-options/](https://www.brentozar.com/archive/2018/11/the-curse-of-cursor-options/)
That would be fine, I would still personally recommend to redact personal information.
I wrote this before elsewhere and apparently was very helpful, so here it goes: Postgresql is awesome. What you'll get after mssql is a formidable infrastructure of functions and types, that will let you operate with pretty much everything; ability to define your own types, and, best of all, an awesome extension infrastructure. Really, it's just out of this world. From GPU queries with pg-strom to mathematical statistical functions of any kind, selecting from any source (file, rest api, etc). And it works great on all operating systems. What you'll be losing is the ability to just write procedural code, return anything anywhere and mix variables with queries with loops and so on. You'll also lose the seamless inter-database queries and the ability to write any silly query and for it to work with the same speed. Postgresql has a picky planner. `where x in (select c from t)` is very different from `join t on c=x` in terms of speed. You'll also lose collations, and case insensitive text comparisons. Think of Postgresql as a database engine for a control freak with a lot of offered possibilities. You don't write procedural code all the time. Only when you say so, and you get to pick the language (plpgsql is the most common, but I've also used python and Java). Every output has to be planned, declared, specified. Partitioning works best when you properly control which route goes where, and which partitions you want to read. The best way to learn it is to work with it, and the basic Postgresql documentation is superb. Just make sure you use the bleeding edge, and use that documentation as well.
[Here is an example of a database Github if you wanted to see one.](https://github.com/Jonathanshaulis) I use this as a piece of my portfolio, I also like to write an article regarding the pieces that are on my Github so there is some context. 
I'm usually not a person to recommend Certifications, but lacking a degree or work experience, it might help you get a foot in the door. The MTA in T-Sql fundamentals is only a single exam ([https://www.microsoft.com/en-us/learning/exam-98-364.aspx](https://www.microsoft.com/en-us/learning/exam-98-364.aspx) ) and is relatively affordable. The books to get ready for it are cheap, and a spending 30 bucks on self study material will allow you to figure out if this is really something your interested in. From there you can branch out into other areas of interest, such as database administration, business intelligence, and database development. Microsoft sql server isn't going anywhere, and while it's not as sexy as things like hadoop, or Postgres, a lot of companies use it, and need more data dogs (a half joking term for data analyst who fetch information from DBs for others). Development is harder to get into, because it's a lot more than just writing queries, so I'd definitely start with analyst positions first. &amp;#x200B; To add an unasked for bit of advice, software testing generally has a lower bar to entry, and knowing sql is usually a big plus. The pay is decent, and while it's not as glamorous as development, there are a lot more openings at the entry level. In fact the demand for testers is really high at the moment, because a lot of people use software testing as a stepping stone into software development. Anyway good luck with your endeavours! 
I believe the save function has been updated to allow you to create categories.
&gt; jobs.Job_ID = employees.Job_ID It's a part of your correlated subquery. This subquery will be executed per each record for the derived table of your 'from' clause (the result set of all joins executed). In your case it is going to be a single table, employees. Since it is executed in the context of a "base result set" record, it can access any fields of this current record (as usual, it's best to use table aliases). The subquery will return a result set (list). Current value of salary will be checked against that list. If condition is satisfied, the record will be included in the output result set. "Select" list will dictate what columns the output result set will have. The end.
Where I'm somewhat lost is that my process uses dynamic SQL in a loop and there are some comments at the bottom of that first link which suggest a cursor would be no more or less efficient... I'll put the full code here for you: ALTER PROCEDURE [SLA].[095a_CycleTime_1_Calc_20](@CalcID int, @SessionID int, @IncrementID int) AS BEGIN TRY BEGIN TRANSACTION SET @IncrementID = (SELECT MAX(IncrementID) + 1 FROM SLA.logProcedure WHERE SessionID = @SessionID) DECLARE @SLAType nvarchar(255) = (SELECT SLAType FROM [AnalyticsMapping].[SLA].[mpCalcs] WHERE CalculationID = @CalcID) DECLARE @ClientList TABLE ( ID int identity(1,1) , Client nvarchar(255)) IF ( SELECT SUM(CAST(isTrigger AS int)) FROM [AnalyticsMapping].[SLA].[mpClients] A INNER JOIN [AnalyticsMapping].[SLA].mpCalcs B ON B.CalculationID = A.CalculationID ) = 0 INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID ELSE INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE isTrigger = 1 AND CalculationID = @CalcID DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @ClientList) BEGIN DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop) DECLARE @TempSource nvarchar(255) = (SELECT TempSource FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @SegmentID nvarchar(255) = (SELECT SegmentID FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @ClientProfile int = (SELECT ClientProfile FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID AND Client = @Client) DECLARE @Clause1 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 1 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause2 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 2 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause3 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 3 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @Clause4 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 4 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @SQL nvarchar(MAX) = ' SELECT CASE WHEN B.SLALabel IS NOT NULL THEN B.SLALabel ELSE ' + '''' + @SLAType + '''' + ' END AS ''SLAType'' , A.DateRange , ' + '''' + @Clause1 + '''' + ' AS ''IDType'' , ' + @Clause1 + ' AS ''ID'' , A.Client , A.Country , A.Region , A.LaborTypeCase AS ''LaborType'' , B.Numerator , B.Denominator , B.MeasureQuantity , B.MeasureDescription , NULL AS ''Goal'' , NULL AS ''isFinancial'' , NULL AS ''Red'' , NULL AS ''Orange'' , NULL AS ''Yellow'' , NULL AS ''Chartreuse'' , NULL AS ''Green'' , NULL AS ''Threshold'' , NULL AS ''ComputedMinutes'' , 0 AS ''isContractual'' , ' + '''' + CAST(GETDATE() AS varchar) + '''' + ' AS ''Mod_Stamp'' FROM SLA.veDate A LEFT JOIN ( SELECT * , CASE WHEN ComputedMinutes &lt;= MeasureQuantity THEN 1 ELSE 0 END AS ''Numerator'' FROM ( SELECT DateRange , ' + @Clause1 + ' , A.Client , Country , Region , A.LaborTypeCase , ' + @Clause2 + ' , ' + @Clause3 + ' , dbo.NetworkMinutes(' + @Clause2 + ', ' + @Clause3 + ') AS ''ComputedMinutes'' , 1 AS ''Denominator'' , MeasureQuantity , MeasureDescription , B.SLALabel FROM ' + @TempSource + ' A LEFT JOIN [AnalyticsMapping].SLA.mpLaborMappings B ON B.Client = A.Client AND B.SLAType = ' + '''' + @SLAType + '''' + ' AND (B.[Labor Type] = A.[Labor Type] OR (B.[Labor Type] IS NULL AND A.[Labor Type] IS NULL)) AND (B.LABOR_TYPE2 = A.LABOR_TYPE2 OR (B.LABOR_TYPE2 IS NULL AND A.LABOR_TYPE2 IS NULL)) AND (B.LABOR_CATG = A.LABOR_CATG OR (B.LABOR_CATG IS NULL AND A.LABOR_CATG IS NULL)) LEFT JOIN [AnalyticsMapping].SLA.[mpGoals] C ON (C.SLALabel = B.SLALabel AND C.Client = A.Client) OR (C.Client = A.Client AND C.SLALabel = ' + '''' + @SLAType + '''' + ' AND B.SLALabel IS NULL) OR (C.Client = ''DEFAULT'' AND C.SLAType = ' + '''' + @SLAType + '''' + ' AND A.Client NOT IN ( SELECT DISTINCT Client FROM SLA.mpGoals WHERE SLAType = ' + '''' + @SLAType + '''' + ' AND Client &lt;&gt; ''DEFAULT'')) WHERE A.Client = ' + '''' + @Client + '''' + ' ' + @Clause4 + ' AND A.SegmentID = ' + @SegmentID + ' ) Z ) B ON B.DateRange = A.DateRange AND B.Client = A.Client AND B.Country = A.Country AND B.Region = A.Region AND B.LaborTypeCase = A.LaborTypeCase' INSERT INTO [AnalyticsMapping].[SLA].[tabDashboard] EXEC sp_executesql @SQL INSERT INTO [AnalyticsMapping].SLA.tabCalculations SELECT @Client , @SLAType , @SQL SET @Loop = @Loop + 1 END COMMIT TRANSACTION END TRY BEGIN CATCH -- Determine if an error occurred. IF @@TRANCOUNT &gt; 0 PRINT 'Error found!...' + OBJECT_SCHEMA_NAME(@@PROCID) + '.' + OBJECT_NAME(@@PROCID); -- Return the error information. DECLARE @ErrorMessage nvarchar(4000), @ErrorSeverity int; SELECT @ErrorMessage = ERROR_MESSAGE(),@ErrorSeverity = ERROR_SEVERITY(); RAISERROR(@ErrorMessage, @ErrorSeverity, 1); END CATCH; My performance is fine, and it runs very quickly so I'm not trying to "improve it" (per se), but I'm interested in the topic and understanding whether a cursor would be better, or worse.
&gt; My performance is fine, and it runs very quickly so I'm not trying to "improve it" (per se), but I'm interested in the topic and understanding whether a cursor would be better, or worse. That's honestly the hard part. If I don't "need" the options for the cursor, I tend to lean towards a loop. If I feel like I need more options or control, I'll lean towards a cursor. At the end of it all, it greatly depends on **everything**. (Statistics, indexes, SQL Version, trace flags, query plans, hardware, etc) The best advice I can give to figuring it out is to switch it to a cursor, try flipping some options based on research, and see what the query plans spit out. Ultimately though, if it does not create enough of a "win" to tune, I'd focus on other projects. From a purely educational or curious perspective, the above is still the best option for figuring out which is best. The SQL optimizer is a mysterious beast and there are so many factors here that I can't give you good insight with the query above one way or another without plugging it in and comparing execution plans. 
For this particular process I have (tried) to stream line things, so all the @'s are touching indexed fields, or primary keys. All the fields on the larger tables are indexed, etc. I've been told in the past that this is how you "trick" SQL into more or less having (some) of the options available that a cursor would have, and that the functionally speaking both approaches are the same. I have never taught myself to write a cursor because I just use a LOOP, and I've never tried a LOOP for something that didn't need to be a LOOP, and therefore have never really had any performance issues. I have a chain of 43 sprocs that cycle through about a dozen loops, and each loop cycles through a variety of clients. Total process takes only a few minutes. The effort to research this would not be worth the time in terms of performance. &gt;Ultimately though, if it does not create enough of a "win" to tune, I'd focus on other projects. From a purely educational or curious perspective, the above is still the best option for figuring out which is best. The SQL optimizer is a mysterious beast The people who taught me to use SQL were fans of forcing SQL to behave in the way they wanted it to, for example they would take a very complex query with many sub-queries and joins and break it down into a series of #tables that construct, and they taught me how to deconstruct and rebuild queries in such a way as to force the optimizer to do what you wanted... based on testing it. This approach was not used for loops, but we only need loops for very specific things when we're creating a model... and they don't need to run very often so it really doesn't matter if the LOOP takes 5 minutes or 12 hours if it only needs to run once a month. Start it on a Friday afternoon and its done on Monday morning. 
As written, this is total insanity. Out of curiosity, what could you possibly mean by "a join is just a cursor at its lowest level". Start with the "lowest level", please?
How does a join work at the engine level? Lets say you have two data sets of 100 rows, DataSetA and DataSetB. When you join, you are comparing 1 row from A, against 100 rows from B. Inner, outer, full, apply... same thing. This is after one join. Now before I proceed, understand that context of the query is very important. The tables in my CORE database represent an abstract concept with a 3 to 4 degree taxonomy attached to it. The people table would not have any columns for a first or last name. The peopleName table would have one name field and a nametype of first, last, middle, middle initial, full, full first name first, full last name first, etc. My ATOMIC database has a AtomicPeopleName_01 table with 50 groups of 8 or so columns. This is the flattened version. So first name could be in strName_01 or it could be in strName_46. It doesnt really matter because there is a relationship that will get you from core to atomic and you can build a human readable view from that. Core is meant to be an ODS if you will. Atomic is a type 1 scd, and Warehouse would be the type 2 and 3. So in core, I can accomplish anything using cursors and recursive procedures that return JSON or XML to the application. This allows me to ,in the Fetch, check to see if there is any security, display translation, time sensitive access, etc. To do that in a join would crush the system. In my atomic database, that spits data out to Reports and Warehouse etc... this data can be picked up and aggregated into more purposeful data, still in a cursor. The idea that is lost on the use of RDBMS is using bulk processing instead of controlled lineage and destination. The cursor allows you to slightly modify the single record that is in memory, @parameter. Instead of waiting for multiple joins to get back to you, that you may have to dump into temp via cte or @/#/## tables for further processing. Due to the normalization techniques, an edit can get to the reports in under 100ms. And once again, through recursive procedures and/or cursors, we can reconstruct any report for any... ingestion? Cursors are faster speed wise, development wise, debug wise, expansion, customization... than a join will ever be... IF you look at the data management system as a data management system... and not as a single business requirement fulfilling item.
Wow thanks for the info
That’s great advice actually thank you. I actually was prepping to take the oracle 1z0-071 exam. I’m most familiar with Postgres and oracle. A lot of people told me that certifications don’t really matter so I decided at the end it wasn’t worth $250. I still did take courses geared for passing the exam. I can’t afford to go to college so I may reconsider taking the oracle certification. I know that transferring between oracle and Microsoft sql isn’t hard but it still does take time and I’m really trying to get my foot in the door. Is it worth learning Microsoft sql or just sticking with oracle?
MS SQL Server 2012. Not sure if this makes any difference, but I'm doing all of this through SQL Server Management Studio Version 11.0.5556.0 on Windows 7 Enterprise. 
I hire people without job experience all the time. It's based on the work of yours that I can see. If you need to do work and aren't hired, do some open source, and put it somewhere that I can see it.
Your site is great!
If you're ever hiring an intern let me know.
I'm not too well versed in xpath stuff, I've used it a few times but never really internalized it. Based on the OPs description I was thinking it might make more sense to try and deconstruct each piece of information into a table and then join those tables up. For example, if he has one huge strong he might write a very simple query that will give him a key, and then take (1) piece of information. Then another query to generate the same key, and the next piece of data. Then another query to do the third piece, etc. Then after just join those together on the key? If there are only 6000 rows you might write a function that takes each row and "pivots it" into multiple columns based on every &lt;space&gt; (simple example) and then just pull out the nth row which is where you should always find the &lt;value&gt; that you're looking for. I'm not sure I fully understand the problem, but if there is only 6000 rows then it really shouldn't matter how long the string is. Just take the string and break it into rows with a key. One in a format like that it shouldn't matter.
I totally agree with this. Normalizing data is kind of what we, as data professionals, are supposed to do. But usually, for optimization, schema changes might effect deployment so it's full on optimization mode.
To add an ELI5 to this, because this is a common question. Basically SQL server excels at being an 'Oracle' (joke intended). You write your query to say "hey I want data from here and I want it to look like this when you're done" and SQL server makes the decisions on HOW that data will be processed. When you use a cursor, you are being explicit in telling the server HOW you want it to be done(e.g. first take this data set, then take this data set). When the best answer might have been an entirely out-of-order set of operations based on an indexing method you didn't even know about. Every time you decide to use a cursor, you need to think to yourself "Do I think that I am smarter than SQL server?" If yes, use a cursor, if no, re-write it to be set-based.
&gt;schema changes might effect deployment so it's full on optimization mode. No offense, but what in the actual fuck are you talking about here? I hear people say stuff like this at work all the time and I generally interpret it as a bunch of nonsense words strung together to sound smart and then think less of them. Can you give me a concrete example where file or table is being consumed by a process to create a new table, with 6000 rows, where first breaking the rows down and assigning a key and then dumping the data into the table would effect deployment? I honestly don't even understand what the word "schema" means, other than to somewhat indicate that you're talking about how to name things organizationally.
Complex text manipulation isn't SQL servers strength. Can you use SSIS and scripting to do the parsing?
I don't even necessarily need to know how SQL does any of that... Your description of how a JOIN works and how it is, at its heart, the simplest description of a CURSOR or LOOP is a very basic computer science principle that I can see. There would be no other way to architect it, in any capacity, in any language. Mathematically it has to be that way. It's like saying addition is really the simplest form of multiplication, or that there is no difference between addition and division. Just because you write a *, a ÷, or a -, doesn't mean anything... they're all the same as using a +. This general discussion is leading me to think a LOOP *can be* the same as a CURSOR, but you would have to structure your data in specific ways to *force it to be*, whereas there are native options for cursors that are not available for LOOPS. However, that is only relevant if you want to let the Optimizer help you, and not to force it to behave a specific way, correct? &gt;hen you use a cursor, you are being explicit in telling the server HOW you want it to be done I wrote all that before I read this line. I can force it to do whatever I want with either a LOOP, or a SUB-QUERY. That was literally how my mentors taught me to write SQL. &gt;Every time you decide to use a cursor, you need to think to yourself "Do I think that I am smarter than SQL server?" If yes, use a cursor, if no, re-write it to be set-based. I will always answer this question with a YES, but most of the time I'll use JOINS even though they're functionally the same. This is for readability, and if I can accomplish my goal in a reasonable amount of time there's no reason to over complicate anything. My only point, which I'm sure you know, is that there are some things which are impractical to accomplish using "set theory" where you absolutely must use a LOOP (or CURSOR) -- and a CURSOR is the lazy way to write a LOOP where you have a ton of options built in, but with a LOOP you can just force it execute precisely how you want it to execute and test multiple different methods until you demonstrably can prove that one is better than another, and force SQL to always do that instead of what it thinks is best.
WHAAAAAAA? 
Ooh this is a nice site 
Schema = model... I think people around me have used schema so much that... that's what I mentally sub out Model for. I used to only know the word schema to mean the namepart before the table name... but it's just a generic word to represent the word model. As for why I wouldn't go directly to another table creation... 1. It's just another table to worry about when deploying out to X production servers. 2. If we create an @ # ## we are flooding tempdb and are not using ram to get the speed. So now there are pagelatch waits and disk bottlenecks. Only @parameters are stored in memory (except for actual memory tables). So if you have an @xml (not a table) you could select @1 = value.()... from @xml... and still keep it in memory. 3. I'm assuming that there isnt an Id that is repeated in the table (my video SpecificDataSetNumber), for physical separation... let alone multiple filegroups / files for domains of data and cluster,blob,column,indexes (my video 002 - files) ... I agree that it should have already been separated though. Then this problem wouldn't exist. A simple procedure FOR XML with good normalization would make this issue nonexistent. 
Oh dang, you even populated the search field with "SQL". Nice.
In general people are right about certifications, employers prefer experience or traditional education, but when it's the only affordable path forward, it's better than nothing. Someone might be willing to take a chance on someone with a cert if the price is right, but autodidacts without education or experience, probably less so. Oracle, Mysql (owned by oracle, and mildly mismanaged by them as well), and microsoft sql server are the top 3 right now, and outside of answering general questions on stack exchange I haven't done much oracle work. Oracle seems fine for a DB system, and have a lot of nice to have features that are better than microsoft sql. However microsoft sql is a piece in a much larger ecosystem, with things like power BI (decidedly meh, but very cheap), sql azure (sql in the cloud which is Serviceable and MS is pushing this one hard), and several other offerings. Of course I'm sure oracle has its own ecosystem, of which I've only heard about their cloud offering because it's not great. Of course I work in a MS dev shop, so I might be a tad biased. Check your local listings on dice and see what's most common in your area, and then target that platform. 
The word model to me is also a nonsense word, kind of. Like, I write models for a living. I don't mean the word model in the same way you do, I mean I take a bunch of data and write a model that predicts something. I suppose you and others use the word "model" to represent how the data is "structured" -- which is a totally arbitrary term to me because all I want is for you to give it to me so I can put it in a "structure" that lets me "model" it. When I hear the word "model", "schema", "snow flake", and "normalization" it really kind of makes me roll my eyes. I realize of those terms that normalization is by far the most important, but to me they are all the same words, for the same thing, and the only thing you're ever really going to get out of that conversation is a savings in size, your ability to "load" the data, "save" the data, "secure" the data, "backup" the data, etc. -- All of which are noble pursuits, but none of it really is going to "use" the data unless you are designing systems that are functioning two ways. For databases that only function one way (i.e. they only accept / load / consume data) the "use" of the data is either preservation, or analytics/modeling. The tools which are generally talked about as being "more appropriate" than SQL will not let you really get in and customize things. Plus a lot of them require the data to be structured in a specific way in order for it to be consumed efficiently by that process. Tableau, SPSS, or SAS are great examples here. Neither one really give a fuck about how you use the word schema, or normalization, and you will have to break normal form, or introduce other "inefficiencies" in order to use them efficiently. To me schema just means names, and does not describe relationships (per se) because relationships are unknown or only partially known at the time the data arrives. So maybe schema to me represents more of an "evolving" term for a "model" -- where you need to build the model agnostic to the schema in order to design the schema. &gt;It's just another table to worry about when deploying out to X production servers. So you're objection here is for two way systems, not a system like I'm describing? &gt;If we create an @ # ## we are flooding tempdb and are not using ram to get the speed. So now there are pagelatch waits and disk bottlenecks. Only @parameters are stored in memory (except for actual memory tables). So if you have an @xml (not a table) you could select @1 = value.()... from @xml... and still keep it in memory. This is where I like to calculate how much RAM and space cost while calculating how much it costs relative to the salaries involved to have this conversation. For me to effective work in the capacity described above I need a very "dedicated" environment where I'm only having to coordinate with immediate coworkers for resources on the server. I believe you might call this a Data Lake? I think that's a bullshit term because it just seems to describe a non-production database where I'm allowed to go fishing, and nothing more than that. &gt;I'm assuming that there isnt an Id that is repeated in the table (my video SpecificDataSetNumber), for physical separation... let alone multiple filegroups / files for domains of data and cluster,blob,column,indexes (my video 002 - files) Why would it matter if an ID is or isn't repeated, but more importantly why would the assumption that there are no repeats make you less inclined to dumping the data into a #table, @table, or dbo.table? &gt;I agree that it should have already been separated though. Then this problem wouldn't exist. A simple procedure FOR XML with good normalization would make this issue nonexistent. My way of thinking is if the row count is only 6000, then lets assume the sum of all the characters in the field is 15000 * 6000 = 90M. Now 90M is a fairly big number, but following your example of all joins being cursors you could arbitrarily deconstruct that down individual rows. So we're talking about an upper threshold of 100M rows derived from 6000 rows, if each string is 15000 characters long. Simple writing a cursor to create a structure such as: | Key | Character | | :--- | :--- | | 1 | H | | 1 | E | | 1 | L | | 1 | L | | 1 | O | | 1 | | | 2 | W | | 2 | O | | 2 | R | | 2 | L | | 2 | D | | 2 | . | This is over complicating things I think, but I imagine this would execute fairly quickly. From there you could just write an algorithm to look for all given sequences of values such as: | Key | Character | | :--- | :--- | | 1 | A | | 1 | C | | 1 | C | | 1 | O | | 1 | U | | 1 | N | | 1 | T | | 1 | | | 1 | I | | 1 | D | | 1 | | | 1 | 1 | | 1 | 2 | | 1 | 3 | Then transform it again. These sequences of "loops" would chew through 6000 lines pretty quickly compared to 12 hours, I imagine.
I cannot tell if you are trying to troll the subreddit, or if you were simply taught horribly wrong. Please stop trying to apply object-oriented principals to your database coding. Stick with the "set theory" since that is how the database engine is designed to work most efficiently with.
I like you. People get mad when you dont know the terminology too. Like.. is it a relationship, bridge, junction or cross reference table? Its data. Hard stop. Lake, pool, mart, warehouse? Nah, this is what you use for your app here... and this is what you analyze. All of my suggestions and reasons consider the worst and overloaded environments. Of course a fresh install would push through 6k. But I have seen an @table of 20 rows take down a report due to tempdb and disk speed. I cannot explain clusters without graphics, but it would make perfect sense to you. Take all your clustered indexes(on one file), put them in one xls sheet, sort them. Now one tables edit its locking a page another table resides on. Give every table a unique bigint (every row has the same int, no table has that same int), cluster on that, and the pk. Put that in an xls and sort... you have every table next to each other and very little page / extent overlap. That's just for clustered indexes. Add nons in there... giant mess.
is there Visa`s or work permits needed for non US citizens to do contract work in the US?
I often feel that it isn't that I don't understand the terminology, it's that the terminology is no different than other terminology we use and I feel that the people using the new terminology don't understand it. So they will just string these filler words together to form sentences that (to me) make absolutely no sense. And I'm often left sitting there feeling like I'm the only one that understands that they aren't saying anything at all, but I'm also the most junior person involved in terms of strict SQL experience. &gt;All of my suggestions and reasons consider the worst and overloaded environments. And all of mine consider ivory towers with no load at all, where the server is idling most of the time. &gt;But I have seen an @table of 20 rows take down a report due to tempdb and disk speed. I cannot explain clusters without graphics, but it would make perfect sense to you. So just put it in a dbo.table? What do you mean clusters? Like indexing? I mathematically understand how different sorting algorithms work in principle, and how regardless of technology, or software program, that there is a limited number of ways to "skin" a cat. So things like clustered indexes (I think, lol) are very intuitive to me. &gt;Take all your clustered indexes(on one file), put them in one xls sheet, sort them. Now one tables edit its locking a page another table resides on. Can you comment on how partitioning differs from clustering? I currently have a table that looks like this: | SegmentID | Field1 | Field2 | FieldN | | :--- | :--- | :--- | :--- | | 1 | blah | blah | blah | | 1 | blah | blah | blah | | 1 | blah | blah | blah | | 1 | blah | blah | blah | | 2 | blah | blah | blah | | 2 | blah | blah | blah | | 2 | blah | blah | blah | | 2 | blah | blah | blah | And I populate it with queries like this: select 1 AS SegmentID, something, x * y / z as KPI from table where client = client and field1 = n group by something select 2 AS SegmentID, something, x * y / z as KPI from table where client = client and field1 = m group by something I have indexes on the SegmentID which is an `int`, but clusters around Client because it is more heavily used by the "meta" process (hate the word meta) -- anyway... the thought has occurred to me that perhaps I may want to partition these tables by SegmentID. Do you have an opinion on this? 
SQL is a Turing complete language, please stop trying to tell me how to code, and what to code in, given the business requirements / limitations that clients have. I've been coding since the 1980s. I've been coding in SQL for about 4 years. If i can do it in SQL, and do it efficiently, your advise becomes nonsensical.
Guru.com
Post the actual error. 
What are you really trying to do? I feel like this is a situation where your code is more complicated than it needs to be. 
Define "user experiences with new features". 
SELECT CASE WHEN DATEPART(HOUR, TaskAssignDate) &gt;= 20 THEN CONVERT(DATE, TaskAssignDate) ELSE CONVERT(DATE, DATEADD(DAY, -1, TaskAssignDate) END 
Is that question? Like from a book or test? I.E. "What are the causes of errors..." If yes, that's a very broad question. What context is it in? 
So clustered indexes are the main physic order of the table. Let's say I have 3 tables all with clustered PKs, 3 rows each. My file would look like. A | 1 B | 1 C | 1 A | 2 B | 2 C | 2 A | 3 B | 3 C | 3 if I added a system unique numeric to each table and clustered on that and the non clustered PK. A | 1 | 1 A | 1 | 2 A | 1 | 3 B | 33 | 1 B | 33 | 2 B | 33 | 3 C | 768| 1 C | 768| 2 C | 768 | 3 This allows us to keep tables physically close and reduce the distance (think platter drives) to retrieve the next table value. I usually have a tenant Id (multi tenancy, maybe client is yours) followed by a taxonomyid (may be unique to my design). I could cluster by this unqiueTableNumber then client to keep tables close together.. Or, client then uniquetablenumber to keep clients together. The correct answer is based on your use. So clusters create pointers. Just another system unique number, that helps point a cluster to a new ordered list (non clustered). Like a compound, composite... insert name here... key. But... if you do NOT have a file for your non clustered indexes... it will disrupt your clustered indexes.. so.. you make another file and put your non clustered indexes on them. If I call these two files, a file unit.. you want multiple file units. One unit for data that doesnt change (reference data, static lookup data like... languages or... w/e) And you want one unit for the following. Date dimension Geography dimension People Relationships (I use 1 relationship table for all relationships, makes sense when you reuse uniquetableNumber and Pk as a parent and the same as a child, then a Type I'd to tell me if its direct, or ancestor or descendant (no on the fly recursive cte)).. this relationship goes deeper so dont focus on this. Also a file unit for your company's money maker. If you sell parts and record orders.. One for orders one for parts. This is a non programmatic way to do partitions and lets you control what goes where. A problem I can see you might have with this, is answered by my table structures. I have a tblPeoPeople tblPeoPeopleName, tblPeoPeopleFlag... and tblRefNameClass, namefamily,nametype,flagclass.... I have peo (one file unit) And ref (another file unit) My ultra wide analysis only tables are like... PEOPeopleHeader PEOPeopleName01.. 02..03.. PrtPartHeader PRTPartFlag01,02... Hope that makes sense. That's the design, now switch mindsets. Is it faster to open a 5gig text document when you query any tabke, or one of 10 half gig documents when you need a specific piece. I havent used partition, I dont know if it creates. New files, but even if it did.. are you partitioning all tables and using other files for indexes. 
&gt; Hope that makes sense. It doesn't but I'll be reading all this tomorrow and probably asking you more questions. On the surface it would seem partitioning a table by an integer would be like adding a 2nd clustering. So I could cluster by Client, and then by SegmentID.
Unless segment appears in another table / cluster.
I don't understand, or I'm not communicating properly. So I have a database where I have a variety of tables, and let's just arbitrarily say these are a few of them: 1. Order 2. Requisition 3. ReqAgg 4. ReqD 5. People 6. Invoice So now you can join these tables a variety of ways in order to calculate a number of KPI's, and each table has its own indexes, *but* consider this: Client A has specific contractual obligations unique to itself, it requires the following query to operate: SELECT blah FROM myTable WHERE Client = 'A' AND SegmentID = 1 Now Client B has different language in their contract, and the need a query like this: SELECT blah FROM myTable WHERE Client = 'B' AND SegmentID = 2 Now in this example the `myTable` object is being populated like this (very simple example): INSERT INTO myTable SELECT 1 AS SegmentID, * FROM realTable WHERE DateRange1 BETWEEN X1 AND Y1 AND CLIENT = 'A' Obviously I am creating indexes on `myTable`, and there are appropraite indexes on the `realTables` that create these "segments," such as: INSERT INTO myTable SELECT 2 AS SegmentID, * FROM realTable WHERE DateRange2 BETWEEN X2 AND Y2 AND CLIENT = 'B' Now the complexity here is that I want to have all the clients for each `SegmentID` because Client A might need to point to `SegmentID = 1` for one calculation, but then `SegmentID = 2` for another calculation. So really what I'm doing is a `SELECT * FROM TABLE WHERE DATE1 BETWEEN X AND Y` and the unioning a `SELECT * FROM TABLE WHERE DATE2 BETWEEN X AND Y`... then later using the `SegmentID` to increment the LOOP and assemble the rest of the dynamic SQL. Does that make sense? I'm clustering around the Client name, not the SegmentID, but I do have an index on the SegmentID and mentally I've always thought a "partition" would make the most sense but I haven't had the time to experiment and performance is already pretty good.
I see. Not an actual PARTITION more like a row number partition by. If your other real tables are on the same file... I'd say no. Move this to a new file and I'd say yes. You'd have client a seg 1 client a seg 2 next to eachother. If all tables are on one file.. and the cluster ' A 1' exists anywhere else... even if different column names.. it'll be slower.
I should do some AMAs here and take files from people and load them lol... live. Twitch sql development. So exciting......
I would really come work with you for (2) years or so to collaborate. I'd take a pay cut.
I wish I was a decision maker. I love to teach. I have an at home sql server. Let's do random projects.
I just started a second series... ripping apart adventure works into this model, I'll RIP apart wide world also so people can see then coinhavit the same tables.
What does file mean? In the "realTable" I have 100 clients... in the "myTable" I have ~20 clients, and I have ~5 segments. So I have all the rows from the realTable for the 20 clients between daterange1, and then all the rows from the realTable for the 20 clients. The cluster is around the client in the myTable, and the SegmentID just lets the dyanmic SQL pick which "SET" to choose for its calculation. In reality the "realTables" are not just (1) table, but a join on multiple conditions between multiple "large" tables. So I'm doing that up front and just dumping it all into a "myTable" and then giving it a SegmentID. That way when the dynamic queries assemble they are essentially pointed at "materialized views' based on the SegmentID around a clustered Client. 
https://docs.microsoft.com/en-us/sql/relational-databases/databases/database-files-and-filegroups?view=sql-server-2017
Let me know, I'll work for free.
So in simple terms a partition creates multiple files, and when using a`WHERE PartitionKey = 1` it segments the segment before looking at the cluster?
&gt; How does a join work at the engine level? &gt; &gt; Lets say you have two data sets of 100 rows, DataSetA and DataSetB. When you join, you are comparing 1 row from A, against 100 rows from B. Inner, outer, full, apply... same thing That's it? And they you give a page-long spiel on how's your architecture is terrible for normal query optimization... No offense - some epic level BS here, I'm impressed.
Backups are your undo button.
No offense back, but can you demonstrate otherwise? In terms of maths I can't see how else it could ever possibly work, and his description of a JOIN being the most simple version of a LOOP seems to be a very core principle of computer science. It really doesn't matter whether we're talking about databases, or kernels, or assembly languages. At the end of the day you're talking about a "physical process" that is somehow being interpreted / compiled. In terms of pure set theory in their simplest forms, a JOIN and LOOP are about as different as addition and multiplication. From there you can go to a lot of different places.
Without getting into whether its a good idea, but imagine you have a query that takes 12 hours to run and changes a table in some way. You run it and wait patiently and after it's done you realize you totally fucked up. Now, you have two options: 1. Undo what you did by restoring a back up. 2. Truncating the table you stored your results in, modifying your query, and then running it again. Undo doesn't really have a place or a context in this conversation, but you could build one. For example if your query was updating a table you might decide to `SELECT * INTO dbo.TableName_BACKUP FROM dbo.TableName` before you run your query, and then "undo" things by just truncating the table and dumping the original data back into it.
Demonstrate what? But anyways - not every join is a nested loop. A "loop" is a logical control structure and as such it will be used in many different algorithms and it's not a defining feature, really. And there is no looping in set theory. I mean, there's not even a single thing I could think I can start with. Maybe start with reading https://use-the-index-luke.com/sql/join Redgate has a bunch of very decent books for free here: https://www.red-gate.com/hub/books/ 
Most database technologies actually do have this functionality. It’s the concept of undo. This is a must requirement for the consistent portion of ACID
It isn't though, you are absolutely using the basic principle of a LOOP in a JOIN. Indexes can help this process, but that is his entire point. The syntax of writing a JOIN versus a deconstructed LOOP using a specific index amounts to the same functional thing in set theory is his point, I think.
You mean error handling, roll backs, etc.? I was more interpreting the question as being, "I did something that was successfully executed, and now I want to undo it." -- Not so much that something is being done, and an error is happening, and that is being handled, or undone because the error happened.
* rollback? * snapshots? * restore backup? 
 Begin Tran Rollback -- here's the undo button Commit -- here's the I'm serious button The long answer involves a pretty in-depth discussion on ACID principles. 
This kinda devolved to something weird. And I have zero clue what a "deconstructed LOOP" could even begin to mean. ( while begin end i 0 &lt; 10 i++) Do you have a specific question that I can answer?
No, but I think you're wrong in your criticism. In terms of pure math a JOIN is essentially a LOOP is my take away. You could demonstrate this fairly simply. I have no idea how it would actually perform, but you could force the server to use an index and execute a LOOP in such a way that would probably replicate a JOIN. That's at the simplest level of looking at the relationship.
None taken. "To know what you do not know is the best. To think you know when you do not, is a disease. Recognizing this disease, as a disease, is to be free of it." I just looked up data vault. The concepts are there, but not all of them. &gt;&gt;Data vault modeling is a database modeling method that is designed to provide long-term historical storage of data coming in from multiple operational systems. Yep &gt;&gt;It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from. Yep &gt;&gt;This means that every row in a data vault must be accompanied by record source and load date attributes, enabling an auditor to trace values back to the source. Nope. Every one of my 16 databases has a purpose and every one of these 16 has a liked name database for history, archive and snapshots (totally 64 dbs). &gt;&gt;Data vault modeling makes no distinction between good and bad data ("bad" meaning not conforming to business rules). Ehh... through tenancy I can compile a master through, but every record has a valid flag, valid meaning you trust it. Close, but I don't care about business rules. I care about sanitation and data managment. Any business rule fits in it. &gt;&gt;This is summarized in the statement that a data vault stores "a single version of the facts" (also expressed by Dan Linstedt as "all the data, all of the time") as opposed to the practice in other data warehouse methods of storing "a single version of truth" where data that does not conform to the definitions is removed or "cleansed". That's called master data management. But.. my system has a database for input, one for an ODS, type 1 scd, type 2 and 3 (warehouse), reports and analysis. Cleansed data is rerouted because every record in every table has a cleanse flag and value, but the 'dirty' value is kept as a way to continue to clean incoming data. Data driven Sanitation while functioning. &gt;&gt;The modeling method is designed to be resilient to change in the business environment where the data being stored is coming from, by explicitly separating structural information from descriptive attributes. Data vault is designed to enable parallel loading as much as possible, so that very large implementations can scale out without the need for major redesign. Yes, yes, yes. Hyper normalization through taxonomy and abstract concepts, and purpose based separation of databases, files, tables (concepts) allow you to move databases to different servers and files to different drives. One of my databases has no tables and holds all the functions and procedures and views that talk to synonyms. That's just Edgar Codds reasons for normalization, though. Source: https://en.m.wikipedia.org/wiki/Data_vault_modeling But... every record in my system has the following flags. Display - does this record have data driven parameterized content (if morning and return customer then "content", if morning then "content", else "content") Planned - does this record have data drive time sensitive activeFlag toggling. Campaigns... preload data and activate it on x date.. password expiration.. Security - does this record have data driven parameterized access limitations. PCI or HIPPA or standard permissions Other - does this record have internal notes, row guids? And using the same patterns that every table in my system follows, cartesian decision trees and data driven calculations. Basically... new features become data entry and not additional programming more often than not. 90% data entry if you are expanding a current feature. Oh.. and I'm not charging anyone for this information. I chose youtube and not udemy. 
I care
Desktop link: https://en.wikipedia.org/wiki/Data_vault_modeling *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^233582
**Data vault modeling** Data vault modeling is a database modeling method that is designed to provide long-term historical storage of data coming in from multiple operational systems. It is also a method of looking at historical data that deals with issues such as auditing, tracing of data, loading speed and resilience to change as well as emphasizing the need to trace where all the data in the database came from. This means that every row in a data vault must be accompanied by record source and load date attributes, enabling an auditor to trace values back to the source. Data vault modeling makes no distinction between good and bad data ("bad" meaning not conforming to business rules). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
How does the engine join two result sets? Is it not For each in dataset A -For each in dataset B -- if a and b then + - End B End A 
a join is a set operation. a loop is an execution control structure. Apples and oranges (well you cant use apples to achieve oranges, but all simulacra fail up close). Think about this way maybe - a loop is like a comma. Using commas is not going to define what genre is your novel. That's the simplest I can chop it down to. My criticism has been about his/her answer - it had nothing about the alleged equality between cursors and joins (also, apples and oranges) and the fact that it had very very naive understanding of the join execution.
I posted some links earlier. Try them?
First one didnt describe the engine workings. But it did say the more you join the bigger the performance hit due to the engine and how the engine can only join 2 items at a time and has to keep track of each pair. The other links were just books. Nothing that answers the question I asked, that you were so adamant i was BSing about. It's ok to say you dont know. I mean, I dont know exactly. Im just using my understanding of assembly code and circuits to deduce... that it has to be a loop, for each, some sort of cursor like item. It's perfectly logical. On off. 1 / 0, match or no match. So, that's my theory. Any information about how the engine itself deals with joins will support or disprove this theory. Pointing to a library and saying the answer is somewhere in there is intellectually dishonest. 
Yes. Functionally the same, if not better, because you are bypassing the optimization engine and taking control of it. Jedi link states the engine attempts to optimize multiple joins using several permutations.. while a lookup is a sleek without the extra steps. 
RemindMe 12 hours
This write-up is fantastic. I can't believe you took the time to do this, but it is very much appreciated, thank you! I am currently working through it all. I had simplified my data a bit in my OP and I actually have 6 dates to work with, but I am making the necessary adjustments. I am pretty new to SQL and all self-taught but your instructions are very clear. One question - I haven't come across the DUAL table before. Is it a sort of 'dummy' table pre-built into Oracle SQL? Is it unique to Oracle? &amp;#x200B;
yes it is possible. Use case statement [https://www.w3schools.com/sql/sql\_case.asp](https://www.w3schools.com/sql/sql_case.asp)
Sorry, I am on mobile. SELECT DATEADD(HOUR, 23, DATEADD(DAY, DATEDIFF(DAY, 0, GETDATE()) -1, 0)) explanation. Going from the inner parenthesis out: find difference in days from getdate and a "zero" date. Add the number of days from the difference above to the "zero date". This basically rounds to today's midnight. So we subtract one to get yesterday. Then we just add 23 hours. You will do the same for the today's 7:00. Just midnight + 7 hours.
To save time extracting data from Oracle and using Excel to do something which I would expect it should be done within Oracle specially if data is going to Tableau or PowerBI. Also I'm trying to mirror how the operation is working which is from 8pm on day 0 to 7:59pm on day 1 
If you absolutely can't use FOREIGN KEYs which is the best way to handle this take a look at EXISTS. Something like the following should work in your case: ``` CHECK EXISTS (SELECT ID_A FROM TABLE_A WHERE TABLE_A.ID_A = ID_A) ```
Okay thank you. I've got SSMS installed and i'll give it a go &amp;#x200B;
Thanks, I'll have a go
I have done but it shows an error on every line
Cheers, i'll give it a go
Do you also have SQL Server installed? Both SQL Developer and SSMS are just GUI tools. They are not the actual database engine.
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 492.5 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about resume advice. I'm ~42% sure of this; let me know if I'm wrong! Have you checked out TalentWorks, /r/resumes, TIME? They've got some great resources: * https://talent.works/blog/2018/01/08/the-science-of-the-job-search-part-i-13-data-backed-ways-to-win/ * https://www.reddit.com/r/resumes/ * http://time.com/money/4621066/free-resume-word-template-2017/
Oh, you're trying to create an artificial break in the day. Like end of the business day or something. 
Why not use Ola's scripts? You're just using a simple UNC right? Super easy with the Maintenance Solution.
just re-read my first statement nas is not on my domain. so i need to input the credential to connect first. &amp;#x200B;
\*I have no idea if this is possible, but...\* &amp;#x200B; Can you map a drive to your NAS at the windows level, then share that mapped drive? Then in SQL backup to the UNC path of your new local shared folder, which is really just a shared-mapped network drive?
Good luck with it, I can answer some questions about the process if you need. Let me know!
Have you tried to use GROUP BY? I’m currently trying to write reports too, and my method of group by-ing seems to work quite well 
Thank you :)
Thank you!
most of the time you can still right click and select modify and it will give you an error message that you dont have permission to modify the sproc but it will still allow you to see the code.
This means that the object is encrypted. See the steps here on how to decrypt: [https://www.mssqltips.com/sqlservertip/2964/encrypting-and-decrypting-sql-server-stored-procedures-views-and-userdefined-functions/](https://www.mssqltips.com/sqlservertip/2964/encrypting-and-decrypting-sql-server-stored-procedures-views-and-userdefined-functions/) 
What he's saying is that regardless of how you think its working, a JOIN is still looping through each row in Table A. At its simplest relationship it functions exactly like a loop to produce the set. &gt;My criticism has been about his/her answer - it had nothing about the alleged equality between cursors and joins (also, apples and oranges) and the fact that it had very very naive understanding of the join execution. I don't think he was trying to say that they were "equal" in use case, but that they were at their cores the same concepts and I imagine you could test this to write a very specific type of loop that would run in the same amount of time as a join. 
I recommend group by combined with a PIVOT function
try TO_DATE('12/05/1998', 'DD/MM/YYY') as your date
no i tried that but it didnt accept,however instead of '12/05/1998' it accepts '12/may/1998'
Hmm, I added in GROUP BY but I have the same list of results.
Group by works until you introduce a column in your select that pulls a unique value. Then you'll have 2+ records. Can you determine what is different between your row duplication? You mentioned using a different machines throughout the day causes record duplication. Are you pulling the machine name in this query? That would certainly cause the duplication. 
Yes, my column Job_Operation.Work_Center pulls the machine names. I could do without Job, Part_Number, and Work_Center if that helps. An example of difference between duplicated rows would be like the % Efficiency will be NULL in the first row and 52% in the second row. In the row with NULL, the Act_Run_Qty and Act_Run_Hrs will be zero while the 52% row will show Act_Run_Qty as 572 and Act_Run_Hrs as 10.7
What does your result set look like? If it's the same info repeated over and over for each person, just add DISTINCT after SELECT. If it's different info you'll need to use [aggregate functions](https://docs.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql?view=sql-server-2017) on the things you want to add up and then GROUP BY those things.
It isn't the same correct info repeated over and over. An example of difference between duplicated rows would be like the % Efficiency will be NULL in the first row and 52% in the second row. In the row with NULL, the Act_Run_Qty and Act_Run_Hrs will be zero while the 52% row will show Act_Run_Qty as 572 and Act_Run_Hrs as 10.7
Yeah, that would work also. Usually DD-MON-YYYY is good. Also it didn't work because I did it wrong. It needed 4 Ys. 
So it doesn’t actually allow me to click on modify as it is just not clickable. I’m guessing this issue needs to be fixed by asking the admin to increase my user permission.
But I think this would be unable to do as I am only down as a viewer?? 
Is it just the efficiency that's different in each row? In that case, sum each of the components within the calculation and then add each component to the group by. eg (and this is rough) &gt; FORMAT (sum(Job_Operation.Est_Run_Per_Part) * sum(Job_Operation_Time.Act_Run_Qty) / NULLIF (sum(Job_Operation_Time.Act_Run_Hrs), 0), 'p0') AS "% Efficiency", &gt; group by Job_Operation.Est_Run_Per_Part, Job_Operation_Time.Act_Run_Qty, Job_Operation_Time.Act_Run_Hrs
Yes , sorry I should mention this on orginal email 
937
I tried adding in the SUMs but now I get "Cannot perform an aggregate function on an expression containing an aggregate or a subquery."
I run a function based of what a query returns and I only want it to run for a certain time of the day when our users arent online. I use this in the where clause to achieve that. It only returns data when the current time is within my window . Not really the same thing but it might give you some ideas. Where (cast(getdate() as time) between '15:00:00.0000000' and '21:00:00.0000000') 
If you do need the values of these duplicate columns, you can use the STUFF function with FOR XML and make the field a character separated list. I’m on mobile, but if you google ‘SQL STUFF comma separated list’ you should get pointed in the right direction.
try removing the nullif and see what you get. That might be your issue.
I don't need them.
Welp, now I get "Divide by zero error encountered."
Yeah it's going to be rather hard to help you out without knowing what the data looks like and what the query produces (and why). Is this query supposed to return stuff per employee and work center? Or is "part" (Job.Part_Number) part of it as well? If it's per employee, try this: SELECT Job_Operation.Work_Center , Job_Operation_Time.Employee , Job_Operation_Time.Work_Date , SUM(Job_Operation.Est_Setup_Hrs) AS TotalEstSetupHrs, , SUM(Job_Operation_Time.Act_Setup_Hrs) TotalActSetupHrs , SUM(Job_Operation_Time.Act_Run_Qty) TotalActRunQty, , SUM(Job_Operation_Time.Act_Run_Hrs) TotalActRunHrs, , ROUND(SUM(Job_Operation.Est_Run_Per_Part), 2, 0) AS TotalEstRunPerPart , FORMAT (SUM(Job_Operation.Est_Run_Per_Part * Job_Operation_Time.Act_Run_Qty / NULLIF (Job_Operation_Time.Act_Run_Hrs, 0)), 'p0') AS "% Efficiency", FROM Job INNER JOIN (Job_Operation INNER JOIN Job_Operation_Time ON Job_Operation.Job_Operation = Job_Operation_Time.Job_Operation) ON Job.Job = Job_Operation.Job WHERE (Job_Operation_Time.Work_Date) &gt;= dateadd(day, -1, CAST(getdate() as date)) AND (Job_Operation.Work_Center LIKE '%CNC%' OR Job_Operation.Work_Center LIKE '%ACME%' OR Job_Operation.Work_Center LIKE '%Sw%' ) ORDER BY Job_Operation.Work_Center GROUP BY Job_Operation.Work_Center , Job_Operation_Time.Employee , Job_Operation_Time.Work_Date You might want to consider placing that `SUM` in efficiency calculation somewhere else, right now it's summing up per part efficiencies as per your formula, you probably want `SUM(...) / SUM(NULLIF(...))` instead but it's hard to tell what you're looking for.
Do you have any data you could share as an example?
It doesn’t matter if SQL can be a Turing complete language because it is first and foremost a declarative language. You need to tell SQL what you want, not how to do it. Your 30+ years of previous programming is apparent in your TSQL writing style. Your abundant experience has inflated your ego here and has made you think you are smarter than the SQL optimizer. While your code will run, it is terribly inefficient and therefore I say you are doing in wrong. When I look at your loop example in another comment, I cringe inside. That code is your stereotypical example of bad coding practices. I challenge you to compare the performance and the query plan of your example against the following rewrite... INSERT INTO dbo.tabTable SELECT * FROM dbo.dataTable A JOIN (SELECT DISTINCT Client FROM dbo.mpTable) B ON A.Client = B.Client
It sounds like you need a window function, or maybe just a subquery. Like, the subquery finds the customers with the lowest scores and then the outer query sums. But it's hard to tell just from this post. Can you provide a few lines of sample data?
If you can write a query that outputs everything, but ordered in the sequence where the customer you want is the first one listed for each account, then you can take that sequence and apply ROW\_NUMBER() or RANK() to it and only select the records where the rank = 1. &amp;#x200B; Something like: ;with cte_Data as ( Select fields, Ranked = ROW_NUMBER() over (partition by Account order by (Val1 + val2) from Data ) select * from cte_Data where Ranked = 1 I used the cte because, iirc, you can't use a window function in a where clause.
Thanks. Yes it’s a local data store. For the most part we have what you suggest set up in a star schema. What is your indexing scheme for setups like this? The reasons for not always using Teradata directly are velocity and availability. Some items we bring in aren’t loaded into Teradata for 48 hours or more and our business users want preliminary data before then. Some other sources are specific to our local market and not yet available on the EDW platform. 
DECLARE @CUTOFF_TIME TIME = '21:00:00' DECLARE @START_DT DATETIME = CASE WHEN CAST(GETDATE()AS TIME) &lt; @CUTOFF_TIME THEN DATEADD(HOUR,21,DATEADD(DAY,DATEDIFF(DAY,0,GETDATE()-1),0)) ELSE DATEADD(HOUR,21,DATEADD(DAY,DATEDIFF(DAY,0,GETDATE()),0)) END DECLARE @END_DT DATETIME = CASE WHEN CAST(GETDATE()AS TIME) &lt; @CUTOFF_TIME THEN DATEADD(SECOND,-1,DATEADD(HOUR,21,DATEADD(DAY,DATEDIFF(DAY,0,GETDATE()),0))) ELSE DATEADD(SECOND,-1,DATEADD(HOUR,21,DATEADD(DAY,DATEDIFF(DAY,0,GETDATE()+1),0))) END SELECT * FROM appl_event WHERE event_ts BETWEEN @START_DT AND @END_DT 
Here is a sample output: &amp;#x200B; Work\_Center Employee Work\_Date TotalEstSetupHrs TotalActSetupHrs TotalActRunQty TotalActRunHrs TotalEstRunPerPart % Efficiency ACME31187 BLANIC 2019-01-22 00:00:00.000 7.25 0 572 10.7 0.01 53 % ACME31187 BLANIC 2019-01-23 00:00:00.000 7.25 0 0 7.05 0.01 0 % ACME31188 FRIANT 2019-01-22 00:00:00.000 4 0 600 10.48 0.01 85 % ACME31188 FRIANT 2019-01-23 00:00:00.000 4 0 0 0 0.01 NULL CNCL30426 PHLTIM 2019-01-22 00:00:00.000 8 4.49 103 5.11 0.15 154 % CNCL30426 PHLTIM 2019-01-23 00:00:00.000 5 4.2 0 0 0.09 NULL CNCL31596 WITDEL 2019-01-22 00:00:00.000 0 0 7 9.84 0.2 17 % CNCL31596 WITDEL 2019-01-23 00:00:00.000 0 0 0 0 0.1 NULL CNCL31598 GHLSWL 2019-01-22 00:00:00.000 2 0 150 0 0.07 NULL CNCL31598 GHLSWL 2019-01-23 00:00:00.000 2 0 171 0 0.07 NULL CNCL31598 LUCBRI 2019-01-22 00:00:00.000 4 0.43 62 0.17 0.13 2,431 % CNCL31762 LUCBRI 2019-01-22 00:00:00.000 1.5 0 260 9.48 0.03 91 % CNCL31762 SWASAR 2019-01-22 00:00:00.000 1.5 0 151 8.5 0.03 59 % CNCL32265 STESAR 2019-01-22 00:00:00.000 3.5 0 133 9.87 0.06 79 % CNCL32265 STESAR 2019-01-23 00:00:00.000 3.5 0 0 0 0.06 NULL CNCL33107 COOTAN 2019-01-22 00:00:00.000 2 0.05 0 0 0.04 NULL CNCL33107 GHLSWL 2019-01-22 00:00:00.000 2 0 244 0 0.04 NULL CNCL33107 GHLSWL 2019-01-23 00:00:00.000 4 0 217 0 0.08 NULL CNCL33107 SWASAR 2019-01-22 00:00:00.000 2 0 0 0.02 0.04 0 % CNCLDEBURR ARCGAM 2019-01-22 00:00:00.000 0 0 200 8.15 0 8 % Looking at it now I see that some of the duplicated are from yesterday and today. I think adjusting how I look for the date will help get rid of some of that.
Implicitly casting from a string to a date is always a bad idea. Use the TO_DATE fuction to explicitly cast: TO_DATE('12/05/1998', 'DD/MM/YYYY') or the date literal, which is always in YYYY-MM-DD: date'1998-05-12'
Lol how
&gt; because it is first and foremost a declarative language. Which is why most of you in this sub-reddit use it in the way that you do. That doesn't mean others don't use it differently, nor does it make it bad. &gt;When I look at your loop example in another comment, I cringe inside. That code is your stereotypical example of bad coding practices. I challenge you to compare the performance and the query plan of your example against the following rewrite... You cannot accomplish what my code is doing with your example. You would need to maintain over 100 small queries like that, and then run them all manually. I can use my code to construct 100 different unique queries that all dump results out in the same format for consumption by the next tool. You would need to have them all daisy chained together inside (1) sproc, which would make updating them a pain, *or* you would need to have them as 100 sprocs. I have (1) sproc, with (1) query, and a few simple mapping tables to maintain all the customizations. 
I also tried changing &amp;#x200B; WHERE (Job\_Operation\_Time.Work\_Date) &gt;= dateadd(day, -1, CAST(getdate() as date)) &amp;#x200B; by taking out the &gt;. This removed all of the NULL repeats / entries from today so I just saw their efficiency from yesterday.
When I want one row, I almost always run to `row_number()` and `partition by` the desired grouping. Typically aliased as `rn`, then after double checking the intermediate select, I filter the first row (`where rn = 1`) from that. 
I think they perform the same functionality - they both restrict rows in your query. However, using one or the other is about readability or the intent of the query. ON is used to specify how two tables are joined together. WHERE is used to restrict rows from a result based on criteria. If you want to specify the columns that are related between two tables, use ON. If you want to restrict your results, use WHERE. For example: SELECT a.column, b.column FROM table_a a INNER JOIN table_b b ON a.id = b.other_id WHERE a.status = 'Open';
I farted around with this for 40 minutes and it's actually kind of difficult. I made a simple, one-column table named \[dbo\].\[Player\] with an INT column named \[PlayerId\]. \[PlayerId\] has four rows: 1, 2, 3, 4. If I am trying to get the sum of the smallest two values for column \[PlayerId\] in table \[Player\], here's what I finally got working (and it's ugly, but functional): --Create a CTE that will select all the PlayerId values as well as a computed column for the value of PlayerId one row down ;WITH CTE AS ( SELECT PlayerId, LEAD(PlayerId,1,0) OVER (ORDER BY PlayerId ASC) AS SecondSmallest FROM Player ) --Take the top 1 from the CTE which will be the lowest value due to the LEAD computed column and add it to the value of the computed column from a subquery SELECT (SELECT TOP 1 PlayerId FROM CTE) + (SELECT PlayerID FROM CTE WHERE PlayerID = (SELECT TOP 1 SecondSmallest FROM CTE)) &amp;#x200B; &amp;#x200B;
They perform two similar but different functions. **ON** is used in a JOIN to determine which fields from each table will be used to reconcile the two tables to each other. **WHERE** is used to determine which records will be returned by the entire query, *after* any joins are performed. Both determine which records are eventually returned, but they affect the query in two different places/ways.
You deserve gold
Wow thanks very much! Glad you found it useful!
What would you want to show if there are two rows with the same policy number but different values in other columns? Would you want to show both records, or only one? You can use SELECT DISTINCT, but this will show you unique results, not unique values in each column. This query will show you unique results: SELECT DISTINCT policy_number, col1, col2 FROM policy; However I'm not sure if those results will be what you need, because if two records have the same policy\_number but different values for the other columns, both will be shown.
Do you think this sheet meets the needs of most people who work with SQL in their daily lives?
Overall I think it does. There's always more I could add. But happy to make adjustments based on any feedback here. What do you think of the cheat sheet?
That’s perfect. Exactly what I needed. Thank you. I needed different values in the other columns if there was more than one. I can make changes to the output in excel as needed for some financial stuff. 
I really really love one of postgre's features related to window functions, namely named windows. You can specify a window and reuse it across multiple window functions, like so: SELECT SUM(salary) OVER w, AVG(salary) OVER w FROM empsalary WINDOW w AS (PARTITION BY depname ORDER BY salary DESC); I have so many queries using `IIF(ROW_NUMBER() OVER (...) = 1, MAX(...) OVER (...), NULL)` or something like that, and maintaining 20 usages of the same `OVER` clause is really tiring.
A noob doesn't even describe how unfamiliar I am with SQL. I was simply asking a question albeit it came off as a little sarcastic (didn't mean it that way).
It definitely does for beginners. This should almost all be like second nature for advanced developers. 
Ah no problem, it didn't seem sarcastic to me! For those new to SQL, I think this is something you could use as you start to learn more about the SQL language. For example, once you get to the point of knowing what a GROUP BY clause does but don't remember where in the SELECT statement it normally goes, this cheat sheet will be helpful for you. But if you don't know what GROUP BY is, this won't help much. Likewise with CREATE TABLE statements - I often forget the exact syntax for parts of it, so this cheat sheet would help me. If you want to learn more about SQL, I've recently created a page that lists 59 places to learn SQL (combination of free/paid, text/video): [59 Best Resources for Learning SQL](https://www.databasestar.com/learn-sql/)
Looks great. I would suggest that you try to describe (in as few words as possible) the logical processing order of the SELECT keywords. Maybe even just a number to the right of each keyword under a column titled "process order" or something. Keeping that order straight helped me a lot when I was learning it.
You can't have something being and "R" _and_ and "O"... can't you use `OR` ? (po_head.rec_type = "R" OR po_head.rec_type = "O")
The SELECT keyword allows you to retrieve data from the database. What does your expression or query look like at the moment? Without the SELECT clause, there are no results to be able to order.
Good luck
I've tried OR but it errors out on me, but I haven't used the parenthesis. Does this do something different with vs. without them?
I should be clear that I'm accessing the database via a proprietary web-based portal that I happen to have admin privileges on. this is the current expression: adddate = CONVERT(VARCHAR, DATEADD(DAY,1, GETDATE()), 101) AND (HService in ('35','36') &amp;#x200B; If I test the expression as is, it returns 22 records, but sorted in a very non-useful way. I just want them sorted based on another field on the table. 
kindly show the entire query
yeah, it limits the "or" part to just the R/O clause... you could also try `1 and 2 or 3` means 1 _and_ 2 need to be true or just 3 needs to be true. But `1 and (2 or 3)` requires 1 to be true and either 2 or 3 to be true
it is no longer unique (some other engines have a similar feature), but MySQL'S `GROUP_CONCAT` is da bomb
as others have said, they perform similar functions but watch out -- a WHERE condition on a column of the right table in a left outer join will effectively turn it into an inner join
Thanks for clarifying. I am guessing that the web-based portal is transforming that expression (which looks like a WHERE clause) into a wider SQL query behind the scenes. So, while in SQL you are able to use an ORDER BY to order your data, I think the restriction of your web-based portal means you can't. I would suggest contacting someone else who uses or supports that tool to see if it can be ordered.
i guessed which has a 1 in 1000 chance of being right which is a better chance than everybody else has who didn't answer at all
Thanks! Good idea - I'll make some updates to include that.
&gt; WHERE is used to determine which records will be returned by the entire query, after any joins are performed The way you write this makes it sound like the engine evaluates all join conditions before any where clause. That's not the case. Forgive me if that's not what you meant.
Is the link already updated?
Holy crap! You did it!!!! Thank you so much. That makes total sense. I knew it had to be something kind of easy but I didn't know I could do that in SQL. Just goes to show how much of a noob I am. &amp;#x200B; Thanks again!
This has been a topic of continued discussion over the last few days. Placing a filter criteria in the ON statement versus the WHERE clause will allow you to filter down an outer joined table without a subquery, while still maintaining the intended function of an outer join.
WINDOW isn't common? Sqlite supports it too...
That’s what I was afraid of. Thank you for the help!
Please post your query. As a general rule of thumb a sub-query will not (always) perform very well, but in general they can do a lot of things. Consider a query that returns the following results: | Client | Date | Revenue | | :--- | :--- | :--- | | ABC | 2019-01-01 | 1200.00 | | ABC | 2019-02-01 | 61.00 | | ABC | 2019-03-01 | 2156.00 | | ABC | 2019-04-01 | 226.00 | | XYZ | 2019-01-01 | 467.00 | | XYZ | 2019-02-01 | 8721.00 | | XYZ | 2019-03-01 | 6818.00 | | XYZ | 2019-04-01 | 2167.00 | | XYZ | 2019-05-01 | 2166.00 | And consider you want to add a new column that has the total revenue by month so you can see what % of the revenue each individual row accounts for. You would need a "sub-select" or "sub-query" or "temp table" to run the monthly sums, then join back to the query you started with in order to accomplish it.
In MS SQL your GROUP BY example will not execute unless you only have (1) department_ID.
What do you mean?
Somebody mentioned about updating it. Not sure if it’s already been updated or still in the process of updating the cheat sheet. 
please post your query sometimes, the use of GROUP BY or DISTINCT can hide an underlying inefficiency
[DataCamp](https://www.datacamp.com) has free and paid SQL courses. This is probably the lowest effort way to get started.
&gt; why 'Avg(salary)' in the subquery did not display the average salary of all employees? because it's a **correlated** subquery, calculating the average only for each employee's department **NOTE** both queries will return multiple salaries in each department, but no indication of which employee has which salary -- that's perfectly valid, but a bit unusual
&gt;why 'Avg(salary)' in the subquery did not display the average salary of all employees? Because of the clause Where b.department_id = a.department_id Since the subquery refers to a table in the main query, it is being evaluated individually for each row in the main query. [This is called a correllated subquery](https://en.wikipedia.org/wiki/Correlated_subquery). Each time it gets called, the where clause restricts it to returning the average of salaries in the department of the parent record. Removing that where clause, so the subquery doesn't refer to any table in the main query, means it is no longer correlated and will just be evaluated once, and get the average across all departments.
&amp;#x200B; Let's say I have a basket of apples, there are 5 apples in my basket. &amp;#x200B; SELECT * FROM BASKET Result: &amp;#x200B; |Fruit|Color|| |:-|:-|:-| |Apple|Red|| |Apple|Red|| |Apple|Red|| |Apple|Red|| |Apple|Green|| &amp;#x200B; Now let's say out of those apples, I want only red apples. SELECT * FROM ( SELECT * FROM BASKET ) Subquery WHERE Color = 'Red' Let's also say I want only red apples in a single query. SELECT * FROM BASKET WHERE Color = 'Red' Both queries are essentially doing the same thing, ideally the optimizer knows that it's not quite efficient to do the subquery here because that means you are grabbing everything in the basket, then, you are grabbing everything in the basket a second time but only where the criteria is met. In complex queries or different database engines, this could be handled differently by the optimizer. The piece you see in the code block that says 'Subquery' is an alias. This is typically required when you have a subquery, this requirement helps keep things un-ambiguos so the database engine knows what column is coming from where. Let's do a more common query example now. Let's say we have a transaction table and a fee table. When a transaction occurs, you associate it to a fee. Transaction table |TransactionID|FeeID|Status| |:-|:-|:-| |2|9878|a| |3|101|b| |98|200|c| |1|1|d| Fee table &amp;#x200B; |FeeID|Fees| |:-|:-| |9878|$1| |101|$2| |200|$999| |1|$0.1| Let's say I wanted to look at all transactions that had a fee of $999 with a subquery. SELECT TransactionID, Status FROM Transaction as Tran WHERE Tran.Feeid in ( SELECT FeeID WHERE Fees = '$999' )FeeSubQuery I'm asking for the TransactionID and Status of the Transaction table, which I am now calling Tran. The where Tran.Feeid is me using the alias and referencing the column from the aliased table. The where clause is utilizing a subquery where we are extracting the FeeID from the Fee table where the Fees matches '$999'. Note, you can actually change this to a join for the same result. SELECT TransactionID, Status FROM Transaction as Tran INNER JOIN Fee as Fee on Tran.Feeid = Fee.Feeid and Fees = '$999' Sometimes it makes sense to use a subquery, sometimes it makes sense to use a join. As another example, I would probably utilize subqueries the most often when dealing with aggregate data. When you deal with aggregate data, you will typically have to group by all columns you are selecting. [Here's a decent Stack Overflow example on this](https://stackoverflow.com/questions/16317814/sql-subquery-in-aggregate-function), let me know if any of this doesn't make sense or isn't clicking.
Welp, already learning stuff, from the major engines that I've used only SQL Server doesn't have named windows. MySQL, Postgres, Oracle and sqlite all have them!
Oh yeah totally, there's `STRING_AGG` in SQL Server but it's only been introduced in version 2017, so probably I'll get to use it 2042 given how fast company I work for progresses on the upgrade path. Postgres has `STRING_AGG` which is even better because it supports `DISTINCT` and `ORDER BY` in it.
MySQL's has always been robust GROUP_CONCAT([DISTINCT] expr [,expr ...] [ORDER BY {unsigned_integer | col_name | expr} [ASC | DESC] [,col_name ...]] [SEPARATOR str_val])
To answer the opposite of your question, after spending most of my career in MS SQL I've moved on and discovered that basically everything else has GREATEST and LEAST functions. 
Can you explain the question, but I have no idea what the OP is asking, or what you're guessing.
Just shared this with 5 of my colleagues who are learning sql! I will let you know if they have any feedback!
sorry, i have no idea about the question... maybe some kind of transpose cipher? as for my guess, i was told it had to be a 3-digit number
Logically the answer seems that it could only be 345.
Try this kind of expression. It will allow a running total or subtotal to be calculated. It will be the same for every line depending on the partition but it means you can subtract the regular from the total. Sum(X) OVER (PARTITION BY y ORDER BY z) https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017 I hope this makes sense. 
Have you tried using PowerShell script in the SQL agent. It is limited but allows more flexibility than command?
He's right in terms of assembly code, set theory, and pure math. You can't do a join without a LOOP (at the most basic level of what a loop is). There is just no mathematical way to handle it without the essence of a LOOP. I think the issue here is that you are treating the term LOOP to be a specific SQL function, whereas he is treating the term to represent a physical function of nature. What he seems to be saying is that you could distill a JOIN into a LOOP that executes the same way (which is an unproven hypothesis), but that it wouldn't be a good idea... because it would be extremely unwiedy / difficult to write, and why would you want to when you could just use a JOIN which is the intended function. What goes on behind the scenes from how you write it to how it must physically behave mathematically is the difference here. He's making a very valid point about JOINS which seems to have very little practical use, but it's something I find very intellectually curious. To be entirely fair /u/AbstactSQLEngineer has not done much to given evidence that his hypothesis is correct, but to be just as fair you have done nothing to demonstrate it is incorrect. If I were going to bet, I would bet on him being correct.
Here is the Microsoft explanation of Coalesce and how different from ISNULL: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql?view=sql-server-2017 
You should add the date conversion codes to this. I always forget if i should use 110, 112, or some other conversion to get the right date format i want. I literally look this up once a week!!
Also, to be clear, I was not using that piece of sample code to compare it to a JOIN, I was using it to compare it to a CURSOR, and the examples of LOOPS which were given in the link provided that did not look similar to my own use case. In another link provided there was a section in the comments talking about using the LOOP for dynamic SQL and pondering whether it changed the execution plan... and since in my real code that is what's going on... you can see why my "sample" was the way it was.
Pretty good! It would be great to have Postgres too. And you might want to make one version for each DBMS—that way, each version would be more compact and people wouldn't have to look through parts they don't need. (I know I'm asking a lot for a free service—these are just ideas to make it better.)
I hate `COALESCE` and tend to avoid it. I have no rational reason for this, but I just tend to see `ISNULL` as a simpler this/that statement, and in terms of readability based on how I use SQL... I prefer to use `ISNULL`. `COALESCE` annoys me to the point where I don't care to learn any of its nuances, and I can accomplish anything I want (for what I do) with an `ISNULL` or a nested `ISNULL`. I find this more intuitive in terms to how I think, and by definition this probably means more people would find it harder to read.
I wanted the world to burn. Unfortunately, it ended up being my world. 
I have a limited experience with ssis, I'd say probably not. I believe SSIS doesn't really use code except for the script task. I don't know how complex some of the stuff you will be building or creating, but you can pretty much copy + paste how to send an email with the script task. If you have to build/maintain Access databases, then yeah I see VB coming in handy. 
Yeah, script tasks are what I was thinking of. But I get the sense the need for complex script tasks isn’t that common. Thanks for the input
I used SSIS for two years at my last job, and on and off before that. In the entire time using it, including for developing SSRS reports that ended up being a custom suite of financial reports &amp; dashboards to replace an aging piece of software that previously was used to generate the reports, I only had to use C# once time. For actual data loads overnight (pull from production into a financial environment), I never had to use C#. It was either straight SQL, stored procedures or little bit of SSIS scripting. And even then, when I redeveloped a piece during my last 2 weeks on the job, I was able to do the same thing with SQL (basically forced retry of a container upon failure, up to 3 times, with a 15 second delay in between each time) with a bit of Googling.
I may be mistaken, but with the correlated subquery you're already "group by" department, so the actual GROUP BY doesn't do anything, there is only one Dept being evaluated at a time.
Sounds like I don’t need to bother. One less thing to worry about. 
Here's one way: count the number of GA areas, compare with the number assigned to a person.
Right, I was actually start to think like varchar100 takes 100bytes divided by 3 that’s 33, then int takes 4bytes 33+4 is 37 but I couldn’t figure out how the first part gives 900
Thanks for the link. I had a quick read and will finish later! 
Cheers for the reply. Yeah isnull seems a lot nicer to use. Not sure why I defaulted to coalesce... Think it is what I was told to use a while ago. I'm still new to making my own queries. 
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Nice work. It starts off with the basics. Each page gets more into depth. By the end you're showing CTEs. That's a solid SQL guide.
Thank you all for your help. I have tried all methods and it could be me doing something wrong or i should have mentioned. I'm using Informix as the database and i believe its t-sql. Does that make a difference to the below suggestions?
This is a perfect explanation, can i post it here or link somehow?
Do the objects exist on another server without the encryption? Or what about source control, could they possibly exist there in a decrypted form?
I would say my SQL skills are pretty good, but date conversion is one of the hardest things to remember, at least for me. Could be that my brain is not that fresh, but still.. One other thing would be few simple checks like how to get DB name (handy when working daily with multiple databases), when you need for whatever reason to get official name. 
If portability is a concern: `coalesce()` is standard SQL, while `isnull()` is not.
Because (relational) databases support transactions. If you want to be cautious, start a transaction run your statements, verify the data then commit or rollback. Several DBMS products even allow that approach for DDL statements.
Don't you just love having to write this obviously not overly verbose subquery `(SELECT MAX(val) FROM (VALUES (value1), (value2), (value3)) v(val)) AS greatestValue`?
How?
Where are foreign keys? How to create one-to-one, one-to-many relationships?
I would say you don’t need to bother. Granted, it’s sometimes nice to be able to write a Script task in C# but those occasions are few and far between. Besides, I think ( and hope) SSIS will go the way of the dinosaur in favor of Azure Data Factory where C# isn’t used at all ( I don’t think). So, I would suggest that if you have extra time on your hands, learn ADF, or the AWS equivalent if you’re in the AWS camp.
Thank you! I kept hoping that stumble onto something like this some day!
Nice one, even includes simple and to the point join explanations. Wouldn't mind seeing some functions like seeing number/character functions with a quick example like length, replace, concat, trunc, floor etc but still love it and downloaded 😁
I always have trouble remembering index and check constraints manipulation; then there's date formatting and other database-specific functions.
This is probably best as a basic inner join. You would only need an outer (left/right) join if there, for example, you wanted all suppliers and software to show up even if they didn't have prices in the table. A join is always between two tables, and once the join is complete you end up with a single table (that includes the data from the tables that were joined). So, think of how suppliers and products connect to each other and write that join. Then you'll end up with something like this: SUPPLIERS SOFTWARE +----+--------------+--------+----+----------+------------+ | id | supplier | active | id | supplier | software | +----+--------------+--------+----+----------+------------+ | 1 | Microsoft | 1 | 1 | 1 | Word | +----+--------------+--------+----+----------+------------+ | 1 | Microsoft | 1 | 2 | 1 | Excel | +----+--------------+--------+----+----------+------------+ (The columns will actually be named table.columm, to avoid duplicate names. E.g. SUPPLIERS.id. This is where table aliases are useful.) Now think of how the table above connects to prices, and add your necessary filters to the `where` clause. 
Wouldn't it just be: &amp;#x200B; SELECT p.Price FROM Suppliers s LEFT JOIN Software sw ON sw.Supplier = s.Id LEFT JOIN Prices p on p.Software = sw.Id WHERE s.IsActive = 1 and sw.Software = 'Word' &amp;#x200B;
If I understand, u only need inner joins. ``` CREATE TEMP TABLE supplier AS SELECT 1 id, 'Microsoft' supplier, TRUE active; CREATE TEMP TABLE software AS SELECT 1 id, 1 supplier, 'Word' software UNION SELECT 2 id, 1 supplier, 'Excel' software; CREATE TEMP TABLE price AS SELECT 1 id, 1 software, 100 price UNION SELECT 2 id, 1 software, 150 price UNION SELECT 3 id, 2 software, 200 price; SELECT s.software, p.price FROM software s JOIN supplier su ON su.id = s.supplier JOIN price p ON p.software = s.id WHERE su.active AND s.software = 'Word'; ```
Note: when you filter left join table on WHERE clause, you do your ```left join ``` works like an ```inner join ```.
The number is still not guessed.... It wasn't 937 nor 345 :)
https://dev.mysql.com/doc/refman/8.0/en/resetting-permissions.html &gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass'; 
I only got this text, with no context....
Thank you. This works well. I will take the other person's comment (left join being like an inner join) to see if I can improve it.
Thanks for this tip.
Thank you for this. I will have a think about what you've said and see if I can get it to work.
There are 3 distinct lengths so that was my guess.
Syntax is right: https://dev.mysql.com/doc/refman/8.0/en/alter-user.html &gt; IDENTIFIED WITH auth_plugin BY 'auth_string' You try it with just `root` instead of root @ localhost? 
When you say Developer, you mean Database Developer? Im technically a DBA now but last job I was mostly writing reports, the first page and last page was stuff I knew or at least viewed a bit. The second sheet though I never really touched on and I doubt as a DBA I will even now.
Here is a list of a lot of books you're searching for, hope this helps! [SQL Books](https://www.datapine.com/blog/best-sql-books/)
&gt; You'll also lose collations, and case insensitive text comparisons. Huh? Isn't ilike Postgres-only? And Postgres has great regex support.
How are your backup &amp; restore skills?
I do well with those. I use the ola Hallengren scripts at the moment but I k ow how to configure maintenance plans for backups of databases and logs to keep them from blowing up and can write backup and restore scripts. I’m not too up on writing good index rebuild scripts as I know the native sql process is not that great for it. 
L3 schools meets my needs daily, especially when I have to switch back and forth between different languages and you get your syntax confused
Yup, I mean database developer. Creating and altering temp tables and tables happens in almost every stored procedure, so a dev should be very comfortable with these statements. How do you like the DBA side? How much of what you do is proactive vs reactive? Is it like an on-call scenario?
How do you do that?
You only need an inner join. It doesn't sound like this specific scenario requires and outer join
so, it would appear that you know the answer therefore, can you tell us what it is AND WHY
ILIKE is not sargable. Neither are regex.
No problem. I teach SQL/Oracle/APEX for a living so I have worked out some of the kinks over the years :P. Dual is a special table with a single row and column. The important thing to remember about dual is we do not care about the data IN dual, we are using the row to compute a value that we need. Sometimes you need to add records or compute values in SQL that do not exist in a table. Here is a classic example of getting the current date from the database. This can run in any oracle instance because every instance has dual. select sysdate from dual sysdate is not a column in dual it is a function which returns the database date. Here is a slightly different example to get the current day of the week. select to_char(sysdate,'Day') from dual Sometimes there is a need to generate several records (like what I did previously) it is generally accepted that you can create some additional dual tables with a predefined number of rows such as dual_10, dual_100, dual_1000. These tables have 10,100,1000 rows respectively and the actual data is not important you just know it is possible to generate a certain number of rows from them. If I had a dual_10 table for the previous example my code would have looked something like this: select rownum as step from dual_10 where rownum &lt;= 4 rownum is an Oracle pseudocolumn(selectable like a column but its not stored in a table) which returns the order the row was selected. This example will yield 4 rows with step incrementing from 1 to 4.
This helps actually, i do have a GROUP BY. Btw whats the best way to post my query here? Direct paste or through a link? Thank you!
Postgres' GIN indexes make ilike comparisons and regexes blazing fast. I don't know any of the in-group terms, so maybe that's not sargable, but I'm also not sure I understand how SQL Server handles these case insensitive text comparisons? Surely 'abc' != 'ABC' without some kind of adjustment?
This was a bit above my scope of knowledge but thank you! You gave me some new avenues to look down. 
let's take the first part, count the number of GA areas: take a set that defines all of your areas per category, write a query that returns the counts per category (access level 1 from your pic). Post what you have so far.
best way is copy/paste the text here, that way we can copy and either test it ourselves (in those cases where sample data is also provided) or at least use it to write a modified/fixed version put 4 spaces at the front of each line and it will show in fixed font SELECT bar.foo FROM bar INNER JOIN fap ON fap.bar_id = bar.id WHERE bar.bat = 3 AND bar.qux = 'ok' GROUP BY bar.foo
I'm mainly an App Admin here but I do DBA stuff as my main title and anything SQL related beyond report writing comes to me. There isn't too much work database wise here but DBA can be fun when I get asked to do data import or ETL tasks. I enjoy SSIS/SSMS and making servers work though I do miss doing report writing sometimes. A lot of my stored procs I make use temp tables though, I had some devs a while ago tell me that temp tables are bad and I instead should be sub querying data or just pull it all into other tables for use. I kind of scratched my head as I am self taught but know that temp tables are great as they're fast and lightweight. They make data manipulation easy enough so I was able to follow a lot of that. How is the Developer side and what do you do with it?
Working with a larger more complex environment will probably require someone taking a chance on you, everyone wants to hire someone cheap that has loads of experience. So first things first know your salary range. Look on Glassdoor for database Administrator average pay near you and be prepared to come in at the low range. Skills wise you want to focus on playing up the idea that you’re all about standard procedures and documentation. Managing sprawling database environments is all about keeping things consistent and being well documented so different members of a team can step in and out of different tasks. Get good with backup and restore methods, disaster recovery methods and terminology, and most of all query performance tuning. Having a familiarity with other database tech is a good idea, I’m finding everyone has a couple of NoSql servers so while they don’t want a mongo expert, having some familiarity with it is a big bonus. Same for cloud stuff most of which is available to practice on if you have msdn for example.
Thanks for responding. I am pretty good at the moment with backups and restores, I document the hell out of everything I do and touch and make sure its available for others. Unfortunately with the size of my environment I don't get to mess too much with disaster recovery but I am hoping in the next year here I get to upgrade all of our environments with SQL2016 and setup some Always On to get used to configuring and maintaining that. I will need to work on performance tuning I think as Im not too up on that one. Thanks for the advice, I really appreciate it!
From an experienced self-taught SQL user/developer’s perspective, could someone please confirm that this is a good list of books to learn SQL?
This is my opinion, but you don't need a book to get going. SQLBolt.com got me started and then once I got a job using SQL, I took some database classes at my local community college. After that I bought a recommended book for my specific SQL flavor. All this was over the course of 1 1/2 years. The rest of my learning is topic specific, but since I have a basic understanding of SQL **and** databases I do well in my current position. 
&gt; without a subquery But why would this be a plus? Subqueries are easier to read, and the optimizer is almost certainly going to evaluate it the same way.
How would you create a `GIN` index on a `TEXT` field? ``` data type character varying has no default operator class for access method "gin" ``` same is for `TEXT`. Maybe you meant `GIST`? But still, even with that, I get 17ms on an exact match query, and 1300ms for an `ILIKE`. It's the first time I hear for an index to be case-insensitive on postgresql. `GIN` works for ranges, but not text. You could add an extension to use trigrams, or use tsvector, but that's a completely different story. I want a fast, exact match on a case-insensitive field. Solutions in postgresql are either using the non-sargable and non-indexable `ILIKE` operator, or you could index on the expression `lower(field)` and join and search on that, which are all workarounds. MSSQL has native case-insensitive text comparison operators. As soon as you declare your collation properly, you can do all the operations without worrying about case sensitivity or insensitivity. The data type declarer has to take care of that ONCE.
Before I knew .NET, I avoided scripts in SSIS (because I didn't know it). I learned .NET, and scripts in SSIS seemed really cool! Now that I've managed some SSIS packages containing .NET scripts... I am back to trying to avoid them wherever possible. From my experience, MS does not make deployment easy for this stuff unless all of your SQL versions and VS versions are consistent and all of your devs are on the same page with versioning. It seems no matter what we're doing, somehow one of the scripts gets corrupted whenever we share work. It's probably a process problem, but it's a huge pain in the ass compared to simply sharing SSIS packages.
One of my favorite Oracle-exclusive features is that whenever you Google `Oracle $topic`, you can learn about `$topic` for 10 minutes before realizing it's a mysql article hosted on oracle.com.
Yes, it's the trigram extension I'm referring to. It doesn't require anything special-- CREATE TABLE test_trgm (t text); CREATE INDEX trgm_idx ON test_trgm USING GIN (t gin_trgm_ops); 
I would tend to agree with your line of thinking - but there are others that think the other method is easier.
Of course there are other ways to learn SQL beside books (online or offline courses, learning on your own with online resources etc). However, if you are more oldschool and want to learn with books, this list should help you a lot ;)
What flavor of SQL are you using and what are you using for the scripts...? This is relevant because MS SQL Server doesn't care if the column has data, you can just drop it without issue... but if you're deploying this with Visual Studio, you might have to go to Schema Compare Options =&gt; General =&gt; Uncheck "Block on Possible Data Loss" or it's somewhere else depending on the version of Visual Studio.
AlwaysOn is great but hard to get "resume quality" experience with until you get to a big company since it's so expensive. You can play around with Sql Developer edition to get some practice though. Work with replication where you can, Sql Log shipping is also used for DR so that's a good tool to get used to. If your company has the need maybe look into setting up a replication publisher/subscriber for reporting workloads. That'd get you practice and something to put on the resume. The other thing I thought about was automation is key. Get experience scripting with powershell and get experience deploying things to sql with scripting. Automating performance monitoring is a good road to go for practical experience. In a big environment we don't get spoon fed a lot of queries with problems, more often it's generic "server B seems slow, what gives?" So having scripted server side tracing, or extended events setup across our farm, allows us to step into a server and start to get a look at heavy reads workloads, high CPU offenders, on demand. It also facilitates our healthcheck work where we can alert on machines that spike CPU usage for instance and we have the data we need to figure out what caused the event, and then can start query performance tuning from there. That kind of "proactive" performance experience is big for DBAs I think. So maybe work to implement that kinda stuff at your job now, again so you can honestly list it on your resume as something you've been doing in your day to day work. Saying "i did x at my old job" carries more weight than, "i played around with x on my home computer" unfortunately.
I appreciate the link! I been trying to learn SQL but was not sure where to start. &amp;#x200B; Thank You,
I believe Amazon RDS is free for small-ish loads - unless you are talking about massive data sizes, I wouldnt go Redshift. https://aws.amazon.com/free/database/ 
Yeah I think so. I recognize a number of the books on here both from browsing and from seeing suggestions for them (the mere mortals one for example). I also personally own “SQL Practice Problems: 57 beginning, intermediate, and advanced challenges for you to solve using a ‘learn-by-doing’” approach by Sylvia Moestl Vasilik and it's a great book, if a little thin.
That's not really equivalent to case insensitivity on MSSQL * You can't `GROUP BY` without specifying `lower()`, in which case you'll lose information on the field * You can't `ORDER BY` without writing an extra `lower()` call * You need indexes where otherwise you wouldn't (I'll explain lower) * `JOIN` operations are abysmally slow So here is my test case: I have a table with more than 1M unique short strings, with some integer values associated with them. I split it into 2 tables, and made this: ``` CREATE TABLE writeable.ad_group_id AS SELECT DISTINCT ON (ad_name) ad_name, ad_group_id FROM ad ORDER BY ad_name; CREATE TABLE writeable.ad_id AS SELECT DISTINCT ON (ad_name) ad_name, ad_id FROM ad ORDER BY ad_name; ``` Now I'll be executing this query: ``` SELECT COUNT(DISTINCT aid.ad_id) FROM writeable.ad_group_id agid LEFT JOIN writeable.ad_id aid ON agid.ad_name &lt;operation&gt; aid.ad_name ``` the `ON` statement will differ in 2 ways: * Using `lower()` for each field or not * Being `=` or `ILIKE` I also add indexes gradually to `writeable.ad_id`, and see how well it performs. Results: * 1.17s for fixed match, no index. Yes, no index, because the values are already physically ordered. Woohoo! Adding a `BTREE` index changed nothing obviously. * FOREVER for ILIKE match, no index. I waited for 5 minutes then just killed it. * 3.67s for lower() exact match, BTREE index on lower() expression * FOREVER for ILIKE match, BTREE index on lower() expression (because ILIKE is not sargable in this case) * And I waited for more than 10 minutes (with no result) for ILIKE with GIN index with gin_trgm_ops. It's using the index only on `WHERE` conditions. But it's not touching it, and doing a nested loop. As far as this goes, for exact matches `lower(x)` indexing with BTREE is far more usable. And on MSSQL you don't even have to think about all of this stuff. If it's declared as `CI`, it will behave as `CI` everywhere with no extra effort whatsoever.
You'll be doing something like this to generate the array within the result: select row_to_json(t) from ( select order_number, name, ( select array_to_json(array_agg(row_to_json(i))) from ( select item_name, price from items where id=customer_orders.id ) i ) as items from customer_orders ) t
Ya I am trying to deploy via Visual Studio. Unfortunately I can't uncheck that box and our dev lead says he doesn't want to uncheck it just for this deployment and we need to find a way to do this for future deployments. Sounds like I might have to go the second route.
Thanks!
are the tables related? can they be joined? can we see what they look like? &gt; I'm using SQL workbench so the DATEDIFF function won't work what makes you think that? doesn't the "SQL Workbench" support SQL?????????
It should, shouldn't it! When I type datediff or timediff into the command prompt, it doesn't recognize either as a function.
Good luck
This worked! Thank you :)
is there a tab or window where you can paste complete queries and press a button or something get them to run, instead of letting the interface build the query for you?
CTE's are a good way to show this: with query as ( select client, month, sum() as revenue from table group by client, month ), montly_totals as ( select month, sum(revenue) as monthly total from query group by month ) select a.*, a.revenue / b.monthly total as Percent from query a inner join monthly_totals b on b.month = a.month
In addition to above, I see a lot of postings for SSRS and working with Azure. I used to do everything; SSIS, DBA, and Front-End work in the same job. I don’t know if you want to always be a DBA. I transitioned full time to Business Analysis and now all I do is write SQL / make Tableau reports. It’s great and I don’t miss the stress of DBA work. I also get paid more!
Something like Run : insertdatascriptlab1.sql
Good idea on the date conversion and simple checks! I'll add those.
Good idea with the separate sheets for each DBMS and one for Postgres! I can make those updates.
Ah yeah I've got some feedback here so I'll make updates in the next few days.
Good idea with the functions! I left them out as they are different for each DBMS, but someone else suggested splitting them by DBMS so I'll do that and add these functions.
Good question. I think the FK is in the Constraints section, but that only shows how to add it to a table as you're creating it. I could add in more info about creating relationships - maybe I'll do this as a separate cheat sheet as the concepts are the same across all DBMS. Thanks for the suggestion!
Awesome! Thanks in advance. What DB are you mostly using?
That would depend the database engine, but I cannot think of any engin today that blocks everything. Of course locking will happen, because ACID (Atomicity, Consistency, Isolation, Durability. See Wikipedia: [https://en.wikipedia.org/wiki/ACID\_(computer\_science)](https://en.wikipedia.org/wiki/ACID_(computer_science))). But concurrent access to the database is kinda the least expected functionality. 
Right now I mostly use Oracle, but use a bit of MySQL and have worked in places recently that use SQL Server.
Cool. I am starting with MySQL. Using MySQL Workbench. And I feel like I need to learn the command line also. 
Do you mean W3Schools? [https://www.w3schools.com/](https://www.w3schools.com/) If so, what do you like about it?
Do you mean from multiple concurrent sessions? It depends on the RDBMS, configuration, and the queries themselves. From a single session? They'll be executed sequentially.
Sorry, yes, typo. I think your guide is absolutely great for beginning to moderate users. W3 schools just has use cases for virtually every SQL variant and function. 
Can't you just copy it all into one script? 
Yea I can but want to keep them in separate scripts. 
In my context: single session. So, you say it'll be blocking, right?
Define blocking.
Define "blocking". Query 2 won't start until Query 1 completes. Everything you send in that batch (request) will execute in sequence. There's no "blocking" in database terms happening here, it's executing each query you sent in turn.
IF you can find a way to call on the .sql file you could declare @script1 varchar set @script1 = print([whatever function you find to open the file]) then execute @script1 ... but I dont think you are going to find that function. You are better off saving script1 as a stored procedure then simply having an execute script1_stored_proc GO script 2 code
Postgresql had this for 10 years now
Except that MySQL would be better without them because they're crap.
Crap? Those functions have saved me literally dozens of seconds.
If you are talking MSSQL, the answer is: Query 2 wont start until query 1 finishes, if both are sent within the same batch Procedure 2 wont start until procedure 1 finishes without an error (depending on catch handling). Checkout maxDOP or parallelism in Sql. Some things run synchronously. It would suck if q2 ran at the same time q1 did, if q1 updated something and q2 read said thing. 
you want to use Relational Division to do this. Read up on it then come back
Are you concerned about the value not looking right in the output? Is this just for display reasons?
Well, the query is part of a business rules engine and that query is going to populate a field in a table that is defined as numeric. The field is numeric bc ultimately it will be used to for calculations. My worry is that converting to varchar would impact calculations. 
If it needs to be numeric, I'd suggest leaving it as number. Ultimately adding the leading or trailing zeros is just a display preference. 
I would love if window functions were on here too! Thanks for this!
Sample schema (simple, no keys or indexes): `CREATE TABLE jobs (` `job_id INT,` `job_name VARCHAR(50)` `);` &amp;#x200B; `CREATE TABLE skills (` `skill_id INT,` `skill_name VARCHAR(50)` `);` &amp;#x200B; `CREATE TABLE jobs_skills_map (` `job_id INT,` `skill_id INT` `);` &amp;#x200B; `INSERT INTO jobs VALUES` `(1, 'Programmer'),` `(2, 'Farmer'),` `(3, 'Bartender'),` `(4, 'Stock Broker'),` `(5, 'Sales Person');` `INSERT INTO skills VALUES` `(1, 'Analytical'),` `(2, 'Interpersonal'),` `(3, 'Early Riser'),` `(4, 'Physical'),` `(5, 'Intellectual'),` `(6, 'Detail Oriented'),` `(7, 'Money Oriented'),` `(8, 'Customer Focused');` `INSERT INTO jobs_skills_map VALUES` `(1, 1),` `(1, 5),` `(1, 6),` `(1, 7),` `(2, 3),` `(2, 4),` `(3, 2),` `(3, 4),` `(3, 8),` `(4, 1),` `(4, 3),` `(4, 7),` `(4, 8),` `(5, 2),` `(5, 3),` `(5, 7),` `(5, 8);` &amp;#x200B; Sample Query. You would filter by jobs\_skills\_map.skill\_id instead of skills.skill\_name, but this demos better. `SELECT` `j.job_id,` `j.job_name,` `COUNT(s.skill_id) skill_matches` `FROM` `jobs j` `JOIN jobs_skills_map jsm ON j.job_id = jsm.job_id` `JOIN skills s ON jsm.skill_id = s.skill_id` `WHERE` `s.skill_name IN ('Analytical', 'Detail Oriented', 'Physical', 'Money Oriented')` `GROUP BY` `j.job_id,` `j.job_name` `ORDER BY` `skill_matches DESC` `LIMIT` `5` &amp;#x200B; |job\_id|job\_name|skill\_matches| |:-|:-|:-| |1| Programmer |3| |4| Stock Broker |2| |2| Farmer |1| |5| Sales Person |1| |3| Bartender |1| &amp;#x200B;
How did you go about going from learning on your own to employed?
They caused my bugs because of how they handled nulls, and how they handle nulls now
Thanks. There's so many cheatsheets that cover queries, but none that cover database design.
Honestly, I'm not sure I'm following you (what's the difference between b1,2 and 3?), but I've approached this issue by chunking it out into separate statements. So IF @Var = 1THEN [WHATEVER B1 IS] IF @Var = 2 THEN [B2 SCENARIO INSTEAD].
tableB1, tableB2 and tableB3 are basically three versions of the same table created for different subsets of the data at different points in time; for example, you can think of them as "table B for values during the year 2017", "table B for values during the year 2018" and "table B for values during the year 2017 but with assumption X". It is the main dataset used for a model, this SP should create additional (optional) variables for that set of observations. After writing this post, I found the IF syntax for SQL Server (seems obvious that it should exist in retrospect but I only found CASE WHEN when searching earlier) and that has actually basically transformed my question into "how do I use if statements in a stored procedure" because my attempts have been woeful.
I don't think there is an easier way than yours. You could maybe try 'IN' (Person1 IN ('Bob', 'Jim') AND Person2 IN ('Bob', 'Jim')) But it cover when both people are Jim or Bob.
Maxdop and query parallelism only apply to the individual queries. SQL Server will execute multiple queries at once regardless of these settings. Maxdop controls how many threads a single query will split into, and cost threshold for parallelism controls how expensive a query has to be to be considered for parallelism. 
You can check before you start to see if the temp table exists and drop it if it does: IF OBJECT\_ID(N'tempdb..#tmp1') IS NOT NULL BEGIN DROP TABLE #LogFileList; END
It doesn't, not before or after the error; the problem is that I am trying to insert into the same temp table in the two different if statements. Changing the second to #tmp2 works, and afterwards only #tmp1 exists. Regardless, I think I figured my way around the problem.
Ah, I missed that you were doing two IF blocks, I read it fast and assume IF...ELSE. Is only one of those ever going to be a true statement? You could change the second IF to ELSE and it would work.
Huh, TIL. Thank you, that variation did not enter my mind, and explains a lot.
 CASE WHEN ... THEN ... ELSE ... END That is the if statements of SQL. so it would be 
 UPDATE loc_mercury SET supply_current = case when supply_current &gt;= 1000 then supply_current - 1000 else supply_current=0 end WHERE item_id = 6;
Perfect, thanks! I knew it had to exist!
I'm not a MySQL person, but you should be able to use a case statement. update loc_mercury set supply_current = case case when supply_current = 500 then supply_current = supply_current - 500 else supply_current = supply_current - 1000 end
Other solutions look good. Or you could even Update ... Set ... = max( 0, supply - 1000 ) Sorry on my phone can’t see column names when responding. There’s probably a thousand ways you could do this :) good luck, game sounds fun
MySQL? you could use the GREATEST function UPDATE loc_mercury SET supply_current = GREATEST(supply_current - 1000 , 0 ) WHERE item_id = 6
Appreciate it, thanks! I'm looking for pre-beta testers to help work out the bugs if you're interested ;) [geministation.com](https://geministation.com)
I’ll check it out! Thanks
Keep it as numeric for any calculations, etc., etc., but if you want to "display" it then just write a view that converts it &amp; adds a `$` and any trailing zeroes. It will look the way you want, but the data will be stored in the correct format for any work.
Well, you can join conditionally, sort of: SELECT main.* FROM mainTable main LEFT OUTER JOIN optionalTable1 op1 ON main.column = op1.column AND @var1 = 1 LEFT OUTER JOIN optionalTable2 op2 ON main.column = op2.column AND @var2 = 2 LEFT OUTER JOIN optionalTable3 op3 ON main.column = op3.column AND @var3 = 3 WHERE op1.column IS NOT NULL OR op2.column IS NOT NULL OR op3.column IS NOT NULL This will only work if you need columns from the main table though (in which case you don't use a join, you use `WHERE EXISTS (SELECT * FROM optionalTable1 WHERE column = main.column)`. If you need something from op1, op2 or op3 then it gets messy because for each column you need something like `COALESCE(op1.column, op2.column, op3.column) AS column`. Alternatively, for that EDIT of yours regarding temp tables, you can create temp table first CREATE TABLE #tmp1 ( -- columns ) IF @var1 = 1 BEGIN INSERT INTO #tmp1 SELECT * FROM tableB1 END ELSE IF @var1 = 2 BEGIN INSERT INTO #tmp1 SELECT * FROM tableB2 END Or if you're too lazy to specify columns for the `CREATE TABLE` you can always `SELECT * INTO #tmp1 FROM tableB1 WHERE 1=0` instead, and then follow with the same if covering various INSERTs.
Glad you got a solution worked out. The error you were seeing with your initial attempt was unrelated to your IF blocks. You need to drop your temp tables after you're done with them as they persist for the duration of your session. If the record set you're inserting into them isn't too large using a table variable instead would also resolve the issue.
Excluding (Bob, Bob) and (Jim, Jim) is as easy as `AND Person1 &lt;&gt; Person2`.
Well, of course! Replace names with IDs that are prime numbers (let's say Jim is 3 and Bob is 7) and then you can simply `WHERE Person1 * Person2 = 21`. Jokes aside, I don't think there's an easier way.
I kinda like this solution :'D 
Well apart from a rather poor database design, you're almost there. `HAVING COUNT(*) = $skillcount` would be fine.
You can do a check with a select all from the table with an ORDER By population ASC/DESC. This will show you (in a different way) if there are any counties with the NULL
In case I mis-read the problem, and you're selecting MIN values from two different columns, that's a lot easier. Just make them variables and add them together: DECLARE @MinValue1 INT DECLARE @MinValue2 INT SET @MinValue1 = (SELECT MIN(ValueColumn1) FROM YourTable) SET @MinValue2 = (SELECT MIN(ValueColumn2) FROM YourTable) SELECT @MinValue1 + @MinValue2 &amp;#x200B;
Are you sure all the values for no population are actually NULL and not blank or a value of zero? NULL and blank are distinctly different in SQL, with a blank being the absence of any data and NULL being a complete unknown. &amp;#x200B; SELECT Country ,Population FROM World WHERE Population IS NULL OR Population = 0 OR Population = '' &amp;#x200B;
To add onto the case/when comments, you can combine/chain the cases too, and `AS` a custom name after `END`: CASE WHEN condition THEN action WHEN condition THEN action ELSE action END AS new_column_name or you can do this: CASE WHEN condition THEN CASE WHEN condition THEN action ELSE action END ELSE action END AS new_column_name
I would use a subquery that makes an array of `from` dates grouped by owner, ordered by ascending `from` date. The main query would then choose the earliest value of that array { `min(array_agg)` in pSQL }. It could look something like this (PostgreSQL): SELECT t1.order, t1.owner, MIN(t1.from_dates) as from FROM (SELECT order, owner, ARRAY_AGG(from) AS from_dates) FROM table GROUP BY order AND owner ORDER BY from) t1 This groups by owner but only provides the earliest date of that owner. I'm not sure how you're creating the `to` column but it should be easy to integrate it
Please do! Also could you add window functions? I always forget them. Thanks!
!remindme 7 days
I will be messaging you on [**2019-02-01 20:05:36 UTC**](http://www.wolframalpha.com/input/?i=2019-02-01 20:05:36 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/aj2y77/sql_cheat_sheet/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/aj2y77/sql_cheat_sheet/]%0A%0ARemindMe! 7 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
If you have TSQL then window functions are your saviour SELECT DISTINCT order , owner , MIN (transactiondate) OVER (PARTITION BY order, owner) as from , MAX (transactiondate) OVER (PARTITION BY order, owner) as to from table
I'm not sure exactly what you're doing in VB, but if you can execute a statement and then pass the output into a variable for VB, you could check the value of the said variable and see if it's greater than 0. select count(*) from orders_table where cust_id=12345; And your customer identifier is probably going to be a variable passed to SQL from your program. Maybe something like Dim sqlText = "select count(*) from orders_table where cust_id=@custIdVariable" 
maybe, I haven't looked at the site itself, but that's only if they even list all countries. I'm assuming you are doing something basic so I would say you are correct (the syntax is off though). but if it was what I think, then you'd have to compare two tables, one with all countries, and the reference table and find the missing countries.
Declare @exists bit = 0 If exists (select 1 from order where customerid=@customerid) Set @exists = 1 Select @exists
Alright I understand, do you know how I can check if the variable is &gt; 0 ?
Is there a way to aggregate based on month and another column? Basically every row has a date and an owner. I'm trying to achieve 2018-12 | ownerA | 1243 units 2018-12 | ownerB | 143 2019-01 | ownerA | 432 
Don't use them if you are not smarter than query optimizer. The data and plans change all the time, but hints (or commands really) stay. Most of the time the problem lies elsewhere - SARGability, old statistics, bad indexes.
the query looks correct (other than left join - why do you need that). Anywho, try SELECT TableB.GroupingName, TableB.FruitVeggie, TableA.Calories, TableA.Carbs, TableA.Sugars FROM TableA right JOIN TableB ON (TableA.FruitVeggie = TableB.FruitVeggie) where TableB.GroupingName = 'Yummy' and post back what you're getting
&gt; Don't use them if you are not smarter than query optimizer Spoiler: Almost no one is smarter than the optimizer.
I'm really unsure what you're asking. Do you want data from all three tables in the same result set? If so UNION is your friend.
Thank you for the assistance, but unfortunately that didn't work. I've also tried ROLLUP/CUBE/Connect By options, but that's not it either. I think perhaps the key thing that I left out, is these stats would need to be aggregated by date, which wouldn't be known until runtime... Honestly, it might be a bit extra, but here's exactly what I'm looking for, but in a stored procedure. DECLARE CURSOR GROUPS_DISTINCT IS ( SELECT DISTINCT TableB.Grouping_Name FROM TableB ); CURSOR FRUIT_GROUPS (p_GROUP_NAME VARCHAR2) IS ( SELECT TableB.FruitVeggie FROM TableB WHERE TableB.Grouping_Name = p_GROUP_NAME ); CURSOR FRUIT_SUMMARY (p_FruitName VARCHAR2, p_ExpirationDate TIMESTAMP) IS ( SELECT TableA.FruitVeggie, SUM(TableA.Calories) AS CALORIES, SUM(TableA.Carbs) AS CARBS, SUM(TableA.Sugars) AS SUGARS FROM TableA WHERE TableA.ExpirationDate = p_ExpirationDate AND TableA.FruitVeggie = p_FruitName GROUP BY TableA.FruitVeggie ); desired_date DATE := '24-jan-19'; hold_calories NUMBER; hold_sugars NUMBER; hold_carbs NUMBER; BEGIN FOR this_row IN GROUPS_DISTINCT LOOP FOR this_fruit_group_row IN FRUIT_GROUPS(this_row.Grouping_Name) LOOP hold_calories := hold_carbs := hold_sugars := 0; FOR this_fruit IN FRUIT_SUMMARY(this_fruit_group_row.FruitVeggie, desired_date) LOOP hold_calories := hold_calories + this_fruit.Calories; hold_sugars := hold_sugars + this_fruit.Sugars; hold_carbs := hold_carbs + this_fruit.hold_carbs; END LOOP; --Sum up all the Calories/Carbs/Sugars for each of the fruit in the Table B group INSERT INTO FINAL_TABLE(Group, ExpirationDate, Calories, Carbs, Sugars) VALUES (this_fruit_group_row.grouping_name, desired_date, this_fruit.Calories, this_fruit.Carbs, this_fruit.Sugars) END LOOP; --Once the grouping has been summed up, "insert" those values into a row END LOOP; --Then move onto the next group of foods. END; It's really simple - give me a list of groups ("Fruits", "Yucky", "Yummy", "Beets") in this case, then iterate through that same list, and in every iteration, do a new group by of TableA where TableA.FruitVeggie = TableB.FruitVeggie. And I "feel" like because there's a logical connection between each cursor, this should be replicate-able in a rather obtuse, but possible query. If there's a better database structure anyone can recommend table wise, I'm all ears. I'm still in the initial set up of this, quasi-data warehouse. We're storing a ton of incredibly minute data that Mgmt now wants mixed and mashed on the fly. 
Thanks for the reply. I have not actually thought of it that way. Assumed blank and null was the same. Will try again tomorrow 😀
http://www.sqlservercentral.com/articles/T-SQL/71550/ Is a method I have used in the past.
Your group by needs to use the same formula you specify in the first column. 
You should have a subquery that selects the max date for each category id and then join that back. &amp;#x200B; So something like &amp;#x200B; \`\` Select sum(quantity) from something t1 ( Select max(date) m\_date, categoryid from something where date &lt;= '2018-04-01 ) t2 on t1.categoryid = t2.categoryid and [t1.date](https://t1.date) = t2.m\_date \`\`\` &amp;#x200B;
It worked! Thanks!!!
No problem! Just remember - you're grouping by values. If the table values are being manipulated in some way (part of a date, case statement, substring, etc), that same manipulation needs to happen in your group by.
If you happen to be on Oracle 12.1 or greater, there's a new feature called `match_recognize`. Introduced in the SQL:2016 standard, but as far as I know no other RDBMS has implemented it. The below returns exactly what you're showing in `my goal table`. with TEST_DATA as ( select 'NCR006015' orderno, 'MRBTRI' owner, to_date('2018-11-01 14:07:35','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'MRBTRI' owner, to_date('2018-11-01 14:07:36','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA1977' owner, to_date('2018-11-01 15:34:56','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA1977' owner, to_date('2018-11-02 14:28:01','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA1977' owner, to_date('2018-11-02 14:28:02','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA1977' owner, to_date('2018-11-02 14:28:09','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUS1850' owner, to_date('2018-11-02 14:29:28','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'MRBTRI' owner, to_date('2018-11-13 20:46:14','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUS1176' owner, to_date('2018-11-20 16:54:36','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'MRBTRI' owner, to_date('2018-11-30 14:06:39','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA1977' owner, to_date('2018-12-12 16:29:48','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'MRBTRI' owner, to_date('2018-12-17 13:56:51','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2045' owner, to_date('2018-12-18 22:46:35','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2031' owner, to_date('2018-12-27 13:15:14','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2045' owner, to_date('2019-01-02 15:00:18','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2045' owner, to_date('2019-01-07 13:41:42','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2045' owner, to_date('2019-01-07 13:41:43','YYYY-MM-DD HH24:MI:SS') transactiondate from dual union all select 'NCR006015' orderno, 'BUSA2045' owner, to_date('2019-01-07 13:42:00','YYYY-MM-DD HH24:MI:SS') transactiondate from dual ) --code starts here select * from TEST_DATA match_recognize ( partition by ORDERNO order by TRANSACTIONDATE measures STRT.OWNER as OWNER , first(TRANSACTIONDATE) as FROM_DT , last(TRANSACTIONDATE) as TO_DT one row per match pattern (STRT SAME_OWNER*) define SAME_OWNER as OWNER = prev(OWNER) ) ; 
Don't put "categoryID" in qoutes
I'm gonna remember that in the future, thanks!!
Trying it again without exotic new features, this is a pretty hard problem. 1. The first step is obviously sorting transactions within an order. 2. Next, you can drop records where `OWNER`ship did not change since the previous record, *except* for the last record (extrapolating, but I assume this is some sort of "Close order" transaction). For now, ignore that exception. 3. What's left is only rows where someone took `OWNER`ship. These are your FROM dates. The TO date is can be retrieve from the next row with `lead`. 4. That pesky close date is annoying, because there will be no next date returned by `lead`. Instead, we have to store off the max date per order and use that. Ends up looking like this: with TEST_DATA as ( --same as above ) --code starts here select ORDERNO, OWNER, TRANSACTIONDATE as FROM_DT , nvl( lead(TRANSACTIONDATE) over (partition by ORDERNO order by TRANSACTIONDATE) , CLOSE_DT ) TO_DT from ( select T.* , case when OWNER = lag(OWNER) over (partition by ORDERNO order by TRANSACTIONDATE) then 'Y' end DROPROW , max(TRANSACTIONDATE) over (partition by ORDERNO) CLOSE_DT from TEST_DATA T ) where DROPROW is null order by ORDERNO, TRANSACTIONDATE ;
Yeah sorry I'm still learning about how to correctly structure things like this. Would be better to have a set of different tables that interact with each other to produce the best result?
The result will still be the same, but you should have separate jobs and skills tables, each with unique job/skill and then a many-to-many relationship table listing which jobs require which skills. Something like /u/Tabooyah mentioned. Right now you're just duplicating all that data (like on your screenshot you have 10+ rows each with the same job title, just the ID referencing all the job details from jobs table would be enough)
Change `and (year(v.FromDate) = 2014 or year(v.ToDate) = 2014)` to `and (year(v.FromDate) = 2014 and year(v.ToDate) = 2014)` 
That is kind of tough... I'd probably start by modifying your data to only include the days in 2014. So if a vacation started or ended in 2014, chop off that vacation period to either Jan 1 or Dec 31, respectively. select count(distinct emplID) from ( select emplID, case when year(fromDate) = 2013 then date'2014-01-01' else fromDate END as "fromDate2014", case when year(endDate) = 2015 then date'2014-12-31' else endDate END as "endDate2014" from vacations where Status = 'Approved' and (year(FromDate) = 2014 or year(ToDate) = 2014) ) where datediff(endDate2014, fromDate2014) &gt; 10; 
Change your where clause on the ToDate portion to use a Case When that will change the ToDate to the last day of 2014 if the ToDate is in 2015 or later 
Where status = 'approved' And fromdate&gt;= 20140101 And todate &lt;= (case when todate &gt;= 20150101 then 20141231 else todate End) And..... etc.
This is interesting. I’m in a similar boat. Published and maintained a lot of BI apps (PBI and QV), but unable to link them externally. Do you have any examples from a site or recommended sample data sets that I can do this with?
Thanks all for the replies. Here is what I ended up with: Select count(\*) from ( select employeeID, CASE WHEN year(fromdate) &lt; 2012 THEN date'2012-01-01' ELSE fromdate END as "fromDate2012", CASE WHEN year(todate) &gt; 2012 THEN date'2012-12-31' ELSE todate END as "toDate2012" from vacations where Status = 'APPROVED' and (year(FromDate) = 2012 or year(ToDate) = 2012) ) as t where datediff(toDate2012, fromDate2012) &gt; 10;
Thanks a lot. This was very helpful
You only want 10 consecutive days? What if they took t days here and give days there?
Oh, yeah you are right. It doesn't work in this case. I guess the struggle continues... :)
well, this is wrong for starters -- SELECT perform_id, title ... GROUP BY movie_id, title
As swan lake hasn't sold any tickets, it won't have an id value in the tickets table. To get the id to come through, you need to reference the [performances.id](https://performances.id) column. Also, because there isn't anything for the sum function to aggregate, it will return a null. If you don't want null values to be returned you could use the ISNULL function. SQL should look like this - WITH res1 AS (SELECT \* FROM performances LEFT JOIN tickets ON (tickets.perform\_id = [performances.id](https://perfrmances.id/))) SELECT performances.id, title, ISNULL(SUM(number\_of\_tickets),0) AS sold\_tickets FROM res1 GROUP BY performances.id, title ORDER BY sold\_tickets DESC
also the table could be achieved without a CTE. SELECT P.id title ISNULL(SUM(number\_of\_tickets),0) AS sold\_tickets FROM performances P LEFT JOIN tickets T ON [P.ID](https://P.ID) = T.perform\_id GROUP BY [P.id](https://P.id), title ORDER BY ISNULL(SUM(number\_of\_tickets,0) DESC
You could also use a case statement to check if vacation time taken covers more than one year and make calculations accordingly 
https://data.gov.uk/ This has open licence data sources from UK government. I used the healthcare prescribing Vs dispensing data as it is interesting to model. Prescribing habits can be mapped onto maps showing clinical need in certain areas. The dispensing data can then be used to calculate market share by pharmacy. There are lots of other interesting datasets but I work in the healthcare sector so those are the most interesting to me.
sorry - the highlights made it unreadable. 51. (price \* tax\_rate) as tax\_amount; 52. (price \* tax\_amount) as total; 
I believe if you declare them as variables prior to the select statement then you can you use them in the given context. DECLARE @tax_amount INT, @total INT; And then inside your `SELECT` you set those variables equal to their given values. @tax_amount = (price * tax_Rate), @total = (price + @tax_amount) I could be wrong, I'm no SQL wizz
Yeah this pretty much does it. 
Hey not bad. I’ve been an oracle database developer for a few years and I use all of them frequently. If you want to learn basic sql you should read this. 
This should do the trick for you as well. &amp;#x200B; DECLARE @price FLOAT = 100 DECLARE @tax_rate FLOAT = 0.07 DECLARE @tax_amount FLOAT = (@price * @tax_rate) DECLARE @total FLOAT = (@price + @tax_amount) SELECT @price AS price, @tax_rate AS tax_rate, @tax_amount AS tax_amount, @total AS total &amp;#x200B;
wrap your INSERT INTO [AnalyticsMapping].[SLA].[tabDashboard] EXEC sp_executesql @SQL in try/catch block (try/catch can be nested) - the 'try' part will execute if @sql is valid, in your catch block you can analyze the error and/or log it some place
I'm still preparing to respond to your last message, but can you give me a sample framework? I have been really struggling with perfecting my TRY/CATCHES because this is a chain of sprocs where I execute 000.global, and then it will decide which sprocs to execute from there. As things currently stand if any one sproc fails in the chain, the entire process is rolling back... That's not ideal, but I can work with it for now, however in a perfect world I would like the "parent process" to always finish running successfully even if there are errors in individual queries that are built from the dynamic SQL, if that makes sense.
A better way of saying things is that I do not fully understand how TRY/CATCH works, and I have them rather irresponsibly thrown around this process based on other processes I've written, but those other processes are just single sprocs that aren't chained together. I realize there is a deficiency to this, but at the time it isn't my highest priority in terms of current development.
Assuming you will add nullable "errorNumber" an "errorMessage" to your [AnalyticsMapping].SLA.tabCalculations (while having "Client", "SLAType", "SQL" columns) you can wrap the 'good' execution path this way: begin try INSERT INTO [AnalyticsMapping].[SLA].[tabDashboard] EXEC sp_executesql @SQL INSERT INTO [AnalyticsMapping].SLA.tabCalculations (client, SLAType, sql) SELECT @Client, @SLAType, @SQL end try begin catch INSERT INTO [AnalyticsMapping].SLA.tabCalculations (client, SLAType, sql, errorNumber, errorMessage) SELECT @Client, @SLAType, @SQL, error_number(), error_message() end catch 
where is error_numer() and error_message() being defined?
Wait, is the error_number, and error_message the actual error #'s and messages generated when I execute a bad query? What is the syntax to wrap the TRY/CATCH inside of a loop that it continues to run?
yes, the errors of latest statements there's no change in syntax
ms sql core
Got you. It will probably be tomorrow when I get around to trying something new. Everything works fine now, unless I have an error in one of the mapping tables. It's fairly easy (but time consuming) to work my way backwards when there is an error to see where its happening so this would be a nice feature to build as more mapped values are being added.
Your answer was very clear, and it worked. Thank you.
Could you elaborate a little further?
Yes you can. It's usually called "dynamic SQL" or something similar. However, the way to create and execute those dynamic SQL statements differs between DBMS. For example, in Oracle you would typically use the "EXECUTE IMMEDIATE" command and pass a prepared SQL statement as a string. So you could simply concatenate your input parameter with the rest of the SQL command to achieve the desired behavior.
dynamic SQL is not necessarily something that's bad on its own - it's not compiled until the execution time though, so make sure your code handles compilation/access errors If the number of permutations of your parameters is low, you can create a series of if statements and "dynamically" choose one of the static query statements.
I am just adding that those are for MS Sql Server. No use to try them on Mysql or Oracle. And as often it's the cases with those DMV, they are mostly cumulative (thinking about the one reporting CPU usage). &amp;#x200B;
I would say "not really". If the number of arguments is high (&gt;=4 is enough for me), I tend to add placeholder comments in the query Something like this: set @query = N' set @query = N' select x,y,z from tblA ta --joinTblB where ta.name like '@searchTerm' --whereTblB '; if nullif(@condition,'') is not null begin; set @query=replace(@query, '--joinTblB',' inner join tblB tb on tb.dataFK = ta.dataID'); set @query=replace(@query, '--whereTblB','and tb.filterField = @condition');s end; exec sp_executeSql @query ,N'@searchTerm varchar(20), @condition varchar(20)' ,@searchTerm, @condition ; It's still dynamic sql, but it reduces the clutter to just what is used, and it helps debugging in my opinion. &amp;#x200B; PS: this syntax is MS Sql server related. It won't work as is in another DBMS.
https://docs.microsoft.com/en-us/sql/reporting-services/create-deploy-and-manage-mobile-and-paginated-reports?view=sql-server-2017 SQL Server Reporting Services. :)
that's a helpful start, thank you
can i pass the result of an expression as a query parameter?
Yes. Depends on the RDBMS you're using as to how this is performed. 
Oh whoops forgot to mention but I'm using postgresql
&gt; If the number of permutations... I considered that. It is _much_ too high, but on the other hand, there are two or three that are common enough, but beyond that, it's wide open. I might fail over to the dynamic SQL when it's not one of those cases, or if the compile time stays under 100ms, it will be okay.
oops, i should have said it's sql server
EXEC7TE IMMEDIATE won't work on SQL Server. 
Declare the variable. Set the variable by setting it from a select statement. EXEC dbo.PROCEDUTE @variable; to pass in the variable. 
&gt; EXEC dbo.PROCEDUTE thank you. was there a typo there? i'm having trouble imagining what that might look like .
No, that's SQL Server syntax. Not sure what you might be confused on. 
You said PROCEDUTE instead of PROCEDURE. 
Didn't catch that. Thanks. 
It be like that sometimes 
No but EXEC does.
Truth. 
Thanks! I'm in a db SQL class and it's very early on. He hasn't mentioned the ability to declare vars yet. Appreciate the help 
Ok so the accurate answer is no But you could use your variable to craft an SQL query and then execute that (dynamic SQL). This usually performs poorly, since the database platform has no way of caching it and very few opportunities to optimise it
The top answer already deals with this, so I think you're ok
How important is it you have to display it on the website? If you just want a button allowing them to export it to CSV, try something like this.. https://stackoverflow.com/questions/1120109/export-postgresql-table-to-csv-file-with-headings 
You can't reliably detect if executing a piece of code will generate an error or not, without running it. This is a common computer science paradigm, known as Turings 'Halting problem'.. https://en.wikipedia.org/wiki/Halting_problem The best you can do is sanitise all inputs to ensure it has the best chance of running, then execute it, and make sure you handle all resulting error conditions (including exception handling). 
 select perform_id, sum(number_of_tickets) as ticket_total from tickets group by perform_id order by ticket_total desc, perform_id asc;
there's no CTE there. that's a subquery. CTEs are sql recursion
thanks for that. i'll stick to IF logic
If you use SQL Server, there may be some benefit in splitting each condition into a seperate stored procedure, and then using your IF to call the relevant one. I'm out of date with SQL Server though, so it may not apply. But historically it only stored one compiled query plan per stored proc, and so it was a decent technique for improving perf for very dynamic queries
A 10 year old question haha Some of the views are less than 10 lines long but at least if that command can be run from a click then it's still better than nothing
If it's at the start in a WITH clause it's a CTE. A CTE doesn't have to be recursive.
Dynamic SQL isn't necessarily bad unless you are not handling parameters correctly. Dynamic SQL can expose you to SQL injection. I usually only use dynamic SQL within stored procedures without parameters for things end users could get data into. If I did, I would validate the supplied parameters to ensure that they did not contain characters that could result in SQL injection. 
Don't forget either using the set clause or throwing an equal at the end of each declare statement with the desired value to set the parameter as. 
&gt; You can't reliably detect if executing a piece of code will generate an error or not, without running it. I think you are misunderstanding the halting problem a little bit here. From Wikipedia: &gt; halting problem is the problem of determining, from a description of an **arbitrary** computer program and an input, whether the program will finish running (i.e., halt) or continue to run forever. &gt; Alan Turing proved in 1936 that a general algorithm to solve the halting problem for **all possible** program-input pairs cannot exist. So we cannot create an algorithm to decide if an **arbitrary** program will halt. However, that does NOT mean that it is impossible to decide this for a specific (set of) program(s). For example, the following Java program will always halt: static void main(String[] args) { return; } Also, I don't think that the halting problem really applies if we only want to decide the validness of a simple SQL statement (i.e., without the procedural SQL parts) because SQL itself is not turing-complete. (Not sure about that though.) 
Looked it up. You're right. Thanks.
Yeah perhaps I am misinterpreteing what the Halting problem is really about, but I still think my point still stands in essence. You can show java code which we know will halt, but given some arbitrary code its impossible to determine if it will halt or not. But taking into account runtime conditions, it still stands. Any Java or SQL code could fail under the right conditions, and it's impossible to determine that before running it. Ok Java and SQL are unlikely to halt, but they could still get to a condition where they cannot execute (java:exception or sql:deadlock/exception)
&gt; You can show java code which we know will halt, but given some arbitrary code its impossible to determine if it will halt or not. Yes, you are right about that. One more thing though: Deciding the "validness" of a program is not the same as deciding if the program will halt. We can decide the validness of an arbitrary Java program or SQL statement by simply "looking" at it (i.e., parsing the program and checking if it conforms to the allowed syntax of Java/SQL). What we cannot decide is, if an arbitrary (but "valid") program will always halt for any input.
Fair point, I'd agree with that 
First off kingdom_gone, You have no idea what you are talking about. Certifications are not easy by a long shot. Especially for a newbie. The tests that you take are developed by people that have been in the industry for years and I am not talking about a simple 5-10 years, I am talking 20-30 years. There are jobs out there that will hire what is called a junior dba. You ever heard of it? Maybe not. I cannot stand people that come on the internet talking about stuff that they have no business discussing. I would hire someone with a few certs because they have most newest information and developments. A combination of certs and a degree would be ideal. But while a person is sitting in the same classroom at a university for 4-5 months, the university textbooks are not turning the books as quick as technology is evolving like a person obtaining a cert. And lastly the exams are not cheap. If you attend schools like New Horizons Computer learning Centers they charge on average of $1700-$2500/Class. Ref: https://www.nhphoenix.com/training-and-certifications/course-outline/id/1003872044/c/10985-introduction-to-sql-databases The course referenced above is almost $1800 for one class so nothing cheap about this. But you are right you shouldn't just try to apply for a straight DBA role because they will require experience. But you can also for a developer entry level position for a few years then go for the bigger positions. But there are organizations out there that will give him a chance.
Certifications are not Useless. You and other people on here need to google the most up to date information on certifications then when you become more up to date on the information then come on here and give people who are serious trying to evolve there career information. Ref: https://www.capella.edu/blogs/cublog/it-certifications-or-it-degrees-for-career-growth/ Ref: https://learningnetwork.cisco.com/blogs/certifications-and-labs-delivery/2017/01/06/certification-vs-degree-what-do-i-need-to-succeed
solved it by replacing **ID** with **team**. This is probably because of postgres is case-sensitive and ID is a keyword (I'm not sure if I'm using the right terminology here. 
This IN construction tends to be really really really slow if you have thousands of values in there. An Uber join would work better on Postgresql. Or to leverage "Distinct on (categoryID) ... Order by date desc"
Depends. Sometimes queries run really slow when they are too complex for the planner to estimate what it's going to work with, the number of rows in each operation, etc. Can happen if you have a join that is supposed to be 1:1 but the planner estimates that you'll either get 10x or 1/10 of the rows. Happened to me that such estimations have piled up and reached 5 orders of magnitude (10000 times fewer rows estimated than really returned). It wasn't old statistics or anything like that. Just complicated queries. When designing such tables I often try to tell the planner as much as possible about the table (constraints), but sometimes (on Postgresql) just disabling merge joins temporily might force another, faster execution plan. Whenever I do this override, I write a lengthy comment explaining it, because it's guaranteed that nobody will get why this was done unless lengthy analysis is performed, or comment is read.
Ola’s maintenance scripts https://ola.hallengren.com DBATools.io got backups and restored are good as well https://dbatools.io Brent Ozar’s first responder Kit and PowerBI dashboard https://www.brentozar.com/first-aid/ https://www.brentozar.com/first-aid/first-responder-kit-power-bi-dashboard/
Thank you very much, I appreciate you sharing these resources with me. I’ll take a look at them now. Thanks for the well wishes too, I think I’ll need it ! :-) 
You’re welcome. Updated with a few more. Just relax and take it a step at a time. I’d suggest going through the first responder Kit checklist first, taking inventory and asking yourself where you are and what you need to do. You got this Chief.
This is a database sub. Cisco certs are more or less required for a networking job, sure. The page you linked also mentioned specifically project managers, security, etc where certs are looked upon more favorably (PMP for PMs and the like) But this is a database sub. I have over 10 years experience as a dba. I've interviewed many times. I've been interviewed many times. Nobody except the initial HR screen has once asked about certifications because everyone in this field knows they're not indicative of knowledge or performance.
For iOS...? Why iOS? There are a few decent ones for Windows/Linux/Mac. Notably Oracle's SQL Developer and Data Modeler have those functionalities.
Not OP, but I often need to quickly sketch a diagram of relation while commuting (e.g. if I get an idea for an app or some problem domain). Since my only portable device is a tablet with iOS (and a phone, which is even less suitable for diagramming), there are no other options, unfortunately.
Android has a few. Never used iOS.
Looks like Lucidchard has an iOS app: https://www.lucidchart.com/pages/tour/ios-app
I thought MIN() is an aggregate function, but a better example is SUM() and AVG(). These give the sum and average values respectively. I am not a SQLite person, but the syntax should be as you described: select AVG(field) Most of the time you're going to be using aggredate functions
I had thought about this and the concept of having to run it twice, first to see if it works, and then a second time to know it does. This problem only exists for me when I have field names that are incorrectly entered. I've considered something where at the very beginning of my process I check all the input values to see if they are valid, if not hold them back from being part of the loops.
To clarify... I realize that if I have functions such as cast(n as int) and point to a field where there is an incompatible value that I will get an error while the query is executing, but not when I try to attempt to execute it. Here I'm not really interested in whether it will successfully execute, but only whether it think it can successfully execute (i.e. all the columns are spelled correctly, the source objects are correctly spelled, etc.) -- but truly I am only interested in this to isolate the ones which don't work. I think the other method might work, and I'm going to play with it today.
an aggregate function is a function that operates over column values from multiple rows so SUM() adds up values, COUNT() counts values, and MIN() and MAX() operate over the entire group of column values to find the smallest and largest so yeah, MIN() and MAX() are aggregate values note that MIN() and MAX() can operate on string and date values too, not just numerics
Yeah this is exactly why. I discovered after making this post that Numbers can do the job really well. Multiple tables within a Sheet and you can add great connector lines between tables. 
Make one in a pdf first, then send it to your device? 
Select c.coursename, count(*) as class count From student as s Inner join course as c On s.courseno = c.courseno 
That we do. 
I presume the issue you are having is that you can sum and group correctly, but when you join in the second table everything goes out of wack. What you want to do is perform your count() and group in either a CTE or a sub query. Then you want to join in the second table. I'm not on my PC so I can't test this and SQLFiddle doesn't seem to work with SQL Server at the moment. But this should work: create table student ( rollno int, name varchar(50), course_no int ) create table course ( course_no int, course_name varchar(100) ) insert into students values (1,'James',1), (2,'John',1), (3,'Michel',2), (4,'Paul',2) insert into course values (1,'Engineering') (2,'Medicine') select s.summed, c.course_no, c.course_name from ( select sum(rollno) as summed, course_no from student group by course_no ) as s join course c on s.course_no = c.course_no
If you’re still in discovery phases on this post, we shamelessly suggest you at least try the Lucidchart app. Pretty easy to drag/drop your shapes and export right from the app for sharing
Thank you so much for this, but, i got this error: &gt; `ER_PARSE_ERROR: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'TABLE Student ( student_id INT PRIMARY KEY, name VARCHAR(30), mavod ' at line 13` on this line: **on s.course\_no = c.course\_noCREATE TABLE Student** &amp;#x200B; Its okay you can take your time with it. I need it by day after tomorrow. This is the only problem where I'm stuck
This wouldn't allow creating the diagram from scratch or editing it from the tablet, though. I'm talking about cases where you cannot do the diagram in advance.
Looks like you're using MySQL. What I have written is for SQL server so I'm not much help here. Can you just run the select statements without anything else? 
If you're talking about generating an ERM of a database, surely you would need the means in advance to be able to connect the different database types?
How's your table-structure/architecture on the database looking so far? Can you post it? I don't see this being an issue if it's architected correctly.
[this](https://imgur.com/IzB63nq) as it is now but [this](https://imgur.com/ee4ds5t) is how i would like it to be 
Is there any reason why that's the way you want it to be?
the architecture is very much open to be modified! i really just want to be able to 1. delete lines of a character when character is deleted 2. delete lines of an act when act is deleted 2. [if possible] have some enforced referential integrity, so eg i get an error if someone tries to add a line with an actID but not a characterID or the other way around. (although, since applications will only interact through SPs and triggers, it should not be possible in practice)
I don't think you need to go to the extent of triggers and stored procedures to keep the ERD in sync from the beginning. That just sounds like a nightmare to maintain. What RDBMS are you on? I'll mock an example up that I've got in mind. 
hi. that is very kind of you indeed. i'm on sql server. i use SPs in any event to provide a safer interface for applications. and triggers, well i guess it's often overkill
So consider this: A line is not directly related to an act. There same line, like "ha!" could be spoken in many acts. It may be better to keep all the lines, just don't keep the act. Under the previous assumption, one character may not only speak one line. I moved production, act, lines, and characters to all be at the same level in the ERD, and applied a foreign key to a new table "spoken language" that you don't have. Spoken language is a joiner table that contains all the keys from the tables you've got in question. With that being said, you could easily delete a character, act, line, or production and all the records from the joiner table would be removed. (I think this is all you care about anyways, ensuring the records that hold the relationship are removed in the event the parent is deleted.) https://www.mediafire.com/file/x1ak641b9559u1r/ZEUGMA25.sql I didn't put all your fields in it, just enough to give you an example of what I had in mind. Keep in mind, with this, you assume that any line can be associated with any act that can be associated with any production that can be associated with any character. Pros: You can delete the parent and it'll be removed from the joiner table breaking the "relationship”. This is also more normalized, so instead of the ACTS, LINES and or CHARACTERS tables growing out of proportion, it would limit it to only one table [SPOKEN_LANG]. Cons: You couldn't delete from characters table and have it removed from lines and spoken language. We wouldn't know if the line was being spoken by another character or not. 
I think you might misunderstand what we’re talking about. We’re talking about creating a diagram of a totally new database that has yet to be created. I’ve just been needing a way to create and edit a diagram of a database I’m planning for a mobile app. I wanted an a mobile app so I could work on the diagram wherever I am.
that's great, thank you for taking the trouble to do that!
No problem. Does all that make sense?
No problem. Does all that make sense?
yes. i was looking at the picture from a fixed point of view and it took your fresh pair of eyes to make me change my perspective.
I'm glad that's helped open your ideas up. 🙂 Hopefully you realize you need more than just character too. A character can be reprised by many individuals. You won't want that table to grow "vertically" either just because an infinite amount of individuals play characters. 
`select col1, min(col2) from table GROUP BY col1` Min is an aggregate function because it is aggregating all the columns which have the same value for col1, and returning a single value for that aggregation. max(), count(), sum(), avg() are other functions. They aggregate the values from multiple rows and return a single value.
Update: I've made updates to the cheat sheets as requested. See the post body above for details.
I've been using the free www.draw.io on an ipad pro with pencil. Love it. Not going to pay $5/month for Lucidchart.
&gt;Can you sort a column using a column alias? Yes. A column alias could be used in the ORDER BY clause. This is dependent on the flavor of DB, no? If you try to directly order a column by it's alias in mssql for instance, you'll get an error. You could use the ordinal positioning, but that's not an alias.
group by c.coursename
Good catch. Thanks 
I bit more information about your organisation setup and reporting environment would be useful. When you say analysts, do you mean people on your team building your models, or the business people consumuing them? Have you got any self-service BI capability in your organisation at all? A data warehouse/vault/lake? And what do you mean by &gt;"has got familiar with every database, they want to go or something new is out" What is a "database" here, do you mean your org is using a lot of different database solutions and they get replaced by newer products often? A couple of things that spring to mind: Rigorous approaches to getting consistency in the data models you're building, and in document and maintaining robust meta data, is important to make sure that what you've built today can be reused tomorrow. Good documentation particularly helps new team members get up to speed. If what you're producing on an ad-hoc basis isn't suitable for in depth modelling and metadata, maintaining a team wiki for code snippets is a good idea. If you've got a ticketing system it's a good idea to use them in tandem: if you invent something new, put some rough and ready wiki pages together containing your scripts/queries/BI report file/whatever, with a how/what/why, and add links to your tickets to the documentation and vice versa. A modelling strategy is important: each time you knock out something bespoke it's an opportunity to consider whether improvements could be made to your existing models that would cover that new use case. Evaluating your strategy is important too: the question of whether to produce many, small, focused models vs. very large, very generalised models might help out. Where practicable you should try to modularise your approach. Instead of doing every request in one big chunk, look at the different logical parts of it. If anything is likely to be reused, pull it out and make it available somehow, such as with staging tables or views.
Learning about additive, semi-additive, and non-additive measures may help you conceptualize aggregate functions. [https://stackoverflow.com/questions/34295293/whats-the-difference-between-additive-semi-additive-and-non-additive-measures](https://stackoverflow.com/questions/34295293/whats-the-difference-between-additive-semi-additive-and-non-additive-measures)
Thanks heaps, Further context is. We have our production databases which are all very different and complicated (the company is working on this) to which most of it is passed through to a data vault. We have a reporting layer filled with self serve BI models customers can use. When those self service tools don't give the client what they want, they come to my team of analysts. Who go away and figure out how to do it using the vault. Sometimes we simply can't others times its easy. I like all of your points, Documentation is my first aim, as their is none. The module approach has been floating around my brain, i supposed the gain their would be turning code into snippets and providing consistency, it wouldn't particularly enable us to hit a 1 billion row table faster. I have started analysing all the scripts written in the last year to see if we can build any half-models or views which will increase our ability to access data faster. &amp;#x200B; But yeah I want to be able to make the team run like a well oiled machine and reduce the overhead of doing the same thing everytime even if the end content is different
What is exactly going on here and how does it work with transactions? For example, if the TRY fails, it automatically inserts the data into the tabCalc table and runs a CATCH, but then the loop continues to function?
Is the enterprise "vault" a structured data warehouse? If not, that is where I would start.
Yes its a structured data warehouse. We use hub sats and links for the production database streams. We also have a large amount of pre-modelled data we can use to avoid reinventing the wheel.
There is some possible uses of modularised approaches assisting performance - if you're often using the same aggregations on your billion row table, and slightly stale data isn't an issue for reporting, you could modularise that and e.g., pre-aggregate it into a single reporting table that gets updated daily. Looking at the scripts that your team has created sounds like a great way to find whether there's options for this.
Cheers for you help :) 
&gt;Cheers &gt; Thanks heaps No problem, my fellow kiwi.
PLOT TWIST! Hey!
Have you tried: WHERE SOUNDEX(actual_name) = SOUNDEX(typo_name);
We had a good thread about it recently: [https://www.reddit.com/r/SQL/comments/ahvpl1/anyone\_ever\_done\_fuzzy\_matching\_in\_ms\_sql/](https://www.reddit.com/r/SQL/comments/ahvpl1/anyone_ever_done_fuzzy_matching_in_ms_sql/) I would pretty much just do what mtx\_ said.
Every time I hear automation I think of Python
Map out the process. Are there any bottlenecks? Which step is taking the longest? Is there anyway that you can improve it? (LEAN methodology is a good tool) (examples could be templates, customer requirement checklist, easier way to output/send the report).. Doesn't have to be too complicated - many little improvements add up to alot (especially if it is the same process happening over and over again)
Yeah that’s peoples common go tos. In a previous role I did a lot of automations of admin tasks just using oracle packages so it’s not just a python thing 
Thanks for the guidance. Will look into LEAN. 
Which DBMS product are you using? 
The Subreddit Wiki has resources on how to learn SQL. You'll also see this listed in the sidebar. What do your queries look like so far?
This should get you started: https://www.w3schools.com/sql/sql_join.asp
&gt;I like all of your points, Documentation is my first aim, as their is none. Are you, me? You have described every organisation that I have worked for. I am having a struggle with my current organisation and I have pointed out that there is no documentation for any of our datamarts. I know roughly where it is being extracted from in the ERP, and I know what the end result is but I, and others before me, have no idea what business rules are being used, the logic behind it, or what steps are being done in the transform, and where to begin looking if there are any new problems with it. We currently have some end users of the data who have gotten very good at teasing stuff out of it but until recently they just believed that the 'raw data' that they are using is straight out of the ERP. 
OMG you must be me. We also have no documentation for our datamarts. The devs just say “go look in the orchestration tool” And yes every job. No documentation. People are so thick saying “the second you write it is out of date” and yet not knowing anything and training everyone from scratch is acceptable 
Never anytime for documentation, but there is always plenty of time to pay the new guy while he reverse engineers everything only to find out that it's a completely bespoke solution that was built in an adhoc way using logic and processes that were only clear to the guy who originally built it a decade ago... 
Exactly. The person I’m replacing has trained 8 staff in 2 years with no documentation :( 
Try being the only one who is somewhat responsible for this. I am not so much a pure data guy as I have a broad IT skill set to fall back on but I always find myself in these roles where there is no documentation and not much support, and with a management team that does not comprehend just how precarious things are. 
Well you understand what needs fixing!!! That’s a start 
There is never enough documentation. Doubly so if they say that they have recently cleansed their data... 
You should definitely have a look at Apache Lucene / Solr; this is a search engine that is specialized in finding similar strings (and it can also be used to store document-like data). In my experience it can take quite some effort to configure correctly, but the default configuration works out of the box to do a proof of concept. I'd think about writing functions in e.g. `plpython3u` to interface from PostgreSQL to Solr. * https://duckduckgo.com/?q=postgresql+solr&amp;t=lm&amp;ia=web * http://lucene.apache.org/solr/ * https://stackoverflow.com/questions/10053050/why-is-solr-so-much-faster-than-postgres 
Do you know about the time that it crashed? Can you historically look up the queries that were running around the time? Why not set up an availability group and replicate the production server data to a server where they can run Excel queries to their hearts content? If it goes down, no one would care, and it's not that hard to set up. 
Download SP_WhoisActive http://whoisactive.com/ Run every minute and pipe output into table. Check what was occurring next time it crashes. Look for: Blocking chains Excessive resource usage (disk IO, CPU). High tempDB usage. You could do your investigation using just SP_WHO2 and DBCC INPUTBUFFER(SPID) if you are on the server as the crash occurs or is building towards a crash. Its quite hard on a properly specced server to get an OOM error although I have seen it done on a SQL box with a quarter of a terabyte of ram.
Have you configured a maximum amount of memory for SQL Server? If not, SQL Server will eat up every byte of RAM and completely starve the OS for memory, which will be a big problem. A user query should not crash the entire instance. If it can, I'm sure Microsoft would like to take a look at it. A query can consume resources to the point where it _seems like_ the instance is dead, but it should be recoverable. I have a system which has _nearly_ brought down SQL Server due to queries demanding too much memory. Enough processes got thrown at the instance demanding double-digit GB memory grants that it was nearly impossible to connect to it and `RESOURCE_SEMAPHORE` waits were all over the place. We only tracked it down definitively (everything else before was a hunch, or "we need more CPU/RAM" or "SQL just needs a reboot") because we have a monitoring suite (SentryOne) watching the instance and it was able to show what was happening in the moments leading up to the condition. Once we identified the cause, the solution was "shut off that system for 5 minutes". I _could_ have played Whack-A-Mole with killing the sessions, but that wouldn't have helped; the app server had run amok and shutting it down gave me a chance to kill the remaining sessions without more coming up _or_ restarting the instance. You can probably do the same for free (but a bit more work) with sp_whoisactive, the First Responder Kit, and/or Glenn Berry's diagnostic queries. If you're using Enterprise Edition, have you considered Resource Governor to limit the resources these Excel users can consume?
There are cases where they are appropriate but they are very rare. Generally index and join order hints are the sign of a developer not understanding the issue but finding a bit of syntax that seems to work for that situation. If any of our devs sent over code with such hints without a very solid reason why it would get kicked back with a more polite version of 'what the hell do you think you are playing at here? Im not deploying this'.
&gt; Its quite hard on a properly specced server to get an OOM error although I have seen it done on a SQL box with a quarter of a terabyte of ram. I've done it on triple that :)
Sorry to hear that, both about the crash and that people are using Excel to access the db. Replication is a good suggestion a quicker solution to implement would be taking snapshots and mounting them in a new server. Call that the reporting server. The real solution is to get people who run complex queries off the production box. Depending on how the application interacts with the db revoking read access for everyone may be a stop Gap solution. That could break the application if it does anything other than execute stored procedure. You can set time out for queries. Maybe set that to 10 minutes and the culprit will show up at your door asking why their query downstairs work anymore.
https://docs.microsoft.com/en-us/sql/t-sql/statements/update-statistics-transact-sql https://www.red-gate.com/simple-talk/sql/performance/managing-sql-server-statistics/
How To Install latest MySQL 8 on Windows https://mysql.tutorials24x7.com/blog/how-to-install-mysql-8-on-windows
Brent Ozar’s first responder Kit and PowerBI dashboard. These help you see what’s actually going on and where you might need improvements. The videos show how to use them. This combined with the SP_WhoIsActive suggestions will get you where you want to go. https://www.brentozar.com/first-aid/ https://www.brentozar.com/first-aid/first-responder-kit-power-bi-dashboard/
What is your role? Why are your "clients" asking someone who doesn't know databases to perform administrative tasks?
I am the new Helpdesk guy on an ECM reseller team. I had to escalate an issue to our vendor who requested our client to do this. Our client does not know how to do this. Unfortunately neither do I.
Then you ask the vendor for documentation, and pass that to the DBA(s) to execute. A "new Helpdesk guy" shouldn't have access to the production database, let alone *administrative* access to it.
The vendor informed me that it is not their place to help with SQL tasks and to suggest that the client consult Microsoft documentation.
So they're telling you to do something to the database, but don't have any documentation to provide you about that process or why it'll help? Honestly, it sounds like they're trying to fault the database for a problem they haven't adequately diagnosed. Where is your client's DBA in all this?
I completely agree. From what I gather, their support teams main goal is to blame things other than their software and have us jump through many hoops before we get any help. I try not to deal with them very much, but sometimes it’s a necessary evil. As for your second question, I’m not sure. It kind of seems like a 1 man show and that 1 man isn’t confident enough to make this change without breaking it. The task has fallen on my shoulders and I am trying to gather as much information as possible. 
Great thank you, &amp;#x200B; Okay, So if I suggest the client to run this command and replace \[database name\] with the name of the database, the statistics on all tables will be updated to 100%?
No, this is just a view that generates the code to update statistics. You’ll want to add the sample size parameter.
Ask your mentor/supervisor. "A .SQL file" could be anything from a single query to a complete dump of a database. Knowing nothing about the contents of that file or the environment you're working in, folks here can't answer that question. &gt; Later I have to put the database in excel No one should be "putting a database" into Excel. Excel is often used as a mechanism to provide data extracted from a database to people who don't have direct access to the database, but the database is the proper place for the data to live.
Did SQL crash (process wasnt running) or was it hung (process is still running)? I would start with looking at the error log as well to check to see if there are any minidumps. Ultimately crashes shouldn't occur and they should be sent to Microsoft. You can also review the dumps as most symbols for sql are public. I've had to do this a few times when I've seen schedule deadlocks. You can also look at the previous system health extended event session leading up to the crash. If it wasnt a crash you may be able to login through the DAC, while the issue is occurring, to check the status of the server and you may be able to KILL activity that is causing issues. If it was a query that caused the issue you could create an extended event session to try to capture some metrics. 
I was looking at this page: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-local-variable-transact-sql?view=sql-server-2017 I thought Id try putting it in curly brackets, like so: DECLARE {@batch_id_table AS VARCHAR(15) }; SET {@batch_id_table = 'PAYMENTS_' + CAST(( SELECT MAX(job_id) FROM syjob WHERE program_id = 'PAYMENTS' ) AS NVARCHAR(6)) + 'a' } SELECT * FROM @batch_id_table It does react to this. It pops up a box titled 'define query parameters', and there is a param called @batch_id_table with a dropdown allowing me to select 'blank' or 'NULL', or to put in a number. I populated it with a valid batch number to test. Then it says there is an invalid or unsupported ODBD escape sequence. It's rather a pain. I think I'd like to try some different software...
If you're finding that you have to rebuild the model (i.e. complex transformations) then I'd try DBT: https://www.getdbt.com/ If your issue is that customers don't know what they want (i.e. need discovery) then you made need a different approach to modeling such that they can build their own reports in a flexible reporting tool (e.g. Tableau/Looker). PM me with more details and I'd be happy to help provide more suggestions.
So aaaahhhh....? :D 
I've started playing with what you outlined above but it seems to be giving me some kind of infinite loop when there is an error. I went into the mapping tables and modified a cell to make it totally nonsensical, and now it just keeps rerunning and dumping the invalid query into the calculations table. Code below: ALTER PROCEDURE [SLA].[090b_OnboardingCompliance_Calc_1](@CalcID int, @SessionID int, @IncrementID int) AS --BEGIN TRY -- BEGIN TRANSACTION DECLARE @SLAType nvarchar(255) = (SELECT SLAType FROM [AnalyticsMapping].[SLA].[mpCalcs] WHERE CalculationID = @CalcID) DECLARE @ClientList TABLE ( ID int identity(1,1) , Client nvarchar(255)) IF ( SELECT SUM(CAST(isTrigger AS int)) FROM [AnalyticsMapping].[SLA].[mpClients] A INNER JOIN [AnalyticsMapping].[SLA].mpCalcs B ON B.CalculationID = A.CalculationID ) = 0 INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID ELSE INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE isTrigger = 1 AND CalculationID = @CalcID DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @ClientList) BEGIN DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop) DECLARE @TempSource nvarchar(255) = (SELECT TempSource FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @SegmentID nvarchar(255) = (SELECT SegmentID FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @ClientProfile int = (SELECT ClientProfile FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID AND Client = @Client) DECLARE @Clause1 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 1 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause2 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 2 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause3 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 3 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @Clause4 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 4 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @SQL nvarchar(MAX) = ' SELECT CASE WHEN B.SLALabel IS NOT NULL THEN B.SLALabel ELSE ' + '''' + @SLAType + '''' + ' END AS ''SLAType'' , A.DateRange , ''OrderID'' AS ''IDType'' , B.ORD_ID AS ''ID'' , A.Client , A.Country , A.Region , A.LaborTypeCase AS ''LaborType'' , B.Numerator , B.Denominator , B.MeasureQuantity , B.MeasureDescription , NULL AS ''Goal'' , NULL AS ''isFinancial'' , NULL AS ''Red'' , NULL AS ''Orange'' , NULL AS ''Yellow'' , NULL AS ''Chartreuse'' , NULL AS ''Green'' , NULL AS ''Threshold'' , NULL AS ''ComputedMinutes'' , 0 AS ''isContractual'' , ' + '''' + CAST(GETDATE() AS varchar) + '''' + ' AS ''Mod_Stamp'' FROM SLA.veDate A LEFT JOIN ( SELECT * , CASE WHEN ComputedMinutes &lt;= MeasureQuantity THEN 1 ELSE 0 END AS ''Numerator'' FROM ( SELECT DateRange , ORD_ID , A.Client , Country , Region , A.LaborTypeCase , ' + @Clause2 + ' , ' + @Clause3 + ' , dbo.NetworkMinutes(' + @Clause2 + ', ' + @Clause3 + ') AS ''ComputedMinutes'' , 1 AS ''Denominator'' , MeasureQuantity , MeasureDescription , B.SLALabel FROM ' + @TempSource + ' A LEFT JOIN [AnalyticsMapping].SLA.mpLaborMappings B ON B.Client = A.Client AND B.SLAType = ' + '''' + @SLAType + '''' + ' AND (B.[Labor Type] = A.[Labor Type] OR (B.[Labor Type] IS NULL AND A.[Labor Type] IS NULL)) AND (B.LABOR_TYPE2 = A.LABOR_TYPE2 OR (B.LABOR_TYPE2 IS NULL AND A.LABOR_TYPE2 IS NULL)) AND (B.LABOR_CATG = A.LABOR_CATG OR (B.LABOR_CATG IS NULL AND A.LABOR_CATG IS NULL)) LEFT JOIN [AnalyticsMapping].SLA.[mpGoals] C ON (C.SLALabel = B.SLALabel AND C.Client = A.Client) OR (C.Client = A.Client AND C.SLALabel = ' + '''' + @SLAType + '''' + ' AND B.SLALabel IS NULL) OR (C.Client = ''DEFAULT'' AND C.SLAType = ' + '''' + @SLAType + '''' + ' AND A.Client NOT IN ( SELECT DISTINCT Client FROM SLA.mpGoals WHERE SLAType = ' + '''' + @SLAType + '''' + ' AND Client &lt;&gt; ''DEFAULT'')) WHERE A.Client = ' + '''' + @Client + '''' + ' ' + @Clause4 + ' AND A.SegmentID = ' + @SegmentID + ' ) Z ) B ON B.DateRange = A.DateRange AND B.Client = A.Client AND B.Country = A.Country AND B.Region = A.Region AND B.LaborTypeCase = A.LaborTypeCase' BEGIN TRY INSERT INTO [AnalyticsMapping].[SLA].[tabDashboard] EXEC sp_executesql @SQL SET @Loop = @Loop + 1 END TRY BEGIN CATCH INSERT INTO [AnalyticsMapping].[SLA].[tabCalculations] SELECT @Client , @SLAType , @SQL END CATCH END --COMMIT TRANSACTION --END TRY --BEGIN CATCH -- -- Determine if an error occurred. -- IF @@TRANCOUNT &gt; 0 -- PRINT 'Error found!...' + OBJECT_SCHEMA_NAME(@@PROCID) + '.' + OBJECT_NAME(@@PROCID); -- -- Return the error information. -- DECLARE @ErrorMessage nvarchar(4000), @ErrorSeverity int; -- SELECT @ErrorMessage = ERROR_MESSAGE(),@ErrorSeverity = ERROR_SEVERITY(); -- RAISERROR(@ErrorMessage, @ErrorSeverity, 1); --END CATCH;
If your .sql file has the contents of your database, you can just simply import it via the terminal for your RDBMS system or use a graphical interface. Check the contents of your file first, though, because it can just contain commands that can make drastic changes to your database. 
How do I check the contents of an SQL file though?
sooooooo.... first of all the terminology they used is kinda weird. dont do it! you dont know what it does, the client doesnt know what it does.by your job title - I assume that you do not have the privileges on that DB to do that (a simple user/password is not enough, you need the a more powerful user to do that). who does this stuff is the DBA of the system, of course its not the vendor of that DB and its not microsoft. it is documented of course, but a DBA should do that. in your organization there is an owner to that DB or an owner to the application that is using that DB. the owner should take care of it. ill be straight with you. if someone from help desk who is not a DBA somehow has a user and password on our DBs and does it just by reading on google and some documents - I will personally fire him on the spot. 
Beware, that doesn't sound like a great idea. Updating statistics to 100% will take a long time even on a moderately sized database and will result in huge amount of I/O operations that can possibly hinder the operations of that database and server. Keeping them updated might also be resource intensive. You can just run `exec sp_MSforeachtable 'UPDATE STATISTICS ? WITH SAMPLE 100 PERCENT, ALL'` if you're really sure you want that.
There seems to be some confusion here. I’m not drubbing the query myself, but my client is asking for a sample query to make the suggested change. 
Hello, You can try the following: &amp;#x200B; select count(distinct EmployeesID) as 'Employees' from (Select EmployeesID, sum(datediff(toDate2014, fromDate2014)) as 'DATE' from (select EmployeesID, CASE WHEN year(fromdate) &lt; 2014 THEN date'2014-01-01' ELSE fromdate END as "fromDate2014", CASE WHEN year(todate) &gt; 2014 THEN date'2014-12-31' ELSE todate END as "toDate2014" from vacations where Status = 'APPROVED') as t where datediff(toDate2014, fromDate2014) &gt;= 1 group by employeeID) as v where [v.date](https://v.date) \&gt; 10; &amp;#x200B; But there is still a case with weekends and holidays I guess.