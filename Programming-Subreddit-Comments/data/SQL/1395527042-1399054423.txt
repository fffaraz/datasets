good point. Depends a bit on the usecases and the indexing, he might be better of with a view (persisted or not persisted). Good point still
I gained permission. I changed it `upload_max_filesize`, but it didn't have an effect in the phpmyadmin site, where it's still telling me that my max is `(Max: 8,192KiB)`. Any other suggestions? [Link](http://us1.php.net/manual/en/ini.core.php#ini.upload-max-filesize) and [link](http://us1.php.net/manual/en/faq.using.php#faq.using.shorthandbytes) for some more PHP stuff I found. Thank you for any assistance.
According to the INI file I modified, "PHP comes packaged with two INI files. One that is recommended to be used in production environments and one that recommended to be used in development environments." Is there a chance I'm modifying the *wrong* INI file?
Also note that depending on how you run php you may need to take one of many actions to get the changes read including but not limited to: * Recycling some windows application pool/restarting IIS * restarting apache * killing off some fcgid processes
Thank you so much! I haven't had time, but I am looking forward to playing around with these CASE statements. Thank you as well /u/ incredibleMouse! Edited for gratitude toward incredibleMouse
That call looks perfectly fine. I am really confused as to why that query won't pull back the right information. Can you run the query directly in the database (using psql or pgAdmin)? Or try the following in your code to see if the join is doing what we wish of it: SELECT * FROM auth_users au1 JOIN auth_users au2 ON au1.email = au2.email OR au1.dti_id =au2.dti_id LIMIT 500; 
Thanks for the help. Learnt something new!
The first one appears to be correct, but you need to change your second answer to reflect the table where doctors reside. Your second statement should be ALTER TABLE OnDuty DROP CONSTRAINT OnDuty_Doc_FK; ALTER TABLE OnDuty ADD CONSTRAINT OnDuty_Doc_FK FOREIGN KEY (docId) REFERENCES Doctor ON DELETE CASCADE;
Yes, this can be done in Oracle with pipelined table functions. Google is your friend.
Looks correct to me. I recommend trying it out in PostgreSQL yourself with a couple of rows of dummy data to be 100% sure.
Oracle won't normally do this. Are colA and colB the exact same data type?
Yes, in fact table2 is the result of a query on table 1.
There has to be something more to it here. Oracle won't round if it's a straight where table1.colA = table2.colB.
After more testing I've found that it doesn't round if I try a third table, which is what table1 and table2 are both based on. Apparently the version of oracle we're using is incredibly out of date, which be causing an issue somewhere along the line. EDIT: Someone else had a similar issue and it goes away on an up to date version of oracle. Damn university is stupidly behind.
LIKE clauses with two % are incredibly slow because indexes don't help the query. For full text search I highly recommend a dedicated search engine. I have a lot of experience with [Solr](https://lucene.apache.org/solr/) and it's very easy to import things into Solr from MySQL.
For clarification, most RDBMSs are written in such a way that leading or trailing wildcards (_ or %) are somewhat performant on indexed fields. Leading *and* trailing wildcards are ridiculously slow in comparison. Depending on your schema and the rest of the query, LIKE with any wildcards can be pretty slow relative to other alternatives, such as a narrower select from sql and then optimized searching in the app, or using another data store like Solr that is purpose built for searching. Sorry it's not the answer you were hoping for, but I hope it helps.
I guess I'd put an analysis widget in your update program or as part of your update scripts. You could do something like use the `INFORMATION_SCHEMA` tables or the object catalog views to get a list of object_ids and their object names, then for each object, send it through `OBJECT_DEFINTION(@object_id)`, send the output through `HASHBYTES('SHA1',@object_definition)`. Then compare it against a table of known hashes for the definitions. Missing and extra objects would be easy to tell, too. You can do that with tables, views, constraints, triggers, scalar and table-valued functions, and stored procs. You can use the metadata functions and object catalogs for anything else, like to see if triggers are disabled. That's if you don't want to just dump sys.all_objects whole hog. There are any number of ways you could get the resulting exceptions table back to you. You can either post the data via JSON to a web service, upload it back to your FTP site, or prompt the customer that the database schema has been altered or corrupted and they should contact a service rep. 
Ultimately you could ensure that all of the schema exists by using IF [NOT] EXISTS and handling the schema update as needed based upon those results. The schema update file well be a bit larger, but it should easily handle this issue.
Thanks. It does help. So what about sorting? Is that also very easy to do on Solr?
Treat the query you've already written as though it was an existing table, and then write the: select BusinessType, Avg(TotalSales) using it. 
I don't have MySQL here to verify my answer, but I hope that I can help you get on the right track. Re-read the question, though. You need to find the total sales for each reseller, then average within business type. Step 1: Total sales by reseller. Step 2: Average the result of Step 1, grouped by Business Type. Step 1: SELECT BusinessType, s.ResellerKey, SUM(SalesAmount) AS TotalSales FROM DimReseller AS R INNER JOIN FactResellerSales AS S ON R.ResellerKey = S.ResellerKey GROUP BY BusinessType, s.ResellerKey Step 2: SELECT BusinessType, AVG(TotalSales) AvgSales FROM ( SELECT BusinessType, s.ResellerKey, SUM(SalesAmount) AS TotalSales FROM DimReseller AS R INNER JOIN FactResellerSales AS S ON R.ResellerKey = S.ResellerKey GROUP BY s.ResellerKey ) GROUP BY BusinessType
Awesome! Thanks for the help! Here is what I ended up with, SELECT T.[BusinessType], AVG(T.TotalSales) AS AvgTotalSales FROM (SELECT [BusinessType], S.[ResellerKey], SUM([SalesAmount]) AS TotalSales FROM [dbo].[DimReseller] AS R INNER JOIN [dbo].[FactResellerSales] AS S ON R.[ResellerKey] = S.[ResellerKey] GROUP BY [BusinessType], S.[ResellerKey]) as T GROUP BY T.[BusinessType] ORDER BY AvgTotalSales 
I don't know of *any* DBMS, where leading wildcards is scaling or performing well. If you have an index on a String column, a string with a trailing wildcard can still be searched for inside the index. For example 'abc%' can still do a seek for abc "something". If you have a leading wildcard, '%abc', there will always be an index scan. You can not search for "something" abc, not inside the index structure. You will have to check every single value contained in the index to do that --&gt; scan. @OP, you can have a look at full-text searches in MSSQL. 
Yes, you can sort by any field as well as special pseudo-fields (like search term relevance!) and you can sort by multiple fields at once, much like in SQL. Also like SQL, sorting is done based on data type, so numeric types are sorted numerically and text fields are sorted ASCIIbetically or alphabetically depending on what modules you are using for it. I have never tried sorting by a combination field like a GPS coordinate (Solr supports localized searches using latitude/longitude, too) but most people never return the GPS coordinate in the results, preferring to return the distance from a given location to the result's coordinates. It is worth noting that you can search by things without returning them in the results and index by things that you don't need to return. The former is something we take for granted in SQL, but the latter is a distinction some database devs don't connect immediately, because in most data stores the set of fields (columns) you store is the same as the set of fields you could return in a result set. Coordinates are usually added in this way: you can query on them (e.g. closer results are more relevant) or filter by them (e.g. exclude all results more than a certain distance away) but often they are not stored for retrieval and cannot be fetched in a result set. This is, of course, just best practice and entirely configurable by the developer.
you might think that's mysql but it isn't
Ya i realized that after i posted. It is just SQLserver. 
... I have my moments...
Does it really throw precisely the same error when you use SELECT TOP 5 with no parentheses, or is the error different?
Thank you!
Have you tried just running this :- SELECT TOP 5 person_id, last_name, first_name FROM person WHERE last_name LIKE '%$q%' OR first_name LIKE '%$q%' ORDER BY last_name 
This gives me the same error as I had mentioned in the original post.
The following query works perfect: $query = "SELECT PERSON.PERSON_ID, PERSON.LAST_NAME, PERSON.FIRST_NAME FROM PERSON where PERSON.LAST_NAME like '%$q%' or PERSON.FIRST_NAME like '%$q%' order by PERSON.LAST_NAME"; This query gives me an error: $query = "SELECT TOP (5) PERSON.PERSON_ID, PERSON.LAST_NAME, PERSON.FIRST_NAME FROM PERSON where PERSON.LAST_NAME like '%$q%' or PERSON.FIRST_NAME like '%$q%' order by PERSON.LAST_NAME"; ERROR Fatal error: Uncaught exception 'com_exception' with message '&lt;b&gt;Source:&lt;/b&gt; Microsoft OLE DB Provider for SQL Server&lt;br/&gt;&lt;b&gt;Description:&lt;/b&gt; 'TOP' is not a recognized function name.' in C:\xampp\apps\wordpress\htdocs\autocomplete-search\search.php:9 Stack trace: #0 C:\xampp\apps\wordpress\htdocs\autocomplete-search\search.php(9): com-&gt;execute('SELECT TOP (5) ...') #1 {main} thrown in C:\xampp\apps\wordpress\htdocs\autocomplete-search\search.php on line 9
What is the sql driver being used in your php.ini file? This would be my first suspect as MSSQL/XAMPP aren't exactly brothers in arms. You may have to upgrade the driver that XAMPP prepackages for MSSQL. [http://samalpramod.blogspot.com/2014/03/connect-sql-server-2008-from-xampp.html](http://samalpramod.blogspot.com/2014/03/connect-sql-server-2008-from-xampp.html) 
I think the best approach to this type of problem is 'Data Warehousing'. instead of writing convoluted queries that need to account for all the inconsistencies of the source systems, you need to write transformation/scrubbing queries once to get the data into an easily queried schema. once your data is in the new schema you would be able to query all the data as if it came from the same consistent source. of course if this is the only query you will ever need on this data, it would save time to just write a 'one-off' ugly query with a bunch of case statements.
thanks for the confirm, i came to the same conclusion....
I would use distinct, but are you guaranteed that prices won't differ for particular colours / sizes?
MSRP will not differ, cost may but i am not looking for cost in this pricebook
It's gonna be the one off ugly query route. There aren't many service types to account for. If anything playing around with the reporting tool will be the most difficult part here. Thank you for your suggestion!
If you're not using any aggregate operators, use DISTINCT so it's clear what you're trying to do.
The idea is to only use `DISTINCT` when you actually need to eliminate duplicates. If you don't need to eliminate duplicates, then you're asking the database engine to do additional work against every row. The DB engine will have to do a distinct sort operation, which is not free. Additionally, finding duplicates when you don't expect them to be there is often a sign of a logic error in your query. Maybe you're inadvertently doing a cross join, which can make a lot of extra work. Or maybe there's a field of the primary key you forgot about. You should use `DISTINCT` when you *know* you're going to get duplicate rows and you *know* you don't want duplicates. TL;DR: A lot of novice SQL developers tend to just put `DISTINCT` on everything without realizing why it may not be appropriate. That's why it's discouraged. As far as the difference, the query engine treats them identically. Look at the [execution plans on this SQLFiddle](http://www.sqlfiddle.com/#!6/2b23f/1). 
I honestly don't like it at all. You have no information about the nesting level of sub-resquests, and I find it hardly readable. The pre-analysis is accurate but I find the solution sub-optimal. 
Several observations about your queries * You are treating dates as strings, when using your like statement, DON'T - that relies on your session date format settings matching DD-MON-YYYY. * You are doing lots of unnecessary joins! Use exists, as this will do a short circuit boolean evaluation, i.e. it will stop looking once the predicate is found to be true. The most efficient solution I can come up with is something like this (untested). On large data it should comfortably outperform your solutions by a large margin. Note, dual is a dummy table in Oracle, that you can select an arbitrary value from. select c.CUSTOMERKEY, FIRSTNAME || ' ' || MIDDLENAME || ' ' || LASTNAME as "Full Name" from DIMCUSTOMER c where 1 = (select sum(EXIST_MARKER) from (select 1 as EXIST_MARKER from dual where exists (select null from FACTINTERNETSALES s where s.CUSTOMERKEY = c.CUSTOMERKEY and s.ORDERDATEKEY between to_date('01/03/1993', 'dd/mm/yyyy') and to_date('31/03/1993', 'dd/mm/yyyy')) union select 1 from dual where exists (select null from FACTINTERNETSALES s where s.CUSTOMERKEY = c.CUSTOMERKEY and s.ORDERDATEKEY between to_date('01/04/1993', 'dd/mm/yyyy') and to_date('30/04/1993', 'dd/mm/yyyy'))))
Having a really hard time following the logical structure of the boolean expressions in the WHERE clause.
that's due to the crappy original sql, not in the new coding style these two syntax errors were in the original sql as well -- and IS NULL(cnd.Doc_Flag,'N')='Y' and s.AdvInvalidated ISNULL 
Not only is this bikeshedding, but you're painting it bright green and making it difficult to actually use. Congratulations. Edit: Apparently it looks less awful in IE, which begs the question, whom does he consider his primary audience?
&gt; bikeshedding TIL a new word, thanks 
Thanks for the bug report.
Please keep in mind I'm still learning, so my replies might not be the greatest. &gt;You are treating dates as strings, when using your like statement, DON'T - that relies on your session date format settings matching DD-MON-YYYY. Noted. One thing though - you're treating ORDERDATEKEY in your revision as if you can directly convert ORDERDATEKEY to a date, but using this DB, you can't. This is why I was joining to the table DIMTIME - the column FULLDATEALTERNATEKEY (which are timestamps like 15-AUG-04 12.00.00.000000000 AM) is the only way to match ORDERDATEKEY to an actual date. For example, the ORDERDATEKEY '1127' is for July 31, 2004. Absolutely no correlation. Anyways... I think I see where you were going with this, but this didn't/doesn't work: SELECT SUM(exist_marker) FROM (SELECT 1 AS EXIST_MARKER FROM DUAL UNION SELECT 1 FROM DUAL ); That's just a stripped down version of the SUM function there, but it always returns 1, even if both of the exists clauses return 1. Changing the second exists to a 2 properly makes this return a 3 if they both exist, however. Was I doing something wrong, or is that just an odd quirk? Regardless, I can't seem to get it to work using exists. They exist parts now look like this. WHERE EXISTS (SELECT NULL FROM factinternetsales S JOIN dimtime ON dimtime.timekey = S.orderdatekey WHERE (c.customerkey = s.customerkey) AND (fulldatealternatekey between to_date('1/04/2003','dd/mm/yyyy') and to_date('30/04/2003', 'dd/mm/yyyy')) But I get an invalid identifier error on c.customerkey, and I can't figure out how to rectify it. Joining dimcustomer to it using "dimcustomer.customerkey = S.customerkey) ends up making the query return every name in the table. Any ideas? Anyways, I wrote another solution this morning that looks like this. SELECT firstname || ' ' || middlename || ' ' || lastname AS fullname FROM dimcustomer WHERE (customerkey, 1) IN ( SELECT customerkey, count(customerkey) FROM factinternetsales JOIN dimtime ON factinternetsales.orderdatekey = dimtime.timekey WHERE (fulldatealternatekey between TO_DATE('1/04/2003', 'dd/mm/yyyy') and to_date('30/04/2003', 'dd/mm/yyyy')) OR (fulldatealternatekey between TO_DATE('1/03/2003', 'dd/mm/yyyy') and to_date('31/03/2003', 'dd/mm/yyyy')) GROUP BY customerkey) ORDER BY fullname; What are your thoughts on this one? It's much faster than my first solution, but I don't know how it would compare to the exists one if I could get it working properly. This one takes roughly **5%**(!!!!) of the time that my original solution did though, even with the ORDER BY. (Also, a reminder that I'm not using the original dates from the question in my solutions, since they don't exist in this DB. The dates range from 2001 to 2005.)
You don't need T-SQL to do this. Just do : SELECT myField + '1111' FROM myTable If you really want to use T-SQL (homework ?), use a cursor to iterate through the records of your table, adding '1111' for each record. However, this solution shouldn't be used in real world because : * It's much more complicated/specific than standard SQL * Cursors are far less performant than a request doing the same thing
 UPDATE tablename SET columnname = columnname || '1111'
One of those days... Of course I immediately go to using the cursor while I could have just used. Thanks for the feedback! UPDATE tablename SET columnname = columnname || '1111'
Don't they both hit TempDB?
I took the code block (unformatted) from the article, tossed it into SSMS and let RedGate's SQL Prompt take a crack at it. It found 3 syntax errors (Dev doesn't know the difference between ISNULL and IS NULL) and when they were corrected, it came up with this: SELECT s.AdvAssignment FROM study s INNER JOIN Assignment_Status cnd ON s.AdvAssignment = cnd.AdvAssignment WHERE s.AdvReference IN ( SELECT adv FROM @userListGroup ) AND s.AdvInvalidated IS NULL AND ( cnd.Digital_Attachment_Nbr &gt; 0 AND NOT ( cnd.Adv_Nbr = 0 AND ISNULL(cnd.Doc_Flag, 'N') = 'Y' ) OR s.Is_Image = 1 ) UNION SELECT s.AdvAssignment FROM study s INNER JOIN Assignment_Status cnd ON s.AdvAssignment = cnd.AdvAssignment LEFT OUTER JOIN UserAssignment us ON s.AdvAssignment = us.AdvAssignment WHERE ( us.AdvUser = @userid AND ( us.assigndate = GETDATE() ) ) AND s.AdvInvalidated IS NULL AND ( cnd.Digital_Attachment_Nbr &gt; 0 AND NOT ( cnd.Adv_Nbr = 0 AND ISNULL(cnd.Doc_Flag, 'N') = 'Y' ) OR s.Is_Image = 1 ) It looks pretty similar, but a lot easier to read in my opinion. I've seen this sentiment a lot lately, mainly from developers of other languages who try writing SQL and experience culture shock because "it doesn't look like C." SQL isn't supposed to look like C.
The is awful
 &gt; WHERE (customerkey, 1) IN ( &gt; SELECT customerkey, count(customerkey) &gt; FROM factinternetsales This won't work, it will only return the correct results if a customer only has one order, multiple orders are fine as long as only during one of those months. Yes, I'm not familiar with your schema, so assumed ORDERDATEKEY was a date. How about this? select c.CUSTOMERKEY, FIRSTNAME || ' ' || MIDDLENAME || ' ' || LASTNAME as "Full Name" from DIMCUSTOMER c where (select count(*) from dual where exists (select null from FACTINTERNETSALES s join DIMTIME d ON d.TIMEKEY = s.ORDERDATEKEY where s.CUSTOMERKEY = c.CUSTOMERKEY and d.FULLDATEALTERNATEKEY between to_date('01/03/2003', 'dd/mm/yyyy') and to_date('31/03/2003', 'dd/mm/yyyy'))) &lt;&gt; (select count(*) from dual where exists (select null from FACTINTERNETSALES s join DIMTIME d ON d.TIMEKEY = s.ORDERDATEKEY where s.CUSTOMERKEY = c.CUSTOMERKEY and d.FULLDATEALTERNATEKEY between to_date('01/03/2004', 'dd/mm/yyyy') and to_date('31/03/2004', 'dd/mm/yyyy'))) By the way, I keep my table aliases short, otherwise there's no point in having them!
OP hasn't a clue.
try /r/programming they come up with ways to "improve" SQL all the time.
If it's all numbers you can simply multiply by 10000 and add 1111. SET ColumnName = (ColumnName * 10000) + 1111
I agree with the general dislike here as well. What he proposes looks horrible to me. I actually prefer his fourth example. And I'm not sure what's meant by a "substantially larger query". I routinely work with queries that can run hundreds of lines. In that case, the suggested styling with that much white space would run on forever and would be a nightmare to try to digest. I would literally hate you if you gave it to me like that. No, your DBA will not have an "easier time".
That technique is far too verbose in terms of lines taken - ok for simple examples, impractical for large reality examples. If I did that with some of the queries used in our production systems, you'd be scrolling pages long. My style.... SQL Keywords are lower case, tables and columns are UPPER. Why? 1. Because DBs store table and column names in upper case, so you are matching how they represent it. 1. Most programming languages language constructs are predominantly lower case (camelCase, InitialCapital case etc.) humans read language constructs better that way. ON clauses appear with the join clause (not a separate line(s) unless the join has many predicates) because they belong to the join. Also the current table context takes priority in the order of the predicate. e.g. ... from T_MY_TABLE1 a join T_MY_TABLE2 b on b.FK_ID = c.PK_ID This is the context of T_MY_TABLE2 b, so I prefer b.FK_ID = c.PK_ID, over c.FK_ID = b.PK_ID, even if logically it makes no difference Long case.... from T_MY_TABLE1 a join T_MY_TABLE2 b on b.FKPART1 = c.PKPART1 and b.FKPART2 = c.PKPART2 and b.FKPART3 = c.PKPART3 Each to their own on styles I suppose.
Do you have a computer and router than you can access off site? MySQL is relatively easy to setup, and you could host from "home", giving you remote access, even after school is over.
Only if it's too large to do in memory, AFAIK. You'll get a warning on your execution plans when the DB engine has to switch to TempDB to do it's sorting. [This](http://sqltouch.blogspot.com/2013/06/order-bygroup-by-operation-spills-in.html) blog post seems to describe the issue fairly well. It suggests creating non-clustered indexes to solve the sorting problem if it keeps coming up. However, if a) you need to run the query, and b) you need to have a distinct list, and c) you don't change the engine behavior by using `DISTINCT` or `GROUP BY` and d) can't create an index... well, TempDB is probably going to have to suck it up. Honestly I've never run into this situation so I don't know how to solve it. My current primary DB server has 16 procs and 24 GB of RAM for a relatively small DB (10 GB) with a fairly complex schema (1000 tables, 250 views). 362 days of the year, our SQL server is fairly bored. The remaining days it's processing year end rollovers and is anything but. 
I'm assuming this is Raiser's Edge? As a data analyst I hate the design of their backend database. But opinions aside, it's likely because if you do CAST('6/30/2013' as datetime), you would miss everything that happened during the day of 6/30 on the BETWEEN clause. It will find everything up to midnight that morning. By doing the whole timestamp, it gives you that whole day. Why they don't do it consistently between the two dates, I have no idea. I'm an Oracle guy mainly, so someone with more SQL server experience might be able to give a better answer
You want a clustered index on that column, but be careful. A poorly chosen physical index can impact performance. You're likely better off doing the sorting in your SELECT queries unless that column happens to be your main join key. 
&gt; I'm assuming this is Raiser's Edge? But of course! &gt; As a data analyst I hate the design of their backend database. Ha, I don't think there is that much design to it. I think it's been added to and cobbled together so much over the last 25 years that it's hopeless and only a complete redesign from the bottom up will fix it. But that's neither here nor there. Thanks for the info - I see what you're saying about the 6/30/13 date, so I guess what I was really wondering was why they didn't just use a timestamp for the 1/1/05 date... Anyway thanks for clearing up part of it anyway...
What he said, including time part in ending date ensures you get activity to end of day... think this part is safely omitted from starting date (1/1/2005 0:00:00.000)
There is no default order by. The clusterd index will sort the data saved to disk to be sorted by the the index, BUT always have an order by in your select.
Thanks. It's interesting to me that the fact that something can be safely omitted would lead them to omitting it rather than just being consistent.
Additionnally, relying on an index to provide a sort on a request isn't a good practice. If you ever remove that index, that could mean some trouble because you're going to lose functionality, which shouldn't happen.
Correct, with a "but": Even if the index never goes away, SQL does not guarantee the order of anything unless you specify it yourself.
There is no final write. I am selecting against an Oracle database primarily. The main concern is being able to break the work up into discreet pieces that can be worked individually to make the resulting query easier to write, update, and otherwise maintain. I have had a lot of success breaking monster queries down into manageable pieces with the use of table variables in MSQL. I could do the same with a temp table but those are verboten. I'm not sure what you are suggesting with the WITH keyword. But I don't think it will help me. I do really appreciate the help though.
I'll research pipelined table functions. Thanks!
A lot of the larger web apps out there are using it now a days... Maybe the more senior devs from those companies have blogs?
Why not just use window functions?
That just sounds spiteful.
Window Functions are awesome, but not all SQL engines (MySQL and MariaDB) implements them. I may cover an article later using window functions to do some stuff!
You'll need to add an ORDER BY clause in the presentation layer. SQL doesn't order data by default. It's not a spreadsheet.
WITH allows you to create a temporary table that exists only for the duration of the query instance. It doesn't write anything to any tables, and doesn't modify any data, thus can be done without having write privileges. You could then write a second query that selects from that. I rarely work with MSSQL, so I don't have a good idea of what table variables are or how they're used. You can't INSERT values into a WITH clause, but it seems like with some manipulation, you could make it do what you want. You can have a series of these temporary tables. For example: WITH temp_table1 as (select 1 as col1, 2 as col2 from dual), temp_table2 as (select col1 + col2 as calc1, (select another_value from some_other_table where condition='this') as calc2 from temp_table1), temp_table3 as (select sum(calc1)/calc2 as final_val from temp_table2) select * from temp_table3 And so on. I'm not sure what kind of calculations you're doing, but you should be able to do what you propose with a series of temporary tables created with a WITH.
Much better achieved with windowed analytic functions or cube / rollup / grouping sets. Although these are not supported by all RDBMS (a few are lagging behind), the article's approach is very dated compared to modern SQL techniques.
And that is a very good reason (as if we needed more) why one should stay far, far away from them.
There is no reason to physically sort your data. Just put an index on the column and MSSQL will optimize queries that use it. If you are worried about the order of your SELECT output, any number of factors can alter that from the physical storage. If you want sorted output, use an ORDER BY clause.
&gt;This won't work, it will only return the correct results if a customer only has one order, multiple orders are fine as long as only during one of those months. Oops, you're absolutely right. How embarrassing. &gt;How about this? I haven't looked at it too much yet (and won't get a chance to for a while yet), but as is, it's very slow (~3.8s). Thanks for your help, by the way. Sorry this is such a slog. (EDIT: Removed inaccurate info about it returning wrong number of results. I forgot to change the dates to what I was familiar with.)
Not sure I follow. Should we stay from anything one language provides that others do not? 
I would say that it's a trade off when using windowed analytic functions. With an OVER statement you can use PRECEDING, which is perfect for rolling averages. However, using the WHERE construct allows for more flexible control over aggregating on a relative timeframe, like say if I wanted to compare one week's aggregate with the same week 1 year prior, one hours aggregate vs 1 month prior at the same hour. Doing this with RANGE BETWEEN is pretty tedious for more complex timeframes. Cube/rollup/grouping set is an excellent approach for MSSQL 2008+/Oracle 10g+; especially for performance. I'm more familiar with MySQL/MariaDB/Postgres/Redshift/Vertica and this article is intended for that audience so I guess it is dated :(
Okay, I've come up with my (I think) final solution, but could you help me understand something? Why does this query not solve this problem? SELECT customerkey, firstname || ' ' || middlename || ' ' || lastname AS fullname FROM dimcustomer WHERE (customerkey, 1) IN ( SELECT customerkey, count(*) FROM ( SELECT DISTINCT customerkey FROM factinternetsales JOIN dimtime on factinternetsales.orderdatekey = dimtime.timekey WHERE fulldatealternatekey between TO_DATE('1/04/2003', 'dd/mm/yyyy') and to_date('30/04/2003', 'dd/mm/yyyy') UNION ALL SELECT DISTINCT customerkey FROM factinternetsales JOIN dimtime on factinternetsales.orderdatekey = dimtime.timekey WHERE fulldatealternatekey between TO_DATE('1/03/2003', 'dd/mm/yyyy') and to_date('31/03/2003', 'dd/mm/yyyy') ) GROUP BY customerkey) ORDER BY customerkey; My thoughts were that the subqueries should return 1 record per CUSTOMERKEY if that CUSTOMERKEY placed an order in the respective date range. The UNION ALL should result in two records of that CUSTOMERKEY if that CUSTOMERKEY placed an order in *both* date ranges. Then it's simply a matter of finding which CUSTOMERKEYs have a count of 1 from that result. However, things aren't working like I thought they would, since if a CUSTOMERKEY exists in both ranges, the UNION ALL'd query is returning only 1 record for that CUSTOMERKEY. As far as I can tell, the problem must exist with the DISTINCT statements. Let me put it like this... DISTINCT on both subqueries = will make the UNION ALL return 1 result if the key exists in either subquery, even for objects existing in both subqueries (which should return 2 results). DISTINCT on 2nd = The UNION ALL will return the same number of records for a CUSTOMERKEY that exists in either date range. In other words, the DISTINCT does nothing. If the 2nd subquery has duplicate elements, **all duplicate elements still end up as part of the UNION ALL result**. DISTINCT on 1st = Same as DISTINCT being on both. Even if the 2nd subquery returns multiple records, the UNION ALL will merge them into one. DISTINCT on none = All matching records returned (expected) It seems like DISTINCT being on the first subquery ends up applying that to the entire UNION ALL. Is this the case, and is that proper functionality? It seems really counter-intuitive. This, on the other hand, despite being only a slightly tweaked version of the above, seems to work (and at least relatively quickly): SELECT customerkey, firstname || ' ' || middlename || ' ' || lastname AS fullname FROM dimcustomer WHERE (customerkey, 1) IN ( SELECT customerkey, count(*) FROM( SELECT customerkey FROM ( SELECT DISTINCT customerkey FROM factinternetsales JOIN dimtime on factinternetsales.orderdatekey = dimtime.timekey WHERE fulldatealternatekey between TO_DATE('1/03/2003', 'dd/mm/yyyy') and to_date('31/03/2003', 'dd/mm/yyyy') ) UNION ALL SELECT customerkey FROM ( SELECT DISTINCT customerkey FROM factinternetsales JOIN dimtime on factinternetsales.orderdatekey = dimtime.timekey WHERE fulldatealternatekey between TO_DATE('1/04/2003', 'dd/mm/yyyy') and to_date('30/04/2003', 'dd/mm/yyyy') )) GROUP BY customerkey) ORDER BY customerkey; 
Other than adding a few values to tables to verify that some queries perform properly, I haven't modified the AW DB at all. So, you're entirely correct that I haven't indexed the tables and foreign key columns.
Adding the indexes makes your query come down to ~.02s, which, like you said, is much faster than any of the other queries listed, although the final one I posted is similar (albeit I don't know how it would scale).
Logically, I think this will work, but it is inefficient - and I can't see how this outperform my query. With **select distinct** the optimiser will either do a **SORT ORDER BY** (it has to sort the records in the SGA and remove duplicates) or **HASH UNIQUE** (where it works out the hash of each value, sorts and removes hash duplicates). On large datasets - where customers have made lots of orders - this is CPU intensive. So you are selecting lots of values, sorting them, removing the duplicates, unioning them, aggregating them - seems convoluted to me. **My query above does the following:** Look for one order that fufills the first date criteria (index range scan with stop key), turn it into a 1 or 0 (null won't happen as aggregation using dual) Look for another order that fufills the second date criteria (index range scan with stop key), turn it into a 1 or 0. Compare the two values. 0 =&gt; 0 = NOT OK 1 =&gt; 0 = OK 0 =&gt; 1 = OK 1 =&gt; 1 = NOT OK How can yours be quicker or more comprehensible? 
&gt;How can yours be quicker or more more comprehensible? I don't know? I'm just reading the execution times from the queries in SQL Developer. I literally just started learning SQL. I have no idea about anything that's going on behind the scenes. If you think I'm BSing you or something, here are screens after adding indexes: Your query: http://i.imgur.com/MSoZaUv.jpg Last one listed here: http://i.imgur.com/Xh6UBQj.jpg 
It will scale badly! See my comments below.
When you have two products (in this case PG and MySQL), one of which is lacking important features, you should go with the one that has the important features. Especially when the other one has some notably bad design choices (e.g. coercing NULLs in NOT NULL columns to 0 or '' rather than rejecting the update). Edit: I think you misunderstood me. I was not suggesting that one should avoid window functions. I meant that one should run screaming from MySQL and its derivatives. 
&gt; However, using the WHERE construct allows for more flexible control over aggregating on a relative timeframe, like say if I wanted to compare one week's aggregate with the same week 1 year prior, one hours aggregate vs 1 month prior at the same hour. Doing this with RANGE BETWEEN is pretty tedious for more complex timeframes. I'm not sure I entirely agree with this as I've done almost similar to the article in production systems using windowed analytics and ROWS BETWEEN and RANGE BETWEEN with where clauses. Sometimes you may need to window on sub queries with case statements, to create your reference dataset, but it will probably still be more efficient. &gt;I'm more familiar with MySQL/MariaDB/Postgres/Redshift/Vertica and this article is intended for that audience so I guess it is dated :( Fair enough, it wasn't obvious you had the restriction of certain RDBMS platforms - so given those caveats your approach may be the best way of doing it. It's well written, but perhaps a contrast to modern SQL techniques might be valuable too. Don't forget, you always have the amazingly useful [**SQL fiddle**](http://sqlfiddle.com) to play with when you don't have access to a platform you want to try out.
Added 300k records (the original was 63k). The last one I listed now operates in roughly ~0.0543s, and the one you posted is sitting at ~0.0275s. So, indeed, your solution scales much better. Thanks for all your help~
in fact, if the query does not include an order by, the execution plan will not contain a sorting. You might end up with the order of the CI, but that goes out the window if the query optimizer decides to for example use a merge join. There simply is no default order by. For all intense and purpose, you will get an unsorted recordset, which is how you have to treat it.
Ah I see. Thank you for clarifying! 
&gt; concatenate '1111' on the end of each record in a certain column but it is good to know if somebody need it 
Need to know the tables involved, how they're indexed, how many records, what kind of updates/inserts are firing the trigger ? Need code for Update_SP.
If I can't insert into it then that would not be able to do the primary task I am hoping to accomplish. To create a simplified example of what I am doing: I have to create a data set that contains the following information: event_id, event_name, event_type_id, event_type_name, event_venue_id, event_venue_name, event_venue_type_id, event_venue_type_name, event_registration_type_id, event_registration_type_name, event_attendance_total, event_gross_dollars, event_tax_dollars, event_venue_rental_dollars, event_administration_fee_dollars, event_profit_dollars, Imagine an event where each attendee is recorded individually. Each attendee buys a different type of ticket for the event with a different classification and a different cost. Further each attendee can pay over the ticket's face value as a donation. Getting the first 12 columns is just an excersize in joining on tables and grouping for the attendance count and the gross. However the final 4 columns are harder. The venue rental is a flat fee plus a percentage of the base ticket cost, but that percentage varies based on what kind of ticket was purchaced and how many of that kind of ticket was purchaced as a percentage of the total tickets. The venue does not get any portion of the donation. The administration cost is a percentage of the donations based on the number of tickets sold regardless of the class. And a percentage of the base cost of the tickets based on the class of tickets, but not the percentage that class of ticket represents as a total of all sales. The administration cost is also partly made up of how much more than the venue cost that was taken in. The tax portion is a percentage of the total, plus a percentage of the administration cost, plus a portion of the venue rental cost, plus a premium above and beyond that if the administration cost exceeds 25% of the total cost of the event. The profit however is just the total remaining of the gross after all the other costs are subtracted. Finally the exact percentages for each aspect of the costs change based on the venue type as well as the event type. This is NOT what I am doing, but it is LIKE what I am doing. In theory I could do this all in a single monsterous query with all kinds of conditionals in it. However, what I would like to do is select the first 12 columns and insert into a temp table (or table variable) with nulls for the last 4 columns. Then simply perform set based actions on it. So for example: UPDATE table_name SET event_venue_rental_dollars = ( QUERY HERE THAT SELECTS AGAINST THE TICKET SALES TABLE DOING THE NEEDED CALCULATIONS. ) WHERE event_type_id = 12 AND event_venue_type_id = 33 Just to a number of actions like that agains the table until I have covered all the needed permiatiations that we deal with. It probably wouldn't be particularly efficient, but should still be more efficient than a cursor, and much easier to build, update, and maintain than a single monster query.
Nested REPLACE is generally the best option. SQL functions can be more readable and be reused in lots of places, but can have a very real negative effect on performance. I've also gotten some mileage out of a CLR RegExReplace function which does a variety of fancy formatting, but it is overkill for this specific question.
How often do you plan on doing this? Is it just once on an entire table? If you need to do it often, then a function is what you're wanting... Take a look at [THIS](http://www.sqlservercentral.com/scripts/String+Manipulation/70558/) function, it will strip out any non-alphanumberic characters like - , etc
This is not readable
Good for de duping
Temp tables are definitely the way to go with this. It's also efficient to create a temp table and use MERGE to introduce the new data. Cursors are so much fun to write, so crappy in practice. 
If you need just a bit of space, I can get you on my server for free. PM me and we can chat a bit about it.
Other than the value in the ID field, which is shared by all records in your screenshot, what makes the two highlighted records special? How are they related in the business process? There does not appear to be any common value in the ValStr field you make reference to. 
It's not SQL-92 compliant, so this is probably the wrong place to ask.
The orange is the voucher number on which an invoice was paid; the green is the accounts payable batch number in which the invoice was processed. The user only knows the voucher number, and needs to find the batch number. I'm sorry that my question was so vague. I edited the OP above with the query I came up with that returns only the value I need. It's messy, but it works!
The green is the accounts payable batch number in which an invoice was processed, and the orange is the voucher number on which the invoice was paid. The user in this case knows only the voucher number and needs to find the batch number. I edited the OP with the code that I came up with that takes the voucher input from the user and returns the batch number. It's messy, but it actually works, and that's good enough for now!
Came to say just take a dump of sys.all_objects, shouldn't be anything too large then just FTP it back to yourself or whatever.
I think you have 2 problems here: 1. You have not described the relationship between these 2 rows properly. I will try that below and you can say if it is correct. 2. you are doing too much in subqueries when you should be using joins against the same table (a lot of people struggle with this). First the problem. - it looks like for a set of invoice rows you can get the voucher number by getting the valstr for the record where the DefId = the same Def Id as a record in the same set where ValStr = 'BatchId'. If this is so, something similar to this should work: SELECT c.Valstr AS ApBatchNumber FROM LLAttrData a INNER JOIN LLAttrData b ON (b.Id = a.Id and b.ValStr = 'BatchId') INNER JOIN LLAttrData c ON (c.Id = b.Id AND c.DefId = b.DefId AND c.ValStr != 'BatchId') WHERE a.Valstr = &lt;userInput&gt; Hope this helps. 
I usually like to format my replaces in a row so they are easier to match, but in your case it's pretty obvious what is happening. *Edit - Nice formatting of something bland/understandable usually outweighs being clever.* Example: *** REPLACE(REPLACE(REPLACE(REPLACE(REPLACE( p.phoneNumber, '-', ''), '(', ''), ')', ''), ' ', ''), '.', '') *** As *jc4hokies* mentioned there are other methods, but most will be slower performing or more time consuming to setup than REPLACE.
Thanks! I like this solution I'm going to use this.
You need to get your grouping of records isolated out first. As you pointed out the thing that makes a machine unique is the combination of machine, side and position. The thing that makes the specific record about that machine unique is the timestamp. So, we'll throw the overall unique records for the machine and its most recent timestamp into either a sub-query, or a temp table (I used temp table here) and then join the full record set to the sub-set on all 4 attributes that make the specific record unique. DECLARE @rec TABLE (machine INT, side INT, Position INT, [timestamp] DATETIME) INSERT @rec SELECT machine, side, position MAX([timestamp]) FROM machines GROUP BY machine, side, position SELECT m.* FROM machines AS m JOIN @rec AS r ON r.machine = m.machine AND r.side = m.side AND r.Position = m.Position AND r.[timestamp] = m.[timestamp] You could also do it with a sub-select like so: SELECT m.* FROM machines AS m JOIN ( --beginning of the sub-select SELECT machine, side, position MAX([timestamp]) FROM machines GROUP BY machine, side, position ) AS r --end of the sub-select ON r.machine = m.machine AND r.side = m.side AND r.Position = m.Position AND r.[timestamp] = m.[timestamp] Obviously, using SELECT * is generally bad, but without knowing the other column names, hopefully the example is clear.
Rank over partition by timestamp desc as rank )where rank=1. It's something like that. Google using ranks in mssql and you'll find it. 
1. Yes, this is exactly right, except I've since found that these DefIDs are shared between all invoice records. But within rows for this ID, that's what I was trying to say. I'm sorry for the vague explanations. 2. So much better and cleaner! Yes, this is precisely what I was trying to do, but didn't know how! To be honest, during my reading I glossed over any references to self joins because I thought it was too advanced and I wouldn't need that information, and when presented with this problem it didn't even cross my mind to join the table with itself. It makes so much sense now. The lowercase letters are different instances of the table? Can you give them different names, or are you limited to characters? Thank you very much for your help. I'm going to try to recreate what you've written in my report. I've learned something new.
Here is a messy looking trick that I've become addicted to. It is equivalent to the ranking solution, but doesn't require a sort. This example could leverage hash grouping (if no suitable index is available) instead of needing to sort by machine, side, position, and timestamp if a ~~rank~~ row_number function were used. SELECT machine, side, position, STUFF(MAX(CONVERT(char(16),CONVERT(binary(8),timestamp),2)+status),1,16,'') AS status, STUFF(MAX(CONVERT(char(16),CONVERT(binary(8),timestamp),2)+efficiency),1,16,'') AS efficiency FROM table GROUP BY machine, side, position; edit: the ~~rank~~ row_number solution for comparison SELECT * FROM ( SELECT machine, side, position, status, efficiency, ROW_NUMBER() OVER (PARTITION BY machine, side, position ORDER BY timestamp DESC) AS rank FROM table ) x WHERE rank = 1; edit2: if your timestamp is a datetime (I read it as a timestamp datatype) (convert style 121 orders properly as a string) SELECT machine, side, position, STUFF(MAX(CONVERT(char(23),timestamp,121)+status),1,23,'') AS status, STUFF(MAX(CONVERT(char(23),timestamp,121)+efficiency),1,23,'') AS efficiency FROM table GROUP BY machine, side, position;
The lowercase letters are just alias's for the table and you can use any combination of characters (within reason) so you could say: INNER JOIN &lt;tablename&gt; &lt;alias&gt; ON (alias.xxxx = ...) You must always use the alias in front of columns from the result of the join. Syntaxically you can also say: INNER JOIN &lt;table&gt; AS &lt;alias&gt; ON (...) Note in the old days we never used INNER JOIN but we just put everything in the WHERE clause - this still works but is considered bad practice: SELECT a.col, b.col, c.col FROM table a, table b, table c WHERE a.col = "abc" AND a.col = b.col AND b.col = "xyz" AND c.col = b.col AND (c.col = 1 OR c.col = 2) ORDER BY xxxx etc. Hope this helps. 
Awesome trick - I wonder how the performance is over the larger sets. I have to remember this, anyway. For the 2nd part though - need to use row_number (vs rank) since multiple rows can share the same rank (potentially). 
The larger the dataset the greater the benefits of aggregation (max) compared to sequencing (row_number). The requirements for a suitable index are also less. It is pretty common to have a suitable index for the partition columns, but not the order column. Aggregation only cares about the partition columns. MAX will work with: CREATE INDEX IX ON table (machine, side, position) INCLUDE (timestamp, status, efficiency) ROW_NUMBER will work with: CREATE INDEX IX ON table (machine, side, position, timestamp DESC) INCLUDE (status, efficiency) *changed rank function to row number is previous post*
http://phphosting4free.com Free web hosting with mySql. No ads. I use it quite a bit. 
lol - this explains a lot. I looked at it in Chrome and thought the guy must have been drunk. Looks ok in IE although hardly a new coding style. 
 SELECT t.item , t.val , t.count FROM ( SELECT item , MAX(count) AS highest FROM daTable GROUP BY item ) AS x INNER JOIN daTable AS t ON t.item = x.item AND t.count = x.highest 
homework?
No, a real problem I'm trying to solve.. SQL isn't my job, but I'm an analyst so I need to do a lot of work with datasets.. I'm paranoid about my identity so I generalized it as much as possible.
You can use CTEs for this. Your subquery is represented as `SELECT ...` in the example below. WITH daTable AS (SELECT ...) SELECT t.item , t.val , t.count FROM ( SELECT item , MAX(count) AS highest FROM daTable GROUP BY item ) AS x INNER JOIN daTable AS t ON t.item = x.item AND t.count = x.highest 
You should specify which database you run. In PostgreSQL this is easy using a non-standard extension. SELECT DISTINCT ON (item) item, val, count FROM test ORDER BY item, count DESC To do the same in ANSI-SQL you will either need to do a join with a subquery with `max()` like /r/r3pr0b8 said or use window functions like the below. SELECT item, val, count FROM ( SELECT item, val, count, max(count) OVER (PARTITION BY item) FROM test ) q WHERE count = max
Now that I've tried it I can't use ctes. Isolated, this works great, but I can't do select * from this. Or use this as a subquery within another query. 
Which database do you use? MySQL does not support most advanced SQL features like CTEs and window functions.
This will find the most recent entry for duplicate MACHINE, SIDE, POSITION in the most efficient way possible (single table scan). In the case of a tie on THE_TIMESTAMP, it will only return one of them. select * from ( select t.*, row_number() over (partition by MACHINE, SIDE, POSITION order by THE_TIMESTAMP desc) RNUM from MY_TABLE t) where RNUM = 1
Rank can return duplicates on ties.
I get "ordered analytical functions not allowed in where clause" when I do that second option
So you are using Teradata then. I cannot check myself how to fix the query since I do not have access to Teradata, but this Stackoverflow question may contain the solution. http://stackoverflow.com/questions/13747015/is-it-possible-to-use-where-clause-in-same-query-as-partition-by If I understood correctly the below should be the solution. `QUALIFY` is as far as I know Terdata specific. SELECT item, val, count FROM test QUALIFY max(count) OVER (PARTITION BY item) = count 
This isn't particularly helpful, but I too have had numerous issues in passing date fields from MS Access/SQL Server to Excel. I do have a functional example in one of my setups- I won't have the ability to check it out and pass along the info until tomorrow morning (10 hrs from now)
I'm nothing if not patient. :) Thanks.
No values return. :(
Through MS Query. For whatever reason, utilizing a SQL connection source without going through MS Query causes epic catastrophes.
You can use co related subqueries for this. Select * from table as outer where count = ( select max(count) from table as inner where inner.item = outer.item) This will give the max of count for each item
That is correct, but I have a minor nitpick. It is called "correlated subqueries".
Has anyone used this? How is it?
The best tools for this kind of work are Extract Transform Load (ETL) software. I am only familiar with Microsoft's version (SSIS) and someone else may know other products if that is not available to you. A common approach for programming solutions is to pull the data from your transactional database into a flat file. Then to load the flat file into your reporting database.
Saweet, I havent brushed up on my Query in a while, might be worht a shot. 
I'll look into those programs. The second option also seems like its viable. Basically the reporting DB will only have a very small fraction of the data from Database A and Database B. DB A and DB B might have thousands of call events and email events and associated parameters. The reporting database will basically take a daily count/ average of maybe 5-6 calculated metrics. I already know the queries to a T in order to retrieve these 5-6 daily figures. Now I just need an automated way for the reporting DB to query DB A and DB B with my two queries, and store that data in itself. Eventually I'll have a connected website display a one-day lag summary from the reporting database, if that makes sense. So I really just need some automated daily job, of the reporting DB adding one record (For the day) based on two queries, one to DB A, one to DB B. The flat file option thus seems fine -- though I'm still no expert in this Database API stuff. Would I run a shell script in Linux, or Windows Powershell, or achieve this in Python/ Ruby? Perhaps I need to talk to the IT team, but they are sometimes behind the curve on this stuff.
Python certainly has standard libraries to handle everything. MySQL may also have ways to script the export and import of flat files (bcp and bulk import are native ways to do this in MS SQL). Windows or Linux commands could be used to kick off the steps in order and schedule the automation.
Maybe look into open source ETL solutions: Talend Open Studio, Pentaho KETL, possibly others... My guess is you need to co-locate tables from both source databases onto a single host so you can join the details from both hosts together in order to build your reports. Both the tools I mentioned can do this, though speed may be an issue if the source tables are large. In essence, you would create a workflow that would extract the data from each source database and load the data from both into another, independent, MySQL instance. With the data now co-located, it can be joined together to build the reports you need (at least in theory). I don't know much about MySQL, but in theory the target MySQL host could be made to reside on the same physical box as the larger of your two source d/bs. This might help to reduce network traffic during the data co-location. Of course, there may be restrictions imposed at your site that make this inappropriate, undesirable or impossible. 
I wonder - would setting up 2 ODBC sources, 1 Access DB with tables linked to the relevant tables in the 2 original tables suffice? Or, if the # or queries is known and (relatively) small, could the same be done in Excel?
do you get an error message, or is it that the trigger is not firing?
Nice, your suggestion led me to the solution, delimiter // CREATE TRIGGER `SalaryCheck` AFTER INSERT ON Employee FOR EACH ROW BEGIN IF NEW.salary&lt;(Select min(Salary) FROM employee WHERE employee.Dno=new.Dno) THEN INSERT INTO company.SalaryViolations (`NewEssn`, `SalaryAmount`) VALUES (new.Ssn, new.Salary); END IF; END;// Delimiter ; finally worked for me after a bit of fiddling, thanks for the help!
Is the first statement consistently 50s and the second is consistently 12s, or the first run was 50s, and the next was 12s? More than likely, the data from the query is cached after the first run, and it can display much faster after that.
&gt; Has anyone used this? How is it? Yes. You can choose DBMS - Oracle, MySQL, Postgres, SQL Server - to solve exercises here http://sql-ex.ru/exercises/index.php?act=learn
Your run-on sentence is confusing. Try rewording.
I have to agree with /u/aheinzm here but I will give you a recommendation that I always do with counts. Sub select. Instead of calling the count within the select you can give yourself more control of the count via a subselect. This way the select that counts is providing one number and not working the entire query. I have always felt counts are easier to work with this way: &gt; SELECT CustomerID &gt; &gt; ,EmployeeID &gt; &gt; ,( &gt; &gt; SELECT COUNT(ShipperID) &gt; &gt; FROM orders &gt; &gt; GROUP BY ShipperID &gt; &gt; ) AS 'desired count' &gt; &gt; FROM Orders 
that GROUP BY is gonna give you grief in what's supposed to be a **scalar subquery**
Is this homework from some kind of programming course? I would interpret it to mean some kind of graphical front end for an end user who isn't a sql programmer, either a web-based html form or some kind of desktop front end. So, for example, some kind of text that asks what you're searching for, and, say, "Year to search:", which would translate into select * from table where column = 'search term from form field 1' and year = 'year from form field 2' It sounds like your task is to write the code that would produce the form, which is outside of the scope of this subreddit.
this is an interview question that just want you to have sql knowledge. It wants you to study the data, patterns and predict the future results; nothing programming related.
This was a quick type just to give inspiration of using subselects. A simple a.key = b.key and there is not problem here.
someone completely new to sql, like OP, is not gonna know that you were just giving "inspiration" and they'll wonder why your query throws an error
OP has proven that they are capable of google first ask second. OP's not an idiot. OP would be fine. 
wat
Restore the backup to a different database, copy the 'live' registration table over to the restored database (using a different table name). Then do something like SELECT * FROM RestoredTable WHERE RegistrationID NOT IN (SELECT RegistrationID From LiveTable) This should pull registration records that are in the backup but not in the live database. Then you can manually add them to the live table in the live database. If you need more details, then we'll need to know what kind of database you are using (mySQL, sqlite, Access) and also the table structure... what's your primary key. 
&gt; we'll need to know what kind of database you are using most likely mysql, because OP is using phpmyadmin in mysql, backups consist of scripted CREATE TABLE and INSERT statements 
Damn interviewers and their questions. I don't think you can create firms in excel, can you? Access forms?
It sounds to me like a database normalization exercise. http://www.studytonight.com/dbms/database-normalization.php http://databases.about.com/od/specificproducts/a/normalization.htm http://support.microsoft.com/kb/283878 http://agiledata.org/essays/dataNormalization.html 
You can create forms but Access is more official for forms. 
Since you gave me an answer, when I use this query &gt; SELECT CustomerID, EmployeeID, COUNT(ShipperID) FROM Orders WHERE CustomerID = '10' GROUP BY CustomerID, EmployeeID; I get a table where it is |CustomerID|EmployeeID|Count(ShipperID)| |:-----------|:------------|:------------| |10|3|1 |10|4|2 |10|9|1 Since there are 4 different instances of a ShipperID where CustomerID=10, I want the 3rd column to display what i have right now, divided by 4. |CustomerID|EmployeeID|Count(ShipperID)| |:-----------|:------------|:------------| |10|3|0.25 |10|4|0.5 |10|9|0.25 And for CustomerID=4, there are 2 total ShipperID, so I want each of the 3rd column values to be divided by 2 for CustomerID=4, and so on for all the different CustomerID's.
you're using single quotes where you should be using backticks -- except you should never need to use backticks your INSERT statement has a syntax error, INSERT does not allow a WHERE clause also, you have (many (unnecessary) parentheses) may i recommend a strategy? test all your sql directly in mysql, i.e. outside of php, and then, when it's working, you can embed it in php and use $variables
Yeah good point on phpmyadmin :)
The layout leaves a lot to be desired. However, the questions are solid and the problems that you have to design queries for get REALLY tough. 
You can do this in two queries... insert into removedEvents (removalReason, field1, field2, field3) select '$removalReason', field1, field2, field3 from events where eventId = '$eventID' delete from events where eventId = '$eventID'; Also look into using PDO/mysqli with bound parameters. The mysql_* functions are dead
Great! Thanks very much :). For some reason my uni still teaches mysql_* functions in some modules and mysqli in others, so we kind of have to switch between, it's very off putting. Thanks for your reply!
I made this to help me get through that part. 1st Part: SQL 2nd Part: "regular" dataset 3rd Part: XML output, by option. It helped me. I still have a printout on my wall. [See if it works for you.](http://i.imgur.com/G9f3VPc.jpg)
Get 100% on last 3 VCEs and you should fine. Research questions you don't understand.
Good stuff, I guess im having the most trouble differentiating between PATH and AUTO, ELEMENTS.
Cool! Thank you, Ill let you know how I fair!
Really? From everything I have heard there are 5+ questions on the test purely differentiating between raw, auto, and path with and without elements. Thank you, Im a bit nervous! I hate tests!
&gt; fair Fare, for future reference. Good luck on the exam!
Thank you!
Microsoft has a much larger question pool, than one person is given; meaning two peopls are unlikely to get the same questions. That, and the fact that they sometimes change the format of the test. (Time total, number of questions and case scenarios.)
I used the same one you linked to and found it adequate. I wrote the exam a month or so ago and scored ~900. I've read other SQL books by Itzik Ben-Gan and have always been impressed. I'm working through the 762 training kit right now and am not nearly as impressed with it. Granted, I'm more of an analyst/BI query writer than DBA to begin with so the subject material isn't as familiar, but the book just doesn't seem as well-written. 
the error message points to the exact place where it barfed -- desc turns out, DESC is a reserved word... who knew? try descr instead, or use those awful backticks, \`desc\`
So it's; `desc` Not; 'desc' I'll give it a go
Thank you! I'd have never have worked that out on my own. I use SqlServer at work so had attempted [desc] but that doesnt work.
No, that's invalid syntax. best bet is to create a string and use exec on the string, but that's usually the answer to the wrong question.
You can accomplish this using dynamic SQL/prepared statements but this approach is often frowned upon for a number of reasons (can't link on mobile but you'll find plenty of advice with those search terms).
I don't follow. Why does Assessments have an Attribute_ID column (when Attributes table already holds Assessment _ID)? It would be much simpler to just post your schema as a DDL, and a few sample rows of data.
By purchasing an enterprise license. 
Partitioning is only enabled for enterprise edition. You have to do it the ghetto way with tables and views.
Isn't it easier to just send the entire RAW file to the DB and then do all processing in just a single DB call? From what I understand of your post, you are creating the query individually for each line and then executing it causing lots of separate DB calls. Feels like this can be better accomplished by sending the whole data in one DB call and put your logic inside a SQL procedure.
I'm surprised that [desc] doesn't work! But in case you weren't aware it's used for ORDER BY [asc|desc]
I may not have tried the brackets on desc actually. There was a field called date right next to it that was my original suspect for the error, I might have tried square brackets on that. I won't be forgetting the consequences of my field names again, even when using an ORM!
Is there any consistancy? Do all the edge servers have their own database on the sql server they replicate to? Is it one database? Different tables? Same tables? Never going to have the same id? unique identifier on the site?
You might want to look into the MERGE command for your upsert logic. It should be far faster than the if...update...else construct. 
What you're describing is merge replication. Google that term and you'll be on the right track. You can set the "Edge" DBs to replicate on a schedule (I've done them nightly), but you can use a continuous replication so that it updates the master DB as changes are made. It's more of a performance hit, but not a huge deal provided your servers can handle your transaction volume.
Hey guys. Update: I passed with an 887. I got the XML stuff down pretty well and feel like I got all those questions correct. The hardest was definitely the interactive questions. Thanks for all your help everyone!
The publisher/distributor/subscriber model may work for you.
All of the Edge servers have independent databases. None of the Edge servers share any info. The master server doesn't need to consolidate everything. Only a few tables and columns. All of the data that is consolidated from the edges needs to be stored in the master. All with a unique ID. So basically, not clone entirely. But cloning &amp; consolidating specific tables/columns. I hope that makes sense.
have you looked into replication? http://msdn.microsoft.com/en-us/library/ms151198(v=sql.100).aspx
I'm by no means an expert but could you not set up linked servers with all of them and use triggers to write off the updated data? edit - doing this would mean any time a row is updated/inserted into a table in one of the edge DBs you could get it to do an update/insert into the master DB.
Hmm. Looking at this, this seems like a good option. Thank you so much! I'll look into this.
I agree. Import the data in its entirety then let SQL Server do what it does best. 
What he said. No partitioning in se or xe. 
A file with 4060 records takes about 3-4 minutes to enter in completely. That should take 0.03-0.3 seconds to insert completely.
Well the problem is the raw data itself. A typical line in the raw data file looks like this: &gt; 10053AFF0A589432;32nd Ave @ Washington -- 10053AFF0A589432 -- 172.22.105.4 -- Yes -- WB/WBLT Phases 3&amp;8 -- 10.1.1 -- 1 -- Terra Access Point NEMA -- 70 -- Yes -- 0 -- ENCORE -- 172.22.105.4 -- 172.22.1.254 -- 255.255.0.0;129;WB_RT;11/18/2013;4:30:00 PM;100;15 Minutes;76;19;A;1.914; I'm not sure if the raw data format affects anything. I've never heard of SQL being able to parse things... but I'm still a SQL beginner. I've only taken one class at my university for it.
Haha... yeah, I know. That's why I'm posting this to hopefully find a solution
I just responded two minutes ago to a similar question, I'll just copy what I just made over for you. &gt; *Well the problem is the raw data itself. A typical line in the raw data file looks like this:* &gt; &gt; &gt; 10053AFF0A589432;32nd Ave @ Washington -- 10053AFF0A589432 -- 172.22.105.4 -- Yes -- WB/WBLT Phases 3&amp;8 -- 10.1.1 -- 1 -- Terra Access Point NEMA -- 70 -- Yes -- 0 -- ENCORE -- 172.22.105.4 -- 172.22.1.254 -- 255.255.0.0;129;WB_RT;11/18/2013;4:30:00 PM;100;15 Minutes;76;19;A;1.914; &gt; &gt; *I'm not sure if the raw data format affects anything. I've never heard of SQL being able to parse things... but I'm still a SQL beginner. I've only taken one class at my university for it.* Again, thanks for your help!
Firstly: Phrasing! Boom!... Ahem. Secondly, 4060 rows in 3 minutes? Please look into bringing a dba into this. Upgrading your server/figuring why exactly this runs so slow is kind of key in here. Thirdly, is there a specific reason for updating a record at all? Admittedly, I'm not familiar with the specifics, but why not have a ref table for collection type and have a very narrow transaction table with location id, date id, time id, collection type id and the collection value (something like number(12,3) will suffice most likely). Pivoting this structure to the original shouldn't be a big deal.
Have a fun one for you: select concat( a1.assessment_id, ': ', a2.attribute_id) from assessment a1 cross join attribute_category a2 left join attributes a3 on a3.assessment_id = a1. assessment_id and a3.attribute_id = a2.attribute_id where a3.id is null 
Have you run the queries manually and compared execution plans? If the same when both run manually, have you tried attaching SQL Profiler and seeing the execution plan generated by the live query and how it's different from when you run it manually? If still no joy, and if the other DB's that are working fine are identical to this one, I'd check free disk space for DB + tempDB (log file location in particular), number / maintenance plan of indexes involved in the new table, etc... but those seem unlikely with just one row inserted. So now I'd be kinda shooting in the dark... perhaps others can help more.
Try select into a temp table and see if the same symptoms persist?
For your first point - I'm not sure what you meant by the first point to be honest, but if I phrased something poorly, I'm really sorry. Second point - I'll look into doing that, though this is a school-related project and I don't know exactly where to find someone that excels in database management. I'll do that as well, if I can't find a solution here. Third point - The reason why we're updating the records is because each raw file will be only for a specific direction (EB, NB, WB, SB). For each file, we'll need to update a record for a specific time and date if there's it's already been inserted.
You can configure the import to use '--' as a delimiter, then your staging table will end up with a bunch of separate fields. You can then use sql string functions (TRIM, SUBSTRING etc) to massage each field and insert it into your table Or worst case just have a varchar field in your staging table which holds each line in its entirety. Its a bit more work to rip out each field then though. 
Yeah, honestly I have no idea haha. I'm new to SQL and database languages in general. I'm using Access to test the concept for now, if that helps. So I assume that's MSSQL?
attach a sanitized raw file, somebody will get bored and write your solution for you.
don't use triggers
I would think an SSIS package should be able to do this much faster than 1000 rows/min.
I might be misunderstanding, but maybe something like this: SELECT SUM(values)/SUM(CASE WHEN filter = 'Criteria' THEN 1 ELSE 0 END) FROM (SELECT * FROM table1 UNION ALL SELECT * FROM table2) x --ect Aggregation around a case statement is pretty common and useful.
Well SUM and COUNT are both valid SQL functions. SUM/COUNT is just AVG, of course. To do the count only where the row meets criteria, SUM a case statement that results in 1 or 0. select avg(COLUMN_NAME), sum(COLUMN_NAME) / sum(case when CRITERIA then 1 else 0 end) from TABLE_NAME group by COLUMNS --if needed. ; 
Technically, it's a predicate. See the ["Language Elements"](http://en.wikipedia.org/wiki/SQL#Language_elements) section on Wikipedia's SQL entry. I normally refer to it as a "conditional," as most people I work with don't have formal training in database and haven't heard the word "predicate" since fourth grade English class.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 3. [**Language elements**](http://en.wikipedia.org/wiki/SQL#Language_elements) of article [**SQL**](http://en.wikipedia.org/wiki/SQL): [](#sfw) --- &gt; &gt;The SQL language is subdivided into several language elements, including: &gt; &gt;* *Clauses*, which are constituent components of statements and queries. (In some cases, these are optional.) &gt;* *Expressions*, which can produce either [scalar](http://en.wikipedia.org/wiki/Scalar_(computing\)) values, or [tables](http://en.wikipedia.org/wiki/Table_(database\)) consisting of [columns](http://en.wikipedia.org/wiki/Column_(database\)) and [rows](http://en.wikipedia.org/wiki/Row_(database\)) of data. &gt;* *Predicates*, which specify conditions that can be evaluated to SQL [three-valued logic (3VL)](http://en.wikipedia.org/wiki/Ternary_logic) (true/false/unknown) or [Boolean](http://en.wikipedia.org/wiki/Boolean_logic) [truth values](http://en.wikipedia.org/wiki/Truth_value) and which are used to limit the effects of statements and queries, or to change program flow. &gt;* *Queries*, which retrieve the data based on specific criteria. This is an important element of *SQL*. &gt;* *Statements*, which may have a persistent effect on schemata and data, or which may control [transactions](http://en.wikipedia.org/wiki/Database_transaction), program flow, connections, sessions, or diagnostics. &gt; &gt;* SQL statements also include the [semicolon](http://en.wikipedia.org/wiki/Semicolon) (";") statement terminator. Though not required on every platform, it is defined as a standard part of the SQL grammar. &gt;* *[Insignificant whitespace](http://en.wikipedia.org/wiki/Whitespace_(computer_science\))* is generally ignored in SQL statements and queries, making it easier to format SQL code for readability. &gt; &gt; --- ^Interesting: [^Microsoft ^SQL ^Server](http://en.wikipedia.org/wiki/Microsoft_SQL_Server) ^| [^SQL:2003](http://en.wikipedia.org/wiki/SQL:2003) ^| [^Language ^Integrated ^Query](http://en.wikipedia.org/wiki/Language_Integrated_Query) ^| [^IBM ^SQL/DS](http://en.wikipedia.org/wiki/IBM_SQL/DS) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cghdsbj) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cghdsbj)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Cool, thank-you
I second the suggestions on installing either SQL Server Express Edition, or one of the full trials. However, be wary of interpreting T-SQL specifics as being applicable to all database platforms. That's fine to do if you'll only ever be working with SQL Server, but that's not how it works out for most database people.
RAW has `&lt;row `. PATH has `&lt;row&gt;`. AUTO has `&lt;alias for table 1 `. AUTO, ELEMENTS has `&lt;alias for table 1&gt;`
With full SQL you can create more advanced queries than you can in access. Using the query wizard you don't control how the joins are made. Also in full SQL you can easily daisy-chain your queries to perform complex tasks in a single hit (access can do this in a similar, but less transparent way). Access also writes horribly long winded SQL, editing it directly isn't nice. There's probably lots more reasons. Access gets a lot of hate but it's great at demonstrating the principles of data management and how I got started. It should give you a really good platform to pick up proper SQL.
While your best option is probably installing SQL Express, or an eval of the full server like everyone is suggesting, here's a site that might help you out a bit in the time being. [SQLFiddle](http://www.sqlfiddle.com/#!6)
2nd person saying this now and I'm far from disagreeing but why? At my work I'm developing an application that is pulling data from one database and replicating it to another through a series of triggers (and transforming it). My understanding is that this is essentially what's being asked. They have a series of databases that are active and they want that data to update dynamically into a "master" db.
I know everyone keeps on saying to download something but it sounds like you may not be 100% comfortable with that yet (maybe as a next step) and just want to play around with the SQL language itself without all the other stuff. Give this a try and see if this is what you were looking for. I know there are some others similar out there for javascript and .net but was able to find this for you. http://sqlfiddle.com/
What happens if network connectivity is lost and you can't access the linked server? Any changes you make wouldn't be duplicated. SQL server has a system in place (merge replication) to do this exact process, except with a lot of oversight and the ability to recover from loss of network connectivity, etc. No sense reinventing the wheel. Triggers have a time and a place, but I wouldn't suggest using them when you're using multiple servers; better to set up an SSIS package to transform and load data periodically, or transform internal to the server and merge.
ah thnx. my understanding was merge replication wasn't dynamic and happen only on what was inserted or updated into a database and would represent a distinct copy every time. That's not the case?
Well, merge replication only happens on rows that are inserted/updated/deleted, but it's not a distinct copy. I'll give you a quick overview of what I use it for. We have a homegrown POS system for two retail stores. Each has a local server and we have a server in the cloud. The cloud server is our publisher and the two store servers are our subscribers. Let's say that an invoice is created at Store #1. That info is then replicated across every server - meaning, a new row is created on the other two databases. The customer can then return stuff at Store #2 - in that case, the original invoice row is altered to reflect the return (no new rows are created). That alteration is then replicated across all servers so that the return can only be done once. Replication currently occurs every hour (although you can make it continuous so that it's fully dynamic).
aaah. interesting. question then, is merge replication available out of the box or is it related to a specific software tier for SQL Server? ie. if we have SQL Server Standard 2008 R2 would it be included in that?
What does "p" mean?
mhm, don't use Access. It's the worst. You are much better off using MSSQL Express edition, it's a limited but free Edition of MSSQL. As memory serves filtered indexes are only available in the Standard Edition and above, but in any case, you are going to be much better off testing and playing arround on a real DBMS which Access is not. 
you can use http://data.stackexchange.com/stackoverflow/query/new there is a point in what the others have said in downloading SQL Server Express thou. 
Exactly! I have no idea. When I run .schema in the ex2.sql file (at the bottom), everything is neat and tidy; but when I try to .read it in the .db file I get a bunch of weird characters. I'm lost.
If you want a one-many relationship you should add article_id to the tag table. Then your FK would be: ALTER TABLE tag ADD CONSTRAINT FOREIGN KEY (article_id) REFERENCES article (article_id); If you wanted to have a many to many relationship you would need a third table "tag_article" (or "article_tag"). This table would be: CREATE TABLE tag_article ( tag_id int(30) NOT NULL, article_id int(30) NOT NULL ); ALTER TABLE tag_article ADD CONSTRAINT FOREIGN KEY (article_id) REFERENCES article (article_id); ALTER TABLE tag_article ADD CONSTRAINT FOREIGN KEY (tag_id) REFERENCES tag (tag_id); Forgive any syntax errors. I currently don't work with MySQL.
What does constraint change? Edit: is the thing I made a one to one?
This looks like some character encoding issues. Try first pasting into notepad then copying that out to be pasted in your command prompt.
The .db file is encoded and not meant to be read byte-for-byte. From what you showed above it looks like that's what you're putting into the db.
CONSTRAINT is required syntax for MS SQL and I believe is ANSI standard. Yes, what you defined is one to one. Both sides of the foreign key are primary keys in their own table.
Thanks for your assistance /u/jc4hokies. Based upon your guidance with the CASE statement I was able to figure out my solution. 
Thanks for your assistance /u/MatthewGilbert. Based upon your guidance with the CASE statement I was able to figure out my solution. 
I got it out based upon the already provided suggestions. Thanks anyway :) 
Thanks, that's helpful. When you say that the query wizard doesn't give you control of how the joins are made, what exactly do you mean? the Access query wizard allows you to perform inner and outer joins, so I guess i'm wondering what other sort of joins full sql would offer that can't be performed in access. This may be a silly question, so again, apologies for my novicehood. Thanks. 
my go to intro to sql is sqlzoo.net
Sign up to sqlservercentral.com and research the questions of the day/read the forum posts from the email. Actually has realworld problems being sorted. Much more useful than textbook examples.
&gt; It sort of seems that SQL should only be utilized by developers that are using SQL in the development of a much more robust program or application using other languages as well. Is this the case? Would it be somewhat useless for an analyst in my role to spend time learning SQL if I do not plan to employ it in other, more advanced programs? Do you always plan to do exactly what you're doing today? MS Access isn't going to teach you anything to grow beyond MS Access. Your skillset is going to plateau simply because there are so things that MS Access cannot do (or cannot do well). Being able to use Access to connect to MS SQL Server and run DML queries isn't going to get you hired anywhere. That's not a marketable skill. Knowing how to use SQL Server Management Studio (SSMS) to write a query, modify a schema, or manage and maintain a database and SQL Agent is. MS Access is great to provide an interface to make learning a bit easier and provide an easy to use interface you can learn with, but it's limited to what is useful in Access. Access is a very limited because it's intended for simple databases. Tasks like writing and executing stored procedures, defining views, and creating indexes are difficult to impossible. Also, 500,000 records doesn't strike me as large by today's standards for a database, especially a multi-year data warehouse. I'd say it's small-medium. 
Teradata's Sql Assistant is a terrible terrible front end. I've been using it daily for about 4 years now and I dream for the days when i can get back to MS SQL Server's version, Query Analyser. (&lt;rant&gt; Little things like code completion and intellisense are missing, but there are MAJOR flaws in it that will allow you to lose any unsaved work if you press the wrong keys by accident. Ex. F7 repeats the last command from history. If you have a full window of code open and you press f7, it will overwrite that code. It is mind bogglingly stupid. &lt;/rant&gt; ) But Teradata the engine,is pretty impressive. I routinely work on tables that approach a half a billion recs and I get pretty good performance. *Yes learning sql is a good idea. Do it* It's too bad that your first development environment will be Sql Assistant. If there is some other client tool, yes, use it. like if Ms Access lets you write SQL and pass it to TD, then as a novice you'd probably end up being more productive in there. 
Wow, I tried it and realized how much I love SQL prompt. Nothing beats SSMS and RG tools. I'm spoiled.
This.
This is actually nice, especially for those who need to test a basic PoC if they don't have access to the real data.
Try this: Make a text file located in C:\DataFiles\ called import.fmt and paste this in: 9.0 25 1 SQLCHAR 0 99 " -- " 1 Field01 "" 2 SQLCHAR 0 99 " -- " 2 Field02 "" 3 SQLCHAR 0 99 " -- " 3 Field03 "" 4 SQLCHAR 0 99 " -- " 4 Field04 "" 5 SQLCHAR 0 99 " -- " 5 Field05 "" 6 SQLCHAR 0 99 " -- " 6 Field06 "" 7 SQLCHAR 0 99 " -- " 7 Field07 "" 8 SQLCHAR 0 99 " -- " 8 Field08 "" 9 SQLCHAR 0 99 " -- " 9 Field09 "" 10 SQLCHAR 0 99 " -- " 10 Field10 "" 11 SQLCHAR 0 99 " -- " 11 Field11 "" 12 SQLCHAR 0 99 " -- " 12 Field12 "" 13 SQLCHAR 0 99 " -- " 13 Field13 "" 14 SQLCHAR 0 99 " -- " 14 Field14 "" 15 SQLCHAR 0 99 ";" 15 Field15 "" 16 SQLCHAR 0 99 ";" 16 Field16 "" 17 SQLCHAR 0 99 ";" 17 Field17 "" 18 SQLCHAR 0 99 ";" 18 Field18 "" 19 SQLCHAR 0 99 ";" 19 Field19 "" 20 SQLCHAR 0 99 ";" 20 Field20 "" 21 SQLCHAR 0 99 ";" 21 Field21 "" 22 SQLCHAR 0 99 ";" 22 Field22 "" 23 SQLCHAR 0 99 ";" 23 Field23 "" 24 SQLCHAR 0 99 ";" 24 Field24 "" 25 SQLCHAR 0 99 ";\r\n" 25 Field25 "" Put a copy of one of your data files in C:\DataFiles\ and call it import.txt Now, run this query in SSMS: select * from openrowset(BULK 'C:\DataFiles\Import.txt', FORMATFILE = 'C:\DataFiles\Import.fmt', FIRSTROW = 1) AS txt; If it doesn't work because you need to enable Ad Hoc Distributed Queries, have an admin turn it on: sp_configure 'show advanced options', 1; RECONFIGURE; sp_configure 'Ad Hoc Distributed Queries', 1; RECONFIGURE; Also, you can change "Field01", etc in the .fmt file to the actual field names you want to return from the select statement. In line 25, your file might end in ";\n" instead of ";\r\n".
To avoid a Cartesian product, write the query just like you did. You don't have a Cartesian product. A Cartesian product from just the first two tables would have about 344,000 rows in it. Post minimal CREATE TABLE and INSERT statements for a more definitive answer.
You don't have a cartesian product. Those occur, usually by accident, when you get a join wrong. like from Table3 join Table1 on Table3.ID = Table3.ID Note that the join condition will always be true, so you'd get all rows from table3 and all rows from table1 yielding 862x399 rows. You don't have that problem. Your joins look fine. It's the underlying data in the tables that is not unique enough which is why you are getting dups. YOu could do giant select asterisk, Count(asterisk) from your result set, group by &lt;all fields&gt; having count(asterisk) &gt;1 and that will list you out the records that are duplicated. That would help you debug it. edit: using a "*" in the syntax above caused some formatting to happen, so i'll change it to "asterisk" 
I would need to see your data to find the duplicates... It could be on any of your joins... Do a select distinct count(*) column_used_to_join against your tables you list... If the count is lower than what you posted, that's your duplicate issue. You would need to select singletons from that table before joining.
You can use Access pass-through queries and start teaching yourself every flavor of SQL (T-SQL, Oracle, MySQL, depending on the server you are connecting to) but stay in the familiar Access environment. Pass through queries actually perform the query server side and return only the results to Access so you get the performance increase you are after. You can also utilize stored procedures if that is something you desire. The main downside of pass through queries is you need to rely on temp tables when using them for update queries. If you decide to go down that road I would suggest getting a text editor (free) like Notepad++ for writing your SQL because Access sucks balls at editing of SQL. http://www.mssqltips.com/sqlservertip/1482/microsoft-access-pass-through-queries-to-sql-server/
The w3c schools site is pretty decent for tutorials in SQL. 
Well, you aren't left joining. That can throw things out when they have patterns like yours. You aren't using table4 in any of your joins. I'm guessing that's a mistake since you query it. I don't know what you are trying to achieve. It seems everything is sourcing from Table3 for their joins except for Table2. Without really knowing the relationships it's hard to say if that is wrong or not.
To explain cart. product, imagine two tables, A and B. The product would bring in every row from table A for every row from table B. So if A has 5 rows and B has 5 rows instead of getting 5 rows joined together, you get 25 back. All 5 matched from B to every row from A. To experience it yourself, join a small table to itself "on 1 = 1" and watch the magic happen. select a.*, b.* from MyTable a join MyTable b on 1 = 1 As was stated already, yours is likely a data issue, not a cartesian product. You should inspect the data closer.
~~I am assuming your multivalue delimiter is a comma, you would change the commas in the below predicate to match whatever delimiter you are using. The reason for the delimiters is to ensure "1" does not partially match "10"~~ where instr( ',' || :my_multi_param||',', ',' || some_col_value ||',') &gt; 0 **Edit** No, there is no way to get max from a string unless you roll your own function 
I don't work a lot in oracle, so forgive any syntax errors. I believe you could do something like this: WITH cte (value, remainder) AS ( SELECT SUBSTR(:my_multi_param,1,INSTR(:my_multi_param,',',1,1)+1) AS value, SUBSTR(:my_multi_param,INSTR(:my_multi_param,',',1,1),LEN(:my_multi_param)-INSTR(:my_multi_param,',',1,1)-1) AS remainder UNION ALL SELECT SUBSTR(remainder,1,INSTR(remainder,',',1,1)+1) AS value, SUBSTR(remainder,INSTR(remainder,',',1,1)+1,LEN(remainder)-INSTR(remainder,',',1,1)-1) AS remainder FROM cte WHERE INSTR(remainder,',',1,1) &gt; 0) SEARCH DEPTH FIRST BY value SET order1 SELECT MAX(value) FROM cte; edit: instr syntax
I normally just have the SQL return all of the values and the filter the results table in SSRS with the multi-value parameter. Depends on the use case, but I've found that to be the easiest way to handle it.
Tested. Single and double apostrohpees make MySQL interpret the desc as a string. The third kind (are they called back ticks?) let you use reserved names as column names. Square brackets are not valid. mysql&gt; SELECT 'desc' FROM my_table; +------+ | desc | +------+ | desc | +------+ 1 row in set (0.00 sec) mysql&gt; SELECT "desc" FROM my_table; +------+ | desc | +------+ | desc | +------+ 1 row in set (0.00 sec) mysql&gt; SELECT `desc` FROM my_table; +-----------+ | desc | +-----------+ | some data | +-----------+ 1 row in set (0.00 sec) mysql&gt; SELECT [desc] FROM my_table; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '[desc] FROM my_table' at line 1 mysql&gt; EDIT: More complete example.
This is also a good resource: http://sqlwithmanoj.wordpress.com/tag/70-461/ I actually just took the 70-461 exam, but didn't pass with a score of 500. I went in not as prepared as I should have been on purpose since I used the free second shot voucher. I wanted to see how the test was and what topics it would cover. The blog I linked you covers most, if not all the topics I encountered.
VCEs? The ones that come with the training kit?
Congrats on your pass! I actually just recently took it.. and failed miserably. I ended up taking way too long on some problems which wasted my time. I also went in without sufficient preparation because I had a second shot voucher, and wanted to see how in-depth the test would be. May I ask what resources you used to study for it? Thanks.
why filtered indexes should help him building a dynamic sql statement?
do you mean something like this? http://sqlfiddle.com/#!6/0f83f/1 remember that dynamic sql could lead to security issues...
Thanks for the help everyone. I just went with a case...when statement that worked backwards through the range of columns in question, copying the value if the cell wasn't null. Practical programming is way different than in my comp sci classes hahah. I spent so much time trying for an elegant solution, when the brute force way is by far the most general solution there is.
Sure, the place where I work (I'm a database developer) was really cool and got me the book, they also got practice test software for anyone interested in the test called "transcender". So other than those two, I also searched through a lot of test dumps on google. I would say the test dumps helped me the most. You can find some with very very similar questions and concepts.
I feel like it it *should* work, but it doesn't. I get an error that I'm missing a closing parens, even though I threw an extra set around it. 
You just want to create a text file with the contents of the stored procedures? How about SET NOCOUNT ON; DECLARE @sp SYSNAME , @spText NVARCHAR(MAX) SET @spText = '' DECLARE spCrs CURSOR LOCAL FOR SELECT name FROM sys.procedures -- Optionally add WHERE name IN ('[...]','[...]') if you only want certain SPs OPEN spCrs FETCH NEXT FROM spCrs INTO @sp WHILE @@FETCH_STATUS = 0 BEGIN DECLARE @tblSpText TABLE (spText NVARCHAR(MAX)) INSERT INTO @tblSpText EXEC sp_helptext @sp SET @spText = @spText + (SELECT spText FROM @tblSpText FOR XML PATH(''), TYPE).value('.','VARCHAR(MAX)') FETCH NEXT FROM spCrs INTO @sp END CLOSE spCrs DEALLOCATE spCrs PRINT @spText Run that, saving the results to a file. Then you can just go back and edit out the lines like CREATE PROCEDURE, etc. (or if you're really ambitious and the SP's are fairly consistently formatted, you might be able to eliminate them in the WHERE clause of the SELECT FOR XML)
I am in the middle of a similar type upgrade and yes, detach and reattaching worked for me. Also what Fozzie33 said should work.
I'm going to give this a shot first. I've already backed up the DB. Thanks for the input!
Fozzie33, thanks very much. That makes sense.
What you need is to emulate the oracle listagg function in MS SQL. (I work in an oracle shop and most days I yearn for *any* other flavour of SQL, but listagg rocks!) I found a stack overflow article with a description of one way that seems to work although it's not very pretty. http://stackoverflow.com/questions/15477743/listagg-in-sqlserver 
Pinal Dave a little cleaner solution: http://blog.sqlauthority.com/2009/11/25/sql-server-comma-separated-values-csv-from-table-column/
Nice! 
Pinal Dave is the man i've used that blog countless times. I'd give him gold if I could. I saw that SO article earlier and decided to come back to it as a last resort. Here is my final query it worked like a charm so thank you both! SELECT SUBSTRING( (SELECT ',' + Cases.Case_Code FROM Cases WHERE ContactID = 15 FOR XML PATH('')),2,200000) AS [Past Cases] 
Here's another question! Is it possible to use Pinal's formula to created a computed column? I have a column titled "Past_Cases" that if possible I would like to default to the results of that query. 
It's pretty straight forward, I would ask the vendor if they have a migration doc too because sometimes applications have specific things that need to be setup again. Logins, etc.
Just query the system tables ... SELECT p.name, m.definition FROM sys.procedures p INNER JOIN sys.sql_moduls m on p.object_id = m.object_id WHERE p.name like '%something%'
Wow, thanks. I got up a database and have it working. Now the question is how do I access that from my app? More specifically with Java? My client ID is 1656264, the hosting domain is www.orbistraveltroop.com, the database name is 1656264_orbis, and the password is, well, I'm not going to give you that, but otherwise.... How would I get my Java program to access this?
Well, for Java to access MySql you probably need to use JDBC or similar, and put in your database details on the connection string. Here is a Java/JDBC tutorial with examples: http://www.vogella.com/tutorials/MySQLJava/article.html The phphosting4free site should give you the host name. When you create your mySql database you set up a user name and password. For example here is my connection details (minus the password) that I feed into a PHP PDO connection string. $v_db_host="pdb6.runhosting.com"; $v_db_user="847711_e20121231"; $v_db_pw = "XXXX"; $v_db_name="847711_e20121231"; Hope this helps. 
There are CLR solutions to this. I have a ListAgg function on my MS SQL Servers.
This is a code snippet I saved a long time ago to do this. I don't recall where I found it. Select Results = Stuff( (Select ', ' + b.CSPI_ID From Benefit b ( nolock ) FOR XML PATH('')),1,2,'')
Oh, silly me- I overthought it a bit... Just run this with results to file: SET NOCOUNT ON; DECLARE @sp SYSNAME DECLARE spCrs CURSOR LOCAL FOR SELECT name FROM sys.procedures DECLARE @tblSpText TABLE (spText TEXT) OPEN spCrs FETCH NEXT FROM spCrs INTO @sp WHILE @@FETCH_STATUS = 0 BEGIN INSERT INTO @tblSpText EXEC sp_helptext @sp FETCH NEXT FROM spCrs INTO @sp END CLOSE spCrs DEALLOCATE spCrs SELECT * FROM @tblSpText
You can use an ORDER BY on your cursor and manually specify the order by the SP's name, I guess... something like: DECLARE spCrs CURSOR LOCAL FOR SELECT name FROM sys.procedures ORDER BY CASE name WHEN '(first sp)' THEN 0 WHEN '(second sp)' THEN 1 WHEN '(third sp)' THEN 2 (...etc...) END Is that what you're asking?
I use with all the time, and I tend to string them together: with one as (select id, this_col, that_col from table), two as (select id, other_cols from another table), three as (select this_col, that_col, another_col from one, two where one.id=two.id) select * from three etc. Typically my with statements create temp tables that can't be created with just subqueries and joins alone. So, usually when I'm pulling a lot of different data from different tables and then doing some kind of manipulation, either lots of case-when, or mathematical calculations, and I kind of "filter down" to a final data set, which I then do a final query from.
Yes, this was very helpful. Thank you so much, or *grazie mille* as the Italians say. I'll let you know if I have more issues, but I should be good for now. 
I also use it for testing and sharing SQL without objects. with test_data(name, age) as( select 'name'||level, 20 + mod(level,26) from dual connect by level &lt;=100 ) select name, age from test_data
Ok, turns out I have two more problems at this point, but I think one of them is just a problem with the installation on my own computer. Here's an example of Java code for connecting to a local database; any idea what the URL would be in my case? I know the driver name doesn't change. The Host Server is fdb7.runninghost.com. String url = "jdbc:mysql://localhost:3306/"; String dbName = "1656264_orbis"; String driver = "com.mysql.jdbc.Driver"; String userName = "1656264_orbis"; String password = "XXXXXXXXXX"; try { Class.forName(driver).newInstance(); Connection conn = DriverManager.getConnection(url+dbName,userName,password); conn.close(); } catch (Exception e) { e.printStackTrace(); }
&gt; String url = "jdbc:mysql://localhost:3306/"; String url = "jdbc:mysql://fdb7.runninghost.com/"; 
The example I gave you uses: connect = DriverManager .getConnection("jdbc:mysql://localhost/feedback?" + "user=sqluser&amp;password=sqluserpw"); so for you that would be: connect = DriverManager .getConnection("jdbc:mysql://fdb7.runninghost.com/" + dbName + "?user=" + userName + "&amp;password=" + password);
For me its not Oracle but postgres. It has the same function. I use them to build smaller datasets from huge tables. I can build the CTE(common table expression) using an index scan when that may not be nearly as easy in the main query. I can then query and manipulate a much smaller dataset. CTE is the term used to describe the "temp table" that results from the with clause. 
This is the first SQL Saturday in Nevada, ever, which has a GREAT line up of speakers. Expect to get there early in the morning for registration and use that time to get the lay of the venue. There will likely be some type of breakfast (bagels and coffee, maybe donuts) and then there might be some type of opening remarks to give you a general feel for the day. There are three tracks, 2 DBA and 1 BI, so figure out where those rooms are early so as not to get lost in between, even if you do, there are tons of folks to help you find where you are going. If you haven't already I would download GuideBook app for whichever smartphone you have, search for SQLSaturday Las Vegas and it should pull the schedule. You can review the sessions now to figure out where you want to go, the app will give you reminders before they start. The speaker line up is pretty good, but with only 3 tracks you are kind of limited as to what you get to listen to. As far as speaker: * Tom LaRock is the president of PASS, which is the international SQL association, and also a great speaker. * I saw Steve Jones from SQLServerCentral.com last week in Madison, he's a great speaker as well. * If you have any SQL Virtualized, David Klee is THE source for that subject, saw him last week in Madison as well. * Joey D'Antoni is big on Architecture and clusters and just big stuff, great presenter. * Karen Lopez is a data modeler extraordinaire, if thats in your wheel house, talk to her. * and Ed Watson is a die hard Gators fan who just left our user group here in Tampa to move to Atlanta, feel free to tell him Gators suck Be sure to ask any questions you have, stop the speakers afterwards if you have followup questions. They love to share and teach. One thing you need to do is ensure you see all the Sponsors, as most of them will be raffling prizes, and every body likes free stuff. 
Oh my bad. I meant between those four. I've done some research but I can't seem to find any decent guides.
Cursor - Kinnda like a SQL For Loop. You make a data set to be looped through, and use fetch status to close the loop when all of the records have been looped through. Procedure - a sql statement stored on the server that can be accessed by the command exec. Often contain parameters, but not always. Function - Kinnda like a procedure, but it can't make any permanent changes to the DB. The advantage is that it can be used inline, so it's useful for doing repetitive mathematical functions, for example convert time from one time zone to another, or hashing a field. Trigger - Like a procedure, but instead of being called manually it's called when a certain conditions are met. The most common kind of trigger is a table trigger, when a table is accessed in a way that matches the triggers conditions, it executes the script. This could be to move a row about to be deleted to an archive table, or change the case of inserted data. It's worth noting that functions and triggers can kick you right in the performance, and cursors are almost always the worst way you can do something because they are slow and resource intensive.
Thanks for the answer! The part about the performance is what I read as well in several guides. Clueless why my teacher would make us implement features that will cut performance but oh well! I will try to find a way to use those in my assignment now.
Try this: SELECT WO.SiteID, WO.WONUM, WO.ASSETNUM, WO.LOCATION, WO.REPORTDATE, WO.DESCRIPTION AS FAILURE_DESCRIPTION, WO.WORKTYPE, WO.BTRELEVANCY AS RELEVANCY, DBMS_LOB.SUBSTR(LD.LDTEXT) AS TECHNICAIN_NOTES, MT.ITEMNUM, MT.DESCRIPTION, SUM(MT.QUANTITY*(-1)) AS QTY_USED, IT.PLUSTCOMP, WO.STATUS, PTC.DESCRIPTION AS SUBSYSTEM FROM db.company_WORKORDER WO LEFT OUTER JOIN db.company_MATUSETRANS MT ON WO.WONUM = MT.REFWO LEFT OUTER JOIN db.company_FAILUREREMARK FR ON WO.WONUM = FR.WONUM LEFT OUTER JOIN db.company_LONGDESCRIPTION LD ON FR.FAILUREREMARKID = LD.LDKEY LEFT OUTER JOIN db.company_ITEM IT ON MT.ITEMNUM = IT.ITEMNUM LEFT OUTER JOIN db.company_PLUSTCOMP PTC ON IT.PLUSTCOMP = PTC.COMPONENT WHERE WO.SITEID='SiteName' AND WO.REPORTDATE &gt; '01-NOV-11' AND WO.DESCRIPTION LIKE '%%' AND WO.WORKTYPE !='PM' AND WO.WORKTYPE!='CMPWO' AND WO.ISTASK=0 AND LD.LDOWNERTABLE='FAILUREREMARK' GROUP BY WO.SiteID, WO.WONUM, WO.ASSETNUM, WO.LOCATION, WO.REPORTDATE, WO.DESCRIPTION, WO.WORKTYPE, WO.BTRELEVANCY, DBMS_LOB.SUBSTR(LD.LDTEXT), MT.ITEMNUM, MT.DESCRIPTION, IT.PLUSTCOMP, WO.STATUS, PTC.DESCRIPTION ORDER BY WO.REPORTDATE, WO.wonum;
Why not use management studio's scripting functionality to dump them into a single or separate files? Right Click Database&gt; Tasks &gt; Generate Scripts... Select only the procedures you want scripted. Adjust the options accordingly for script formatting. Using TSQL to do this instead of the wizard seems less ideal unless you want it automated or something.
&gt;A Query that works That did it. I didn't realize that I needed to do the Group By statement to include all items - I was trying to use it just to create the WONUM group only (the query I posted was the one that got me any results). 
Sorry, I'm not sure I understand your reply. Are you saying it worked or are you saying there is something still wrong with it?
Yours worked, mine was failing when I tried to GROUP BY because I only did a GROUP BY WO.WONUM. 
look at the execution plan with and without the index. it will let you know for sure if its being used or not. 
Ty for the response!
This might be a longshot, but try removing the index, then manually creating statistics on the indexed column. It could be possible that even if the index is not suitable, the query optimizer uses the statistics from the index to avoid a bad execution plan.
\#1 = Redgate
Yup. As far as I'm concerned there is no 2 through 5. Redgate's stuff is fantastic. We use multiscript, sql prompt, sql compare and some others of theirs, too at my company. I'm also a fan of AdeptSQL for database compares: http://www.adeptsql.com/
In a school project you will not notice the performance hits by triggers and cursors. In fact we use triggers, functions and cursors with our work system, they are unavoidable. It is just something to consider when you are building something, but I have yet to work in a system that does not use them. Learning them is very important. 
Their [SQL Source Control](http://www.red-gate.com/products/sql-development/sql-source-control/) and [Deployment Manager](http://www.red-gate.com/delivery/deployment-manager/) are game changers for me. They've completely changed how we develop and deploy databases and .NET apps.
Just don't give them your telephone number. They have a pretty proactive telesales team. And by proactive I mean a nuisance.
Redgate, Idera, and the guy who makes SSMS Tools Pack.
I don't know of any software. Maybe Redgate has something, but its going to be tricky anyway Without constraints, its possible there is broken referential integrity (ie orphaned records in child table, which developer never intended to happen), meaning you cannot create constraints even if you identify them. I think I'd start by setting up SQL Profiler and capturing a trace of typical workload. Save it to a file and then write a little script that parses out any join clauses, particularly inner join, which in many cases will identify the relationship. 
Confio ignite has made my life much easier. Execution plans are killing me.
I used to use Visio for this but their latest version sucks ass. I've used dezign http://www.datanamic.com/dezign/ for forward and reverse engineering. I am not affiliated with the company, just took advantage of their free trial when I needed it.
Redgate SQL Prompt is best in class for what it does. I do more development than actual admin work and it speeds up query writing immensely. SSMS tools is another staple for me. The only reason I use it is for the automatic query saving to the hd which has saved me hundreds of hours. Adept SQL Diff is what I use to compare/deploy SQL objects. I've tried the Redgate one and the built-in VS one, but the simplicity and ease of use of SQL Diff makes it an easy choice. 
Love the Solarwinds~~Conifo~~ stuff
I prefer SQL Compare from Redgate, AdeptSQL is good too
You would call the function it the SELECT right after OrderID and pass in the columns that have the appropriate data as the values for the function. SELECT OrderID ,dbo.OrderCost(col1, col2, col3) FROM ...
Commenting to save this for work Monday. Looks promising.
It actually works very well. You can generate ERD's for review and when done, forward engineer it to the SQL script of your choice. Once I do more custom development at my work, I'll be getting a license for it. I'm currently stuck in O365 AD Sync hell. :(
Yeah. I don't get that. Everybody I know in tech follows the "hard sell = no sale" rule. I mean, if you call me once and I say "no, thanks" then don't contact me again. If you do, the answer changes from "no" to "hell no". I suppose their sales team is run by MBAs who are hoping to contact other MBAs who took the same sorts of classes that teach that hard sales is effective and respectful? 
Easy enough? SELECT A *2 AS colname FROM tblname
Oracle Data Modeller can reverse engineer and do this. It's free and although written by Oracle, does work on other RDBMS platforms (SQL Server, MySQL, etc). http://www.oracle.com/technetwork/developer-tools/datamodeler/overview/index.html?ssSourceSiteId=otnru
So if I have a balance/transaction sheet, where changing one element in a column will cause changes across multiple columns, and I want to make a new row below it for the balance value after last change, can I automate this? You've just defined it in the statement, what about making the relationship permanent?
I think you'd probably be better off creating a VIEW to do what you want.
I know what you mean. A few years ago we bought some equipment from a company and they have called ever since. I've made it a point to never buy from them again because of their constant calling. Their emails reek of desperation. "Hey this is X at [company Y], just seeing if there's any purchases I can help you with". No.
&gt;Your database shouldn't be responsible of identifying who is submitting the data. Thanks- important distinction. I wasn't sure if the solution would best be found in the php, or sql, or some other place.
the php file is on the server side, presumably in your db connect statement. a user will not see the text in the file. there are ways that a user can access your database and it's info if you are not protected against sql injection. in general you want to validate/restrict any user input that could pass sql code from your form elements.
Some times they are unavoidable, or are a one time pull so performance isn't an issue so you use them to avoid over engineer the solution.
No, sql server doesn't work in excel-like fashion like this. In excel you can reference any cell in the sheet, it is just a 2d (or 3d if you will) array of unstructured data, sql tables on the other hands are very specifically defined lists of rows and every row in table have to has the same e structure. it's a very bad practice to have rows that have different meanings in one table (having one row as a sum of different rows from the same table is just a very bad idea even if in theory you could abuse some of the sql server functionality to achieve this). Sql server is nothing like excel and you shouldn't force it to behave like it.
The php code is interpreted by the webserver and the user's web browser should only being seeing html, javascript, css, images, etc.; not your php code.
The standard way to do this is creating a view.
Uh huh. Thanks. Is that so? I have all my .php files right in the public directory with index.html.
Plsql or tsql? Either way, I don't understand your confusion. You can google how to write max min stddev and group by queries. 
without the AND 
There's no difference for inner joins when conditions are applied. You can pretty much think of them as "logically cross join all tables, then apply all conditions". The SQL engine is supposed to figure out the optimal vs. literal way of doing this, of course (sequence of tables accessed, indexes, method of joining, etc.). There is slight difference in outer joins ANSI syntax, since the 'where' conditions are supposed to be applied only after all joins were processed. Again, in real life, the executing SQL engine is supposed to take care of optimizing this simplistic approach. Here's a link to a rather old MS SQL technote: http://technet.microsoft.com/en-us/library/aa213235(v=sql.80).aspx 
Get a solid understanding of GROUP BY first; MAX(), MIN(), AVG(), and the rest will fall into place.
The thing with installs for any software is first making sure your system meets the requirements. http://docs.oracle.com/cd/E11882_01/install.112/e16774/pre_install.htm#NTCLI1256 Look at the x86 for your machine. Installs can be really challenging, this is true if you were installing on hardware in a real environment. Fish through the Oracle support documentation. Make sure you have the required system specs, then read the Oracle supports on how to install for Windows. Getting comfortable with the support docs will put you leaps ahead of your classmates. 
They meet system requirements. Well above them. 
Every attempt I've made with DBCA stats the listener is either not up or my database isn't registered to it. I have no clue as to how I start a listener for my database.
Sorry, it was 8.1 pro.
http://docs.oracle.com/cd/B28359_01/win.111/b32010/ap_net.htm "Listener Requirements In Oracle Database 10g Release 1 (10.1) or later, the listener is set to start automatically at system restart. If you intend to use only the listener for all of your databases, ensure that only the Windows service for the listener, as listed in the Control Panel, is set to start automatically. Oracle usually recommends that you only have a single net listener service running on a Windows computer at any one time. This single listener can support multiple databases. If you need to have two different net listener services running on a Windows computer at the same time, make sure that they are configured to listen on different TCP/IP port numbers. If the same IP address and port are used for different listeners, you might expect that the second and subsequent listeners would fail to bind. Instead, Windows allows them all to listen on the same IP address and port, resulting in unexpected behavior of the listeners. This is a suspected Windows operating system problem with TCP/IP and has been reported to Microsoft." .. is this starting up automatically on your computer? and from the command line can you run this.. lsnrctl start and post what the message is?
Yes, I've gotten into Oracle's database control center and it states the Listener is running but my server is not as well as a listener error.
I'm deleting the current database (the SID does not match the one listed in the Control center) and seeing if I can't figure this out again.
Ok so this is what my issue seems to be: My listener isn't looking at the newly created Database I just made. I'm unable to edit the .ora file because the Listener is still up and running and I don't know where to shut it down from. Database control also thinks my SID is still orcl. It's not. So that's not updated either.
I just have no clue as to how to fix this. The database is there, I can 'edit' it with DBCA. It never completes though because the listener just doesn't want to cooperate or whatever. I can't use any of the tools to connect to this database because it's not running. I'm about to just give up....
What's the output of "lsnrctl status" at the command prompt? And get used to using the command prompt for Oracle - it's still the best way (and sometimes only way) to administer the database. 
I'm going to try installing 12c instead, as it official supports Windows 8.1. There shouldn't be any difference in output logs correct? Because I have to hand those in for my assignments.
Thanks! Question though, why would an SQLer mind subqueries?
If you don't want an integer, multiply by 1.0 as well. SELECT 1.0*(SELECT COUNT(1) FROM FilteredContact WHERE new_age &gt;= 60) / (SELECT COUNT(1) FROM FilteredContact WHERE new_p_rel_hh = '1 - Head') AS result
Thank you so much for the tool. I have tried it (after spending a few hours figuring out how to connect to MS SQL!) And it works, but not when the names are different. The application in the other comment does that actually!
Oh I gave up yeah, went to 12c which went much better. And no, the listener was starting, I just couldn't get it to listen to my database. I tried logging in from SQL PLUS as such: &gt; sqlplus SQL&gt; conn sys / as sysdba; And it would always return as an invalid login. 
wow thanks, using MySQL though. How does TSQL compare to MySQL from your experience?
The main reason would be performance, though. If you think about the process in the DB, the DB has to perform a query for each subquery, then insert the results in the surrounding query and perform that. In this case it would have to run a total of three queries to get the overall result. Of course query optimizers have various ways of doing this better, but in my experience avoiding subqueries makes for a better performance.
I'd explicitly cast one to float so you're not dependent on your RDBMS's order of operations (though as far as I know, all the main ones will either go left-to-right or multiply first, so you'll be fine).
I got it working with 12c but I was about to run a VM.
Other than RedGate this is the first tool that I have seen in a while that made me sit up. Very awesome, Idera is useless.
Oh yeah, and it's beautiful. Here's a before &amp; after: http://imgur.com/a/Jes3P
:) Much nicer for sure! 
You could also store them into variables then divide them after the fact.
I don't see any way of accomplishing this query without scanning `new_age`, scanning `new_p_rel_hh`, and then doing a scalar calculation. Sure, you could put them into a single query that uses `SUM()` and `CASE` for the two fields, but now you have to scan `new_age`, scan `new_p_rel_hh`, run a case statement against both, and calculate a sum. If you had a covering index on either `new_age` or `new_p_rel_hh`, only the subquery method would actually use it for the aggregate function. You could certainly push the scalar calculation off the RDBMS, but it's not a particularly resource intensive query to divide two numeric values. I mean, it's O(1). It doesn't get much simpler than that. I mean, am I wrong? What am I missing? 
I meant to say that generally it's usually a good idea to avoid subqueries. Didn't even read this particular example with care, tbh.
yep, I share that sentiment.
VCE files are used with the VCE Exam Simulator software. You can download vcefiles free off the internet.
Running through 100 iterations of both queries on tables from my database, I show that the first query is faster by a fair margin. Looking at the execution plans, it's because it is able to parallelize table scans in the the first query, but not the second (my date columns are not indexed). 
In general, the preference is towards doing things in a single query where that makes sense. The second method feels like a hang over from procedural programming where you're often storing intermediary results in variables rather than dealing with sets. However, I've also come across plenty of situations where storing an intermediary value has dramatically improved the performance of a stored procedure. It's generally annoyed me when that kind of solution has worked but I think once you reach a certain level of query complexity you find the query optimiser can be assisted by that sort of approach. 
The paper: http://www.complang.tuwien.ac.at/anton/euroforth/ef13/papers/nelson.pdf
depends on if there is a foreign key constraint on that table or not. for data consistency its generally a good idea to keep that constraint in tack but you can always remove it if need be. If you are getting that error then you do in fact have a FK constraint. 
I've found a piece of beauty today too.... I'd guess its not as bad performance wise than the abomination you posted, but it makes me want to cry all the same SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO create function killSpaces ( @stringValue varchar(100) ) RETURNS varchar(100) AS BEGIN DECLARE @retVal varchar(100) declare @idxStart int set @retVal = @stringValue while charindex(' ', @retVal) &gt; 0 BEGIN set @idxStart = charindex(' ', @retVal) set @retVal = stuff(@retVal, @idxStart, 1,'') END RETURN @retVal END 
You're welcome
The only time you really need a function like this is when you want to replace multiple characters (and we're talking many - nested replace() statements are still better up to a point), or need to use regex to do something like remove non-alphanumeric passwords. This is a straight replace() statement...the SQL engine is very efficient with that.
Why do u need keys?
&gt; Thanks 
Can't drive without my keys
Here's what I came up with, to relate the SUPERSSN field with the SSN field you can join them. I chose a left join in case there are supervisors who have not had entries in the employee_t table for whatever reason. To distinguish the two tables I've given them what's called an alias, this allows you to reference specific fields in specific tables regardless of how many tables share the same column names. I left it very basic, but you could also add in NVL()'s which would replace null values (missing supervisors) with a default name or message like 'N/A'. SELECT EMP.FNAME AS EMPLOYEE_FNAME, EMP.LNAME AS EMPLOYEE_LNAME, SUP.FNAME AS SUPERVISOR_FNAME, SUP.LNAME AS SUPERVISOR_LNAME FROM EMPLOYEE_T EMP LEFT JOIN EMPLOYEE_T SUP ON SUP.SSN = EMP.SUPERSSN; 
So, as someone who is completely scrapping and redesigning my company's entire database structure from scratch, and migrating everything to the web. Would you recommend some of the more complex calls to stay as views/stored procedures inside of SQL server, or would I be better off doing all the heavy lifting inside of my webapp? For example, the database I'm currently working on has a monstrous amount of views for some of the queries that are needing to be created, and many of these views call several other views is strange arrays of joins, encompassing a massive array of aggregate functions. The entire instance could've been built inside a single database with around maybe 60 tables. However, the previous developer built 20 databases with close to 200-300 views per database, all tied to access front ends. There are cases where we'll need the same data 12 times, but only when a certain field matches, so for example - WHERE [Mod] = 'D1', WHERE [Mod] = 'D2', etc. I was thinking of compressing these into a couple stored procedures to do all the heavy lifting and just passing a parameter to return the appropriate view. Is this a good option, or should I just created tailored select statements inside of my application code to select the appropriate information?
I figure your speaking metaphorical but its not really helping...
I have a question, since when is this sub for homework help
This sounds like a hw assignment. 
Perfectly fine, but it's worth noting that you are running at risk of potentially losing transactions until the replica is in synch. Whether that is acceptable is up to the business.
Thanks man! 
To order a record set by random, ORDER BY newid(). To generate a number sequence row_number(). If you can't figure it out from here, reread the last chapter of your course material 
Well nesting views is usually a bad idea. A view gets inlined in the select much like a subquery. After a certain amount of nesting, you are going to end up with "to complex to be optimized to run efficiently SQL" for the optimizer. I'm a fan of stored procedures. They prevent SQL injection, they are parameterized, enable you to do row level security and are a great vehicle to filter data on certain conditions. In my environment, the frontend user doesn't even have select permissions on the tables, everything goes trough stored procedures and the odd table valued function. 
Returning ROW? That's totally inconsistent with the rest of SQL select clauses! I'd definitely prefer: returning * into MyRecord;
Oh my.
Well is there a SQL help sub? for java there is one. IT said on the side here that one can request help or ask questions. And its not like i said can some one do it for me i did post what code i have so far.....
It is are we not allowed to post help for HW assignments? Thought one could ask questions here but guess not.. as much BS as im getting from everyone so far the only help full person has been /u/svtr . If i posted to the wrong sub just let me know....
One of their excuses for so many views is that they said SQL server was complaining due to too many aggregate functions. That, and they needed the same data for 10 different queries (different parameters). After having to repair some of their broken reports, I've determined that they used the query designer to build every single view, which, in this case has made code hideous, inefficient, and sometimes incorrect.
Yes, that would match PostgreSQL, for instance.
You are allowed to ask questions pertaining to homework assignments, but you didn't do that. You posted the objective, some code, and said thanks... That's it. In the future (or now), explain where you're stuck and the specific questions you have. The people of this subreddit are very knowledgeable and helpful, but they're not going to do your homework for you. 
Not very Forthy. You cold do the same with any text substitution mechanism.
The reason homework threads are not well liked is that usually its really really simple stuff *, and one can usually taste the amount of lazy spawning them. In your case you at least provided sample data, and a "here is what i got this far", so to me that's fine, so I tried to give you a little push into the right direction. Or rather one of the right directions, I can from the top of my head think of 4 different ways to solve this. *specially after you heard for the hundredth time that "oh yeah I know SQL, I had a course in collage". Trust me, database development is nothing like OOP...
If you're in synchronous mode, then transactions are not completed on the primary node until the secondary also reports them complete. You will not lose data in sync mode at the cost of slower performance. 
You get the C# programmer who says ,"Today, I shall create a cursor that will do everything ever needed to be done to my data tables"
&gt;the info in the MySQL table doesn't update until I refresh the page sounds like a problem in your javascript. &gt;fairly familiar with many programming languages as I am an IT student HAHAHA thats a good one
I guess you may be right. If I am using PHP to connect to my MySQL database and read from it when the page is initialized, will I be able to continue to query this connection every 5 seconds to get data that is up to date? Or do I need to open a new connection every time?
well, I don't "do" webdevelopment (I hate it). What I can tell you is, that php gets executed on the server side. So loading the page, your php runs. To have the browser, which is on the client side, do stuff, you will need to use javascript. If you plan on querying the database every 5 seconds, well, you better not plan that thing to scale to many users. Your Mysql server is going to melt. What I would do, is to write a web service, which queries the database and caches the data, having the clients asking the webservice for the current set of data. That way at least the logic on the DB side wont get executed over and over and over again. If you had have lets say 500 concurent users which is not THAT much for a website, you'd be quite fucked once you do updates, since page / table locking won't be your friend. But well, not my department, you'll be better off asking that particular question on some webdevelopment sub.
This code is awful, a lot of completely convoluted and unnecessary date arithmetic, the OP doesn't understand how Oracle calculates dates. I can achieve better with a simple query (untested). with qryNIC as (select '900780826v' as NIC from dual), qryDateInfo as (select to_date('19'||substr(NIC, 1, 2)||'0101', 'YYYYMMDD') as BIRTHYEAR_EPOCH, to_number(substr(NIC, 3, 3)) as DAYS_OFFSET from qryNIC) select case when DAYS_OFFSET &gt; 500 then 'Female' else 'Male' end as GENDER, BIRTHYEAR_EPOCH + case when DAYS_OFFSET &gt; 500 then DAYS_OFFSET - 500 else DAYS_OFFSET end - 1 as DATE_OF_BIRTH from qryDateInfo
Is this for school?
Are you proposing storing "Trips" as an entity of some sort? Are you trying to give individual staff instructions on which order they need to pick things? I've only worked with one retail database system and in that there was no record of "Trips" in the sense you're talking. Individual orders had pick orders though, which was done (from what I can recall) based on some ranking system so the products were sorted so aisle 1 stuff appeared before aisle 2, etc. That was for a smaller warehouse than what you might be envisioning though and there was nothing stopping the order picker doing it in a different order than suggested (and if they processed orders quickly enough without quality dropping I don't think anyone gave a shit). A lot of the more complicated rules took place at the Product level - essentially some products couldn't be picked with others because of their size while others had special instructions for dispatching, etc. The core concept was the Order which was used for most core tasks (e.g. stock ordering, stock replenishing, stock disposal, etc). 
Im, not sure what goes here: to return 2 print 'Ther are ' + + 'total' + ' classes ' + 'between 10 and 20'
Im, not sure what goes here: to return 2 print 'Ther are ' + + 'total' + ' classes ' + 'between 10 and 20'
Let me preface this by saying I have no experience with picking systems at all so this is just a guess on my part on what might work. You could have something like Basic Entities: Customer Product Employee Entities with Parents (FK field in Brackets): Order(CustomerId) Trip(EmployeeId) OrderProductTrip(OrderId,ProductId,TripId(nullable)) So Customers Have Orders (via CustomerId in Orders) and Orders have Products via OrderProductTrips Employees have Trips (via EmployeeId in Trips) and Trips have Products via OrderProductTrips I think for this to work TripId would need to be nullable in OrderProductTrip as you would build the order before you build a set of trips for it. You could avoid the nullable column by having an OrderProduct table separate from the OrderProductTrip table but I think thats probably not going to be of any advantage. I guess it would really depend on how much additional information you needed to record against each product on a trip. Pickup time, Instock etc etc. If there was a lot of information that was specific to the trips relationship with the product you might want to split it off. But even with this basic structure you should be able to handle some change to the way things are done. Even build trips that cover multiple orders (say get frozen products for orders 1, 2 and 3 in one trip)
phpmyadmin: what do you mean code? are you using phpmyadmin to create a table then using php or some other 'code' to modify the table?
You get this error when you are trying to set up a foreign key constraint to a table but the referenced value doesn't exist. That is there are values in the referencing table that does not exist in the referenced table. To query for which values are missing: SELECT Client_ID FROM dbo.Clients AS c WHERE NOT EXISTS (SELECT * FROM [yourReferencedTable] AS r WHERE r.Client_ID = c.ClientID)
Our teacher gave us a sql, every time I try to change something, the change doesn't appear on the website. For examble: INSERT INTO `polls` (`PollID`, `UserID`, `Title`, `Question`, `Answer1`, `Answer2`, `Answer3`, `Answer4`, `Votes1`, `Votes2`, `Votes3`, `Votes4`, `StartDate`, `EndDate`, `PollOpen`) VALUES (4, 'BillGates', 'A poll about Bill Gates.', 'HOW is Bill Gates?', 'He''s the greatest!', 'He''s the best thing since sliced bread!', 'I wish I could be him!', 'I heard he wrote most of the Beatles'' songs!', 6, 7, 6, 8, '2012-01-01', '2012-01-02', 0). If I Remove the "HOW" the change doesn't appear on the website. Why? 
Sorry for the format mess up. I am new to reddit.
you can test this, put wireshark on your client, and see if it sees traffic between your browser and server after your page loads... if not, it isn't the database. if you are seeing traffic, then check the database logs, you should see a connection. Basically, narrow your problem down to the actual problem. This guy used JQuery to update the chart in realtime.: http://ngo-hung.com/blog/2012/07/19/create-a-simple-highcharts-bar-chart-with-real-time-update setInterval(function() { getData(); }, 30000); function getData(){ console.log("retrieving data from server "); var url = "&lt;some server url&gt;"; $.getJSON (url, function (jsonFromServer) { var data = &lt;get data from the json results &gt; // update the series data pollChart.series[0].setData(data); }); } 
Thanks, I'll take a look at this!
Have you considered using COUNT() on the customer ID's?
Keep in mind that this command inserts a new record and doesn't change any existing records. Each time you run this command, you should see a new record in the table when you use the SELECT command: select * from polls
I apologize for my lack of constructiveness here, but I think we need to attack the root of the problem. It is apparent to me that either your teacher has done a terrible job of explaining these things to you, the books you've been provided are not sufficient, or you are not asking for clarification from your teacher on points that you are not understanding. If any of these cases or similar cases is true it will be in your best interest to address them ASAP. To your question: php**My**Admin is a GUI based tool used to manage MySQL databases. Whether you use phpMyAdmin, mysql command line, or write a program of your own to alter the database, the results should appear the same in any tool. This is because no matter which tool you use to access the database, you're still accessing the same database. If you walk to your friends house, drive there, teleport there, or fly there you're still arriving at the same place.
That's pretty similar to what I had in mind. If you do it that way with order_details separate from order product_trips you will need an Id on the order_details table. Then you can use that as a foreign key in the order_product_trips table. That's because you need to link the product from an order to the trip. In the example you have you are just linking the whole order. You can then add one entry in the order_product_trips table for each of the products from an order that you want to add to the trip. As a side note just be careful with your naming conventions. It may not seen all that important now but it will make all the difference down the road. Make sure you use all singular Trip or all plural Trips not a mix. Why call the table order_details when you call the table linking it order_product_trips. It's either order_products or order_details, Pick one. Why customer orders. Aren't they just orders? I would also use underscore not spaces as spaces in names are a pain down the road. They are minor quibbles but database design is all about consistency consistency consistency.
Looks good to me. I am guessing the first order Id being 1 instead of 12 is just a typo but you could have trips cover multiple orders, if that's something you wanted to do.
You probably have to commit your new row. Some clients will auto commit while others will not. If your insert runs but is not showing up you most likely have to commit the new record. Either that or you are violating a constraint and it is generating an error that you are not paying attention to.
SQL server - sorry I should of specified. 
Grammar error detected. [What is it?](http://www.reddit.com/r/SpellingB/comments/22bwnw/homophone_error) **should have** *Example:* I should have never thought horseback riding would be any better than ziplining. *** ^(Parent comment may have been edited/deleted.) ^[STATS](http://www.reddit.com/r/SpellingB/comments/22o42h/stats/)
Bascially, it's shit. Does your application require instance level collation or database? 
Well shit. Thank you for explaining that!
Wow, exactly as needed. But beats me why. In the table, empl.name is never null...
If your logic allows, why not have separate triggers that call a stored proc with IsUpdate or IsInsert as a parameter. That would centralize your code base and keep your triggers separate without the need for if exists. Just an idea. I don't know all the concepts of the involved task so that idea might not be sound in your situation. As for multiple exists, yes you can have them. In fact it might just be better to use an AND/OR instead of nesting them. IF EXISTS (SELECT * FROM table1 WHERE table1.col1 = 'BLAH') AND EXISTS (SELECT * FROM table2 WHERE table2.col1 = 'BLAH') UPDATE table4 SET col4 = 'newvalue1' ELSE IF EXISTS (SELECT * FROM table6 WHERE table6.col1 = 'BLAH') UPDATE table4 SET col4 = 'newvalue3'; OR IF EXISTS (SELECT * FROM table1 WHERE table1.col1 = 'BLAH') BEGIN IF EXISTS (SELECT * FROM table2 WHERE table2.col1 = 'BLAH') UPDATE table4 SET col4 = 'newvalue1' ELSE UPDATE table4 SET col4 = 'newvalue2'; END ELSE BEGIN IF EXISTS (SELECT * FROM table6 WHERE table6.col1 = 'BLAH') UPDATE table4 SET col4 = 'newvalue3' ELSE UPDATE table4 SET col4 = 'newvalue4'; END; Does this help or am I missing the mark?
In SQL Server, the database collation is really the "default" collation when you create the table. You can have a French collation for a database and create a Latin table/column. Having a mixed collation database can be tricky. For instance, equality operators on join conditions will result in an error if you are joining on columns with different collations. You will need to collate one column to coincide with the other.
Try SQL Exercises: http://sql-ex.ru/. Start with http://sql-ex.ru/learn_exercises.php
Aha, now I get it :) Meanwhile, I tried a few things and applied this query to my exact needs and works as expected. TIL, totally. Thank you kind stranger!
edit: I am assuming that you're locked out of the instance. Use the method below to get back in &amp; reset SA normally Totally doable, will require a little downtime though Pre-req - be local admin on the server SQL is installed on Start SQL server in single user mode with -m Login via sqlcmd create login (or use existing one in the next step) grant sysadmin to login restart SQL Server in multi user mode http://blogs.technet.com/b/canitpro/archive/2012/11/26/the-sql-guy-post-30-how-to-recover-from-a-lost-sa-password-in-sql-server-2012.aspx
This will work to replace all those values with blanks (or whatever you want in SQL Server. Pretty sure it should work in MySQL too. UPDATE table1 SET column1 = REPLACE(column1, '', '') 
Depending on your needs, you may have to export all the data into a new database with the desired collation. It's not terribly hard to do using the export/import data tools in SSMS, but it can take a while depending how large the database is. 
64-bit SQL Server?
This should be totally doable as the other comment points out but if you keep running into problems you always have the migrate option rather than uninstall reinstall. This could be to a new machine if resources allow or even to a second instance you install on the same machine(yucky - last resort) Also, This is the perfect opportunity to use to try get a new server that isn't running windows 2003 and maybe even go right for SQL 2012. I have done this in the past with a problem that is easily fixable but management won't pull the trigger on the oh so needed upgrade. 
Yeah this works on 2008 too. You need the semicolon if you're amending the startup params in the config manager, but not if you're starting SQL on the command line
Its probably not a good idea to just go deleting them. Find out why they're there in the first place. Its probably a mismatched character encoding between the input file and database Also check its actually visible in the raw database, and not just appearing on the web page (since it could just be the html character encoding which is wrong).
Maybe I am missing something but you can't use a merge statement instead?
Implausible .... but interesting. I wonder what a single quote mark in the middle of the numberplate would do?
Here is my suggestion, unfortunately I don't have Postgres so you may have to tinker with the syntax to get it to work. Perhaps I am missing something but I could see the need for the windowing. SELECT COALESCE(t.task_id, tl.task_id) as all_task_ids ,t.desc ,tl.emp_id ,tl.max_solve_date ( SELECT task_id ,desc FROM tasks ) t FULL OUTER JOIN ( SELECT task_id emp_id max(date) as max_solve_date FROM tasks_log ) tl ON t.task_id = tl.task_id 
LOL OMG OP, where did you find this? Hard to believe this image has never graced reddit before! Oh wait... it has about a million fucking times. http://karmadecay.com/results/u3140169
Thanks but it does the same thing as my SQL. Empty lines are returned only for tasks not completed by anyone.
So he's assuming that's the database name and the cameras account has admin rights.
You don't need admin rights to drop a table. The camera must have write/update rights since it has to write new plates to the db.
And your question would be?
I would imagine the same team that wouldn't sanitize / parameterize correctly to allow this type of thing to happen wouldn't be too careful about security either. Why waste time with grants when sysadmin works? 
People give me crap about this but I do most of my query writing in Oracle SQL Developer design mode. If I need something the designer can't do they I modify the code directly. I use it for Oracle and SQL Server(with the right drivers). 
Thanks! im assuming form submissions, such as logging on or creating an account in most cases are direct fed into a database as well? hence the possibility for SQLinjections? is the programming to take the data directly from the user input to input into the table somtheing that is coded in SQL or is that something another language would deal with?
I am not sure if I understand your question correctly, but a couple of things I noticed are 1) you generally do not want to use `DISTINCT ON` at the same time as window functions, you should usually use one or the other and 2) shouldn't you join on both task_id and emp_id to get what you want? I imagine you want to do a cross join first between tasks and employees for the case where no task has been completed at all. Suggested solution if I understood the problem correctly: SELECT t.id, t.desc, e.id AS emp_id, last_time FROM tasks t CROSS JOIN employees e LEFT JOIN ( SELECT task_id, emp_id, max(date) AS last_time FROM task_log GROUP BY task_id, emp_id ) last_tasks ON t.id = last_tasks.task_id AND e.id = last_tasks.emp_id EDIT: A solution without sub queries. SELECT t.id, t.desc, e.id AS emp_id, max(date) AS last_time FROM tasks t CROSS JOIN employees e LEFT JOIN task_log tl ON t.id = tl.task_id AND e.id = tl.emp_id GROUP BY t.id, e.emp_id
It is usually found in your database driver in your programming language as parametrized SQL or prepared statements. It could look something like this text = "'); DROP TABLE users; --" connection.execute("INSERT INTO data (text) VALUES (?)", text) and we would get the text `'); DROP TABLE users; --` inserted into the database without any risk for SQL injection. What you want to do to avoid SQL injections is to build queries with normal text handling functions and instead let your database driver do the work. Dangerous code looks something like this. text = "'); DROP TABLE users; --" connection.execute("INSERT INTO data (text) VALUES (" + text + ");")
7 times? And appears to be a completely different article, actually, just a picture.
Before anyone asks, I have googled this at least 4 times and only found one "result", it didn't work for me. I've been doing SQL for school using Oracle using 10g express, but the code I upload for the class comes up in a single line, all red square of text and I'm wondering if this is creating problems. I can only run it by going to the "view" dropdown menu and selecting detail view. Has anyone else experienced this and fixed it? Edit: To clarify why I'm wondering after all this time of using it "as is", I've run into a few problems running scripts that I practically used word-for-word by the textbook examples and just today I went to "results" tab of the scripts and it states that of the 179 statements in the script, all are failed and contain an error of some sort. 
The text is just the script, which is too long for me to make a post on (&gt;1000 chars), all the scripts that I've downloaded from my college website show up as this in the script editor. If you mean an actual command and error then here's one in particular I've tried numerous ways to type up and failed with the same error: SELECT COLUMN_NAME, TABLE_NAME, DATA_TYPE FROM DBA_TAB_COLUMNS WHERE COLUMN_NAME = 'BOOK_CODE', 'TITLE', 'PRICE'; ORA-00933: SQL command not properly ended 
We see that there's an error message, but what exactly are you asking? Everyone's situation and environment is different. http://www.sscce.org/
Well, upon going into the script editor to copy/paste the code in hopes I could fix it, it brings me to the blank red square in which I can't type anything so I'm guessing this is in relation to it not cooperating with my computer somehow; when using it at school it's just a normal white text-box but on here it's this unusable red block of pissing-me-off.
&gt; On the create tables don't define columns as char btw instead use varchar2, as they are fixed width I.E they will store the text with trailing white space. While it's considered better practice to use varchar, there's nothing about using char that should cause an error. If he's just learning SQL and copying commands, he should be using the table definitions given to him for the assignments.
agreed, I was not attempting to state that was a problem causing his error.
I've actually tried a few different browsers with no luck so far; I'm concerned it might be the OS as the Oracle program (10g) is rather old but I tried a newer version and it wouldn't work for me.
I will suspect that the file is being saved as lines ending with just a carriage return, and not a carriage return/line feed combination, which is causing them to all appear as a single line when you upload them. What editor are you using to create the file that you are uploading? 
why don't you upload the scripts somewhere where it can be visible?
Below your "group by" use the code below. This will remove 0 counts. HAVING Count(*) &gt; 0 In your where statement use the code below. AND 1 in (nc.new_cat1_Rape,nc.new_cat1_Incest,nc.new_cat1_ActsofLasciviousness) This will force it not to count X. In your order by, you should change your THEN in each of your statements to do 1, 2, 3 so you actually have an order by.
No, I actually want to have the 0 counts. But Thanks! Your adjustments didn't show X. 
&gt; No, I actually want to have the 0 counts. change this -- FROM FilteredContact fc RIGHT JOIN new_casecategory nc ON nc.new_EntryId = fc.new_EntryId WHERE fc.new_age &lt;= 18 to this -- FROM new_casecategory nc LEFT OUTER JOIN FilteredContact fc ON fc.new_EntryId = nc.new_EntryId AND fc.new_age &lt;= 18
I'm not sure if it is possible to place a case instead of a variable like that, but here's a way to do what you want with not many more keystrokes: WITH CTE_DateType (DurationType, Duration, EndDate) AS ( SELECT 'd', 11, '2014-04-14 11:22' UNION SELECT 'm', 22, '2014-04-14 11:22' UNION SELECT 'y', 33, '2014-04-14 11:22' ) SELECT *, CASE p.DurationType WHEN 'd' THEN DATEADD(DAY, -p.Duration, p.EndDate) WHEN 'm' THEN DATEADD(MONTH, -p.Duration, p.EndDate) WHEN 'y' THEN DATEADD(YEAR, -p.Duration, p.EndDate) END AS StartDate FROM CTE_DateType p
&gt; User has a one to many connection to Article I think it should be: ALTER TABLE user ADD CONSTRAINT FOREIGN KEY (user_id) REFERENCES article (user_id); but something is array.
1 User -&gt; Many Articles So the reference is on the article side . The User Id in Article references to the Primary key in User . 
can you explain it to me a bit more, I am a bit confused as to why the switch worked?
You do not get the 0 count because : FROM FilteredContact fc RIGHT JOIN new_casecategory nc ON nc.new_EntryId = fc.new_EntryId WHERE fc.new_age &lt;= 18 you right join, but filter on the left table. Since NULL &lt;= 18 does not equate to true, you do not get the types for which no case is found. Move the filter condition to the join condition, then it will work. I'd like to add, that I would move those 4 lines of case expression to a subquery, would make it SO much more readable
yeah I just realized that the code I used is kinda making things complicated..
I can try it (unfortunatly im not a native speaker ) : You said its one to many relationship: (1:n) so a user can exist without having an article a user can have 1 article and a user can have multiple articles . Now u tried to set the userID from User as Foreign key pointing to userID from Article. Thats not how "1:n" works. Cause in this case u cant have a user w/o an article. When u have "1:n" the one on the many side (article in your case) always has the reference to the primary key from the one side (user). User can exists w/o an article. (One) Article cant exist w/o an user. Is this any help for u? http://en.wikipedia.org/wiki/Cardinality_%28data_modeling%29 http://www.techopedia.com/definition/25122/one-to-many-relationship 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Cardinality (data modeling)**](https://en.wikipedia.org/wiki/Cardinality%20%28data%20modeling%29): [](#sfw) --- &gt; &gt;In database design, the __cardinality__ or fundamental principle of one data table with respect to another is a critical aspect. The relationship of one to the other must be precise and exact between each other in order to explain how each table links together. &gt;In the relational model, tables can be related as any of "one to many" or "many-to-many." This is said to be the __cardinality__ of a given table in relation to another. &gt;For example, consider a database designed to keep track of hospital records. Such a database could have many tables like: &gt; &gt;* a *doctor* table with information about physicians; &gt;* a *patient* table for medical subjects undergoing treatment; &gt;* and a *department* table with an entry for each division of a hospital. &gt;In that model: &gt; &gt;* There is a __many-to-many__ relationship between the records in the doctor table and records in the patient table because doctors have many patients, and a patient could have several doctors; &gt;* A __one-to-many__ relation between the department table and the doctor table because each doctor may work for only one department, but one department could have many doctors. &gt;A "one-to-one" relationship is mostly used to split a table in two in order to provide information concisely and make it more understandable. In the hospital example, such a relationship could be used to keep apart doctors' own unique professional information from administrative details. &gt;In data modeling, collections of data elements are grouped into "data tables" which contain groups of data field names called "database attributes". Tables are linked by "key fields". A "primary key" assigns a field to its "special order table". For example, the "Doctor Last Name" field might be assigned as a primary key of the Doctor table with all people having same last name organized alphabetically according to the first three letters of their first name. A table can also have a __foreign key__ which indicates that field is linked to the primary key of another table. &gt;A complex data model can involve hundreds of related tables. A renowned computer scientist, [C.J. Date](https://en.wikipedia.org/wiki/C.J._Date), created a systematic method to organize database models. Date's steps for organizing database tables and their keys is called __Database Normalization__. Database normalization avoids certain hidden database design errors (__delete anomalies__ or __update anomalies__). In real life the process of database normalization ends up breaking tables into a larger number of smaller tables, so there are common sense data modeling tactics called __de-normalization__ which combine tables in practical ways. &gt;In real world data models careful design is critical because as the data grows voluminous, tables linked by keys must be used to speed up programmed retrieval of data. If data modeling is poor, even a computer applications system with just a million records will give the end-users unacceptable response time delays. For this reason data modeling is a keystone in the skills needed by a modern software developer. &gt; --- ^Interesting: [^Data ^model](https://en.wikipedia.org/wiki/Data_model) ^| [^Unified ^Modeling ^Language](https://en.wikipedia.org/wiki/Unified_Modeling_Language) ^| [^IDEF1X](https://en.wikipedia.org/wiki/IDEF1X) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgs0dxc) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgs0dxc)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Thanks murmix Ill check it out.
That's overly complex for my tastes. update churnedmarch31toapr13 set startdate = CASE p.DurationType WHEN 'y' THEN DATEADD(YEAR, -1 * p.Duration, c.enddate) WHEN 'm' THEN DATEADD(MONTH, -1 * p.Duration, c.enddate) WHEN 'd' THEN DATEADD(DAY, -1 * p.Duration, c.enddate) END from churnedmarch31toapr13 c inner join Products p on p.productid = c.productid where startdate is null It's a little less compact than what you wanted to do but the intention is far more clear (IMO) than what Murmix suggested and it's more compact.
This is basically the same thing as my suggested solution, except you have reimplemented `MAX` + `GROUP BY` with window functions. A useful excerise to udnerstand SQL better, but hardly recommended in production code.
This. Vendor software is terrible for this scenario happening. *'Oh you're having trouble installing? Try giving the installer the SA credentials and perform the install again.'* Edit - *'Hey just letting you know, we had a report that our software is vulnerable to an eight year old SQL injection technique on its web interface. Don't worry about it though, it's so old nobody probably tries it anymore.'*
Well hot damn I just tried this at school and that made it work! These examples I've been referencing had no "IN" statement so it never would have occurred to me, thank you!
Down to food where the every query will have defined animal,species,age and I want to find the "most matching" set. EDIT: I apologize for my lack of proper terminology but I'm still learning.
This is what I assume you are trying to do. Correct me if I am wrong. &gt;Given 3 values "animal", "species", and "age", find the best food for them. All three values are optional. If no value is provided then any match will do. An empty value in the table should also match any requested value. DECLARE @Animal VARCHAR(20); DECLARE @Species VARCHAR(20); DECLARE @Age INT; SET @Animal = 'tiger'; SET @Species = 'bengal'; SET @Age = NULL; SELECT T.food FROM Table1 T WHERE COALESCE(T.animal, @Animal, '') = ISNULL(@Animal, '') AND COALESCE(T.species, @Species, '') = ISNULL(@Species, '') AND COALESCE(T.age, @Age, 0) = ISNULL(@Age, 0); The COALESCE and ISNULL statements will cause the clause to return true if the values are null. As /u/r3pr0b8 said, you should research these functions they are very useful for queries like these.
All the difficulty is coming from the fact that in SQL, null is "unknown" and not "any". You can never say whether or not some arbitrary set that you know nothing about contains a particular element. The only test you can perform on something that might be unknown is to ask whether or not it's unknown or not (`someValue IS NULL`). If you want a value that means "any", you're better off choosing some value as the stand in for "any" rather than using null, since `null = null` is false. For example, let's use the value "#any" for the varchar columns and -1 for the int columns. Now your first query might look like this: SELECT food FROM SomeTable WHERE animal IN ('tiger', '#any') AND species IN ('bengal', '#any') AND age IN (3, -1) In this case you should get the exact same result as all the values are explicitly stated. When you parameterize this query, it becomes much more flexible: SELECT food FROM SomeTable WHERE animal IN (@animal, '#any') AND species IN (@species, '#any') AND age IN (@age, -1) Now you could pass in `@animal = 'tiger', @species = 'bengal', @age = -1` (your second example query) and you will match every row. You can use `null` to mean "any", but it gets messier: SELECT food FROM SomeTable WHERE (@animal IS NULL OR animal IS NULL OR @animal = animal) AND (@species IS NULL OR species IS NULL OR @species = species) AND (@age IS NULL OR age IS NULL OR @age = age) If you want to start joining on those conditions, it starts getting messy fast. In my experience, the optimizer is far less likely to choose some crazy plan that performs poorly when you use a tokens rather than null. I have to admit, I would really love to see an equivalent value to `null` that means "any" in SQL. I have to use patterns like this a lot and would pretty nice if you could more natively express that intent in SQL (and also have the optimizer recognize and handle that better). Basically, I want a "`null`" where "`null = null`" and "`'anything at all' = null`" are both true. That would make my life a lot easier.
At first from looking at COALESCE when the first commenter mentioned it, I did not think it was what I needed. I didn't realize you could have COALESCE in WHERE. Is COALESCE better than CASE or is it situational? Should I write them both out and see which has better performance?
Coalesce is effectively a built in case function for nulls prpbably best if you are dealing with nulls and want a garanteed value (you can always set a static value as the last case)
SQL Server runs as individual instances that are specific to the SQL version, so yes you can remove the SQL 2008R2 instances, just be wary of any data you might lose when uninstalling it. Are you sure you have 2008 instances? Depending on where you're looking, the start menu is not a good identifier as the installation of 2008R2 also adds a 2008 item to the start menu.
I understand table normalization, but I don't think that relates to this question specifically.
it depends on how often you retrieve rows of one type versus the other also on how big those extra columns are if they're in a different table, then table scan type searches on the common table should be (marginally?) faster as there will be more rows per physical block but i wouldn't split them until i actually thought it would make a measurable difference in performance
It depends on a few things.. - how you plan to query them? If every query will always select either Pics or Videos, then it may be worth splitting. But if you want to say, query the 10 most recent media uploads by a user (pics and videos), then you'll benefit by them being in one table. - It also depends on how much schema is shared between video and pics. You say pics have 5 columns which will be NULL, but how many columns do they have in common, or rather in terms of byte size really? - One last thing to bear in mind is cacheability. If pics is getting 1000 uploads an hour and videos is only getting 4, you may benefit from splitting it. This way you can cache video results independantly (ie in memcache), without having to invalidate the cache just because an image was uploaded. - Splitting could also improve the hit ratio of mysql query cache, although in most cases the query cache is next to useless anyway. [Edit: Im speaking in terms of MySQL here. For MSSQL the more intelligent caching would provide a much stronger argument for splitting... more compact rows means it can keep more rows per page in ram] 
I would suggest using a SSIS package. What are the errors the wizard were giving you? If all else fails you could use the generate scripts wizard. Right click a database in SSMS &gt; Tasks &gt; Generated Scripts...
that *is* a lot simpler. thank you. I cant believe I didn't think of it. i gots the dumb.&gt;.&lt;
If it's a separate instance: yes. If you had 2008 and upgraded it to 2008r2: no. Note that a 2008r2 install will list some components as 2008. 
Practice databases comes with most sql software 
Just to be clear, I wasn't thinking about putting picture and video data in separate tables. Pictures and video share about 10 columns of data, but there are an extra 5 or so which only apply to videos. I was thinking of making another table with an id that points to the main picture/videos table id of video rows, and then that table would only have the 5 columns of video-only data. This will not be a high-traffic table... probably averaging 100-500 database read/writes a day. Could go up to 20,000 or so on a big day. But generally I'll be searching both videos and pictures at the same time so I want them in the same table, it's just when I call for a video's information, I'll need those extra five columns.
The extra columns won't be that big. Probably none bigger than 50 bytes.
You can play a game with SQL @ http://schemaverse.com That's a pretty fun way of learning the language :D
Install wamp or lamp on your box. 
You are not being clear if you want to the schema or the data. You can right click on the database. Choose "Tasks"-&gt;"Generate Scripts". You can script out anything you want. In the advance options you can choose "Schema and Data". This will generate both scripts to create the tables and a line for each row being inserted. This is fine for small tables and databases but not realistic for large tables. Why don't you tell us what you are ultimately trying to do and we can point you in the right direction.
Absolutely ! And you normally need another language, to call SQL commands; if you're doing web apps, it's simpler to start with something like php or asp.net, and do SQL calls directly -- this becomes error prone with big programs, so you switch to mvc frameworks, like rails, django or asp.net mvc In a previous life, I wrote this tutorial for php and PostgreSQL http://okaram.spsu.edu/~curri/classes/13/summer-13/DB/StudyNotes/Ch8-WebApps/Ch8-WebAppsNotes.pdf 
SQL server offers an 180 day free trial
I'd recommend PostgreSQL over MySQL; it's more consistent with the SQL Standard and has [great documentation](http://www.postgresql.org/docs/9.3/static/). You can [download and have a Virtual Machine running in minutes](https://blogs.vmware.com/vfabric/2013/02/new-vmware-vfabric-postgres-9-2-release-install-a-postgres-machine-in-5-minutes.html). Edit. [Here](http://www.commandprompt.com/ppbook/booktown.sql) is a decent sample database to start with.
I think datguy's question was more about how to PRACTICE on MySQL. And I'd like to know too. 
I just added the CTE for testing, as I had no "churnedmarch31toapr13" table of my own. Other than that, our case/dateadd logic is identical.
You are right. I can't remember how I was reading it yesterday, but looking at it now, I must have been reading it wrong.
How accurate do you think the VCEs are? I took the test back in Oct. and failed by like 4 questions. I had the Microsoft 70-461 training kit and felt like I was very prepared. I had taken the practice exam that came with the kit until I could consistently get 100% on it. When I took the exam I was completely dumbfounded by how different it was compared to the M$ practice exam. I felt like I had prepared for the wrong test. I'm getting ready to take it again and would like some more true to real exam practices. Will the VCEs be what I'm looking for?
Is VBScript permitted within your environment? I've done a ton of DTS and SSIS which ultimately called out to "Execute Process Task" to run a .vbs that either I or someone on the dev team has built.
run it in a batch or you can try something like autohotkey 
No idea how I missed that part. This would explain why no one else suggested this. Whoops.
If you first download and install VirtualBox from Oracle, then [this page](http://virtualboximages.com/VirtualBox+MySQL+Pre-Installed+Virtual+Appliances) has virtual machines with MySQL included. 
Create a powershell script that connects to your SQL Server instance, and does the copy and paste, then after it closes connection with SQL Server, have it then open the actual program. Or have it do it in vice versa. Then, create a shortcut of the script on your desktop with this as the target: powershell.exe -command "&amp; 'C:\Path to your script\YourScript.ps1' -AnyArgumentsYourScriptMightNeed" For the section that executes the opening of the Program, this might help: Start-Process -FilePath ".\PathToYourProgram.exe" -ArgumentList /passive, /norestart -Wait
You don't compile SQL scripts. You compile a program that runs an SQL script. So go learn VisualBasic, C, C#, whatever, and learn how to connect to an SQL engine from that programming language, and write a program that connects and runs the script.
Agreed, pg is better than mysql. See: http://sqlfiddle.com/ for running sql against a db without installing, and http://pgexercises.com/ for practice exercises. 
You want `AFTER`. `FOR` is for DDL triggers (when you want to trigger on something like a `CREATE TABLE` statement).
I got a temp position as a Report Writer/Junior DB Analyst just by going through a temp agency with very little experience. Turned into a Junior DBA position and they are paying for me to get my SQL Server certifications. Just get out there, talk to a recruiter, submit some resumes.
The WHERE clause always goes at the end of the statement - be it a single table select, or a multi-table join. And yes, you can always use &lt;ALIAS&gt;.* to return all rows from that table. Doing so will not return all rows from all tables, so long as you use the &lt;ALIAS&gt; (ie. 'Patient').
glad you confirmed! so AFTER will run my sql statements once the insert or update is completed and committed to the database. I thought that is how i read it but on several forums they said the opposite. 
That was very clear. Thanks. Writing the trigger now and test it out later today. 
That was exactly the information I needed, thank you!! :]
You're welcome.
I started in a support role for a company that required a little sql knowledge to extract data and modify records for customers. Gradually doing less support and more database work. . If you aren't too worried about salary just start studying the cert material and applying for junior roles a lot of companies will take on those with less experience if they show that they are willing to learn and want to go somewhere. Nobody likes a coaster :) 
What cert are you referring too? Thanks
whoa, let's be a little more clear here &lt;ALIAS&gt;.* will return all **columns** from that table
I picked up the mta database fundamentals. And then started on my mcsa as my work uses ms sql. But im sure the oracle cert would work the same. Look at jobs in your local area and see what systems they are using. A cert won't garantee you a job but it does demonstrate you are willing to put your own time in to improve which helps
if its MS SQL, I use OSQL.exe. I'm sure there's something simular for other dbs. You pass it everythiong needed to execute your SQL command, connection info, input/output filenames. Quite simple, great for scheduled tasks.
Many of the questions in VCEs are good. Some answers are wrong and require research. Getting comfortable with the last 3 is recommended.
on SQL Server you can design and execute SSIS packages, which are ment to do ETL things like this. An other option would be to write a CLR in .Net and have that do it. CLR's are pretty much DLL's you write to extend the functionalities of SQL Server. I'd go with SSIS if possible thou.
There is a meme with that British dude from Virgin Airlines and he says, do what you have to do to get the job, and you will learn there... I have lied my fucking ass off to get jobs and after two months I am at par. At six months, I am a genius. Seriously, most will probably say I am wrong and they could be right. I don't learn well when everything is running smoothly. I have to be in over my head and have to figure it out myself. That way, I will never forget it... You might have someone from this subreddit challenge you for kicks... Shit, I would appreciate it to! Good luck to us bud!
Thank you. But I failed to see much change in your code.
Thanks! I'll look into this. SsIs sounds familiar
this explains to me, how companies can end up with a backup &amp; restore plan of "we have raid 5 so we're good right?"
Every major company has a database. Also all of [these](http://en.wikipedia.org/wiki/List_of_reporting_software) companies have entry level positions doing development or support or consulting.
Other than mine is concise, elegant and doesn't rely on about 50 lines of convoluted, bizarre and unnecessary array based PL/SQL date arithmetic? 
Cheers, I got rid of the instance etc and re-ran the installer. Turns out the solution is to change the credentials *prior* to installing, and not after as the MSDN post suggests. Also, seems to only apply if the user profile is overridden by AD configs. Turns out ours at work is... "messed up".
For the most part, that's pretty much the case, for a developer. The only thing that comes to mind that wasn't in those courses when I first took them, was the use of linked servers, and the syntax behind OPENQUERY. If you're looking for an IT position, you're pretty much selecting and inserting within your code. The difficulty level comes in from reading from older tables that have kept their name and structure, even though the business has changed. Then linking that data to data in a newer server, db, or table. Where I work, loan number can be either numeric, or a varchar 10, and don't have the same column name across tables/DB's. When I worked for the state, there was an old state identification number, that exceeded it's pre-determined maximum. It was 10 digits, and new ones were 15. So some DBA way back when decided to create a "StatePrefix" column in the old table. Whenever you read from 1 table to another, it'd be something like: WHERE (A.StatePrefix) + abs(A.StateID) = B.StateID 
Interesting. Thanks for answering me. Yea my position would likely be an entry-level IT position working with SQL and .NET.
Loads of fun to be had there for sure!
There's no way to do that in straight SQL. You're either going to have to write out the query or write procedural code to generate the select statement from the results of the system table query.
Perhaps this is overkill but also have a look at ETL tools such as Pentaho PDI. You can use a batch file to run jobs you have defined.
Yes and no. Had a very quick look at the site and looks to be comprehensive enough. For entry level its ideal, but just be aware this is a continuous learning process. You know enough to get started now. You will start too look at where to do your SELECTS or INSERTS from. Do you do it in code (.NET) or on the SQL database via Stored procedures. You will find that code (C#) is generally much quicker than running it off the db. Strange that right... or is it? (Check when it is quicker and when it is not) Do you keep the code in the db or in your C#? What is best practice? Think of the next programmer after you... Also, the site does not include "newer" functions that updated versions of MS SQL have, things like RANK OVER PARTITION BY etc. Amazing functions that make my life much easier. (This is now as a data analyst not as a C# coder.) Good luck have fun. 
Learning Java solely for DBA work is like learning logistics management when your profession is diesel repair. Java is a programming language and framework for development. have you ever done any programming?
I had one programming class where I used C+ to make a deli order menu. I switched degrees right after that class ended. I hated programming. 
In my experience (2000, 2005, and 2008) this can only be done with an expensive self-join or with a trigger. 
Here's the expensive way... SELECT TOP (100) PERCENT dbo.emp.employee, dbo.emp.manager, COUNT(emp_1.employee) - 1 AS group_ctr FROM dbo.emp INNER JOIN dbo.emp AS emp_1 ON dbo.emp.manager = emp_1.manager AND dbo.emp.employee &gt;= emp_1.employee GROUP BY dbo.emp.employee, dbo.emp.manager ORDER BY dbo.emp.employee, dbo.emp.manager Sub in your column names as needed.
If you're looking for a simple intro to Java there's O'Reilly [Head First Java](http://www.headfirstlabs.com/books/hfjava/). I do not think Java is frequently used for DBA work, although I'm sure it is possible. You are likely better off with Python, Powershell, another scripting language or whatever SQL dialect is supported by the DB vendor you are working on (ex T-SQL on MS SQL has a lot of proprietary functions for admin work).
 SELECT EmployeeID, ManagerID, ROW_NUMBER() OVER(PARTITION BY ManagerID ORDER BY EmployeeID ASC) - 1 [Order] FROM [...] http://technet.microsoft.com/en-us/library/ms186734(v=sql.100).aspx
That works and is very cool. Heading on over to your link to see if I can understand it. Thanks for the help!
Window functions like `ROW_NUMBER()` and `RANK()` are a bit weird. The syntax is pretty unique. The important part, I think, is that `OVER()` is a clause with a lot of purposes. http://technet.microsoft.com/en-us/library/ms189461.aspx 
Excellent. There's a piece of 2008 I hadn't seen yet. Thank you da_chicken.
This is the shortest path. OSQL in a cmd file.
Well, SQL is still a type of programming, albeit not quite like C++. Are you sure you are actually interested in learning SQL / about DBs? 
Ouch.
I fucking love OVER() and use it every day
SQL is easier to learn and I have already passed one class. I just don't want to learn Java, C+, or C++. I was even excited about learning web design, but Java freaking blows my mind. It makes no sense.
So I can learn oracle instead of Java?
Thanks, I'll try this out today. I have to play with B-Trees and heaps today. He gave us a few programs to play with.
Learn php...you can do so much with it...though it's difficult because C# and Java are the industry standards.
And of course, now I can't figure out how to change the SELECT query into an UPDATE query. How?
You should really study up on the new features in 2005 and 2008. http://technet.microsoft.com/en-us/library/ms170355(v=sql.90).aspx Always, when a new version is released, make it a point to look at the "What's new in xxxx" and look for the T-SQL changes. The link I provided gives you an overview with drilldowns for specific features. The 'PARTITION BY' was introduced in 2005, not 2008.
I jumped this company direct from 2000 to 2008, and I'm admittedly still catching up even now (SQL admin, sysadmin, exchange admin, security manager, and chief bottle washer kind of thing). Thanks for the link!
Hmm we have done this at my last company. We also had crazy hostnames and all of our linked servers used the DNS alias. As long as all relevant machines can all resolve the same DNS alias it should be fine.
That's a good move. My company was on 2000, switched to 2005 in 2009 and wanted to go live with a new product on 2008 that was just released 6 months ago. I pushed for them to go straight to 2012 and skip 2008, and they finally relented. It feels good, man. 
Ding, this was it. Brilliant, thank you. This is a massive query and the execution plan was frustrating. Very much appreciate it and sorry for the late reply.
That is more complicated. Use a self join. UPDATE M1 SET Field1 = {value} [...] FROM MyTable M1 INNER JOIN ( SELECT EmployeeID, ManagerID, ROW_NUMBER() OVER(PARTITION BY ManagerID ORDER BY EmployeeID ASC) - 1 [Order] FROM MyTable) M2 ON M2.EmployeeID = M1.EmployeeID AND M2.ManagerID = M1.ManagerID WHERE M2.Order [...] That works on SQL Server, but it's not standard ANSI SQL. Standard SQL usually doesn't allow joins in an `UPDATE` or `DELETE` query. 
 CREATE TABLE ##t( tablename nvarchar(200) ,col int ) DECLARE @tableid varchar(200); DECLARE C CURSOR FAST_FORWARD FOR SELECT tableid FROM Table_A OPEN C; FETCH NEXT FROM C INTO @tableid; DECLARE @sql nvarchar(5000); DECLARE @tablename nvarchar(200) WHILE @@FETCH_STATUS = 0 BEGIN SET @tablename = N'data_' + @tableid SET @sql = N'INSERT INTO ##t ' + N'SELECT ' + @tablename + N', col ' + N'FROM ' + @tablename; EXEC(@sql) FETCH NEXT FROM C INTO @tableid; END CLOSE C; DEALLOCATE C; SELECT tablename, col FROM ##t; DROP TABLE ##t;
This is too basic for this subreddit.
Thank you! As I was driving home, I was debating about the collection of the data and realized there would have to be a temp table of sorts in the mix and it looks like you had a similar thought. Oh, and syntax, you actually had some that looks correct as mine was off. I'll test it later tonight!
If you don't want to use a cursor, you can also stuff all the select statements into a single dynamic SQL query (and skip the temp table): DECLARE @Sql nvarchar(max); SELECT @Sql = COALESCE(@Sql + ' UNION ', '') + 'SELECT * FROM data_' + tableID FROM TableA EXEC(@Sql) The code above assumes that all the tables can be unioned, but it's easy enough to modify if they can't
That is crazy, I don't think I would have thought about that ever. I'm going to have to do some reading on coalesce. 
COALESCE just returns the first NOT NULL argument in its argument list. This is a SQL server specific kludge for concatenating strings from multiple rows. I've seen a specific name for it, but I'm blanking on it at the moment. You can do the same thing using STUFF and FOR XML, as outlined [here](http://fergusondigital.com/Blog/post.cfm/using-for-xml-path-and-stuff-to-coalesce)
Looks right. Except for that top 1. The OP is looking for one row for each server. When you get a max() and group by all the other cols, you're already getting only one row for "each" of the items in your group by. When you tack on that top 1, you'll only ever get one row with one server. I assume the OP wants to view the whole list of servers, with their max(stat) - so I'd remove that top 1, which also removes the need for all looping.
Ah yea, well spotted. The TOP 1 needs to be removed
&gt; put a little more effort into your spelling and grammar. that's awfully difficult when you're typing with only two thumbs 
Good call. Yes, I posted this on my phone. I have a feeling gsxr_jason may be correct but if I do put in some extra effort on content hopefully that makes the difference.
Assuming you just want to compare two columns, I would first import the CSV file as a temp file. select temp.DP_OBJECT_NAMES , decode(temp.DP_OBJECT_NAMES,oracle.DP_OBJECT_NAMES,'Y','N') from CSV_DP_OBJECTS temp left join DP_OBJECTS oracle on oracle.DP_OBJECT_NAMES=temp.DP_OBJECT_NAMES I'm assuming something like that should work. The decode function will return a 'Y' if there is a match and an 'N' if there isnt a match for each corresponding DP_OBJECT_NAMES record in your CSV temp file.
Or they could use a CASE statement instead: (CASE WHEN TEMP.DP_OBJECT_NAMES = ORACLE.DP_OBJECT_NAMES THEN 'Y' ELSE 'N' END) "EXISTS" The only reason I bring that up is because I've had to re-write sooooooo many DECODE statements in the past couple of months that don't work the way the original authors thought. CASE statements are far easier to think of in IF...THEN...ELSE pseudo code, and actually work that way (for the most part). Unfortunately this is in an application used to pay contractors, so most have been majorly overpaid for YEARS...
You can open a cursor on this query, and each time you fetch the table name into a variable, you can execute a dynamic SQL statement on that variable. Syntax is something like this (I have not tested this, play around with the syntax till you can get it to work, but I wrote a similar program to execute dynamic SQL on refcursors so the idea should work). declare x_exec varchar2(4000); x_table varchar2(4000); x_count int; cx_reg sys_refcursor; begin x_exec := 'select count(REG_NUM) into x_count from :table where REG_NUM like ''12345'''; open cx_reg for... --your query; loop fetch cx_reg into x_table; exit when cx_reg%notfound; EXECUTE IMMEDIATE x_exec USING x_table; DBMS_OUTPUT.PUT_LINE(x_table || ': ' || to_char(x_count)); end loop; close cx_reg; end; /
**Edit**: I stand corrected... Is that null value or empty string? Not sure if that matters. Not enough info at the moment. ~~You can't use a case statement to conditionally change the where clause unless you are building the query dynamically and that presents its own challenges.~~ However, you can validate the variable value prior to running the query. if (@playerid&lt;&gt;'' and @playerid is not null) begin /* Run a particular query */ end else begin /* run a different query */ end 
&gt;You can't use a case statement to conditionally change the where clause That's not entirely true: AND (CASE @FILTERBY WHEN 1 THEN W.F WHEN 2 THEN W.M ELSE NULL END IN (@FILTER)) To be fair, this is used in the select for a dataset for an SSRS report, and @FILTERBY has set values, and @FILTER represents values from another dataset, but the WHERE clause does change dynamically based on the value of @FILTERBY.
What r some of the excellent SQL ebooks in ur opinion?
 DECLARE @PlayerID as INT SET @PlayerID = null SELECT * FROM Player WHERE (@PlayerID IS NULL OR PlayerID = @PlayerID) If you start stacking those kind of conditions up though, it will perform poorly. I fall back to dynamic sql if I have any more than about 3 of those clauses.
That's interesting. Any idea how this might affect the execution plan and thus performance?
I totally agree with this. External tables are amazing, manifesting csv data into queryable tables.
A few thousand columns???
Well, ok. All you are doing is a SELECT colA, colB FROM tableA right? How often is this data updated? 
T-SQL Server 2008
You want this website http://www.seanlahman.com/baseball-archive/statistics/ And the book Baseball Hacks (OReilly) If you get any use out of the work of Sean Lahman, drop him a $1 or $2 for his trouble or at minimum a note of thanks.
Note for anyone reading: "*Better*" isn't just an opinion word, there in the title. A well-normalised oltp database can be a pleasure to work with at an application level, ad-hoc query out of, and ETL out of into your BI consumers' hands. Normalisation isn't just a SQL thing, every well normalised database is an efficient-stored, efficient-find, efficient-clean database. Duplicated data is isolated out, and things have a logical storage structure. Additionally, it is natively balanced for *SELECT*, *INSERT*, *UPDATE*, **and** *DELETE* functions as a product of implementation (if not by deliberate design intent). **Thing to bear in mind:** Higher normal-forms (a later theory lesson I'm sure) can lead to a "many tiny tables" problem. So functionally it can take very many joins to recreate a business-familiar representation of the original dataset. Joins can be expensive at a performance task at runtime, but are **WHY INDEXES EXIST AND YOU SHOULD LEARN ABOUT THEM BEFORE YOU CONSIDER ALTERING YOUR DATABASE STRUCTURE AWAY FROM A NORMALISED PATTERN**. Favour using an index over de-normalisation if you want to improve performance, or subsequent developers, analysts, and business people may grow to hate you for reasons you never appreciated. (However, if you have inherited a database from someone previously in your role, at least have an understanding of _why_ it is patterned in that way before you go about making changes...) **De**-normalisation should only be considered as an option of last resort, you're typically altering the database logical model in favour of optimising for exclusively *SELECT*, *INSERT*, *UPDATE*, **or** *DELETE* functions, but at the cost of **all** of the others.
If you want play by play data, go to retrosheet.org. with some googling you will be able to find some tools people have put together to create csv's out of the data.
Thanks for the offer of advice. I have a question that I struggle with and have seen others struggle with and that is deployment! What are the best practices to keep track of all the changes (DDL) and data changes one has to do as part of the development process? And in a way that allows it to be deployed to an existing system. Schema diffs work but then you have to remember all the stuff you don't want to deploy and there doesn't appear to be a simple way to always filter those out. Data changes too are a hassle and I've resorted to having to write small packages representing an API to the tables; which is probably a good idea anyway but it is more baggage. Finally, the actual deployment even with diffs and scripts seems to be a risky process and I end up running a few changes at a time rather than feeling confident enough to run the script through in one go. And rollbacks seem to be impossible after a certain point so I usually proceed carefully. Duplicating the entire schema along with data seems to be an overkill and hard to automate but is that a realistic possibility for small databases? A lot of questions I know but I'll take any advice at this point!
Check out fine grained auditing , that might help out to track changes during the Dev process 
How often do you use Java at your job, if ever. I know this is an SQL subreddit but I am going to school to be a DBA and am taking a really hard Java class for stored procedures, creating heaps, and stacks of data.
I'm an experienced developer but I've only picked up relational database development rather recently in my career. I don't have any particular questions for you at the moment, but I'd like to know if I'm thinking about any of this wrong. &gt; How do you go about deciding which tables to create? Do you do this youself? I think of a table as a collection of all instances of some type. When I create a struct or value class, I might create a table to go with it, or to replace it. Pointers/references are inverted to become foreign keys. &gt; Is database devleopment the main part of your role, or are you &gt; primarily a (java/.net/whatever) programmer? The latter. Relational databases are another (power) tool in my toolbelt. &gt; What do you struggle with most when writing SQL queries? The typical approach to tackling a complex problem is to break it down into managable pieces and address each piece in relative isolation. I'm still struggling with breaking down complex SQL queries into small enough parts that I can reason about them well. Thinking declaratively is something new. This is bar far the hardest part for me. &gt; Do you get involved in managing database performance? If so, how? I haven't really gotten this far yet. Queries haven't yet been the slowest part of any of my applications, so there's been no need. &gt; Have you taken any courses on database design/development? If so, &gt; what was good/bad about them? Would you take one again? I self taught through a few books and some online articles. 
&gt; I find myself having to break down the query into multiple steps with intermediate temp tables. I hope this is for reporting/data warehousing situations and not OLTP?
Personally I use Oracle's Data Modeler[http://www.oracle.com/technetwork/developer-tools/datamodeler/overview/index.html?ssSourceSiteId=otnru]. In addition to being free to use it allows you make your changes in the tool then compare them against a target database and generate the change script. You can restrict this to just the modified tables, meaning you don't have loads of changes to wade through as you would for a full schema diff. Data changes are messier as they can be time sensitive and affected data differences between live and test. Writing small API packages is a good idea though. Having a duplicate QA/UAT database to test run deployments against is a very good idea to help mitigate the risks you've identified. This is very feasible with small databases as you can generally find sufficient storage. It's when you get to very large databases that you run into issues as businesses are not always willing/able to fund a complete duplicate of live. When it comes to rollback, if you're on Oracle 11gR2+ I would recommend looking at edition-based redefintion. This allows you to create to "versions" in your database so you can "hide" (behind views) table changes to some extent. I've found it more useful when it comes to stored procedures though, as you can have two versions of the code in one database. You can then smoketest changes before putting them live. Another option is to create "guaranteed restore points" prior to release. These enable you to quickly revert the database back to the state of the restore point if the rollout goes wrong. Doing so does require you to shutdown the database, so may not always be a feasible solution if uptime matters. (there is an equivalent in SQL server, but I can't recall what it's called at the moment).
&gt; Data Modeler I'll check that out - thanks. But that just deals with schema changes though, not packages or function or procedures. &gt; Having a duplicate QA/UAT database to test run deployments Yep, I'm coming to that conclusion. For the tiny changes I make, there is no risk of getting it wrong, or if there is then I can fix it quickly but it does seem to be a bunch of work. edition-based redefinition sounds very interesting. Maybe it's just getting a good combination of techniques together. guaranteed restore points is not possible because I'm working in a multi-user environment. Perhaps the new 12c pluggable databases might work. It's no wonder people that don't understand databases get so frustrated with rollouts! There are some I've done where it takes multiple steps to re-remember all the changes they've done! And I'm no expert but it does all seem to be a bit of a crap shoot that requires a great deal of attention and care. Still, the productivity gains are worth the trouble though. Thanks for the tips.
I'm relatively new to the world of Oracle databases and PL/SQL, I've had no proper formal training for Oracle databases, but one of the biggest obstacles I've found is the lack of good documentation and help for Oracle databases! What would you recommend to use for Oracle documentation? It seems that StackOverflow is dominated by MS-SQL devs as well! 
I doubt you can help me here - just needed to rant for a moment. I recently transitioned from an actuarial job with a flavor of database management into a position that's essentially database engineering, but for actuaries. The worst part for me is having a plethora of tables with no documentation as to their source. That way, when something breaks, I wind up tracking down a guy who sends me to Accounting who eventually lets me know how we get that info from the mainframe. A full day process that could be prevented by one paragraph of documentation =(
But just think of all that time I saved bot leaving you any documentation! At least an hour... I feel your pain.
Documentation links for all Oracle's products is available for http://tahiti.oracle.com. Look for "Oracle Database, VER Release X" where VER and X are the verison and release you use. StackOverflow does seem to have an MS bias, but there are several great people who answer Oracle qs there (I've been known to myself ;) Alternatively you could check out the Oracle forums (http://forums.oracle.com) Is there anything specific you're looking for?
I feel your pain too. Unfortunately this is really a business/process problem than a technology one. You can add comments to tables and columns though. If there's any that are particulalry unclear in their meaning you could add these when you figure out what they are.
sqlite (https://sqlite.org/) - perfect for what you need - it's the same database probably being used by the browser you are using and the phone (one db for each app frequently) you have. I believe there are drivers for most languages and the wonderful thing is that the database becomes really just a file (so backup is easy) and there is a simple command line interface.
a cross post from /r/cheatatmathhomework seriously? Do your own damn homework ffs
It is not explicitly banned on the sidebar (for now) so I won't remove this post, but blatant requests for homework solutions are discouraged on this subreddit. I encourage students who are struggling with assignments and are seeking help on this forum to be forthcoming about it being an assignment and to seek additional instruction rather than outright answers. 
I don't completely understand your question because I'm too damn lazy to read it all and you are too damn lazy to ask it the right way. BUT, here is something I can thing about: treat each entity as a sphere and intersection of all spheres is your possible answer. Edit: I could be wrong but I will welcome corrections
If possible, I would use postgresql with the postgis extension. It contains a plethora of spatial functions that will do exactly what you want.
Thanks! I just needed to add a + 0 to the totaldisabilities because the disease columns are bits. Now I will transfer this value to a table to count how many patients have multiple diseases.
Very useful function for deduping too
I started in operations, got decent at sql and took a junior level dev job. Now I'm mid level and have learned a shit load by asking the right people questions. And by breaking things.
If its something you need in several queries, it would be better to a) create a computed column http://technet.microsoft.com/en-us/library/ms191250(v=sql.105).aspx or b) create a view, which fabricates this sum, and reference the view instead of the table. Then when you later add a new illness, you only have to amend the computed column or view, instead of changing all your queries
Sweet cripes, that's an horrible table structure, have you heard of UNPIVOT?
I know right? I am only an intern and they are making me generate reports of these databases. I have heard of UNPIVOT but haven't really used it yet. 
I get the feeling from the lack of detail that OP doesn't know what he has.
That's why I included the line about the Start Menu not being accurate. One of the things I do regularly is install SQL Server for clients and many of them do not like extra things being installed and half of the time they think I've installed it two versions for no reason.
Ok. Great! I looked at a bit so far and it looks like it might work. Thanks! 
I worked in my university tutoring beginner SQL to students. From my experience, the main thing that people really struggled with to begin with is table structures and table design. Most people can learn query language pretty easily, since its always the same and specific parts have specific uses (joins etc.) but people seemed to really struggle with tables since they seemed to think they had no knowledge to relate to it. There is a bit of finesse to structuring data and requires an understanding of the data behind it which a lot of students didn't grasp well. 
I know this isn't a serious thread, but I feel like when this happens, the answer is to build temp tables and index them.
beautiful
Hah. Fascinating. In our office, we used to not reason about long running queries all that well and occasionally leave things running and break our nightly ETL processes.
Unfortunately, this did not do the track. It seems to not like your Inner Join statement (if I take out my temp table that is). It is throwing up a bunch of errors. Msg 207, Level 16, State 1, Line 3 Invalid column name 'SNAME'. Msg 209, Level 16, State 1, Line 3 Ambiguous column name 'ServerName'. Msg 209, Level 16, State 1, Line 5 Ambiguous column name 'ServerName'. Msg 209, Level 16, State 1, Line 1 Ambiguous column name 'ServerName'. Msg 209, Level 16, State 1, Line 5 Ambiguous column name 'ServerName'. Are you certain that I do not require the Temporary table?? Maybe I need more explaining - The purpose of the temporary table is to store all of the Server Names because the Server Stat table is large and I only require a single (max) value for the stat to show its peak over the course of a day. In trying to experiment with what you've given me I've run into some trouble.. Basically -- the first example you shown has helped - but the ServerName is staying static. I want 1 row for each server, and to loop through my list of server names.. So should I create a "counter" column within the temp table?? And loop through the list?
Actually when I wrote the query, I hadn't noticed that ServerStats is the same table you are using for the query, and for the temp table. You definitely don't need a temp table, and Id advise against it because its more Disk I/O, and will result in less concurrency while tempdb schema is being created. Avoid loops and cursors whenever possible. They will always perform much worse than the equivalent set-based operation. You cant always avoid them of course, but in this case you can. Why not just do... SELECT MAX(cast(StatValue as int)) as 'CPU Utilization', DateStmp as 'Time', ServerName as 'Server Name' from dbo.ServerStats where StatName = 'cpu_util' and DateStmp between '2014-04-04 00:00:01.000' AND '2014-04-04 23:59:59.999' GROUP BY ServerName, DateStmp ORDER BY ServerName, 1 DESC;
The reason I don't do this is because it returns every Stat Value for Every Timestamp - and duplicate Server Names. I only want one value - the peak - which is why I am sorting and selecting the top row. The goal would be to have this automated by using the looping and temp table - so that if anyone were to add a new Server into the mix - I wouldn't have to add any lines of code. The reporting would be much more flexible that way. Top 1 gives me exactly what I need. But it won't give me every distinct Server along with it. Thats why I am trying to build my list, get the stats, and then report on it.
It's all fun and games until someone pulls out the 'EXCEPT' function when querying national healthcare databases.
http://sqlfiddle.com/#!2/464533/30/0 SELECT `posts`.`title` FROM `posts` WHERE `posts`.`new` = 1 AND `posts`.`title` NOT IN ( SELECT `posts`.`title` FROM `posts` INNER JOIN `blacklist` ON `posts`.`title` LIKE CONCAT('%', `blacklist`.`value`, '%') ); 
What is the Primary Key on ServerStats table? And if one server recorded 99% cpu on two separate dates, which one would you want it to show? 
Date, ServerName, are both PK's.. The goal is to run this as a standing report - the data that feeds into the database could down the road add server names. The between dates would a static query (date - 1, or something so that every day it would report previous days stats). I just can't seem to get it to work how I'd like.. I've gotten closer.. SELECT MAX(cast(StatValue as int)) as 'CPU Utilization', ServerName as 'Server Name', Date from dbo.ServerStats where StatName = 'cpu_utili' group by ServerName , Date order by ServerName; But this brings undesirable results: 2 xxxx 2014-04-22 15:50:06.000 3 xxxx 2014-04-22 16:20:06.000 2 xxxx 2014-04-22 17:20:05.000 3 yyyy 2014-04-22 16:20:06.000 2 yyyy 2014-04-22 17:00:07.000 2 yyyy 2014-04-22 15:50:06.000 1 zzzz 2014-04-22 15:50:06.000 It was suggested to me to use the Date without the time, so to Format the date so that Im returning only that day - I am going to research that a bit. (THANK YOU SO MUCH for having responded and been kind enough to help me through this! I really appreciate it Carpii!)
I just don't understand why SQL makes this so tricky... It seems super basic in terms of steps, but the execution/logic is throwing it off.. #1 get my list of server names #2 step through servers names, grabbing top row from each. Once you throw in a few constraints like date, everything gets out of whack. I just wonder why it is causing so much frustration.. Probably because there are so many avenues I could approach this from.. I was wondering, is there a way where a subquery would do the trick? I just can't get my gears working..
Thanks for sharing Andrela, that's interesting. Do you mean they weren't able to identify which attributes (columns) should be placed on which tables? Did you cover normalization at all?
See if this is any help? SELECT MAX(cast(StatValue as int)) as 'CPU Utilization', DateStmp as 'Time', ServerName as 'Server Name' from dbo.ServerStats s where s.StatName = 'cpu_utili' and s.DateStmp &gt;= '2014-04-22' AND s.DateStmp &lt; '2014-04-23' and s.StatValue = (select MAX(a.StatValue) from ServerStats a where a.ServerName = s.ServerName and a.DateStmp &gt;= '2014-04-22' AND a.DateStmp &lt; '2014-04-23') GROUP BY s.ServerName, s.DateStmp ORDER BY s.ServerName, 1 DESC; Note there are two places you need to specify the dates now, instead of one. I've also ditched BETWEEN in favour of using &gt;= and &lt; which is more accurate. 
Your best bet in this case would be dynamic SQL I'd say. Be sure to use sp_executesql instead of just contatinating the sql command and then exec(@sql), its parameterized which is ... well... better, for many reasons. I would not do that in the Frontend, but that might just be me, I've been burned by "the frontend handles that" to often... Freetext searches don't really fit this usecase I'd say, since you have a dynamic list of search parameters and fields to be checked. This is usually handled by dynamic sql, and concatinating the where clause to fit the supplied parameters
The problem is really the GROUP BY In order to display date field, it has to be in the GROUP BY. But because its being grouped, you're getting 1 distinct row for each Server/Date combination. My query above filters this further to just show the dates which match the top Stat for that server
They simply couldn't grasp relationships between data. This was to begin with, by the time I'd gotten to normalisation they were ok with the concepts but it was definitely one of the major stumbling points and took a long time before everyone got it. I think they couldn't really get a sense of how data is stored, not many of them had done programming or anything like that before so it was a real struggle to get these concepts across. 
Really odd.. I mean, as I stated in a reply (prior to reading this), I am finding the StatValue returned to be incorrect. for instance.. the results from your query look like this: SELECT MAX(cast(StatValue as int))as 'CPU Utilization', ServerName as 'Server Name', TimeStmp from dbo.ServerStats s WHERE s.StatName = 'cpu_utilization' and s.TimeStmp &gt;= '2014-04-22 00:00:01.000' AND s.TimeStmp&lt; '2014-04-23 23:59:59.999' AND s.StatValue = (select MAX(a.StatValue) from dbo.ServerStats a where a.ServerName = s.ServerName and a.TimeStmp &gt;= '2014-04-04 00:00:01.000' AND a.TimeStmp&lt; '2014-04-04 23:59:59.999') GROUP BY s.ServerName, s.TimeStmp ORDER BY s.ServerName, 1 DESC; 9 xxxx 2014-04-22 00:10:07.000 9 xxxx 2014-04-22 00:30:05.000 9 xxxx 2014-04-22 12:30:06.000 9 yyyy 2014-04-22 12:40:06.000 9 zzzz 2014-04-22 12:40:06.000 SELECT TOP 1 MAX(cast(StatValue as int)) as 'CPU Utilization', TimeStmp as 'Time' from dbo.ServerStats where StatName = 'cpu_utilization' and ServerName = 'xxxxxxxx' and TimeStmp between '2014-04-22 00:00:01.000' AND '2014-04-22 23:59:59.999' GROUP BY TimeStmp ORDER BY 1 DESC; 32 xxxx 2014-04-22 00:10:07.000 
What data type is StatValue ? You're casting it to an int, which makes me think it might be a varchar in the table. If so that would explain a lot of performance problems But probably youd have to also CAST it to int in my queries (both the s.StatValue and the MAX(a.StatValue))... SELECT MAX(cast(StatValue as int)) as 'CPU Utilization', DateStmp as 'Time', ServerName as 'Server Name' from dbo.ServerStats s where s.StatName = 'cpu_utilization' and s.DateStmp &gt;= '2014-04-22' AND s.DateStmp &lt; '2014-04-23' and CAST(s.StatValue as int) = (select MAX(CAST(a.StatValue as int)) from ServerStats a where a.ServerName = s.ServerName and a.DateStmp &gt;= '2014-04-22' AND a.DateStmp &lt; '2014-04-23' AND a.StatName = s.StatName) s.DateStmp = (select a.DateStmp from ServerStats a where a.ServerName = s.ServerName and a.StatValue = s.StatValue and a.DateStmp &gt;= '2014-04-22' AND a.DateStmp &lt; '2014-04-23' AND a.StatName = s.StatName order by a.StatValue DESC LIMIT 1) GROUP BY s.ServerName, s.DateStmp ORDER BY s.ServerName, 1 DESC; Also I've amended this query since it was not limiting it to the same StatName in the sub queries. This could explain the 0 rows you were getting. My data only used 1 stat name which is why I wasn't seeing the issue. 
www.wordsforthat.com
Bingo - you were right... Thanks alot Carpii! I am going to have to have a talk with some DBA's... son's of guns giving me junk data!
 DECLARE @Addr1 varchar(100), @City varchar(50), @State varchar(2), @Zip varchar(10); SELECT * FROM Address a WHERE (a.Addr1 LIKE @Addr1+'%' OR @Addr1 IS NULL) AND (a.City LIKE @City+'%' OR @City IS NULL) AND (a.State = @State OR @State IS NULL) AND (a.Zip LIKE @Zip+'%' OR @Zip IS NULL);
Another example that may be more efficient considering the join in the above subquery: SELECT `posts`.`title` FROM `posts` WHERE `posts`.`new` = 1 AND NOT EXISTS ( SELECT 1 FROM `blacklist` WHERE `posts`.`title` LIKE CONCAT('%', `blacklist`.`value`, '%') ); This article explains some different MySQL/MariaDB scenarios as well: http://explainextended.com/2009/09/18/not-in-vs-not-exists-vs-left-join-is-null-mysql/
Sounds like from the other comments the whole point is to get each servername? We don't need a loop or a temp table for this, the values already exist in the table we are starting with. I think it's simpler than you are making it. If my answer below doesn't work, please provide an example of what ur data looks like when u start and what u want it to look like when u are finished. Try this below (without temp table): select serverName, max(statvalue) as CPU_Utilization, max(DateStmp) as Server_time from dbo.ServerStats where lower(statname) = 'cpu_util' and datestmp &gt;= '2014-04-04 and 'datestmp &lt; '2014-04-05' group by serverName; I am not sure if there is anything else you are trying to achieve, but just pulling top based on server name should give different results every time. What I wrote above is the max stat_value and date stamp per server name. Which it sounds like is what you want. I set statvalue and datestmp to their max so u can group by the servername making ur results distinct on server name. I assume you don't care about the othe two fields being a specific value since u were originally just taking the 'top 1' which can produce wildly different results for those two columns each time you run it. EDIT: You stated you are using the tmp table because the table is large. How big is large, and what is the primary key? Are there any indexes currently? If my query above causes performance issues. You could try to index your original table on server_name, if that is an impossibility, then make a copy of with only the columns you need and index that on servername.
what you need is the having clause, I don't lke to give out solutions streight up, it doesnt teach as well as doing it yourself. If you need more help, ask... 
YAY temp tables! 
Get the high scores by country in a subquery, join to the scores table again on country.
This was my initial thought from previous programming courses however we have not yet been taught sub queries, so in theory should not need one. EDIT: I have got it working with a sub query... SELECT sc1.name, topscores.country, topscores.country_highest FROM scores sc1, (SELECT hs.country, MAX(hs.score) AS country_highest FROM scores hs GROUP BY hs.country HAVING MAX(hs.score) &gt; 10 ) AS topscores WHERE sc1.country = topscores.country Thanks for the idea.
the Where clause will filter as the rows are gathered. The Having on the other hand will filter afterwards. That enables you to use aggregates in the having, for example : SELECT id FROM Table t HAVING t.column1 = max(t.column1) In the where clause you can not do that. Obviously since the Having does not reduce the rowcount your recordset as it gets returned by the joins and indexes, the WHERE is much better for performance.
Thanks for your answer. But I get the same result. I guess because there are several rows where INSTR(p.title, b.value) &lt; 1 doesn't give a result. If I remove the distinct and add b.value to the select, then I see every title multiple times with every value of the blacklist table (except the one where the blacklist is like the title). 
Thank you, that one worked very well! :)
I see where the having is better, I actually was getting the wrong results (missing some players). So I can move: WHERE hs.score &gt; 10 to HAVING MAX(hs.score) &gt; 10 However I am not sure how to get the high score back into the country, without a sub query (which we have not yet been taught). EDIT: I have got it working with a sub query, I can only think of doing it this way! SELECT sc1.name, topscores.country, topscores.country_highest FROM scores sc1, (SELECT hs.country, MAX(hs.score) AS country_highest FROM scores hs GROUP BY hs.country HAVING MAX(hs.score) &gt; 10 ) AS topscores WHERE sc1.country = topscores.country Is there a simpler way I am missing.
SELECT TOP 3 ....
my guess is that you would want the score on the test in the select as well, to compare the individual score against the top score of the country, otherwise the query is a bit useless. To answer your question, it depends on the DBMS you use. On MSSQL you can use "window aggregates", in essence aggregate functions that are grouped by individual expressions. Let me give you an example, much easier : /*sample data*/ DECLARE @scores TABLE (score INT, country VARCHAR(10), NAME VARCHAR(10)) INSERT INTO @scores SELECT SCORE = 19 , COUNTRY = 'UK' , NAME='tom' UNION SELECT SCORE = 25 , COUNTRY = 'UK' , NAME='tom' UNION SELECT SCORE = 85 , COUNTRY = 'UK' , NAME='tom' UNION SELECT SCORE = 64 , COUNTRY = 'UK' , NAME='Mike' UNION SELECT SCORE = 40 , COUNTRY = 'Italy' , NAME='Mario' UNION SELECT SCORE = 100 , COUNTRY = 'Italy' , NAME='Luigi' UNION SELECT SCORE = 57 , COUNTRY = 'USA' , NAME='Joey' /* your query */ SELECT sc1.name,sc1.score, topscores.country, topscores.country_highest FROM @scores sc1 INNER JOIN ( SELECT hs.country, MAX(hs.score) AS country_highest FROM @scores hs GROUP BY hs.country HAVING MAX(hs.score) &gt; 10 ) AS topscores ON topscores.country = sc1.country /* example for window aggregates*/ SELECT sq.name , sq.country , sq.score , score_to_beat = CASE WHEN sq.country_max &gt;10 THEN sq.country_max ELSE NULL END , personal_best = sq.personal_max FROM ( SELECT sc.name , sc.score , sc.country , country_max = MAX(sc.score) OVER(PARTITION BY sc.country) , personal_max = MAX(sc.score) OVER(PARTITION BY sc.name) FROM @scores sc )sq Note that the 2 aggregates for country_max and personal_max are grouped by a *different* field. You could not do that in the group by clause, you would have to write subqueries to do that. On my example you could remove the subquery and write the case expression this way : case when MAX(sc.score) OVER(PARTITION BY sc.country) &gt; 10 then MAX(sc.score) OVER(PARTITION BY sc.country) ELSE null END however, it would not make a difference in how the query gets executed, and its a bit ugly to look at, hence I would use a subquery here. 
I guess I should rephrase. (I'm not overly proficient in Access, I prefer SQL Server lol) This is from an Excel Spreadsheet that was put into Access to SQL Query the data. The problem I'm facing is the one sheet has Multiple rows for each company, Containing the same information except for 5 fields referring to contact information primary, I'm looking at the Contact Type field. I want to limit the Field using an In Operator (Which I have done), however I want to limit the number of results the In Operator provides based on the Company, however - I feel like I'm at a loss. I could use the TOP 3 but from how I was thinking of using it, it only returns the top 3 records for the entire sheet. Essentially, MS Access newbie looking for some guidance.
I don't think Access supports window functions or CTEs so you'll have to do each query separately and union them together I think. If it did you could do this: WITH cte as ( SELECT Stuff , ROW_NUMBER() OVER (PARTITION BY Company ORDER BY Something) AS Num FROM YourTable WHERE Company IN ('blah', 'blah2', 'blah3') ) SELECT Stuff FROM cte WHERE Num &lt;= 3 
You need to join product and vendor on vendor code to avoid pulling a Cartesian product. 
Indeed, you need to join the tables VENDOR and PRODUCT and secondly I think you mean to SUM() and not COUNT() the quantity on hand. 
you need a where clause to join your two tables. This expresses how they relate to eachother. SELECT v_name, count(p_qoh) FROM vendor, product WHERE vendor.v_code = product.v_code GROUP BY v_name; and because I am sure someone is going to bring it up, here it is again in ANSI join syntax: SELECT v_name, count(p_qoh) FROM vendor JOIN product ON vendor.v_code = product.v_code GROUP BY v_name;
Thank you so much guys! The nudge was exactly what I needed to understand. And great job catching my mistake of trying to use COUNT instead of SUM. ____ Here is how I did it: SELECT v_name, sum(p_qoh) FROM vendor v, product p Where v.v_code = p.v_code GROUP BY v_name;
You were absolutely right
Thank you. This was incredibly helpful, as I learn by best reverse engineering or breaking down existing script. I commented with my [final script](http://www.reddit.com/r/SQL/comments/23ry9d/oracle_question_regarding_count_with_grouping_data/cgzzu9n).
Can you show the output of actual execution plan? I'm have some suspicions about divergence in the execution plans prior to the aggregate step.
You can use Database Diagrams from Sql Server management studio to define tables, joins, and indexes.
Glad to see you got it working. I was going to add aliases like you have but I didn't want to change the focus if you had not covered that yet.
Probably Java.
Tough question. I suppose it depends on what you want to do. Python was already a great choice, as there are a lot of data analysis/machine learning packages for Python and Python has a good web framework (Django). What is it that you want to do?
I was learning it as I went. All makes sense to me now. I appreciate the help. You guys are awesome.
Python + JS
A few things: a) In SQL (Oracle at least) you must always SELECT &lt;x&gt; FROM &lt;y&gt;, you cannot simply select something out of thin air. So your first attempt at a query should technically work if you simply add "FROM DUAL" at the end (DUAL being a "dummy" universal table that holds no values and returns a single row) b) when you use the LIKE operator you usually add at least a percent (%) sign in the string literal that you're using so Oracle knows how to match that string. If you use LIKE 'TIES' it will return only the rows where the TYPE is exactly 'TIES' (case sensitive). c) if your report consists of a fixed number of columns that you want to hard-code (such as Qty of Ties, Qty of Shoes etc) and your result is a single row (which means you're not grouping by anything) then you could try something like this, so that you go only once through the ASSETS table: SELECT SUM( (CASE WHEN UPPER(a.TYPE) LIKE '%SHOES%' THEN a.QTY ELSE 0 END) ) as Qty_Of_Shoes, SUM( (CASE WHEN UPPER(a.TYPE) LIKE '%TIES%' THEN a.QTY ELSE 0 END) ) as Qty_Of_Ties FROM ASSETS a;
I've added the below line to the second query: WHERE ROWNUM &lt; 2 This has solved my problem, but it doesn't really satisfy me. I just feel there's a better way to do it. 
What is it I want too do, such a tough question. I wish i could say I was a stary eyed dreamer with a big plan, but really, I just know, I am good at programming, I know SQL and like working with data, and most importantly I want to make $$$. I guess my main goal is to make myself the most marketable in the long run. For background, I work now about 70% SQL and 30% VBA/Excel, i am looking to make the next step in my career and most places want me to get about 1 year more Exp, so i figure let me tack on another language while i do that. It will likely be in the medical industry. 
Well. I would say if you want to go down the data analysis route, I would learn R (which basically means learn stats.) If I was going down the business intelligence route, I would learn the existing technology out there as well as Ruby/Python and Javascript to do things like d3 visualizations. If you want to go down the application development route and you want to stay in the healthcare space, I would say learn Java. Where are you geographically located and what is your current gig?
ok, I will try tomorrow when I come back to work, but it will be veeeeery big
When you say SQL Developer, is it MS SQL Server or another one? If it's MS SQL Server, then I say if you want to become more marketable, the .NET languages such as C# will probably be the best route, as usually when companies use 1 Microsoft product, then tend to use others as well.
I am a developer in the medical industry. C# or Java would be your best bet. I'm a .net developer and you can have my Visual Studio when you pry it from my cold, dead fingers, so I'm a little biased in that direction. A good developer in either of those languages with good SQL skills is very employable and will continue to be so for a good long while. 
Great! I'll have to try your second suggestion. The report is only taking about 2 seconds to complete, but faster is always better. I'm actually drawing from a join between two large tables and using more/different qualifiers than "LIKE 'TIES'." But your point is noted. 
Right now, I am based out of So Cal, USA. I used to work for a medical company but followed the $$ to a web based company that is getting old now, I was hoping for an IPO but that doesn't seem to be happening. So, I figured with my past EXP medical it would be the best get back into it and with how things are going will be lucrative for the rest of my life, well at least until stem cell research and 3D printers make us all immortal. 
Yeah, I guess there is no perfect answer although it seems java is just staring me in the face every were I look. 
What should the desired output look like?
Can you add a top 1? (Not an oracle guy) or some wonky FROM like: FROM ( SELECT 1 AS FOO ) A ?
Yes, SQL Server 2012 is the main thing I use, but also, SSRS, SSIS, SSAS. I have some links too going to to Excel files so people can push data into the server without having to make a portal or anything. I have never learned anything of the C languages, are they similar at all to VB/VBA? Not that VBA is a real language but i basically mastered it and have dabbled in VB.
Just a quick roll up of quantities for 13 types of assets. 13 columns with one row. People before me were running a detailed report for each type, then exporting to excel to sum up the quantities for each, then taking those sums and putting them into a chart. With this, I'm just getting the quantities up front without the unnecessary detail. 
/u/LittleRedDot showed me "FROM DUAL." I think that's in line with what you're saying. 
The query that /u/littlereddot provided is exactly what you want, you just have to add the remaining types. This would be an excellent use case for [PIVOT](http://www.oracle-base.com/articles/11g/pivot-and-unpivot-operators-11gr1.php) but you have to manually pivot the data because you are using the LIKE operator. 
How big is the table of companies? And is this a one time query?
If you need totals or sub-totals then you could just use GROUP BY ROLLUP and get those totals along with the details. SELECT a.type, SUM(a.qty) as Total_Qty FROM Assets a GROUP BY ROLLUP (a.type);
If you have a test database (and I'm assuming its a SQL backend). You can put a field value on a claim of "bootybootybooty" for example and run the following. I usually use this for database remapping purposes but it can identify the table and column where a value exists. Don't forget to set the @minimumlength if you know that the field can contain up to so many characters this will greatly limit how many tables it must query. SET NOCOUNT ON DECLARE @sampledata as nvarchar(1000), @minimumlength as int, @db_name as nvarchar(100), @table_schema as nvarchar(100), @table_name as nvarchar(100), @column_name as nvarchar(100), @SQL as nvarchar(4000) --Sample data is sample ID codes you are looking for SET @sampledata = '%bootybootybooty%' --Minimum Length looks at columns with a length this value or higher SET @minimumlength = 15 PRINT 'Your sample data is ' +@sampledata IF OBJECT_ID('tempdb.dbo.#TablesWithRef', 'U') IS NOT NULL BEGIN DROP TABLE #TablesWithRef END IF OBJECT_ID('tempdb.dbo.#TablesOfType', 'U') IS NOT NULL BEGIN DROP TABLE #TablesOfType END CREATE TABLE #TablesOfType (db_name nvarchar(256), table_schema nvarchar(256), table_name nvarchar(256), column_name nvarchar(256), processed int) CREATE TABLE #TablesWithRef (db_name nvarchar(256), table_schema nvarchar(256), table_name nvarchar(256), column_name nvarchar(256), numofrecs int) INSERT INTO #tablesoftype SELECT Table_Catalog, Table_Schema, Table_name, Column_name, 0 FROM information_schema.columns WHERE data_type in ('char','varchar','nvarchar','nchar') and CHARACTER_MAXIMUM_LENGTH &gt; @minimumlength While (Select COUNT(*) from #tablesoftype where processed = 0) &gt; 0 BEGIN SELECT TOP 1 @db_name = [db_name],@table_schema = table_schema,@table_name = table_name,@column_name = column_name from #tablesoftype where processed = 0 SET @SQL = 'INSERT INTO #TablesWithRef SELECT ''' + @db_name + ''',''' + @table_schema + ''',''' + @table_name + ''',''' + @column_name + ''', COUNT(['+@column_name+']) from [' + @db_name + '].[' + @table_schema + '].[' + @table_name + '] WHERE [' + @column_name + '] like (''' + @sampledata + ''')' PRINT @SQL exec (@SQL) UPDATE #tablesoftype SET [processed] = 1 WHERE [db_name] = @db_name and table_schema = @table_schema and table_name = @table_name and column_name = @column_name CONTINUE END SELECT * from #tableswithref where numofrecs &gt; 0 ORDER BY NUMOFRECS DESC, TABLE_NAME ASC 
I played around with kiwid SQL Fiddle and modified my answer above. 
English
I wouldn't recommend C++ unless you were planning on doing something with 3d rendering or specific peripheral interaction. That's like saying ,"I'm going to sit down and read something. How about the dictionary? It has lots of words!" If you know python, stick with python. You can build applications, web based applications and it has a large support base. Really the question should be ,"What flavor of SQL server should I compliment my python application?"
Right now I am using SQL Server 2012 if that is what you mean, but that is not a choice I have. It is what my company uses, not sure what the next company will use. If I misunderstood please explain. 
From what I've heard, C# is very much like Java, and not as difficult to learn as C++ or C may end up being. 
C# is very similar to the C++ and Java. If you're more familiar with the VB stuff you can learn VisualBasic.NET which is the .NET version of the VB stuff. But not as many companies use it as much as C#. You can use VB.Net to get your feet wet with the whole world of .NET and once you feel more comfortable to start switching to C#, because then it's just a language difference.
I am a little confused with question 1 to 2 ... In one we create a list of assets that have both a contract RoleID of 3 and 7. But in 2 it is asking to filter the list down to those with only RoleID of 3. But the list only contains those with both? Should part one be "OR" not "AND" ? 
How about the geospatial side of SQL? I find Postgis a very helpful extension to PostgreSQL. 
That's a solid plan, but another option is MDX and/or DAX. If you can think multi-dimensionally it will make you a better database developer. 
Use C#. It has native support for using SQL Server extremely easily, besides being, in my opinion, the best mainstream language around bar none.
Yea, I know row by row isn't the best approach, or even necessarily a good one, but I'm somewhat constrained by legacy crap that I can't change. Still, this isn't my area of expertise, and I've not used Table-Valued Parameters. So the basic idea is that there's a stored procedure who's purpose is to rate a product we sell. It takes a handful of parameters describing the product, and spits out the rate. I can't change it, and I need to see what it says about 600,000 some odd products. Would these Table Valued parameters help me in that instance, or is there really any way to do that without row by row?
Nope, TVP won't help, that requires changing the procedure. It's possible you have a way out, but I don't know what it is. You've been set up to fail, your only chance is to get lucky. If you PM me your procedure code I MIGHT be able to help.
The goal of 1) To find the True Owners that correlate to Recorded Owners. The goal of 2) To find other properties with Recorded Owners that Correlate to a True Owner (through #1) but is missing (it has to be manually added). There is no key between Recorded Owner and True Owner so there will be times that Recorded Owner = Fake.LLC and Matches a True Owner = Real.corp. But, not all the Properties that SHOULD have Real.corp listed are showing Real.corp, but at least are showing Fake.LLC. Hope that makes sense. 
Instead of using IN could you instead use an inner join to a top 3 sub query.
SQL Developer query builder ftw
I'll add that I recently learned that when you create the table first you can add PK and indexing quicker then doing the select into and adding those fields after. Not an issue unless you're dealing with a large record set (millions).
I hate people like you.
To be clear 'A' is a variable defined as an integer . Another sql statement selects an auto key value(produced by a sequence command) and the goals is to run the SQL anytime the return is a null value on the auto key value. The SQL statement creates a record on a particular table with certain values pulled from various tables, all related to each other through various auto key (sequence) values. 
Does your process insert rows into a table from which you are querying for your cursor? I've witnessed odd side effects from doing so in the past; it could be your problem. Also, try staging the new rows in a temporary table, rather than inserting them directly into the destination table one at a time. After you have completed running through the cursor, then insert all of the new records all at once into the destination table. This will greatly reduce the number of index updates and (potentially) fragmentation on the destination. 
Do you feel better after making that comment? 
No, the destination table is a brand new one just for the purposes of holding these results. That also means there's no indexes on it as of yet, and I'm not doing anything to prevent fragmentation. It should be able to insert wherever it likes, just as if it were a temp table. In any case, we're into the regime where I can just wait it out now, so the question has become academic.
Maybe try count(1) instead of count(*) to eliminate possibility of null values?
Brio will do what your looking for I think.
&gt;uses a cursor I think I found your problem. Look, I don't know what operation it is you're doing to each record, but the whole concept of database design is to do things in sets, not record-by-record. You should not be doing record-by-record unless there is absolutely no way to do it otherwise, and if you're doing 1000s of records per minute, you need to step back and do it right. 
It could be a page-split problem. Look up fill factor, and change the table to have maybe a 30% fill factor. More seriously, I'd change the stored procedure into a function. Then use the function to do a t-sql solution. Eg. you are doing it like : Call spoc with values a, b, c, d; sproc returns x. then you insert a, b, c, d, x into the table. Instead do something like Insert into table (a, b, c, d, x) Select a, b, c, d, function foo(a, b, c, d) from table foo The function returns x, takes parameters a,b,c,d. That way you could do all of it at once - inserting all records at once instead of one at a time. This may sound like work, but let's say you find a band-aid that makes it limp along a while longer. Later you data volume *will* increase, and you'll have to do this anyway. On edit, you don't need to change the stored procedure, you could just build a function, copy/paste the logic inside the stored procedure, make a few syntax changes, and hey presto you're golden and you haven't touched the original sproc. 
You're not wrong. The thing of it is that this is a test of a procedure that is intended to run one business transaction at a time. In production it needs to do five or ten transactions per minute tops. Seems like the answer is that there's really no good way to simulate us doing three month's worth of business in a few hours without changing our rating procedure such that it invalidates the test. That's fine, I was just hoping for a miracle.
nothing changes
here is execution plan for [count](https://dl.dropboxusercontent.com/u/2167742/count.sqlplan) and for [select](https://dl.dropboxusercontent.com/u/2167742/select.sqlplan)
Could you repost the select? The select link is also for the count query.
count(*) is always record count. count(column) doesn't count nulls.
done. try now
Which database are you using?
that's like if I asked you why my car didn't start this morning. A bit hard to diagnose I would say. 
How would the code in "C" look if I have several conditions when pulling which quantities to sum? One of my select statements looks more like this: SELECT ( SELECT SUM(ASSETS.QTY) FROM ASSETS, STORES WHERE ASSETS.STORE_ID = STORES.STORE_ID AND ASSETS.ASSET_TYPE IN ( '12345' , '98765' ) AND ASSETS.SEQ_ID &lt;&gt; ASSETS.DELETE_FLAG AND STORES.SEQ_ID &lt;&gt; STORES.DELETE_FLAG ) as Qty_Of_Shoes 
What's your goal ? Do you have a more concrete - and also correct, your column names/content don't match - example ? What do you mean by "combine" ? You want to store the combination in a table (bad practice, look up for normal forms), or you want this in the result of a query (very simple sql, look up for inner joins) ?
It depends. Are these conditions common for all the subqueries you use in your select? ASSETS.STORE_ID = STORES.STORE_ID AND ASSETS.ASSET_TYPE IN ( '12345' , '98765' ) AND ASSETS.SEQ_ID &lt;&gt; ASSETS.DELETE_FLAG AND STORES.SEQ_ID &lt;&gt; STORES.DELETE_FLAG
The ASSET_TYPEs will change. In another similar report I will be doing the same thing, but adding another condition that will change: AND STORES.REGION IN ( 'AA1' , 'AA2' ) The REGIONS will change, so I can break the summed quantities down by region as well. Everything else will stay the same.
sounds like the UPDATE statement was missing the correct WHERE condition hard to tell because of the lack of actual information 
No problem, this is the right place to ask ! I won't do homework, but I'll always give pointers to the solution. Don't hesitate asking :)
You can plug the conditions that change in the CASE statement and keep that ones that don't in lower the JOIN/WHERE clause: SELECT SUM( (CASE WHEN a.asset_type IN ( '12345' , '98765' ) THEN a.QTY ELSE 0 END) ) as Qty_Of_Shoes, SUM( (CASE WHEN a.asset_type IN ( '54321' , '56789' ) THEN a.QTY ELSE 0 END) ) as Qty_Of_Ties FROM ASSETS a JOIN STORES s on a.store_id = s.store_id WHERE a.SEQ_ID &lt;&gt; a.DELETE_FLAG AND s.SEQ_ID &lt;&gt; s.DELETE_FLAG
Off the tip of my head 1) you're inserting too many rows 2) the where clause on your update is broken 3) the join on your select is broken But you haven't given very much to go on. 
The filter on row_number() over (partition by appid order by userid) = 1 is filtering out two additional records. The only explanation I can imagine is a tough one to follow, but here it goes. So I'm guessing the combo appid and userid is not unique. The other part of the filter is to take data from ActivityContacts not AppointmentToFund. So my theory is that a couple of the same appid/userid have data in both ActivityContacts and AppointmentToFund. The count execution plan does some right outer joins where the select execution plan does some left outer joins which are equivalent, but have subtle effects such as data being in different order. In this particular case, this has the side effect of AppointmentToFund records getting a row_number of 1 in the count plan where in the select plan an ActivityContacts record gets a row_number of 1 instead. This would cause additional records to be lost by the count. A solution/test would be to filter ActivityContacts/AppointmentToFund before filtering row_number.
you right! I wrote [here](http://www.reddit.com/r/SQL/comments/23rw6r/ms_sql_strange_behavior_of_count_with_like/ch0tb6b) that we have fixed this issue with changing ROW_NUMBER. But I just accidentally guess this solution, and you explain why it is working this way. Thank you, you must be the master of execution plans :D
An interesting question, and a good solution. If the pieces of the row_number (partition by and order by) represent unique rows, you are guaranteed consistent results.
This is not clear at all and probably why you haven't had any replies. How does A get set in particular from Delphi? Anyway, I'm making lots of assumptions, because you haven't given a concise explanation, but my guess is you've declared A as an integer, but it has to be a variant in Delphi, because integers don't support null. An integer represents null as 0, which will fail your check. Your Delphi code should be something like this.... var A : variant; begin {some code} A := MyDataSet.FieldByName('AFIELD').AsVariant; {some more code} if VarIsNull(A) then.... 
Check out the ~~USER_NAME()~~ `ORIGINAL_USER()` system function.
I'm trying: SELECT SUM( (CASE WHEN a.REGION = 'SOUTH' AND TYPE in ('12345', '67890') THEN a.QTY ELSE 0 END) ) as SOUTH_SHOE_QTY FROM MERCH_WAREHOUSE a WHERE a.AS_OF_DATE LIKE '01-DEC-10' And I'm getting the error: "ORA-00904: "A"."QTY": invalid identifier"
This is getting out of my comfort zone here which is a good thing :) Is it something that can be put into a trigger to copy off at the time of action?
I too am interested in hearing from anyone that's done this. We currently have an SQL Server 2008 R2 install with 2x 8 Core Intel Xeons and 64 GB of RAM - we'd like to see about spinning a second instance of SQL Server on the same machine to give selected users SQL-runtime access without allowing them direct access to the production DB (if this is even possible).
Yes. Try this to get an idea of how it works: SELECT ORIGINAL_USER() 
My situation is the exact same scenario
Replace a.QTY with whatever quantity field you have in that table. Also, date fields don't work like that. If AS_OF_DATE is a DATE field you'd be better advised to use WHERE a.as_of_date = to_date('01-DEC-10', 'dd-MON-yy')
I believe by default, the maximum memory for a SQL instance is set to an insanely high number. Basically, so it always uses as much memory as it can. I had to configure both instances to a finite amount, because I thought if I left both of them at the default, it would cause problems.
Are the columns text/char type? You have to be careful with linebreaks when you export-import or excel will have them all jumbled up. 
Here is a good article about memory usage from Johnathan Kehayias [link](http://www.sqlskills.com/blogs/jonathan/how-much-memory-does-my-sql-server-actually-need/) Using his formula on the first line, you can figure out how much to leave for the system, then allocate the remainder how you see fit to the two instances. &gt; reserve 1 GB of RAM for the OS, 1 GB for each 4 GB of RAM installed from 416 GB, and then 1 GB for every 8 GB RAM installed above 16 GB RAM 
Correct, it is set to the upper limit of an INT by default. Leaving this value will cause the poor performance as documented [here](http://blogs.msdn.com/b/sqlsakthi/archive/2011/03/12/importance-of-setting-max-server-memory-in-sql-server-and-how-to-set-it.aspx?Redirected=true) This is one setting that should ALWAYS be changed from default. There are a couple of other settings but this one is one of the big ones.
I have no source to cite on this but I remember reading a few years ago that defining the temp table before inserting into it was faster. That's the way I have been doing it ever since. I would like to see some real data on this if anyone has it. 
Something must have happened to your data. That query will return unique rows and will not mix source rows as described. As a side note, distinct is not a function and does not operate on a single column. It operates on the entire row. There is no need for parenthesis, they are just a distraction. This is effectively the same query select distinct ((((((col1)))))) ,(((col2 ))) ,(col3) from tab1;
What database are you using?
define the temp table. That way you will know the datatypes and lengths of the columns, instead of digging trough code to find the implicit conversion that crashes your application
SQL Server 2008 R2
But what if they are just INTs and DATETIMEs? I guess my real question is does it take longer to create the table shell first or let SQL figure it out? Intuitively I think manually creating the table is faster, but I wasn't sure if SQL had some optimization that does it quicker.
Instead of defining, do a SELECT column_name1, column_name2, column_name3 into #Temp from TableName where 1=0 This will create an empty table, and pre-set the datatypes and lengths to match the columns in the table you're pulling from. Run the following script, for proof of concept: SELECT sc.name,st.name as type_name ,sc.max_length FROM tempdb.sys.columns sc inner join sys.types st on st.system_type_id=sc.system_type_id WHERE [object_id] = OBJECT_ID('tempdb..#Temp'); 
I just tried and got the following - Msg 195, Level 15, State 10, Line 1 'ORIGINAL_USER' is not a recognized built-in function name.
What version of SQL Server are you on?
SELECT INTO #tmp is bulk logged and potentially has some performance gain over INSERT INTO #tmp. INSERT INTO #tmp WITH (TABLOCK) can be bulk logged as well, so either method could work fine. Bulk logged operations significantly reduce the load on log files, but this usually only has a minor impact on the overall query since log files are not normally a bottleneck.
The definition of a temp table using CREATE TABLE or SELECT INTO is insignificant compared to the physical insertion of data, even if it's only a few rows.
Try SUSER_SNAME().
You're the man. I originally wrote it the way I have up top, and it took 8.5 mins to run. I just ran it after rewriting it using the CASE WHEN statements and it took 25s. Thanks for all your help. I will keep this in the toolbox!
Fair enough. I was trying to paste just what I thought was important. Here is the full code for the procedure PROCEDURE BtnOKClick(Sender: TObject); VAR OD:TOracleDataset; OC:TOracleDataset; A: integer; BEGIN OD:= TOracleDataset.Create(nil); OD.SESSION:= FORM.OCSESSION; OD.SQL.TEXT:= 'select pjl_auto_key from project_links where key_value = ' + form.dataset.fieldbyname('SOH_AUTO_KEY').AsString; OD.OPEN; A := OD.fieldbyname ('PJL_AUTO_KEY').asinteger; IF(A is null) then BEGIN OC:= TOracleDataset.Create(nil); OC.Session:= Form.OcSession; OC.SQL.TEXT:= 'insert into project_links (pjl_auto_key, order_type, key_value, pjh_auto_key) select e.pjl_auto_key +1, "SO" AS order_type, b.soh_auto_key , e.pjh_auto_key from cq_header a, so_detail b, so_header c, cq_detail d, project_links e where b.soh_auto_key = c.soh_auto_key and b.cqd_auto_key = d.cqd_auto_key and a.cqh_auto_key = d.cqh_auto_key and a.cqh_auto_key = e.key_value and c.soh_auto_key = ' + form.dataset.fieldbyname('soh_auto_key').AsString; end; END;
select SUSER_NAME() that worked :)
Also, you should almost always parameterize your SQL, DON'T build it like you are doing with string concatenation. Bad bad bad! Sql injection is one of the main reasons for security issues in the world, and your DB can't cache cursors so it's inefficient. 
I'd try playing with fill factors both at the page and at the index level. Another thing to look at to improve this 'slowdown' factor is to see if there are any extraneous indexes you don't need. You could run this in a test perf scenario by simply dropping all indexes on the inserted table and seeing if you see the same 'drop off' behavior. If you don't, it's the indexes, and you can partially mitigate the behavior by dropping any extraneous indexes and setting the [fill factor](http://www.brentozar.com/archive/2013/04/five-things-about-fillfactor/) on the non-leaf level of the indexes. Another thing to consider doing for this specific test is to drop all indexes including the clustered index (this can be tricky on some platforms if you have a PK defined on the table) and have the inserted to table be a heap table only. Maaaybe you need the indexes in actual production, but if you're doing test runs anyway drop the indexes and see if that is in fact the problem. 
Are you using any Scalar Value Functions? Are your transaction logs growing? Are you using Instant File Initialization? http://dba.stackexchange.com/questions/4680/instant-file-initialization-lock-pages-in-memory-issues
temp tables can be vastly more performant. Spooling happens in the temp db too, don't forget that
Try updating statistics on table1 and table2. If statistics are out of date, sql optimizer might assume there is no data &gt; XXXX where there is new data. If optimizer thinks, based on statistics, that no data will be returned it sometimes does things that would very quickly confirm if there was no data, but fail miserably when it unexpectedly encounters thousands of records. This method shift from a good execution plan to a poor execution plan could happen suddenly and without any code changes.
I believe that this was the case in SQL Server 7.0, but it was changed in SQL Server 2000. So it hasn't been an issue in about 15 years. Edit: a word
SQL Server 2008
Just out of curiosity, would this work to capture who is running a pass-through query from Access?
Honestly im not quite sure yet. Its team based and I'll know more about it next Tuesday, but thank you for your input. Does the language differ very greatly depending on os?
&gt; Does the language differ very greatly depending on os? not "very" -- just enough to be a pain in the ass 
The #1 thing you should master first is join logic. I cannot stress just how important it is to wrap your head around the basic joins and the impact that each one has on the result set. Also, ANSI SQL-92 join syntax is going to be the same in all current implementations of SQL, so it'll be valuable to learn no matter if you end up using MySQL, PostgreSQL, Access (shudder), Microsoft SQL (T-SQL), Oracle SQL (PL/SQL), etc.. Some implementations will offer syntactic shortcuts, but the ANSI SQL-92 syntax is what you should learn, as it is considered current industry standard. A good visual demonstration of join logic, with code, is available here: http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/ 
Look up how to use the CASE statement. It is the SQL equivalent of an IF THEN.
Views don't normally include an ORDER BY clause is why you can't create the view.
still get this message SELECT CustNo, SUBSTR(firstname,1,1) || ' ' || (Lastname) * Error at line 2: ORA-00998: must name this expression with a column alias
Check the sidebar for Learning SQL. Since you're a programmer, SQL is different from the languages you know now. Some things you might want to know: Don't: * A. select * from tableA * B. select columnA from tableA inner join tableB on tableA.key = tableB.key where tableA.columnB = tableB.columnB * C. Try not to use Dynamic SQL queries * D. Don't ever use arrays * E. where datepart(year,datecolumn) = 2012 A. this is a bad practice: don't query for columns you don't need it takes extra processing time also if the table changes your query might stop working B. put that WHERE clause in the join C. the query optimizer can't do its work properly then D. Just use another table, [normalize your data](http://en.wikipedia.org/wiki/Database_normalization) E. Don't change the indexed columns datatype unnecessarily, use: where datecolumn between '20120101' and '20121231' instead Do: * Check the table for indexes and use them * Use the indexed columns for your joins and where * Create functions, stored procedures, jobs and views to make life easier * Test your (large) queries on a test database
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Database normalization**](https://en.wikipedia.org/wiki/Database%20normalization): [](#sfw) --- &gt; &gt;__Database normalization__ is the process of organizing the [fields](https://en.wikipedia.org/wiki/Field_(computer_science\)) and [tables](https://en.wikipedia.org/wiki/Table_(database\)) of a [relational database](https://en.wikipedia.org/wiki/Relational_database) to minimize [redundancy](https://en.wikipedia.org/wiki/Data_redundancy). Normalization usually involves dividing large tables into smaller (and less redundant) tables and defining relationships between them. The objective is to isolate data so that additions, deletions, and modifications of a field can be made in just one table and then propagated through the rest of the database using the defined relationships. &gt;[Edgar F. Codd](https://en.wikipedia.org/wiki/Edgar_F._Codd), the inventor of the [relational model](https://en.wikipedia.org/wiki/Relational_model), introduced the concept of normalization and what we now know as the First Normal Form ([1NF](https://en.wikipedia.org/wiki/First_normal_form)) in 1970. Codd went on to define the Second Normal Form ([2NF](https://en.wikipedia.org/wiki/Second_normal_form)) and Third Normal Form ([3NF](https://en.wikipedia.org/wiki/Third_normal_form)) in 1971, and Codd and [Raymond F. Boyce](https://en.wikipedia.org/wiki/Raymond_F._Boyce) defined the Boyce-Codd Normal Form ([BCNF](https://en.wikipedia.org/wiki/Boyce%E2%80%93Codd_normal_form)) in 1974. Informally, a relational database table is often described as "normalized" if it is in the Third Normal Form. Most 3NF tables are free of insertion, update, and deletion anomalies. &gt;A standard piece of database design guidance is that the designer should first create a fully normalized design; then selective [denormalization](https://en.wikipedia.org/wiki/Denormalization) can be performed for [performance](https://en.wikipedia.org/wiki/Computer_performance) reasons. &gt;==== &gt;[**Image**](https://i.imgur.com/zpFXTuS.png) [^(i)](https://commons.wikimedia.org/wiki/File:Insertion_anomaly.svg) --- ^Interesting: [^Relational ^database](https://en.wikipedia.org/wiki/Relational_database) ^| [^Relational ^model](https://en.wikipedia.org/wiki/Relational_model) ^| [^Third ^normal ^form](https://en.wikipedia.org/wiki/Third_normal_form) ^| [^Second ^normal ^form](https://en.wikipedia.org/wiki/Second_normal_form) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ch1lh70) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ch1lh70)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I'm not sure, but I doubt it. 
Ok, thats good to know. I stopped using MSSQL a few years ago now, so It would appear Im out of date with things. I was fairly sure SQL 2000 did suffer from this problem though, as its the last version Im familiar with
Personally I prefer all in one as when u need to make an update your only doing it in one spot instead of 4. 
I would wrap those or statements in ISNULL
I'm not sure I see where that's necessary. If @iEmployeeId, for instance, is NULL the first OR statement will return all records. If it's not NULL, it will return any records with a matching Employee ID. 
Update stats appears to have done the trick. Thank you good sir.
Nope. jc4hokies advice about updating stats appears to have done the trick
Neccessary probably not, but in a quick test, the ISNULL does come back *slightly* faster, and IMHO makes the code more readable. select * from account where person_id=@person_id or @person_id IS NULL select * from account where person_id= ISNULL(@person_id,person_id) &gt;Table 'account'. Scan count 25, logical reads 205150, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt;SQL Server Execution Times: CPU time = 2653 ms, elapsed time = **273 ms**. &gt;Table 'account'. Scan count 25, logical reads 205150, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt;SQL Server Execution Times: CPU time = 2153 ms, elapsed time = **226 ms**.
Its pretty common to do that yes. However, the more optional filters you introduce, the worse the performance is going to get. Dynamic searches are very often done via dynamic SQL for that reason. declare @debug bit declare @SQL nvarchar(max) declare @Params nvarchar(max) set @Params = ' @dynSqlParam_EmployeeId INT ,@dynSqlParam_DeptId INT ,@dynSqlParam_JobTitle INT ' set @SQL = ' select * from tblEmployees where 1=1' IF NULLIF(@iEmployeeId,0) IS NOT NULL BEGIN @SQL = @SQL + char(13) + char(10) + ' AND EmployeeId = @dynSqlParam_iEmployeeId' END IF NULLIF(@iDeptId,0) IS NOT NULL BEGIN @SQL = @SQL + char(13) + char(10) + ' AND DeptId = @dynSqlParam_iDeptId' END IF NULLIF(@iJobTitle ,0) IS NOT NULL BEGIN @SQL = @SQL + char(13) + char(10) + ' AND JobTitle = @dynSqlParam_iJobTitle' END IF @debug = 1 BEGIN PRINT @SQL --or select whatever you prefer END ELSE EXEC sys.sp_executesql @SQL , @Params , @dynSqlParam_zEmployeeId = @iEmployeeId , @dynSqlParam_iDeptId = @iDeptId , @dynSqlParam_iJobTitle = @iJobTitle END Note that by using sp_executesql and not directly concatinating the variables you do not open yourself up to sql injection and it will be a fully parameterized query. Its much much better than exec(@sql) this way
Ah, I was trying to figure out what you were wrapping in ISNULL and it didn't click till I saw your SQL. Yes, I agree that would be a better way to formulate it.
You are correct. I was assuming that he would only have 3 - 4 columns to search on, but I shouldn't be making that assumption. I just hate seeing SQL littered with unnecessary IF statements.
Doesn't that run the risk of having a bad plan used? (Like, a user calls the proc with an employeeID but the query plan does a table scan)
Im a big proponent of readbility. In this case performance is a nice addition.
This is just to get you in the mind-frame for the move to SQL, which should help if you don't know the basics yet, especially since you don't know which particular type SQL you will be using... First of all, keep in mind that SQL is a totally different beast to any of the programming languages you already know, you need to start afresh with it, so don't bring across programming logic to SQL or you will be looking for features it doesn't have and missing features you should be using. As Wepp said, the very first starting block is joins, understand joins, and learn the basic structure of a query, and most of the rest of it fits into place nicely. Once you are happy with building basic queries, look into grouping and data manipulation, set yourself targets, things you want to be able to interpret, and build a query to do it, play around with creating and altering stored procedures and functions (but focus more on stored procedures), by this point you will be quite familiar with everything and your employer will probably be trusting you with more complex reports. A good starting point is [http://www.w3schools.com/sql/](http://www.w3schools.com/sql/) which runs you through the building blocks in all of the more popular types of SQL. There are different 'flavours' of SQL, they all use the same basic logic (joins for example), but a lot of the other syntax will be different, in much the same way as programming languages function in a similar way but with different syntax, and slightly different features. ANSI SQL-92 will get you moving along, but as soon as you pick up the basics, focus on the flavour of SQL you will be using, for me that's MS SQL Server 2005 &amp; 2012. I can't stress enough, once you get the basics, everything else comes easily, all it takes is time, and any employer who knows your background will expect this, so don't worry if things start slowly. We recently started a guy here from an Oracle background and he has picked up SQL Server very well and very quickly, because he understands how SQL works in general, despite the differences between the systems, once you know the basics, you will learn very quickly. Another thing to note, the SQL scene tends to have a very good community, so get involved. Find a large community online, go to meet-ups, etc. Most employers will allow you company time to attend meet-ups as long as you're not too busy at the time and it is relevant to you. For example I frequently log on to http://www.sqlservercentral.com/ and when I can, I attend the SQL Relay events here in the UK, this is all specific to MS SQL Server. Meet-ups are free because they are sponsored by big companies in the scene, they do a bit of product pushing, but you also get a lot of talks on things you wont know about, you'll learn and pick things up that you may not be able to in the office, it can be very worthwhile. Bottom line, familiarise yourself with the basics (over-do it even), and everything else will flow, use the internet, find communities, and enjoy yourself, most SQL DBAs and developers love their work, it can be slow at times, it can be stressful at times, but in general, it is very fulfilling, I'd much rather be doing SQL than programming. 
Once again man, you're a terrible programmer and you should feel bad.
This I hope this gets upvoted as this would be a major concern.
The answer to this question depends on what you want to do with SQL. There's a huge variety of things you can do as an SQL Developer. Invest in SQL Server Developer Edition (or SQL Express if you are OK with the space limitations), and start building hobby projects. Do you like statistical analysis and/or data analysis (Business Intelligence)? Try downloading stock market data and figure out how to identify and ignore errors in source data, then compute various technical indicators. Do you like the idea of developing database-driven ERP projects? Try designing a database for tracking project status, vendor data, employee data, sales data, etc. Do you find it rewarding to optimize slow queries and procedures? You should be able to find people looking for help in this area on websites such as www.sqlservercentral.com. 
Thanks! This is a great response as I'm still weighing which direction I'd like to go. If anyone else has high-level suggestions like these, please share. 
A lot of positions do ask for a BA degree or equivalent experience. I personally got into it by way of a technical support position that utilized SQL to investigate issues in the software. From there I moved into a junior SQL developer job at the same company.
I write T-SQL for a living. Have for almost a year. I got a job doing so after taking 3 classes on SQL from a community college. Being able to use the SQL skills I was learning at school at my previous job helped a TON. If I wasn't able to use SQL at the previous job I bet it would have been a few more classes. I have a liberal arts bachelors degree, I don't think it gained me anything in terms of this job placement. They only cared how skilled I was at SQL. My current employer gave me some good instruction and I've learned a HUGE amount by reading other people's code and finding stuff online. Talk to a recruiter at a professional staffing agency. I was applying at companies from online want ads but I never heard back. The agency I went through gave a proficiency test when I registered with them. Robert Half Technology is who placed me. The agency had me placed within a month.
Did a Google search found a few school projects online. http://1000projects.org/sql-projects.html
I have an added question then, where should we look for SQL jobs?
I will look into this. I am not sure what you mean though to tell you the truth. I wouldnt even come close to calling myself a developer or anything. Ive gotten myself this far using templates and day to day learning and just plonking around. 
As soon as you said it I smacked my head. Im a long way removed from math classes. I changed the as null to = 0 and it worked. I also ran into an issue with the statement "SO" in there. I ended up having to do ''SO'' with two separate apostrophes vs quotation marks. Again... not sure the difference but obviously makes all the difference. Two new things for me to go read up on. 
I have no related degree. Spent about 8 months learning SQL by doing projects on my own outside of work. Applied and was given the job as a data analyst. It was an internal transfer from a non-technical position, but I had samples of a couple things I had done and some references from volunteer work I had completed
I always used to troll through indeed.com with search terms like "SQL Analyst", "SQL Developer" etc
But 0 could be a valid value for the integer, so could give a false positive. Null is not 0. You should do it the way I suggested.
There are some good sample data sets out there you can download. Especially in the sports realm if that's of interest to you. But I'm sure in other areas as well. My recommendation would be find one that you're curious about and load it up into a database (a local instance of MySQL or just an MSAccess DB if you have to) and just start querying. I learned primarily by playing around with Lahman's Baseball database. It's free and has tons of great information. Coded a query GUI and just started looking for trends and information that I was interested in. You can easily download a tool for ODBC querying, Oracle's SQL Developer is free and can connect to Access, but for simple things like that I always like to code them myself just for the practice 
Ohhhh... I see what you are saying. Well, the code is working now but what its probably doing is everytime I hit a certain button its creating the record I want over again. Even though its basically a duplicate record. Obviously not ideal. I will go back again and ready some more about paramaterizing and why it is security risk from sql injection. I am curious about that. Thank you for your help. 
[Schemaverse!](https://schemaverse.com/)
without significant programming knowledge/experience, I'd say the best bet is business/data analyst or reporting related positions. Those are commonly found in medium to large companies and often require knowledge of SQL. Career of those jobs are significantly different from that of engineers/developers. Those positions are more like traditional business positions.
great argument you have there.
Formatting varies a lot from person to person and shop to shop. I wouldn't worry too much about that as much as getting the logic of the query down. I cant figure out the formatting thing here sorry but the description on the right is the style I use. Essentially KEYWORD is all caps and gets a new line and tab in the following non keyword lines. When your query is short it doesn't matter for the formatting, but when they get to be more complex it's best to break them up in new lines to make them more readable in case someone else has to modify your code. 
Thank you for the reply. So based on what you are saying, my formatting was ok? My query had SELECT, FROM and WHERE each in their own line. The tabbing part I have not learned yet but I'm sure I'll pick it up as my queries get more advanced. I'll definitely stick to getting the logic of the queries down first. Thanks again.
Yeah IMO your code it totally fine. 
&gt; So based on what you are saying, my formatting was ok? My query had SELECT, FROM and WHERE each in their own line. absolutely fine here's an example of a somewhat more complex query, "people who bought item 2 also bought these 5 other items" -- SELECT otheritems.item , COUNT(*) AS freq FROM vstats AS thisvisit INNER JOIN vstats as othervisits ON othervisits.item = thisvisit.item AND othervisits.visitor &lt;&gt; thisvisit.visitor INNER JOIN vstats AS otheritems ON otheritems.visitor = othervisits.visitor AND otheritems.item &lt;&gt; thisvisit.item WHERE thisvisit.item = 2 GROUP BY otheritems.item ORDER BY freq DESC LIMIT 5 note `LIMIT` is mysql the formatting is a big part of making this query understandable imaging if it were given to you like this -- SELECT otheritems.item, COUNT(*) AS freq FROM vstats AS thisvisit INNER JOIN vstats as othervisits ON othervisits.item = thisvisit.item AND othervisits.visitor &lt;&gt; thisvisit.visitor INNER JOIN vstats AS otheritems ON otheritems.visitor = othervisits.visitor AND otheritems.item &lt;&gt; thisvisit.item WHERE thisvisit.item = 2 GROUP BY otheritems.item ORDER BY freq DESC LIMIT 5 
^ What he said. That's way fancier tabbing than I write.
You can define fields as varchar, which are variable length single-byte strings. Alternatively there is nvarchar, which are variable length multi-byte strings (for utf, and internationalised character sets). Now, when you type a string literal in your query, SQL Server cant decide if its supposed to be a varchar or an nvarchar. Hence the N prefix before a string literal, tells sql server you want it to treat the string as an nvarchar 
Its often easier to put brackets around your ON clause too. Heres how I'd format the same query (just personal choice). Note how you can see a quick overview of the query, just by scanning the keywords in the leftmost 'column' SELECT otheritems.item, COUNT(*) AS freq FROM vstats AS thisvisit INNER JOIN vstats as othervisits ON ( othervisits.item = thisvisit.item AND othervisits.visitor &lt;&gt; thisvisit.visitor) INNER JOIN vstats AS otheritems ON ( otheritems.visitor = othervisits.visitor AND otheritems.item &lt;&gt; thisvisit.item) WHERE thisvisit.item = 2 GROUP BY otheritems.item ORDER BY freq DESC LIMIT 5
It depends how often you are applying. Banking and healthcare industries are always looking for SQL programmers. You might have to look under analyst positions and those usually will state SQL is required. Now some places might require a college degree, not usually in SQL but just as a prerequisite. I have a BS in Marketing and a MBA but learned SQL 14 years ago and been doing it ever since.
Buy or rent (from the library) a good sql book. Something for dummies, maybe, or a big book defining the standard. Take a look at the first few chapters, and see of you can follow along. Since you're looking to make a career change, try this (it worked for me): use your current job to develop your skills. Look for ways that you can improve your job or workflow by using sql. Become a person who looks for solutions to problems in novel ways, our who finds things to improve upon, and learns the skills needed to create those solutions. That's the kind of thing that can get you hired even if your skills are rudimentary. 
First off, sorry for using imgure to show the problem, I couldn't figure out how to use SQLfiddle with the school's uploaded script. The problem I'm having is that I'm wanting the results of the table to show the 3 different lengths (40,30,25) and the total rental_fee for those respective lengths but as you can see in the photo, the rental_fee is still being split instead of summed. 
Take the rental fee out of the group by. Should do the trick.
You need to find out the means of extracting parts of your database's DATE or TIMESTAMP column into constituent parts. AFAIK, it's often something like: SELECT EXTRACT(YEAR FROM DATE_COLUMN) AS EXTRACTED_YEAR ,EXTRACT(MONTH from DATE_COLUMN) AS EXTRACTED_MONTH FROM YOURTABLE; Then you will need to combine these into a single value (maybe something like EXTRACTED_YEAR*100+EXTRACTED_MONTH). That will give you a number such as 201307 (July 2013). Alternatively, CAST your DATE datatype to a CHARACTER value then SUBSTRING out the YEAR and MONTH part. SELECT SUBSTR( CAST(DATE_COLUMN AS CHAR(10),1,7) /* assume date is presented in ISO format YYYY-MM-DD after CAST() */ FROM YOURTABLE; You may end up with a query something like this SELECT ITEM /*, LOCATION */ , SUBSTR( CAST(DATE_COLUMN AS CHAR(10),1,7) AS YEAR_MONTH , SUM(SALES) FROM YOURTABLE GROUP BY PRODUCT, /* LOCATION, */ YEAR_MONTH ORDER BY PRODUCT, /* LOCATION, */ YEAR_MONTH ; HTH 
Thanks for the advice on the books. I just downloaded a couple of basic SQL e-books that focus on the basics of SQL. One is Discovering SQL A Hands-On Guide for Beginners by Alex Kriegel and the other is SQL and Reltaional Theory by C.J. Date. In real estate there is definitely a very large amount of data that is created and used. I still have not found out how I can apply SQL knowledge to it but I'll see what I can do. Thanks again for your input.
glad to see you are using apex, but version 2!
Group by the Length only.
I can't believe how something so simple changed this. Thank you! I honestly feel like I've tried it this way but I'm guessing something else was wrong at the time lol
If this is Teradata, either your base data is skewed (maybe an inappropriate primary index on the table you are reading from) or your spool is skewing as your query runs. 
Try /r/datasets ...
easiest would be to have only one table for the player stats, instead of a table for each player
Does ' mean something else in SQL? I don't see an apostrophe in the code. SELECT Count(Goals) AS GamesPlayed, Sum(Goals) AS GoalsScored AS MinPerGoal INTO NewTable FROM LionelMessi HAVING ((GameDate)&gt;#12/31/2013#) AND ((GameDate)&lt;#12/31/2014#) That will create a new table, but I still can't copy and paste that and change the player name. I also can't do FROM Neymar, LionelMessi. 
I'd do that but I'm not quite advanced enough in SQL to add an extra step to gather the games one player has played from a table of all games. 
My bad, the ' is in the error message
you might then use a form where you can input the names you want and the results will display
 ;WITH CET_Sums as( SELECT Left(CAST(Date_Field as Varchar(20),4) + RIGHT(Left(CAST(Date_Field as Varchar(20),7),2) as YearMonth, Product, Sales FROM tableA ) SELECT YearMonth, SUM(Sales), Product FROM CTE_Sums Group BY YearMonth, Product
good detailed list (Y)
you need a semicolon after the first SQL statement. You have to end the statement, the error is showing the end of the first statement and beginning of seconds. Since, there is no semicolon to end something, you have this problem. Now, i'm assuming you are doing this for a class. As a professor, this isn't great database design. You could sum this thing up into 1 or 2 tables. You'd have a PlayerTable(ID, FirstName, LastName, OtherStuffAboutPlayer) GamesTable(GameID,PlayerID,GameDate,Goals,Assists,MinutesPlayed,anythingElse) Then, your query would be pretty much the same, except, SELECT Count(g.Goals) AS GamesPlayed, Sum(g.Goals) AS GoalsScored, Sum(g.Assists) AS AssistsGiven, Sum(g.MinutesPlayed) AS Minutes, Round(Count(g.Goals)/Sum(g.Goals), 3) AS GoalRatio, Round(Sum(g.MinutesPlayed)/Sum(g.Goals), 3) AS MinPerGoal FROM PlayerTable p, GameTable g Where p.ID = g.playerID Group BY p.LastName HAVING ((g.GameDate)&gt;#12/31/2013#) AND ((g.GameDate)&lt;#12/31/2014#) 
 ;WITH CTE_SUM AS ( SELECT 'LionelMessi' as Player, Goals, Assists, MinutesPlayed, gamedate FROM LionelMessi UNION SELECT 'Player2' as Player, Goals, Assists, MinutesPlayed, gamedate FROM Player2 UNION SELECT 'PlayerN' as Player, Goals, Assists, MinutesPlayed, gamedate FROM PlayerN ) SELECT Count(Goals) AS GamesPlayed, Sum(Goals) AS GoalsScored, Sum(Assists) AS AssistsGiven, Sum(MinutesPlayed) AS Minutes, Round(Count(Goals)/Sum(Goals), 3) AS GoalRatio, Round(Sum(MinutesPlayed)/Sum(Goals), 3) AS MinPerGoal Player FROM CTE_SUM GROUP BY Player You can add them all to a common table expression, or really it would be better to store them all in one table with a player Name or PlayerID field. You may need to add some more group by criteria above.
Personally it sounds more like you need a view to use for the select than a store proc to return the data. 
agree with this generalization but be wary of Oracle, if you don't have the $ to throw at it or the expertise in house, it will be very very frustrating... trust me we use all 3 + another db called PVX... PVX is used for a very specialized ERP we use and it only runs on PVX, we have a few web apps including Magento on MySQL and have no complaints, our PLM is on Oracle and it gives me the hardest time. I'm a MS sysadmin by training so my opinion on MS SQL may be bias, however it is the easiest DB for me to administer and i also write reports and custom apps in .NET &amp; T-SQL, T-SQL is the reason i recommend MSSQL, but the bulk of my network is on a Microsoft stack. Oh, and i also import data from all the above databases into MSSQL using SSIS to build OLAP cubes and make basdass reports using SSRS.... but again I'm Microsoft Bias :) edit: a word
Or Postgres if you want something similar to mysql, without all the gotchas.
[Comparison of different SQL implementations](http://troels.arvin.dk/db/rdbms/)
&gt; be weary of Oracle wary FTFY :) 
Here is what you should do, go to your local Sql User Group, ask people very nicely. My googlefu netted this : http://www.tampasql.com/ A nice side benefit of this is, that you don't get random people that might or might not know much about sql responding, the people that go to user groups usually are the creme de la db nerd (and some recruiters...). Having a conversation will improve your odds as well I'd say.
Thank you all for your answers. This is a for a paper for school and I currently am working with Oracle databases. But I am still learning about Oracle and Microsoft SQL server. Again, Thank you
There are more MySQL DBAs than PG. Unfortunately.
That's true, but I have a theory that a very significant portion of the "MySQL DBAs" are, well, to the category of DBAs as MySQL is to the category of RBDMSs.
Have you tried a Ctrl+ click on that box? You can usually find some helpful info in that dialog box that Pops up
Thanks, I should have read the formatting rules better. 
That's not really fair. I know some brilliant admins and devs who work with mysql. I'm not a big fan of the mysql, but you can still do some amazing things with it despite its shortcomings.
this will only work in sql 2012+ mind you, but it should be a hell of a lot faster : create table #bunchOfRowsInLogFile (dateOfVisit date, email varchar(255)) insert into #bunchOfRowsInLogFile Values ('2013-01-01', 'email1@example.com') , ('2013-01-06', 'email1@example.com') , ('2013-03-01', 'email1@example.com') , ('2013-04-01', 'email1@example.com') , ('2013-05-01', 'email1@example.com') , ('2013-06-01', 'email1@example.com') , ('2013-07-01', 'email1@example.com') , ('2013-08-01', 'email1@example.com') , ('2013-09-01', 'email1@example.com') , ('2013-11-01', 'email1@example.com') , ('2013-12-01', 'email1@example.com') , ('2020-01-01', 'email1@example.com') , ('2000-01-01', 'someOtherEmail@example.com') SELECT * FROM ( select email , visitedAt = dateOfVisit , previousVisit = lead(dateOfVisit, 1,null) over(partition by email order by dateOfVisit desc) from #bunchOfRowsInLogFile )sq where datediff(month, previousVisit, visitedAt) &gt; 12 or previousVisit is null --or 1=1 order by visitedAt http://data.stackexchange.com/stackoverflow/query/edit/186821 for syntax highlighting If the logic fits your need, and you got a recent sql server edition, it shouldn't be to hard to take this example and adjust it to your specific need /Edit i assumed you where running MSSQL, "IF OBJECT_ID... " made me think that
Sadly, it's for one of my classes here at college and the teacher INSISTS we use nested loops. I asked what the point of this was, and he didn't really give me a direct answer as to why we use cursors at all. Can you explain that to me? edit: I tried putting it int the first fetch, but I didn't get the proper results. I feel like I messed up somewhere.
Thank you so much. I had a feeling it was something with partition by, but i hadn't thought of that. 15+ min ...now 12 seconds. 
I sit next to OP, he is truly grateful. 
despite the fact that you're teaching people sql by using mysql, it still comes off as an sql tutorial rather than a mysql tutorial, so you should take steps to ensure that any sql you use for examples is actually standard sql, and not mysql-proprietary thus, string literals should use single quotes, not double quotes, e.g. 'Grilled Cheese' instead of "Grilled Cheese" &gt; The PRIMARY KEY forces every row to be a unique value. actually, it forces every value in the designated column to be unique rows themselves can be made unique only if the PK covers all columns, and also when using the DISTINCT keyword for result rows &gt; There is additional documented on types here. documentation &gt; Note that we do not ensure uniqueness of ingredient_price because multiple recipes can have the same price. ingredients, not recipes &gt; Notice that our PRIMARY KEY is for recipe ID and ingredient ID pairs. We require only that the combination is unique because each recipe can have a particular ingredient only once. bravo! many people don't understand this &gt; There is an AUTO_INCREMENT command which can be used to let SQL automatically pick the recipe and ingredient IDs to ensure uniqueness. this is mysql-proprietary, so you shouldn't say "let SQL" here 
The only hope is he wants to show how bad they perform against a set based solution. 
very welcome, saving the planet one cpu cycle at the time... Btw, imagine how grateful his dba / server admin is gonna be :P
Last question I think, what does: lead(id,1,null) do/mean?
Does Pacos suggestion create the desired answer? I haven't been able to replicate it. SQL isn't my strong suit.
just popping in here to say thanks for using the leading comma convention i wish more people knew about it
LAG() and LEAD() are 2 functions introduced in sql 2012. The signature is this : LEAD ([the field returned], [offset], [defaultValue]) They will give you the value in the previous or following row of a recordset, based on the order by. Think of it like this, you put a row_number over your recordset, the lag and lead will give you the row by the specified offset in that ranking (specified in the over() clause). It's a self join on a ranked subquery pretty much 
I fucked up at work today on 5 production machines, I gotta iron out my karma ....
Got it thanks again.
I like to indent the AND rows but yep fully agree
Well, your solution worked. However, now the results aren't grouped properly. ART103A S1001 Smith, Tom ART103A S1020 Rivera, Jane CIS235A S1001 Smith, Tom CIS235A S1020 Rivera, Jane CIS251A S1002 Chin, Ann HST205A S1010 Burns, Edward HST205A S1020 Rivera, Jane MTH101B S1001 Smith, Tom MTH101B S1020 Rivera, Jane MTH101B That's what I get now. One step closer. edit: I had to use a break to get to this point.
Penile Dave for the win! 
excellent point except sometimes i start my WHERE clauses with 1=0 can you guess when?
update and delete ?
Give us at least : * a description of the tables * what exactly you are trying to output
I'm not an Oracle guy but I recall there being a free x86 download of their engine, kind of like SQL Server Express Edition. If you don't have that already, I might suggest that?
&gt; Don't guess what exactly the teacher is trying to teach. okay, thanks, i won't oh, and your magic internet minus points, they do nothing 
 WHERE 1=0 OR name LIKE '% Todd %' -- OR city = 'Butte' OR foo = 'bar' 
I'm not an oracle guy either. If you where talking about MSSQL which I am a guy off, I couldn't give you a better answer either thou. Frist things first, know how little you know. Being able to write a 5 line SQL query does not make you someone that knows SQL. Sounds simple and streight forward, but you'd be amazed how many don't even know that. Lets move on to the more intresting. Do you want to work on databases full time? If yes, read further, if no, go and deepen your knowlage on java, php, c#, vb.net, c++ or what ever have you. Now, you read on, so I'm gonna assume you want to work with and on databases. First real question : Why? Simple as it is, very good question. Do you know anything about the work? Second question : Why should a company invest money to get you trained : You have to show on your cv, that you are into database stuff. That will put your application on a trainee position over some other guy. How can you do that... well, I do NOT recommend some open source bla bla thingy, that doesn't work very well for database development and administration as far as I've seen in the past. If I where you, I'd try to do some internship, try to get a part time job, write a bot for schemaverse. Something like that. If you are lucky, you got a SQL User Group near by, if so, GO THERE ! http://www.sqlpass.org/ this is MSSQL specific, but well, I'm from the mssql world, so I know pass, i dont know if there is something similar for oracle. In such user groups you can network (i hate that word), you can meet people. People that are usually happy to give you pointers, and that are very well connected themself. (hint : internship) Third question : What can you do to train yourself Thats a tough one. Databases are pretty dry. Its hard (at least to me) to pull a good practice example out of where the sun doesn't shine. The best answer I can give would be to download a sample DB and get a Book. Again, MSSQL bias, but installing MSSQL Express (its free), downloading AdventureWorks (free), and buying the lower tier self training books for the Microsoft SQL Server certifications (used on ebay, close to free), would be a good start. That would teach you the basics. Your course did not teach you even the basics as far as I am concerend. After that, well, read blogs, read and read and read an read. I'm a fulltime dba, and I still do that. I'd really like to stress, that you should try to get an internship thou. You might not *like* database stuff after all. You really should get a picture of your possible futur, before you go all in on it. Local Usergroups are a GREAT opportunity for that as well.
thanks for the advice! I admit, I'm so green, I don't really even know what I don't know. I really wish I could find an internship, but a), I'm not in a position to work without pay, but also b), there's really no internships in my area, so I'm kind of on my own there.
If you know that you don't know much, you're already a step ahead of most "fresh out of collage &lt;very professional and competent&gt;" people I've had to endure in my past. There is a hint of sarcasm in there... I don't know enough about, my guess, the US jobs market, but you might get lucky to get a payed internship. Payed internship would still be next to nothing, but not exactly nothing. Try it thou, you might get a "code monkey" part time job with a bit of luck, or get an internship that, if it doesn't pay, does take your time schedule into account (they aint paying after all). Try to get in thouch with a local usergroup, thats the best resource you can have. If its an hour away, try to get to a meeting once or twice and just talk to people. Getting young people that are intrested early on, can be an intresting thing to companies that train their own staff. And after all, you don't have much to loose In any case, just "cold call" IT companies, and even the bigger non IT companies in your area. Just ask if they could use an intern during your spring break or whatever you got. Nothing to loose, and doesn't cost you anything but a bit of time. Well, that and the wohohoooo spring break I'm getting wasted blabla. If you are willing to not do that, thats why you get an internship. 
Postgres
Right, this is the opposite of the 1 = 1. You can now filter out each of your "or" operators individually, including the first.
i will readily accept that my comments may have no effect, but they are not clueless -- i know full well how dumb cursors are 
Like I said, I'm not an Oracle guy, so...sure?
what is the difference? don't they all work basically the same?
&gt; Go read the comments besides your own, including the OP. you mean comments like this? "Your teacher should be forcefully removed for teaching cursor use like this" or this? "The only hope is he wants to show how bad they perform against a set based solution." 
Totally different. SQL Developer is Oracle's free GUI toolset for working with SQL databases, not a database itself. Oracle Express Edition http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html will get you a free edition of Oracle to run on your own machine.
I am an Oracle Certified Professional DBA. Oracle is a difficult piece of software to work with. It is the most powerful RDBMS around, but also the biggest pain to admin. It is also the best paying. You should really consider whether it is the best target for you. A good DBA needs to have a very wide range of technical skills, an Oracle DBA even more so. Other RDBMS have a somewhat lower barrier to entry, provide fantastic functionality and can be the basis for a great career. If you are genuinely interested in being and Oracle DBA, start [here](http://www.amazon.com/OCA-Database-Administrator-Certified-Associate/dp/111864395X/ref=sr_1_3?ie=UTF8&amp;qid=1398738550&amp;sr=8-3&amp;keywords=oracle+certified). I have not used the 12c guide, but the previous editions are the best I know of outside of actually attending Oracle University training. Get that book, read it cover to cover, then read through everything you don't know by heart again. Do all of the practice tests until you think they're too easy, because they're easier than the test. Oracle's certifications are sought after and intentionally difficult. You can download an installer and begin playing with the RDBMS at home. Try writing some complex reports. Make sure you understand function syntax. Learn the underlying OS of your choice. Once you've got that stuff down, you may be able to get a job as a Jr DBA. Best of luck!
The error was Must declare the scalar variable "@order". Yeah, that's how it was spelled in the database... For now only one person will ever use the app at any given time. OrderID is the primary key of pizzzpipelineorder I'm going to try your suggestion now..... Update: I got the error: Conversion from DBNull to type Decimal is not valid. Must declare the scalar variable "@order".
SQL Developer is just a tool used to connect to the database system. You need to [download](http://www.oracle.com/technetwork/database/enterprise-edition/downloads/index.html) and install the RDBMS itself. I forgot to mention earlier, go ahead and bookmark tahiti.oracle.com if you haven't already. You'll be spending a lot of time there. The quality of documentation is pretty damn good, and there's a lot of it. [Here](http://docs.oracle.com/cd/E16655_01/nav/portal_11.htm) are the installation guides. 
I'm confused as to the difference between the two, I guess. 
Get thee to tahiti! Simple explanation, the RDBMS is the actual database engine. It is what people mean when they say they use Oracle. SQL Developer is an ancillary tool that is used to connect to the RDBMS and interact with it. SQL Developer requires the RDBMS to be useful, the RDBMS does not require SQL Developer.
 select FirstName, EnglishName, ScientificName from User u join WhaleSpotted ws on u.UserID=ws.UserID join Whale w on ws.WhaleID=w.WhaleID Should do the trick. If duplicates are a possibility you can add the following to the end: group by FirstName, EnglishName, ScientificName edit:formatting 
I am an Oracle girl and I can agree that learning java, c# or the like will enhance your skill set. Also, I'd start looking for an internship. Paid or free the experience will be invaluable. I have my OCA and I can say of the two parts, I found the SQL part the most challenging. I might recommend looking at a kill test. The cert will help your resume, but the internship will be what gets you to your first job. IMO. 
If you're up for a bit of advanced SQL, this is the perfect place to use a view. Something like this: /* you may also wish to include u.UserID and/or w.WhaleID in your column list */ create or replace view view_whale_spottings as select u.FirstName, w.EnglishName, w.ScientificName from User u join WhaleSpotted ws on u.UserID=ws.UserID, join ws.WhaleID=w.WhaleID (I like to start my view names with "view_", but you don't need to do that if you don't want to). Then you can just say, select * from view_whale_spottings And off you go.
Ah nice, thanks very much!
I'd say that no matter what exactly one wants to teach, programming is process where you have to find workable solutions to a given problem. Being forced into "do it with nested loops because reasons" has nothing to do with what I call programming. Finding a solution is the interesting part, the rest is just syntax. As to the question of cursors vs no cursors, a professor that doesn't know that cursors are the last thing one should use in SQL, since they are iterative and we kind of do SQL because its set based, has not the first clue, and should not be teaching. Btw, /u/r3pr0b8 is anything but clueless when it comes to databases, while based on your comment history, you are self admittedly "still somewhat new to SQL". So well draw you own conclusions on that *fact*.
The difference between a table, the database engine and SQL Developer is the difference between a text file, your operating system and notepad.
yeah, but the problem is, the use of aliases in /u/crile's code is inconsistent -- they're used in the FROM clause but not the SELECT clause nothing more annoying than picking up somebody else's query and having to figure out which table a column comes from also, choice of alias names is important yes, more signal and less noise is important, and in this example, u, w, and ws are fine, but have you ever seen a query where the aliases are a,b,c,... or t1,t2,t3... ?
I think the error is actually in your VB and not in the SQL. For Each product In Chkslice.Items If Chkslice.SelectedIndex = True Then .AddWithValue("@product", Chkslice.SelectedItem) .AddWithValue("@order", decorder) .AddWithValue("@quanity", Ddlquanity.SelectedValue) End If .Clear() Why are you clearing the parameters after adding them? And you also don't need the for loop at all.
If this is all you have so far, then I would suggest adding a "sighting_date" to the WhaleSpotted table. Also some kind of boat ID and a location marker. Just basic stuff, really. 
I could be wrong but isn't this the same result without the subquery? --RETURNS THE LOC ID OF THE TRUE OWNERS FOR ALL PIDS W RECORDED OWNERS-- SELECT P2.PropertyID as PropertyID, MAX(PC2.LocationID) AS T_Owner FROM enterprise.dbo.Property AS P2 left join enterprise.dbo.PropertyContact As PC2 on PC2.PropertyID = P2.PropertyID GROUP BY P2.PropertyID HAVING MAX(CASE WHEN PC2.ContactRoleID = '7' THEN 1 ELSE 0 END) = 1 and MAX(CASE WHEN PC2.ContactRoleID = '3' and PC2.EndDate is null THEN 1 ELSE 0 END) = 1 **EDIT:** I fixed the PC2.Enddate is null This does 1 read against the database. It aggregates your output and only returns the property owners who have a contactroleid of 7 and contactroleid of 3 and do not have an enddate. Ultimately, when you start optimizing code in sql. It usually comes down to reducing the amount of reads that have to be performed to perform the action. Your subquery is not a bad code at all, its not a correlated subquery and in small datasets it will perform well. However, when you start getting into millions of records, 1 read can be a matter of 2 to 4 minutes even with well indexed tables. 
&gt; Views provide abstraction over tables. You can add/remove fields easily in a view without modifying your underlying schema The opposite can be true as well. You can make a view which will always have the same fields even if you modify the underlying schema (assuming the view is updated to match the schema changes). 
&gt; It all works great and looks right untill i try to fill some of the Tables with data, namely: &gt; INSERT INTO KUNDORDER (ordnr, persnr) &gt; VALUES (MYSEQ.nextval, '9045123256'); &gt; and: &gt; INSERT INTO KUNDVAGN (radnr, ordnr, artnr, antal) &gt; VALUES (MYSEQ.nextval, **'2'**, '23452', '1'); I think that the insert into KUNDORDER resulted in an ordnr of something other than 2. You need to look at KUNDORDER and match your ordnr for the insert into KUNDVAGN with an actual ordnr that exists in KUNDORDER. Edit: You want to make the PK of ARTIKELBILD "artnr" for it to be one to one. One to one means the PK is also a FK.
&gt; I think that the insert into KUNDORDER resulted in an ordnr of something other than 2. You need to look at KUNDORDER and match your ordnr for the insert into KUNDVAGN with an actual ordnr that exists in KUNDORDER. not sure, i get the error after : INSERT INTO KUNDORDER (ordnr, persnr) VALUES (MYSEQ.nextval, '9045123256'); OFC! yea you were right but its the persnr that needs to match a persnr in the KUND table! Thanks! **EDIT:** so this means i have to "SELECT * FROM KUNDORDER" and look at the number in ordnr. Is there a way to not have to look up the number manually every time? can i just use the select statement while i do the insert? like: INSERT INTO KUNDVAGN (radnr, ordnr, artnr, antal) VALUES (MYSEQ.nextval, SELECT MAX(ordnr) FROM KUNDORDER, '23452', '1'); ?
Try this. INSERT INTO KUNDVAGN (radnr, ordnr, artnr, antal) SELECT MYSEQ.nextval, MAX(ordnr), '23452', '1' FROM KUNDORDER;
hmm i got this back: 2287. 00000 - "sequence number not allowed here" *Cause: The specified sequence number (CURRVAL or NEXTVAL) is inappropriate here in the statement. *Action: Remove the sequence number.
Sorry, Oracle's not the environment we use. Someone else may be able to help with specific syntax.
Create an A_B_C_MAP (or whatever suffix for relationship maps is in your naming convention) with A_ID, B_ID and C_ID as the composite key. Depending on what are you designing and design conventions, you might want to create a separate synthetic ID for the records and have (A_ID, B_ID, C_ID) as an alternative key. If there are attributes that describe relationship ( so the key for them is truly (A,B,C)), put them into the map table. 
INSERT INTO KUNDVAGN (radnr, ordnr, artnr, antal) values (MYSEQ.nextval, (select ordnr from KUNDORDER where persnr = '9045123256'), '23452', '1' ); commit;
One possibility is to script the database objects and data for deployment.
What area are you in? It's a hard road without having any experience in the IT world. I never think free is a good option, but there aren't any big companies near you? Do you know anyone you can "shadow" a few hours a week? Something that would impress employers is having a work portfolio. Set up a website and have a working db in the back end. Have it insert information and fetch it .. 
I'm in Montgomery, al. the only it industry we have in any numbers is the military, and the state government. while they do offer internships, the fact is I'm 28 years old, and most internships are aimed at high school students, or normal-age college students.
What is the error that you are getting? I don't think you should have a problem restoring old DB to a newer version. Its only when you restore a newer DB on an old version that you have troubles.
I was 39 when I got an internship with a big telecom company, and it was paid. In fact it was paid at a higher rate than I had been making. Although it was for my masters. I had no real db experience prior to that, just building things on my own. I worked with undergrads who were also being paid. Don't discount what you bring to the table being a non-traditional student. 
I just did a quick google search and found a systems developer internship with Booz Allen Hamilton in Montgomery ... I'm sure they pay. 
That's what he's talking about. Restoring a new version 2014 on 2012 server. I'm only adept when moving "up" on restores. However, I agree with /u/jc4hokies that scripting would be the easier route.
oh i misunderstood. thanks.
Well, by coordinates for instance. Longitude + latitude. Or get a nautical chart and see if you can load it into the database. Bonus points if you can find a digitized chart that you could easily load up. Also, more than one user may report a sighting at any one time, that's why I figured you should group them up under a common entity, like a boat for example. You could add weather info etc etc the possibilities are many.
I think its much easier to think of a NULL value in the terms of "unknown" instead of "lack of a value". what do you get from : "unknown" + 5? well "unknown" of course. Does "unknown" = 5? the answer is not true/false, its again "unknown". The same goes for the negation, does "unknown" != 5? --&gt; "unknown". What do you get from 'some string' + "unknown" ? also "unknown" I find that a much easier way of explaining why NULL behaves the way it does. 
That's what the article says right at the beginning, right? &gt; NULL is like an "unknown variable" in algebraic equation And further down: &gt; It does not make sense if we treated NULL as "unknown" or "irrelevant". But it makes sense if we treat NULL as "could be anything".
How to port your database to PG
I'm not sure what you mean by the last part, but this will probably get you close. SELECT employee_legal_names, COUNT(*) AS Cnt, COUNT(DISTINCT employee_nicknames) AS NicknameCnt, MIN(employee_nicknames) AS Nickname1, MAX(employee_nicknames) AS Nickname2 FROM employees GROUP BY employee_legal_names HAVING COUNT(*) &gt; 1
 SELECT employee_legal_names , employee_nick_names FROM employees WHERE employee_legal_names IN ( SELECT employee_legal_names FROM employees GROUP BY employee_legal_names HAVING COUNT(1) &gt; 1)
My elevator pitch for null is "it's the difference between not knowing how much is in your checking account and having $0." Mentally, I've always found it more useful to use "anything" or "unknown" instead of "no data" whenever I get bitten by, say, 5 + null = null. Algebraically it behaves more like infinity than unknown X. (5 * inf = inf vs 5 * x = 5x)
Well the bottom on is an inner join written as described in ANSI 89 standard. The top one is a left outer join in ANSI 92. Now it is a question of Left vs Inner. I am a firm believer that ANSI 89 needs to hurry up and die. The bottom query could be written as SELECT s.name AS starname, ((s.class + 7) * s.intensity) * 1000000 AS startemp, p.name AS planetname, ((((s.class + 7) * s.intensity) * 1000000) - (50 * p.orbitdistance)) AS planettemp FROM stars as s INNER JOIN planets as p ON s.starid=p.starid WHERE s.starid&lt;100; Inner joins will will only return a result from the table "Stars" if there is an associated record in "Planets" and the Star ID is less than 100 Left Outer Join will return results for every Star ID less than 100 regardless if there are planets associated with it. However, since there is math involved, and I dont know how SQLite handles NULL values when it evaluates expressions, you may get a divide by zero error for the PlanetTemp column, or more likely NULL. [Different Types of Joins](http://en.wikipedia.org/wiki/Join_%28SQL) 
&gt; How to port your database to PG I liked that one :P
that infinity methapor is quite nice I have to say. I usually just spell it out, NULL = "i do not know". 5+ NULL = "I still do not god damn know (+5)"
Tables: 1. Pet (**PetId**, PetName, *PetTypeId*, *PetBreedId*, PetDOB) 1. PetType (**PetTypeId**, PetType) 1. PetBreed (**PetBreedId**, PetBreed) 1. Owner (**OwnerId**, LastName, FirstName, Phone, Email) 1. PetOwner (**PetOwnerId**, *OwnerId*, *PetId*) 
This is why we give people a SQL exam when they are on site to prove their knowledge...
/u/kittyclawson, the Reddit Grammar Police Bot has detected an error in the title of your link: &gt; Hi guys. I don't know if this is the right place to ask but I am desperate and ~~your~~ [**you're**] the best place I could think of asking this. But could you possibly help me with some SQL homework? As indicated above, we recommend that you use desperate and [you're] the best instead. 
These are pretty basic... not sure why they would give you a cheat sheet if they intended to screen with them since they are super easy to Google etc. 1. You can have multiple unique indexes as well but only one Primary. 2. I think you might want to use the word Intersect instead of Link but that might just be personal pref. Also you need to separate them first. 3. I would prefer Left Joins instead of "self joins" again could be personal pref. 4. "...and any applicable data." should be WHERE clause
Yeah, but I put my answers there... just want to make sure they're good ones :)
Thanks!
Left join and self join are not at all equivalent terms. 
thanks for the heads up! unfortunately, by looking at the listing, its obvious I'm underqualified by, like, a looooot.
One last tip.. everyone is under qualified for all jobs. If you have at least one of those skills .. you should consider applying. What is the worse thing that can happen? Nothing? They write those descriptions to catch someone that has something ... you will hardly ever meet someone who has all those qualifications. And, with internships, they like to "train" you into a job. Most of my friends were interns. All of us got job offers either before graduation or after with the interning company. Don't sell yourself short. 
I think OP's answer is better - a unique index can have one null. Otherwise it's indistinguishable from a PK index when you go under the sheets. 
this! Self join is a concept of joining a table with itself. You can use left join, cross join, right join. It depends on the required result.
Question 1: What is a windowing function in SQL Server? Why would you want to use a rownum() and partion by clause? Answer 1: A windowing function is an additional function applied to a portion of the result set. You would want to use these to keep the results organized. Question 2: What is a CTE? Why would we use it? How is it different than a with statement in Oracle? Performance? Ease of use? Self-documenting? What are the alternatives? Answer 2: CTE stands for common table expressions, it's a temporary separate set of results that can reference to itself. (This is the one where I need the most help) Question 3: What is a Cartesian product? Why would it be good to be aware of this? Answer 3: A cartesian product is when a cross join is performed and all data in both tables is returned. Usually happens when there is no relationship between the two tables trying to be joined. Question 4: What is the purpose for normalizing tables? For denormalizing tables? Answer 4: Normalizing tables is to reduce redundancy and increase efficiency and usually keeps table size small. Denormalizing tables can improve database performance, cuts down on the need for joins and improves reading data. since it's readily available. Question 5: How does analytic SQL differ from simple aggregate/grouping? Answer 5: Analytic SQL your are dealing with aggregates using grouped data. Simple aggregate/grouping you are dealing with aggregates but with a row-row interaction. There hope it helps others to read it.
Any half way competent interviewer will see trough your "learning the answers to questions" to fake knowledge with just one followup question. Are you sure you want to go that way? I wouldn't if I were you.
use a row_number() to number the duplicated rows. If you get a row_number of 2, its a duplicate
Why not just select distinct * into newtable from existingtable? 
CREATE TABLE [reddit]([type] [nchar](50) NULL,[name] [nchar](50) NULL,[color] [nchar](50) NULL) ON [Data Filegroup 1] GO INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Fruit','Apple','Green') INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Fruit','Apple','Red') INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Fruit','Apple','Green') INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Fruit','Banana','Yellow') INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Fruit','Banana','Yellow') INSERT INTO [reddit] ([type],[name],[color]) VALUES ('Veg','Potato','Yellow') GO CREATE TABLE [redditnew]([type] [nchar](50) NULL,[name] [nchar](50) NULL,[color] [nchar](50) NULL) ON [Data Filegroup 1] GO insert into [redditnew] ([type],[name],[color]) select * from reddit group by [type],[name],[color] having count(*)&gt; 1 delete reddit from reddit as a inner join (select * from reddit group by [type],[name],[color] having count(*)&gt; 1) as sqry on a.type = sqry.type and a.name = sqry.name and a.color = sqry.color insert into reddit select * from redditnew
be fair to your readers -- mention that your code isn't SQL, but T-SQL
and you feel it necessary to share that one line of code with the rest of us?
Start by formatting your statement if you want help ! Also, what's the exact error ?
The best way to do this is to join the table to it's self. Hopefully you have a PK on this table. If not add one. WITH CTE_Unique as( SELECT MIN(PK) as PK, Field1, Field2, Field3 FROM TableA Group By Field1, Field2, Field3 ) --DELETE A SELECT * FROM CTE_Unique U JOIN TABLEA A ON U.Field1 = A.Field1 ON U.Field2 = A.Field2 ON U.Field3 = A.Field3 WHERE U.PK &lt;&gt; A.PK
Thanks!
Here you go! ;WITH CTE AS ( SELECT 65 LetterCount UNION ALL SELECT LetterCount + 1 LetterCount FROM CTE WHERE CTE.LetterCount &lt; 90 ) SELECT CHAR(LetterCount) FROM CTE WHERE LetterCount &gt; 75 AND LetterCount &lt; 77 UNION ALL SELECT CHAR(LetterCount) FROM CTE WHERE LetterCount &gt; 78 AND LetterCount &lt; 80 UNION ALL SELECT CHAR(LetterCount) FROM CTE WHERE LetterCount &gt; 75 AND LetterCount &lt; 77
So basically a dumbed down version of http://technet.microsoft.com/en-us/library/ms179859.aspx? Also, if you want alphabetical range restrictions, why not do SELECT fld FROM tbl WHERE fld BETWEEN 'apple' and 'banana' which totally works (up to collation, duh) and will include things like "applesauce" but not "bangle" (i.e. you can restrict on more than just the first character)?
Also you probably want some parenthesis in your where clause. Mixing ANDs and ORs without ()s is bad news bears.
That, + sp_rename for existing and new tables (dont forget to recompile all objects, especially views). If this has to be in-place, another decent option is to use good hashcode for all record values to detect uniqueness. 
I agree. I know that when we interview people at my place of work, we run multiple follow up questions as well as ask for examples of how it would actually be used in code if applicable. Interviewers aren't looking for memorized textbook answers. They want to know if you actually know the underlying concepts and actual use of those concepts.
actually, this particular WHERE clause is fine -- notice the repetition of one condition in both sides of the OR
Thriven, Thank you. It definitely saves time (by almost 50%), but when I ran the queries side by side, my result-sets were different (approximately 30% fewer results in your version). I'm not sure why yet, looking through to figure out what the issue might be. Also, I've never used a "HAVING" clause like you have above. Could you explain what specifically this does? 
does the stupid part come from adding the NSFW tag without reason?
as ridiculous as it sounds, some part of me believes they might literally hold me up to public ridicule-like seriously put me in stockades in the street and yell "hey everybody! this guy doesn't know pl/sql! can you believe he applied for this job?!"
 with Duplicates as ( select a.*, row_number() over (partition by FIELD1, FIELD2, FIELD3) as DUP_NUM from MyTable a) delete from Duplicates where DUP_NUM &gt; 1 
you previously posted this article in this same subreddit, and i advised you to inform your readers that the article wasn't about SQL, but rather T-SQL you responded by deleting your post and re-posting it here, in the same /r/SQL subreddit, with the [MS SQL] in the title dude, i meant the readers of your article, not the readers of this subreddit -- it still looks like an SQL article, *but it isn't* and if you don't know what i'm talking about, then you have no business promoting your article at all go ahead and write all the articles you want, just don't expect a lot of uptake if this is gonna be the type of quality
It's like walking through a minefield. It can work out fine if you make the right steps, but it's dangerous and unnecessary.
Dude, you're talking to somebody who thinks, "Hey, here's this really well known feature that you should totally already know" is worth a blog post.
You need a unique identifier ...without that this is impossible... with it, it is extremely easy.
oh, i agree but sometimes you find yourself in a minefield (somebody else's query) and decide to try to get across it safely (don't re-write large portions of it) instead of taking the time to dig up all the mines 
I saw I had it evaluating EndDate is not null in the first having clause that may be why it had bad output. However, after I wrote that code I realized that I probably shouldn't max on location id. Try the following... --RETURNS THE LOC ID OF THE TRUE OWNERS FOR ALL PIDS W RECORDED OWNERS-- SELECT P2.PropertyID as PropertyID, PC2.LocationID AS T_Owner FROM enterprise.dbo.Property AS P2 left join enterprise.dbo.PropertyContact As PC2 on PC2.PropertyID = P2.PropertyID GROUP BY P2.PropertyID,PC2.LocationID HAVING MAX(CASE WHEN PC2.ContactRoleID = '7' THEN 1 ELSE 0 END) = 1 and MAX(CASE WHEN PC2.ContactRoleID = '3' and PC2.EndDate is not null THEN 1 ELSE 0 END) = 1 Your join of &gt;enterprise.dbo.Property AS P2 &gt;left join enterprise.dbo.PropertyContact As PC2 on PC2.PropertyID = P2.PropertyID is a one to many relationship. So if you ran just the following code: SELECT P2.PropertyID as PropertyID, PC2.LocationID AS T_Owner, ContactRoleID, PC2.EndDate FROM enterprise.dbo.Property AS P2 left join enterprise.dbo.PropertyContact As PC2 on PC2.PropertyID = P2.PropertyID Your dataset my be like this PropertyID, PC2.LocationID, ContactroleID, PC2.EndDate 1 , 1 , 7 , NULL 1 , 1 , 2 , 14-12-2010 1 , 1 , 3 , NULL 2 , 4 , 7 , NULL 2 , 4 , 3 , NULL 3 , 5 , 7 , NULL 3 , 5 , 4 , NULL The HAVING clause allows you to use filter criteria like the WHERE clause but it only applies to aggregates. Using MAX(CASE WHEN PC2.ContactRoleID = '7' THEN 1 ELSE 0 END) = 1 and GROUP BY P2.PropertyID and PC2.LocationID allows you to for each P2.PropertyID and PC2.LocationID evaluate if PC2.ConctractRoleID exists in your case statement, if it does it gives it a 1, else 0. So using the sample data above, each row is evaluated. For propertyid of 1 and location id of 1 the statement does the following. 1 0 0 Now take the MAX of 1,0,0 and you get 1. That evaluates that for propertyid of 1 and location id 1 a contact id of 7 exists. In that same read, the second HAVING filter looks at MAX(CASE WHEN PC2.ContactRoleID = '3' and PC2.EndDate is not null THEN 1 ELSE 0 END) So using the same sample dataset shown above. Your second statement in the HAVING clause will evaluate propertyid of 1 and location id 1 as so. 0 0 1 The MAX of 0,0,1 is 1. This identifies that propertyid of 1 and location id 1 have in the recordset a row that has PC2.ContactRoleID = '3' and PC2.EndDate is null. I hope that explains what the HAVING clause is doing.
I might be missing something here, but what article are you talking about? the only thing I see when following that link is a 1-line where clause.
I updated the article with some more stuff. Thanks for all your feedback. I appreciate it
yup, think of null as something that is "not equal or unequal to anything else, including null"
question on your link: why not just wrap nvl(field,'~') around the two columns?
ROWID is an Oracle concept, not MS SQL Server.
mybad. but it just shows you get what you pay for...
That's still not getting entries in an alphabetical range. That's getting data in which the first character falls in an alphabetical range.
I think the Stanford class you're talking about is [this one](https://www.coursera.org/course/db), which is available on Coursera.
Not sure why would you say going Dev path is 'up'. To me it sounds like a side-grade at best. SysAdmin -&gt; Systems Architect seems like a more 'intuitive' upwards career path, for example. Regardless, getting to know relational technologies would be a complementary skill, I think.
A quick google search shows that nvl is an oracle function? In that case, this would make the query non-sargeable. I'm not familiar with Oracle though.
I like the simplicty of this code. But what if there are a lot more fields inthe actual table. Do I have to write all of them out?
Yes, which is why **[my solution](http://www.reddit.com/r/SQL/comments/24fov8/ms_sql_how_do_i_remove_duplicates_only_when_every/ch73bju)** is better.
select DATETIME (2000-8-1 4:00) YEAR TO MINUTE - 10 UNITS HOUR This works for me, I get: 2000-07-31 18:00