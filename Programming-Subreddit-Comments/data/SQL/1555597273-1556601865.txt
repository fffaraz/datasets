This is probably a great suggestion, and it's where I need to go next. As you can tell, I have a minimal understanding of SQL and am struggling with basic tasks. I didn't even know that integration services was a thing!
You were 100% right and I feel dumb for not thinking of that. Thank you!
Ah, sorry i thought it was homework. Just seemed like a crazy homework scenario! But yea I would tackle it like this: 1. Do the select and unions and either put into temp table or subquery. 2. In the main query, I would change your case statements to this: SUM(CASE WHEN COMPANY = 'COMPANY 1' THEN SALES ELSE 0 END) AS SUM_COMPANY_1
You'll need a plan for handling the IDs or other unique keys. If the databases have overlapping keys values, you can't just UNION them together into the same table because then the keys won't be unique. An order might then be linked to 3 different customers with the same ID. Joining a customer table to an address table might give you 3 addresses for each customer.
No worries, you're trying to help. &gt; Do the select and unions and either put into temp table or subquery. When I do this step, I'm running into trouble because sales should be aggregated, and I can't make the group by work. Previously, I had the wrong example code: SELECT * INTO #tmpConsolidatedSales FROM (select Customer Number, Customer Name, Sales from SalesTableCompanyA UNION select Customer Number, Customer Name, Sales from SalesTableCompanyB UNION select Customer Number, Customer Name, Sales from SalesTableCompanyC) as tmp It should have been something like: &amp;#x200B; SELECT * INTO #tmpConsolidatedSales FROM (select Customer Number, Customer Name, sum(Sales) from SalesTableCompanyA group by Customer Number, Customer Name UNION select Customer Number, Customer Name, sum(Sales) from SalesTableCompanyB Customer Number, Customer Name UNION select Customer Number, Customer Name, sum(Sales) from SalesTableCompanyC Customer Number, Customer Name) as tmp When I do this I get an error code that says, "Each GROUP BY expression must contain at least one column that is not an outer reference."
Ok, that may explain the error I'm getting. It appears that the UNION path won't work, as there are going to be multiple entries per key (customer number).
Is this what you want? WITH cteAllContacts AS ( SELECT ContactID , MasterContactID , ContactStart , CONVERT(int,NULL) AS Duration , 1 AS IsActive FROM ActiveContactsTest UNION ALL SELECT ContactID , MasterContactID , ContactStart , Duration , 0 AS IsActive FROM CompletedContactsTest) , cteRecursion AS ( SELECT c.ContactID AS TopContactID , c.ContactID , c.MasterContactID , c.ContactStart , c.Duration , c.IsActive FROM cteAllContacts AS c WHERE c.MasterContactID = c.ContactID UNION ALL SELECT r.TopContactID , c.ContactID , c.MasterContactID , c.ContactStart , c.Duration , c.IsActive FROM cteRecursion AS r INNER JOIN cteAllContacts AS c ON c.MasterContactID = r.ContactID WHERE r.ContactID &lt;&gt; c.ContactID ) SELECT r.TopContactID , COUNT(*) AS TotalContacts , SUM(r.Duration) AS TotalDuration , MAX(r.ContactID) AS LatestContactID , MAX(r.IsActive) AS IsActive FROM cteRecursion AS r GROUP BY r.TopContactID;
One simple but potentially important difference is that you dont have access to the fields in the subquery. It also can be a matter of performance, although SQL optimizers have come a long way, and in many cases, subqueries and joins are now compiled to the same execution plan.
firstly, only "simple" object names (that don't contain special characters, etc.) can be used as-is (see SalesTableCompanyA, Sales). If you have a space in the name, use double quotes around the name, like this: "Customer Number". secondly, in order to use aggregate functions (sum, max, min, avg, etc.) you need to "group" your data into "buckets". This is done via the "GROUP BY" clause. Aggregate functions then create a single result per "bucket" based on values from that "bucket". so, if you need to get sums per customer number/name, use "GROUP BY "Customer Number", "Customer Name".
I marry I have nothing relevant to say that that analogy is amazing.
Good catches. I have cleaned up the object names in the query I'm using on my local machine. I completely omitted the group by in my original post. Oops :( I've added them in subsequent responses/queries. Thanks again!
This is pretty much exactly what I ended up with. I was not 100% sure if the second block of code that you gave me last time already accounted for what I needed now since the requirements were different. Amazingly, it sure looked like it in my testing yesterday, but I do not use recursive CTEs very often and wanted some confirmation.
The first result is not being cast to datetime, you're just calling the field that is returned "datetime". since it's just a concatenated string you can store any value you'd like in there, so you see the result you are getting. Try this instead and you'll see the same result: select cast(concat('2019-04-18',' ','23:59:59.999') as datetime) as test
Set theory and logic. You don't need more than addition, multiplication, and remanders in programming. Take CS maths or statistics for engineering if you want to get heavy but it's set theory relationships and learning your systems business logic.
You could use the multi-tenant concept and include a tenantid in this new table, and add an additional 'externalId'... to make this data both backward compatible and segmentable.
From the perspective of a BA with lots of SQL Server experience and some Sharepoint, option #2 seems like the more solid way to go. If you're going to the trouble of upgrading, you may as well do it all at once instead of piecemeal. The process of installing the updated version of SQL Server, copying databases, and repointing Sharepoint will be a little tedious but shouldn't be especially difficult. See [here](http://www.sharepointdiary.com/2016/01/move-sharepoint-databases-to-new-sql-server.html) for an example.
This is the exact process we followed when I was working for a fed department in canada. Installed new server, migrated databases and repointed sharepoint. Might sound tedious but there is a TON of documentation and examples of people who have gone through this before.
I think what you want is PIVOT
IMHO doing the in-place upgrade is the "insane" option. Stand up a new VM, then copy everything over using `Start-DbaMigration` from the [`dbatools`](https://dbatools.io/) module. This will copy **everything** over in one command - databases, security, Agent Jobs, Linked Servers, database mail (though from 2008R2 to 2012, you'll have to re-enter the passwords on those), literally **everything**. And IIRC, /u/thebeersgoodinbelgium created it originally _to move her Sharepoint databases_. When will your Sharepoint 2010 support end? How about SQL Server 2012? Get upgraded to the newest possible version of SQL Server here - by migrating to 2012, you're only moving the goalposts a little bit and you're going to find yourself upgrading both SQL Server and Sharepoint relatively soon.
My understanding is that Windows and SQL 2008 R2 are end of life starting 2020. I think Windows 2012 is end of life 2023, and SQL 2012 is out 2022. Seems like if you're going through this trouble, you could save yourself more by going to 2016?
Correct, PIVOT is the easiest way to accomplish this. [Here](https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-2017) is a link to Microsoft's documentation on the PIVOT and UNPIVOT functions for your edification :)
Could you just use a subquery? SELECT * FROM ( SELECT Year(LEDGER."document date") [YEAR], Month(LEDGER."document date")[MONTH], LEDGER."item no_", LEDGER."location code", LEDGER."lot no_", Sum(LEDGER.quantity)[QTY], LEDGER."remaining quantity" [Remaining] FROM "companyserver".dbo."item ledger" AS "LEDGER" WHERE (LEDGER."item no_" LIKE '11%' ) GROUP BY LEDGER."item no_", LEDGER."lot no_", LEDGER."location code", LEDGER."remaining quantity", Year(LEDGER."document date"), Month(LEDGER."document date") ) SUB WHERE QTY &gt; 0 ORDER BY [item no_] ASC, YEAR ASC, MONTH ASC
Just to update all. We are in the process of migrating to SharePoint 2016 on SQL 2016. This is a stop gap measure so we don't get charged for the unsupported platforms. However the migration project to SP 2016 is stalled due to competing priorities. Of course...
Thanks for the tip and link. I am leaning towards moving to a new servers now.
Appreciate the advice. After more research, I agree.
Good luck!!
See the update above. This is temporary while we move things to SP 2016 and eventually move to SP Online.
No, you can't move system DBs between instances. You can use `Start-DbaMigration` from the [`dbatools`](https://dbatools.io/) module to migrate from one instance/server to another. Why do you have 141 servers with only three people using them?
SQL Server 2008R2 goes EOL in 3 months.
SELECT * INTO #tmpConsolidatedSales FROM (select 'A' AS Source, Customer Number, Customer Name, Sales from SalesTableCompanyA UNION select 'B' AS Source, Customer Number, Customer Name, Sales from SalesTableCompanyB UNION select 'C' AS Source, Customer Number, Customer Name, Sales from SalesTableCompanyC) as tmp You can add a static column like that to differentiate the source, you might want more descriptive values and column name though. This will help you keep a record of where it came from as well as adding it to your key should preserve uniqueness.
it was a surprise from audit.... should have refused the gift
Since LEDGER."remaining quantity" is not an aggregated field, couldn't you just place LEDGER."remaining quantity" &gt; 0 into your WHERE clause or am I missing something here?
Oh! Tha should work too, I just figured out another work around too SELECT Year(LEDGER."document date") [YEAR], Month(LEDGER."document date")[MONTH], LEDGER."item no_", LEDGER."location code", LEDGER."lot no_", Sum(LEDGER.quantity)[QTY], LEDGER."remaining quantity" [Remaining] FROM "companyserver".dbo."item ledger" AS "LEDGER" WHERE (LEDGER."item no_" LIKE '11%' ) AND LEDGER."lot no_" IN ( SELECT LEDGER."Lot No_" FROM "companyserver".dbo."item ledger" AS "LEDGER" WHERE (LEDGER."Item No_" Like '11%') GROUP BY LEDGER."Lot No_" HAVING (Sum(LEDGER.Quantity)&gt;0) ) GROUP BY LEDGER."item no_", LEDGER."lot no_", LEDGER."location code", LEDGER."remaining quantity", Year(LEDGER."document date"), Month(LEDGER."document date") ORDER BY LEDGER."item no_" ASC, Year(LEDGER."document date") ASC, Month(LEDGER."document date") ASC
üò¨
Sounds like you can/should do some consolidation.
Yeah. Anyone who wasn't actively working on a migration by the beginning of this year is cutting it _really_ close.
Can someone explain the purpose of this?
Rate RateName|RateFactor :-|-: Rate1|0.1 Rate2|0.2 Data DataID|Rate1|Rate2 -:|-:|-: 1|100.0|50.0 2|200.0|300.0 SELECT d.DataID , u.RateName , u.RateValue , r.RateFactor , u.RateValue * r.RateFactor AS Product FROM Data d UNPIVOT (RateValue FOR RateName IN (Rate1,Rate2)) u INNER JOIN Rate r ON r.RateName = u.RateName;
Came here to say this. Another keyword to search with might be CROSSTAB, but I think that depends on the flavor of SQL you‚Äôre working with.
Either way seems sane enough. Document your service account setups for SQL, and for Sharepoint as that tends to get lost in the ether during these migrations. In place is going to be easier in my opinion than repointing, but that's because I manage more SQL than I do sharepoint. The idea being that it will be listening on the same port, same IP, just reattach the DB's and make sure the permissions are good.
Basically, you have 2 data sets: 1 - "members" 2 - "members who are referrers". Both data sets are pulled from the same table. Your next question - how do you link them? "Member" data set has the link to the referrer in the "recommendedby" column, so you need to follow that link, i.e. take the recommendedby from the "member" dataset. Your former example took a link from the second set and followed that, essentially linking "Member" set as the referrer for your second ("rec") set.
What resources did you use to prepare?
Well it‚Äôs called a table variable but I agree, I‚Äôve always seen insert into used.
You can use INSERT or INSERT INTO.
 SELECT Year(LEDGER."document date") [YEAR], Month(LEDGER."document date")[MONTH], LEDGER."item no_", LEDGER."location code", LEDGER."lot no_", Sum(LEDGER.quantity)[QTY], LEDGER."remaining quantity" [Remaining] FROM "companyserver".dbo."item ledger" AS "LEDGER" WHERE (LEDGER."item no_" LIKE '11%' ) GROUP BY LEDGER."item no_", LEDGER."lot no_", LEDGER."location code", LEDGER."remaining quantity", Year(LEDGER."document date"), Month(LEDGER."document date") HAVING Sum(LEDGER.quantity) &gt; 0 ORDER BY LEDGER."item no_" ASC, Year(LEDGER."document date") ASC, Month(LEDGER."document date") ASC
IN is a way of saying ‚Äúexists in this list‚Äù. So select from the grocery store where food item IN (‚Äòmilk‚Äô, ‚Äòeggs‚Äô, ‚Äòbread‚Äô) would get you anything / all that contains those attributes. So it will return both because it‚Äôs inclusive- like an OR statement
‚ÄúI‚Äôm getting every active employee with either code‚Äù That‚Äôs exactly what IN means. You could rewrite WHERE x IN (condition 1, condition 2) as WHERE x = condition 1 OR x = condition 2
As others have said, IN in this case is just saying if the PAYRCORD value is IN your list, which is why you are getting either code. There is nothing saying the employee has to have both. I'm assuming the employee has two rows in the Pay Code table, one for each PAYRCORD? I'm assuming MSSQL? I think something like this would work: SELECT UPR00100.EMPLOYID, reg.PAYCORD 'PayCode REGHRS', sal.PAYCORD 'PayCode SALARY' FROM UPR00100 emp --employee table INNER JOIN UPR00400 reg on emp.EMPLOYID = reg.EMPLOYID AND reg.PAYCORD = 'REGHRS' --inner join to only the specific pay codes INNER JOIN UPR00400 sal on emp.EMPLOYID = sal.EMPLOYID AND sal.PAYCORD = 'SALARY'
Basically nothing It's nice to have a reasonable understanding of AND/OR/NOT/XOR logic, and any understanding of set mathematics helps... but calculus, statistics etc are mostly useless unless you're doing some fairly niche stuff. Databases are mostly about sets of data, not math
IN is an OR condition. Sounds like you‚Äôre looking for all employees with two records in the pay code table. If that‚Äôs what you want, do a select from the employee table and then an inner join on the first condition of the pay code table and another inner join on the second condition of the pay code table. That will give you all employees that have both records in the pay code table.
IN is just syntactic sugar for multiple OR statements. So take: Where a.field1 = ‚Äòwhatever‚Äô And a.field2 in (‚Äòfirst‚Äô, ‚Äòsecond‚Äô) This is functionally equivalent to: Where (a.field1 = ‚Äòwhatever‚Äô And a.field2 = ‚Äòfirst‚Äô) OR (a.field1 = ‚Äòwhatever‚Äô And a.field2 = ‚Äòsecond‚Äô)
Avoid using in. Table scans. Ick.
 INNER JOIN UPR00400 --pay codes ON UPR00400.EMPLOYID = UPR00100.EMPLOYID WHERE UPR00400.PAYRCORD = 'SALARY' --condition 1 INNER JOIN UPR00400 ON UPR00400.EMPLOYID = UPR00100.EMPLOYID WHERE UPR00400.PAYRCORD = 'REGHRS' --condition 2 Incorrect syntax on the second INNER. Did not expect that. Must have misunderstood.
You can't use where within a join. Change it to AND. INNER JOIN UPR00400 --pay codes ON UPR00400.EMPLOYID = UPR00100.EMPLOYID AND UPR00400.PAYRCORD = 'SALARY' --condition 1 INNER JOIN UPR00400 ON UPR00400.EMPLOYID = UPR00100.EMPLOYID AND UPR00400.PAYRCORD = 'REGHRS' --condition 2
 INNER JOIN UPR00400 --pay codes ON UPR00400.EMPLOYID = UPR00100.EMPLOYID AND UPR00400.PAYRCORD = 'SALARY' --condition 1 INNER JOIN UPR00400 ON UPR00400.EMPLOYID = UPR00100.EMPLOYID AND UPR00400.PAYRCORD = 'REGHRS' --condition 2 Damned close but I've never understood, "The objects "UPR00400" and "UPR00400" in the FROM clause have the same exposed names. Use correlation names to distinguish them."
Argh! PIVOT!! Thank you. I feel like a tool. I couldn‚Äôt seem to get my head around it and thought it had to be something complicated. Much appreciated.
https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://www.amazon.com/Exam-70-764-Administering-Database-Infrastructure/dp/1509303839&amp;ved=2ahUKEwjbjv6Cn9vhAhVFSK0KHcLxDgkQFjALegQIBBAB&amp;usg=AOvVaw35f0Kr3_EKwGTZNC8hFsWx This book and the practice exam they link on Microsoft website. If you get this book, really focus and memorize the queries , the test questions were at least 50% query based
Give them an alias to distinguish them as 2 separate tables. INNER JOIN UPR00400 A ON A.EMPLOYID = UPR00100.EMPLOYID AND A.PAYRCORD = 'SALARY' --condition 1 INNER JOIN UPR00400 B ON B.EMPLOYID = UPR00100.EMPLOYID AND B.PAYRCORD = 'REGHRS' --condition 2
I always think of it as "Is X `IN` this list?"
Use intersect. Seriously. Condition 1 query INTERSECT Condition 2 query;
What do you mean with "Access Predicate" or "Index Filter Predicate"?
Is there a way of doing so with an arbitrary number of rates?
For the IF NOT EXISTS, change the SELECT * FROM to SELECT 1 FROM Declare the fields you're inserting into, at the Mch_ManRoland1 insert. Declare the fields you're selecting from @DT_eachcellgrid .. avoid select * from *** Your error is occurring in the NOT EXISTS statement. You refer to Mch_ManRoland1 but haven't bound it to @DT_eachcellgrid in any way .. you have to link it to @DT_eachcellgrid .. maybe via INNER JOIN, or LEFT or FULL OUTER .. it depends, you'll need to test. INNER would be more performative but might omit stuff Good luck
You have a sqlite database file and you want to quickly expose it over the network and serve queries.
Please show that IN always table scans and never uses indexes. Otherwise it's just your experience that it table scanned on a db you worked on
I never said IN ALWAYS table scans. I advise against them, because what people do who aren't familiar with writing TSQL do is fill a huge group of names inside of an IN statement and complain when it runs slow - and I find that in the majority of cases, they are looking at a column that isn't included in the index. &amp;#x200B; Of course IN doesn't always mean a table scan.
&gt; You can't use where within a join. flat out wrong
Access Predicate is when the index is used for finding the range of leaf nodes to be searched. Index filter predicate is when it is used for filtering the leaf nodes. https://www.qwertee.io/blog/postgresql-b-tree-index-explained-part-1/
So at the end of the day you gave some terrible advice, then complained when nobody got the full richness of the meaning of the 6 words you chose &gt; Avoid using in. Table scans. Ick. I was just checking if your original post was terrible advice, which I thank you for better qualifying, but still think isn't the answer, or if there was some deep knowledge of SQL server implementations. The thing to do when you find IN with table scan is add an index and boop, you're done, database handles the rest just fine.
*indices
https://www.giantstride.gr/sql-indexing-part2/
Simply adding an index to a table isn't always the best solution. As with everything else it depends on the problem and the requirements.
Accepted that shitting indexes everywhere is a poor idea. General case for someone not running `EXPLAIN` on their queries I'd argue they probably are not using `ADD INDEX`, so will generally see a benefit Further, I'd suggest that any case I can think of right now, and INDEX not being the right answer would suggest a poor database design to start with. Happy to learn though. What situation(s) were you thinking of?
So, the way that I would interpret this is as follows: the logic behind SQL queries and relational algebra are the same but the symbols differ widely, like using the capital pi character meaning projection, which maps to (if I remember right) SELECT DISTINCT. If this is for a class assignment, I would assume your class would have gone over these symbols. I don't find myself needing to express queries this way at my job, rather there od just write the query.
You would have to dynamically build the query using system views to insert lists of columns in the right places.
Yes, and the best way to go about it is to stop trying to model arrays by multiple columns (not good, bad, clunky, ugly, use-a-fork-to-clean-your-ears method of modeling arrays), store your data in a (DataID, RateName, RateValue) table and then the result is just a join away.
Relational algebra is a (totally worthless) way of writing a SQL query. It it only use in school, and once you start working in the real world, you will never use it again.
Can spell it either way.
&gt;There are plenty of reasons due to which users need to convert database of SQL Server in CSV format I'm curious as to what these reasons are. Because I only see "here's an easy means by which to exfiltrate data." No user should have a need to dump an entire database and in the process lose all security, data type information, table relations, constraints, defaults, and everything else kept in the database schema itself. If they need to move a database elsewhere, there are much more appropriate ways to do it. If all they're doing is going on a fishing expedition, I have to question the intention.
`IN` is a string of `OR` Literally. That's what the SQL engine does behind the scenes. `UPR00400.PAYRCORD IN ('REGHRS', 'SALARY')` means `UPR00400.PAYRCORD = 'REGHRS' OR UPR00400.PAYRCORD = 'SALARY'` Also, I see your GP queries. Good luck with the payroll stuff. Hopefully you don't have to do any 401k queries... they still give me nightmares.
Can you create a cluster index using the PK and several FK on a table?
 WHERE ColumnA = ColumnB and ColumnB = ColumnC Not sure why you would need partitions?
So in this example would you want to show rows 1 and 2 because they have the same values?
Correct , but I wouldn't show the 3rd row because its the only one with those values in the first 3 columns.
 SELECT * FROM Table1 a WHERE (a.ColumnA,a.ColumnB,a.ColumnC) IN ( SELECT ColumnA, ColumnB, ColumnC FROM Table1 GROUP BY ColumnA, ColumnB, ColumnC HAVING Count(1) &gt; 1 )
Check out PIVOT. May do what you're wanting. &amp;#x200B; [https://www.sqlshack.com/multiple-options-to-transposing-rows-into-columns/](https://www.sqlshack.com/multiple-options-to-transposing-rows-into-columns/)
ahh.. I think this should work . I should've mentioned that columnA, Column B, and Column C are all columns from different tables that I joined , how should I reference them? Thank you for all the help!
[This might help](https://soundcloud.com/user-19077365/the-truth)
Weird bot...
Might be better to create the original query as a view, then just reference the view... or change your original join to have two EXISTS clauses to a void having to clean up the data post-join.
You could use a CTE to do your joins and then reference it as Table1 like in the query
That got it! had to change line two: emp.EMPLOYID Also managed to pull only active employees. Thank you so much.
For future reference Use this line to troubleshoot when it's not running. Print @sql You'll easily see where you're missing quotes. Also dynamic SQL is a hot button topic. I'm off the school of thought that's it's best to be avoided. It's a pain to troubleshoot and whoever inherits the code will hate you. Usually there's a solution that can be found where you don't need dynamic SQL.
I was using SELECT @SQL and then copying it out to test. Just gets really annoying when you need to use multiple `'`'s. &gt;Also dynamic SQL is a hot button topic. I'm off the school of thought that's it's best to be avoided. Sometimes it is unavoidable. For example, that snippet above will run ~500 queries and log the variances... and I'm not going to manually write 500 queries, and run them. &gt;It's a pain to troubleshoot and whoever inherits the code will hate you. Usually there's a solution that can be found where you don't need dynamic SQL. Not my problem. No offense.
The number that you're looking for is 9, not 3 ''' + --&gt; ''''''''' +
Not sure I follow or not, but 3 works just fine?
[Is this you](https://i.imgur.com/Ub6BZHe.jpg) OP?
Nvm please - thought you wanted to get quoted column values from the tables.
If you are doing dynamic SQL this way, you are making it harder than it needs to be! You can operate on the result like its a string and use the REPLACE() function at the end. Also, this isn't going to work for you, but if you were using dynamic SQL with real parameters and not just replacement table/columns, sp\_executesql lets you pass variables to it. One final note - Google SQL injection.... sanitize your inputs! DECLARE @SQL nvarchar(MAX) = 'SELECT ''@SourceUAT'' , ''@ColumnUAT'' , COUNT(*) FROM ( SELECT @ColumnUAT FROM @Source EXCEPT SELECT @ColumnUAT FROM @SourceUAT UNION ALL SELECT @ColumnUAT FROM @SourceUAT EXCEPT SELECT @ColumnUAT FROM @Source ) X' SET @SQL = REPLACE(REPLACE(REPLACE(@SQL, '@Source', @Source), '@SourceUAT', @SourceUAT), '@ColumnUAT', @ColumnUAT))
Agreed, GP is a mess. Been learning SQL so I can tame this animal.
How am I going to execute the @SQL to get the query to run? And how is using REPLACE() simpler? This is not for a stored procedure, and I'm not sure how there is any threat of injection. In the upper part of my query the @Souce parameters are being generated from SYS.TABLES.
My full code is below. I need to remove the @UATList piece because I can just modify @SourceCols, but you can probably see what I'm using this snippet for: DECLARE @UATList TABLE ( ID int identity(1,1) , TableName nvarchar(255)) INSERT INTO @UATList SELECT TABLE_SCHEMA + '.' + TABLE_NAME AS 'TableName' FROM AnalyticsMapping.INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_NAME LIKE 'ds%_UAT' DECLARE @SourceCols TABLE ( ID int identity(1,1) , TableName nvarchar(255) , ColumnName nvarchar(255)) INSERT INTO @SourceCols SELECT SCHEMA_NAME(A.SCHEMA_ID) + '.' + A.NAME AS 'TableName' , B.NAME AS 'ColumnName' FROM SYS.TABLES A INNER JOIN SYS.COLUMNS B ON A.OBJECT_ID = B.OBJECT_ID INNER JOIN @UATList C ON C.TableName = SCHEMA_NAME(A.SCHEMA_ID) + '.' + A.NAME DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @SourceCols) BEGIN DECLARE @Source nvarchar(255) = (SELECT TableName FROM @SourceCols WHERE ID = @Loop) DECLARE @SourceUAT nvarchar(255) = (SELECT SUBSTRING(TableName, 0, LEN(TableName) - 3) FROM @SourceCols WHERE ID = @Loop) DECLARE @ColumnUAT nvarchar(255) = (SELECT ColumnName FROM @SourceCols WHERE ID = @Loop) DECLARE @SQL nvarchar(MAX) = 'SELECT ''' + @SourceUAT + ''' , ''' + @ColumnUAT + ''' , COUNT(*) FROM ( SELECT ' + @ColumnUAT + ' FROM ' + @Source + ' EXCEPT SELECT ' + @ColumnUAT + ' FROM ' + @SourceUAT + ' UNION ALL SELECT ' + @ColumnUAT + ' FROM ' + @SourceUAT + ' EXCEPT SELECT ' + @ColumnUAT + ' FROM ' + @Source + ' ) X' INSERT INTO #UATValidation EXEC sp_executesql @SQL SET @Loop = @Loop + 1 END
&gt;Sometimes it is unavoidable. For example, that snippet above will run ~500 queries and log the variances... and I'm not going to manually write 500 queries, and run them. What do you mean 'code reuse'? I have one structure that will move data from any table to any table. And another that will dynamically build the nested where ins. All dynamic sql. My only suggestion, for ease of readability. Is that you get rid of ' + @var + ' and replace it with some sort of token like... &lt;&lt;SELECT LIST&gt;&gt;. then repace(@sql_tokenizedscript,'&lt;&lt;token&gt;&gt;',@var). Easier to see the quotes too.
At the end of the day, you're still doing `EXEC(@SQL)` right? Using this REPLACE() technique just makes the stringy SQL easier to handle than polluting it with double ticks and pluses. If your way is now working, that's fine.
I guess what I'm trying to say is that I am trying to get the values in the @SQL field, not just adding them on to the end of the string.
Can you give me an example? I'm genuinely curious but not sure what you mean by "token."
@sql = 'select &lt;&lt;select list&gt;&gt; from &lt;&lt;table&gt;&gt;' @select list = @select list + column + ',' from x Replace @sql, '&lt;&lt;select list&gt;&gt;', @selectlist
https://youtu.be/sZiK81u0vJc 17 minutes in or so
How does dynamic SQL evaluate a `&lt;`? This approach might actually address an issue I'm going to have in the second part of this UAT.
No no. It's not syntactic. I could do $$token$$ the idea is to just create something to replace() that wont ever appear in code. I posted a vid. Show it in a cursor,running, all that stuff.
I'll check it out, swamped today, we have a major database migration happening, as well as a switch to a new ETL tool at the same time. But IT has changed multiple columns and data types for the ETL integration... and we're supposed to somehow test to see if the new data is the same or not. We have about 60 tables which are used in some form or another to populate over 500 visualizations. No bandwidth for our team to do UAT on all the visualizations, and doing a straight SELECT * FROM TABLE EXCEPT SELECT * FROM TABLE_UAT will always fail because of the changes they made.... except they can't tell us what they changed. So I figured we'd check column by column for our base tables, and any columns which are flagged as changing we can evaluate on a case by case basis. The problem is that it went from 60 except queries using *, to over 500 except queries using column names.
Excuse my error if I'm mistaken; but, isn't there a wizard to export to fixed width flat text file? I'm assuming you're doing SQL Server
What flavor of SQL are you using? If you're using MSSQL Server look into using `bcp` to do this or SSIS.
SQL Server and yes, there is a wizard but I'm just an apprentice. He left before I could finish learning SQL from him and when I tried to do the wizard I got stuck for hours. I'm struggling to get the export working even once.
Ok. I have no experience with that but I'll try Google.
I use "{}" since you can then use python f strings to generate the code.
Sounds good, thanks
Thanks. This is a good start. How do I refine my query to only show top values when I have duplicates? For instance, if I have five loc1 values and another column with dates, how do I show only the loc1 with the most recent date?
I use json in sql, sometimes {} breaks it, so I use &lt;&gt; to prevent xml use hehe.
Lol! Nice...json is so much better than xml. Just FYI, if you want to escape use {{}}. At least in python, I don't really use SQL to generate SQL anymore since python is better for that in virtually every way.
Select TOP 1 (distinct(someShit)) Order by DateColumn Desc
You should be able to schedule reporting in the wizard if I‚Äôm not mistaken.
If you're using TSQL, the **string\_split()** function can be used ( [https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017)). This became available in Server 2016. SQL Fiddle example: [http://www.sqlfiddle.com/#!18/57af6/5](http://www.sqlfiddle.com/#!18/57af6/5) CREATE TABLE table1 ([id] int, [vals] varchar(1)) ; INSERT INTO table1 ([id], [vals]) VALUES (1, 'A, B, D, E'), (2, 'A, D'), (3, 'B, C, E, F'), (4, 'A, B') ; select * from table1 cross apply string_split(vals, ',') If this function is not available to you natively, you could create a new scalar value function yourself. Google around for something like fn\_split() or split() and you'll find loads of examples. Hope this helps!
Have never worked with bcp, but in xml null is implemented with either lack of element or `&lt;field xsi:nil="true" /&gt;`. `xsi` here is xml namespace `xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"`
What variant of SQL are you using? EXPLAIN isn't something I'm familiar with. To answer your question, I live in an OLAP world. Generally speaking the data doesn't change from moment to moment, thus if a query is performing poorly it's more likely to be an issue with the query than with the design / indexing strategy My first port of call is, generally speaking, to look at the queries and then at the design. Especially if we're dealing with databases in the TB size. Often times it's just not feasible to add an index to a very large table.
Is the company not replacing him? Seems a lot to throw on your shoulders.
Ya it's not fun. I started as a Business Systems Analyst 2 years ago officially. Now I'm taking over the duties of someone with 35 years experience plus my own stuff. I think I figured it out but my brain hurts so I'm going to see if it makes the file tomorrow morning at 6 am.
 *There are plenty of reasons due to which users need to convert database of SQL Server in CSV format* uw0tm8?
I would concatenate before you pivot. So with the data on the right make a column that is Attribute1||‚Äô-‚Äô||Attribute2 then pivot on
I've been downvoting every post by this user, who is the same as Vidyakant. All garbage. Hopefully mods will start removing the posts for excessive self promotion.
Linked servers. Idk how to set them up, but that‚Äôs probably the way you want to go. You can also access databases across servers without creating linked servers but I don‚Äôt remember the syntax at the moment.
also, it's a table variable, not a temp table.
Here's what I threw together... it's not my best looking code, but hey, it's a holiday! Put it inside a cursor loop to set @t1 and @t2, or whatever floats your boat. The code below assumes that the column ordinals are the same between tables, but you could also use the column names (commented out below). I tried it on some sample tables and data that I made up, but I'm not sure how that maps to your situation. Anyway, here it is... LMK if there is something missing or misunderstood. &amp;#x200B; declare @t1 nvarchar(200), @t2 nvarchar(200), @sql1 nvarchar(max) = N'', @sql2 nvarchar(max) = N'', @sql nvarchar(max); set @t1 = 'Table1'; set @t2 = 'Table1\_UAT'; &amp;#x200B; select @sql1 += N', \[' + [c1.name](https://c1.name) \+ N'\]', @sql2 += N', \[' + [c2.name](https://c2.name) \+ N'\]' from sys.columns c1 cross join sys.columns c2 where c1.user\_type\_id = c2.user\_type\_id and OBJECT\_NAME(c1.object\_id) = @t1 and OBJECT\_NAME(c2.object\_id) = @t2 and c1.column\_id = c2.column\_id; -- [c1.name](https://c1.name) = [c2.name](https://c2.name); &amp;#x200B; set @sql1 = RIGHT(@sql1, LEN(@sql1) - 1); set @sql2 = RIGHT(@sql2, LEN(@sql2) - 1); \-- select @sql1, @sql2; &amp;#x200B; set @sql = '(select ' + @sql1 + ' from \[' + @t1 + '\] except select ' + @sql2 + ' from \[' + @t2 + '\]) union all (select ' + @sql2 + ' from \[' + @t2 + '\] except select ' + @sql1 + ' from \[' + @t1 + '\])' \-- select @sql; &amp;#x200B; exec (@sql);
I'm going to be awarding things on Sunday just to see what comes in over the weekend, but cool approach to start. Thanks for your contribution.
I am facing the same issue.....Want to register for the 98-361 but the Pearsonvue Link to Register the Exam doesn't work(I actually went to Personvue site and was sent back to Microsoft) ...the other option left is for the Microsoft Employees....Does the Microsoft Online Exam work?.....FYI Passed the System check...Has anybody been successful with the booking ? Any Advise
Thank you very much for your help.
This was my thought as well. Never worked with bcp, but sounds like some research needs done there. Not sure this can be solved just by trying to sub out a string in SQL.
Can you publish the data to your final reporting tool and let that tool whatever it is handle the logical joins between the different data sources? It's pretty trivial with powerbi and ssrs. And I've had to go that route at some jobs before because it was just a pain in the ass to have the network and security admins manage the linked servers.
Cross server joins are notoriously poor performing. Pull everything you want from one server and stick it into a #table. Join the tables in that server to the #table. If you're doing complex stuff, you'll want to check out ssis or another similar ETL tool. In order to set it up, you'll want to research Linking servers.
Thanks R/sql family. I can use PowerBI to do some of the final joins. It‚Äôs really a Cartesian join I wanted to do to create a semantic type of skeleton table upon which I write Dax from other tables. I did find a way to do that with M in Power Query today, so it will function. It will be a bit more convoluted, but will work. It‚Äôs essentially the solution two of you suggested. Again, thanks so much for the guidance.
You're looking for an "anti-join". It's a concept and not a discrete thing in the language, so you won't actually type "ANTI JOIN" in your code, and there's more than one way to do it. In your case, I would probably just start with message, left join to exclusion on all of the columns you need to be a match, then have a condition in your where clause that exclusion.id is NULL.
Okay, I'll give that a shot. Thanks!
Why???
Sqlite
Why is the second way better?
‚Äúthe statostical probability of a select‚Äù this doesn‚Äôt make sense. What do you mean you want the top 1? You only want a single value but you want it to be random every time you run the sql query? You can order by weight DESC if you want to and select the top rows. But I don‚Äôt know if that‚Äôs what you‚Äôre trying to accomplish.
Step 1 is going over what needs to be done with your manager while being very frank about how much you are having to learn, get the prioritizations and his buy in on setting realistic timelines. Things where you don't know what you need to learn usually take at least a week: they need to make the phone calls to say things will be delayed while you figure out how to get them done, assuming they cant help. The name of the game is automation. Get it working without you having to touch it again. Your top 3 things to learn are (I'm assuming you are on a Microsoft sql stack if you us oracle you should be calling the headhunters now, and asking your manager for an experienced contractor to help stabilize, you probably should ask for that anyways just so they understand the pickel you are in.): 1 SSIS, so you can automate what you are doing, 2. the SQL task scheduler, so you once you have it working right you just get an email telling it's done, or a different email telling you it failed 3. SFTP for transfers that leave the company and or SSIS script task and any APIs it needs to talk to if your exports go to external automated systems depending on how it's set up. For your fixed width ASCII it's going to be a data type thing, and be careful you don't open it as a csv and then save it: excel will trim the extra white space if there are not text qualifiers i'e double inverted commas/quotation marks.
This would really be something to solve in the application layer I would think, but maybe something like this would do it? WITH PercentChance AS ( SELECT id, name, weight, SUM(weight) OVER(order by id rows between unbounded preceding and current row) AS WeightSum FROM dbo.randomizer ) SELECT TOP 1 * FROM PercentChance WHERE weightsum &gt;= RAND() * 100 ORDER BY WeightSum
Thank you so much. This is good advice, beyond the issue I'm dealing with. I think I have it mostly figured out. Still want to figure out how to email the file to a co-worker somehow otherwise I'm good.
Oh wow brilliant... What do you think of this? Select top 1 name from table Order by abs(checksum(newid())) / [weight]) When run stats it‚Äôs kind of off by 5-10% off above their weights. I did look into a CTE but yours looks quite logical if the variability is better than what I have.
Okay so something to be really mindful of: data latency. How fresh does the data need to be. A lot of people will want live data, that cost PowerBI licenses, and if you tell them you can only deliver that a cost of 10$/month/seat (it might be better now) of people who can see it they will calm down. If you go the live route performance might be an issue. When I worked with power BI I put a data freshness date on the visualization.
You are very welcome, and while the boss needs to do the schedule negotiations DO reach out to the business customers or their reps on requirements you don't understand.
Just tried your code but I keep getting a static result. No variability on weight :(
Oh, it seemed to work in mssql. What rdbms are you using?
I‚Äôm using SQL 2017. That is weird.
What database are you using, what is the error, what do your table definitions look like, and what is your code with the join? There isn't enough information to make a diagnosis.
My guess is he is dummying up some data.
[removed]
My car isn't working, can you tell me what's wrong with it?
&gt; &gt; &gt;SELECT Tour\_Details\_Guest\_Sales\_Rep, &gt; &gt;COUNT(CASE &gt; &gt;WHEN(DATEDIFF(month, '2019-03-01', Tour\_Details\_Tour\_Date) = 0 &gt; &gt;AND Tour\_Details\_Tour\_Date &lt;= '2019-03-01') &gt; &gt;THEN 1 &gt; &gt;END) AS MTD, &gt; &gt;COUNT(CASE &gt; &gt;WHEN(Week\_number IN A1.Week\_number &gt; &gt;THEN 1 &gt; &gt;END) AS WTD &gt; &gt;FROM Tour\_Details &gt; &gt;JOIN Sales\_Rep ON Sales\_Rep\_Name = Tour\_Details\_Guest\_Sales\_Rep &gt; &gt;AND ISNULL(Manager, '') &lt;&gt; '' &gt; &gt;AND Sales\_Rep\_Status = 'Active' &gt; &gt; &gt; &gt;left join (select distinct Week\_number, Sales\_Rep\_Name from Tour\_Details where Tour\_Details\_Tour\_Date='2019-03-01') A1 on A1.Sales\_Rep\_Name = Tour\_Details\_Guest\_Sales\_Rep &gt; &gt; &gt; &gt;WHERE Tour\_Details\_Guest\_Status IN('Qualified', 'Courtesy Tour') &gt; &gt;AND Manager = 'SHANE' &gt; &gt;GROUP BY Tour\_Details\_Guest\_Sales\_Rep; &amp;#x200B; Check this one. You need to put your select into FROM. You cant put selects in aggregate functions so all the stuff you need you must delcare in FROM
Thanks Man! It's working but i need to edit lil bit SELECT Tour_Details_Guest_Sales_Rep, COUNT(CASE WHEN(DATEDIFF(month, '2019-03-01', Tour_Details_Tour_Date) = 0 AND Tour_Details_Tour_Date &lt;= '2019-03-01') THEN 1 END) AS MTD , count(CASE WHEN(a.Week_number IN (A1.Week_number)) THEN 1 END) AS WTD FROM Tour_Details a JOIN Sales_Rep ON Sales_Rep_Name = Tour_Details_Guest_Sales_Rep AND ISNULL(Manager, '') &lt;&gt; '' AND Sales_Rep_Status = 'Active' left join (select distinct Week_number from Tour_Details where Tour_Details_Tour_Date='2019-03-01') A1 on 1 = 1 WHERE Tour_Details_Guest_Status IN('Qualified', 'Courtesy Tour') AND Manager = 'SHANE' GROUP BY Tour_Details_Guest_Sales_Rep;
You're welcome :) Have fun with SQL :)
Little more info would be useful. But, in general, joining on an nvarchar is no different than joining two int fields, etc. Also, email addresses can be way longer than 50 chars.
Every time I press the pedal it turns left.
I think your car is actually an airplane. Ticket closed, please submit your question in be airplane forum.
0/5 on survey. Tech was very rude and did not assist with my motorcycle issue.
So, i have to so this project for my college in which i have to create a db for a city hall. I tried to create it from scratch and this is what i came up with. There are salaries (salarii), is the asministration of the city hall, like mayor and his staff.(administratie) there are projects for the city. I don't know how or between which should i do the join. Please help
That will *probably* depend on which DBMS (MySQL, SQL Server, etc.) you're using. Try googling "[name of DBMS] string as primary key" and see what results you get.
I wouldn't think so. Maybe if it were a `varchar`, but that's just a suspicion. The best way to find out is to measure!
this is difficult to help since I don't understand any of the variable names
Do you mean a join statement or do you mean you need to link three tables together in the database structure?
Agreed. This is just very vague. Best I could come up with is googling SQL46010 and got syntax error on SQL Server and another error on Oracle. If the problem is similar, it could be that they need to place brackets (or use QUOTENAME) around a column that is also a reserved keyword (user, system, etc). https://stackoverflow.com/questions/41453259/sql46010-syntax-error-when-creating-a-stored-procedure-in-visual-studio-15 If it‚Äôs on the Oracle side (which I‚Äôm doubting at this point): https://docs.oracle.com/html/E17766_01/e46000.htm Then it seems to be that a value is too long for the column it‚Äôs being inserted into.
An example with table1, table2 and table3 would help me alot üòÅ
A join statement, between pro_administratie, pro_salarii and pro_angajati, if is possible
Without a primary key your table is a "heap". If you create a primary key there will be a clustered index created automatically. This will make your query if your predicate is your PK much quicker. There is a lot lot more to this and other people will expand but this answers your question.. I suggest doing some basic tutorials on index and execution plans
Honestly, I have never derived any value from charts like that. You can discard that forever if you want to. Saying "I need to do the join" doesn't really tell us what you need. There's lots of plausible joins here. A good question will look something like this: &gt; I have these tables for city hall. Three involved tables from the set are the administrators, the projects, and the salaries. I need to find the salary of all administrators working on projects starting after October 4. Or something like that. And then we'd say something like select sum(salary.amount) from projects join administrators join salaries on projects.owner = administrator.id and salaries.holder = administrator.id where start_date &gt; date("July or whatever") Your homework isn't "implement a join on three tables." That would be like if you were being taught to drive, saying "I'm supposed to grab this wheel and turn." If someone doesn't know you're in a car, they'll have no idea what to tell you to do.
Thank you! We are studying this for 9 weeks now, believe me i have no ideea what i am doing there...
nine weeks? what the fuck, this should take two or three days
Slow down your query versus what? Is this versus choosing a different candidate key with a different data type? Or having no primary key at all? If you do not set the string at the primary key, will it still be indexed in any way? The question is very open ended and depends on a lot of factors. However, one overarching fact is that 10,000 entries are not very much, even for wide rows. So more than likely, you should see the same or better performance.
Nine weeks with 2 hours/week. They did not teach us practical stuff, just theoretical.
ArticleCode is PK, but what if I instead use a auto generated PK and ArticleCode as just an UNIQUE field for ex. Would that increase performance? Because I query in my code on articleCode and not on ID if that makes sense.
No, certainly not at that size. That table is tiny. The main limiting factor for key performance is usually the number of bytes they use, especially if they are going to be used as foreign keys. Integers are good because you can fit a lot of them into 8 bytes (32-bit integer) very efficiently. String codes have a lot of wasted space since a lot of characters are not used for coding. The issue is that wasted space still takes up disk, memory, and IO time. Most RDBMSs have key size limits, so you should make the field as short as practical. Longer than 25 characters and I would be suspicious. I would not have any concerns with a table that only has 20,000 records, unless those codes were going to be used in dozens of tables or many millions of rows.
Just the simple act of making a new column as a surrogate key and a PK should not increase performance. If you change all of your joins to reference the new surrogate int column, it would probably increase performance on a theoretical basis in which you wouldn't even notice. Also, using an auto-generated int would potentially make inserts faster on at least SQL server, if you followed the typical practice of clustered index on PK and then you could have a fill factor of &lt; 100% on your ArticleCode index, to reduce page splits.
when I was job hunting it looked like that‚Äôs what a BI Developer is: report development and automation. So you gotta know a bit about architecture to set up pipelines for automating reports, and then also be able to build dashboards, like in Tableau
Awesome. Great feedback!
Whether a primary key results in the creation of a clustered index is dependent on db engine settings. For a PK an index doesn't have to be clustered and the benefit of a CLUSTERED index is dependent on the data and the queries against the table.
Your PK should be meaningless
Please please, please use left outer join or inner join not just join
look, sql's easy. strap in; we're gonna move fast . # How to think about SQL fundamentally SQL is a collection of bags. you call them tables (or views or whatever, but shut up; we're marching.) a bag is a grid. it's mostly rows, but someone drew columns on them those rows are often ordered. they don't have to be, but it makes a pretty walloping big performance difference, so, they usually are. most commonly, there's just a zero-indexed integer counter (typically called id, index, or rownum.) those counters are usually pretty close to dense, but you should be ready for gaps, because shit gets deleted, ***mang*** *chicka chicka* sql's original job was to staple those bags together on request in (often temporarily) meaningful ways get your staple gun, fucker. . # How to think about queries generally speaking, the form of most queries to databases is "give me these rows, given that they match those rows but not those other ones, group them by this thing, pull this number out, and i dunno, do a waltz i guess" the database's job is to provide you what you asked for, and if it's not mssql, to be fast about it a well designed database (read: none of them, ever) is basically just a collection of fact tables, which has been whittled down to prevent repeating information, because repeating information is dangerous (what do you do about disagreements? this table says bobby was born on the 25th, but that one says the 23rd) . # How to think about tables when i say fact tables, i mean data. sql's main job is to know things for you. so a table might be something like "here are our customers." when you list the customers, you'll list the important singular facts about them. their username. the email their password resets go to. their childhood fears. how much storage they pay for. the notes from your blackmail book. i say singular facts because some facts aren't singular. by example, i have three phone numbers (home, cell, work.) other people have two, or four. databases in sql have fixed column counts, so either you be the actual worst person and say "we'll have nine phone number columns," or you do it right (because eventually you'll have a customer call in like "my tenth phone number doesn't work") non-singular facts get their own table, which is tied back to the original. so i now also have a "phone numbers" table that is just rows of a phone number and the id of the person they're tied to. a well designed database (read: none of them, ever) has *lots* of narrow tables like that. it probably sounds messy now, but when you start tying things together you'll get it; it's actually way, way cleaner. did you get your staple gun? well hurry the fuck up, we're almost there. . # How to think about queries so the gag with SQL is you can make arbitrarily long chains of "this, tied to that, tied to this other, but only under these circumstances, and only on a tuesday." so it's actually perfectly reasonable to want something like "show me every manager in the company who's been here between two and four years, hasn't gone through safety training, is at a site containing any one of these six dangerous industries, is on a team without extended insurance coverage, and is up for a training package" what SQL will do is a little hard to understand at first, but once you do, it's super duper straightforward . # We'll need some example data start with just "the staff at the locations" your staff table is going to look something like id name role salary training location - - - - - - 28 mary stevens site manager 181,000 chemist chapel hill 29 ayombe mulimine head of engineering 178,000 administrator chapel hill 32 fang-yu tham safety coordinator 193,000 consultant pittsburgh 33 dan merriweather engineer III 165,000 chemist chapl hill 35 tommy alwinde engineer II 143,000 chemist pittsburgh there's actually a pretty serious mistake there (normalization) but i'll get back to it because it won't make sense yet. but it will apply to the location that dan merriweather finds himself in, because that typo is intentional. there's an "e" in chapel. it starts at 28 because we fired all the founders. 30 and 31 got caught having sex in a stairwell and got fired too. 34 was redacted by the NSA. with a gun. (normally we'd just add a column "deleted" and keep the data, or move it into a history table, but i'm trying to make a point about skipping IDs) your location table is going to look something like id name - - 1 pittsburgh 2 chapel hill 4 le fronce, all of it 5 miami we deleted the city of minneapolis, which is why there's no 3. no, not the row. the city. . # We'll also need a task so like suppose i want to know how much money is being spent per location. conceptually, there's about four-ish steps there: 1. figure out the lists of people and locations 1. cross-reference them 1. bucket them according to location 1. sum the salary column per bucket . # So fucking do it Which turns out to be easy. Step 1 means "put the data in the database, asshole." Step 2 is `... people JOIN locations ON (people.location LIKE locations.name) ...`. What you're saying there is "when we do the thing, treat the people table's location column as if it was matched to the location table's name column." Step 3 is `... group by location.name ...`. What you're saying there is "when we do the thing, treat location as buckets, and any stuff we do to lots of rows, bucket a value from this column to figure out which ones." Step 4 is `... sum(salary)...`. What you're saying there is "when we do the thing to lots of rows, it's a sum over the salary column." What you end up with is select sum(salary) from people join locations on people.location like locations.name group by location.name; . # Why did that look easy? And that's kind of really how you do SQL: write out what you need in a pedantic and boring way, then slog through it one lego at a time. It looks hard because it's so different, but it's actually rly easy. Everyone's problem is they try to look at an SQL query like it's one line of code. Look at an SQL query like it's an entire fucking program. Adding a line is adding a clause. Do it step by step, and use the SQL console like a repl. Using the SQL console like a repl is ***a really big deal***. Do not attempt to skip it. I've been doing SQL 20 years and I still struggle if I don't have a console to work in. Go to your console. Type `mysql` or `pgsql`. If it doesn't work, get some shit installed until it does. . # Member berries, SQL edition select sum(salary) from people join locations on people.location like locations.name group by location.name; That query will give you three rows. The salaries for `pittsburgh` summed, the salaries for `chapel hill` summed, and the salaries for `chapl hill` summed. &gt; Record scratch. It's that fucking typo in fucking Dan fucking Merriweather's fucking row. Fuck. . # That's not Old Man Witherspoon. That's ... So you know how I told you that SQL's job is to know things? SQL is Jeff Albertson. SQL's finger is *always* in the air. SQL's favorite word is "actually," but more specifically in *that* voice. Even when you're right, you're wrong, and it's SQL that's going to know why. And you should value this. This is incredibly important. If you're a bank, you want *no* errors in your data. SQL is an amazing language for making large classes of error impossible. And so what you need to do is keep JeffQL in the loop. If you tell Jeff "this is a person and his name is Bob Jones," JeffQL will shrug and say "okay." If on the other hand you say "this is a green lantern and his name is bob jones," JeffQL will spring into action. "Ekshully, there exists a list in issue 418 of all Green Lantern planets with Earth-like naming systems. Only six sectors containing such planets exist: Earth and Ungara in sector 2814, Kaban in 1561, Thanagar in 2682, Rann in 430, Korugar in 1417, and Pharma in 2345. The Greens Lantern of Earth, Ungara, Kaban, Rann, and Korugar are well known. Pharma's green lanterns are trapped in the dying universe. Thanagar only produces female Lanterns. As such there can be no Green Lantern named Bob Jones. I decline to add this to our data." Then JeffQL will chuckle and sip its soda, and go back to watching 80s godzilla cartoons. This is that "normalization" thing I was talking about earlier. . (continued, too long)
# But I wanted to be done half an hour ago Here are our two tables again: `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 chemist chapel hill 29 ayombe mulimine head of engineering 178,000 administrator chapel hill 32 fang-yu tham safety coordinator 193,000 consultant pittsburgh 33 dan merriweather engineer III 165,000 chemist chapl hill 35 tommy alwinde engineer II 143,000 chemist pittsburgh `location` id name - - 1 pittsburgh 2 chapel hill 4 le fronce, all of it 5 miami The `location` column in `staff` is poorly chosen. Because it's a string, typos can be a thing. Also, how do you know if that's Pittsburgh PA, or Pittsburgh CA? (It's PA. It's always PA. Just saying, in principle.) So instead of having that string there, what we should actually do is have the `id` of the location, instead of its string name. And because there is no table entry for `chapl hill` (again note the misspelling,) the mistake in dan merriweather's record becomes impossible. `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 chemist 2 29 ayombe mulimine head of engineering 178,000 administrator 2 32 fang-yu tham safety coordinator 193,000 consultant 1 33 dan merriweather engineer III 165,000 chemist 2 35 tommy alwinde engineer II 143,000 chemist 1 Similarly, we would want to make a table of rules, and a table of trainings, and indicate those also by ID. And so now we match with `=` instead of `like`, because `like` is just for strings. select sum(salary) from people join locations on people.location = locations.id group by location.name; Our data has just been "first form normalized" - we removed a piece of repetition. Repetition is dangerous. We don't repeat city names anymore. We just have a table of cities, and give them by their ID number. Much safer. This also means that later, if we decide to rephrase "engineer III" to "senior engineer," we change the label in one place, every single fucking query gets updated immediately and universally, and no meaningful data is changed at all. it's also hella fast to fix. . # But I don't know what "location 2" is So then, you just ask SQL to ship the string back out with the answer. We used to have: select sum(salary) from people join locations on people.location = locations.id group by location.name; So we'll add a bit to the select on the front asking for the name as well select sum(salary), location.name from people join locations on people.location = locations.id group by location.name; And now our result should be two rows of two each columns: `(chapel hill, 524000), (pittsburgh, all the money)` . # Chaining queries for maximum justice And look, we could keep chaining the query up from there. What if you only want the answer for chemists, and to exclude consultants and administrators? select sum(salary), location.name from people join locations on people.location = locations.id where people.training like 'chemist' group by location.name; Or, you know, maybe you already extracted training out into a table, so it'd be two joins instead. That would be better - your data should be as normal as is practical. Then we would have `training` id label - - 1 chemist 2 physicist 3 administrator 4 accountant 5 consultant 6 kung-fu historian Which means you also have `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 1 2 29 ayombe mulimine head of engineering 178,000 3 2 32 fang-yu tham safety coordinator 193,000 5 1 33 dan merriweather engineer III 165,000 1 2 35 tommy alwinde engineer II 143,000 1 1 And then you'd ask the database something like select sum(salary), location.name from people join locations on people.location = locations.id join training on people.training = training.id where training.label like 'chemist' group by location.name; *Much* nicer. `:)` . # Mind opening meme X-Ray Storing an address as text Brain art Storing an address as several text fields Brain lit up Storing the zip code as a length-limited text field Brain with lines Storing the zip code as an integer Sine waves behind Using the word "whomst" Chakra guy Storing the zip code as a non-negative, size limited integer Union with universe Making a table of zip codes and joining to it, to exclude Centralia Titan reaching to sun Stored trigger that dispatches a human courier to verify on insert Big bang guy Remembering that you have Canadian customers who don't have zip codes . # Who really is Jeff Albertson SQL has four jobs. 1. Keep the data durably. If there's a power outage, hackers, and the datacenter gets hit by a bomb, I expect you to restore gracefully. 1. Refuse to accept any data that doesn't fit the rules we set. (Corrolary: set *extremely* strict rules when you are able to. Postgres is very good at this.) 1. Find fast ways to get answers to really complicated, frequently one-off questions 1. Provide a high security isolation environment that no programmer since the late 80s even knows exists . # In summary First, make sure that the thing you're asking for is something that a person with no context will feel like is answerable. "I have this specific data that I need out of the database. It should be shaped like this. Here are the considerations to keep in mind." No, don't count joins. Focus on what information you're attempting to receive. That's what this tool does. After that, stop trying to do your homework. Start trying to do *the first step* in your homework, instead. SQL doesn't happen all at once. It's the most incremental programming you've ever done. And now you speak SQL.
I hope so, I'm a report developer. I dislike reporting from live databases so I usually create reporting environments, pulling data using ETL's, ssis jobs and occasionally use R. Then we get on to the reporting tools, ssrs, tableau etc. It really can be a multiple disciplined position.
&gt; But I wanted to be done half an hour ago Here are our two tables again: `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 chemist chapel hill 29 ayombe mulimine head of engineering 178,000 administrator chapel hill 32 fang-yu tham safety coordinator 193,000 consultant pittsburgh 33 dan merriweather engineer III 165,000 chemist chapl hill 35 tommy alwinde engineer II 143,000 chemist pittsburgh `location` id name - - 1 pittsburgh 2 chapel hill 4 le fronce, all of it 5 miami The `location` column in `staff` is poorly chosen. Because it's a string, typos can be a thing. Also, how do you know if that's Pittsburgh PA, or Pittsburgh CA? (It's PA. It's always PA. Just saying, in principle.) So instead of having that string there, what we should actually do is have the `id` of the location, instead of its string name. And because there is no table entry for `chapl hill` (again note the misspelling,) the mistake in dan merriweather's record becomes impossible. `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 chemist 2 29 ayombe mulimine head of engineering 178,000 administrator 2 32 fang-yu tham safety coordinator 193,000 consultant 1 33 dan merriweather engineer III 165,000 chemist 2 35 tommy alwinde engineer II 143,000 chemist 1 Similarly, we would want to make a table of rules, and a table of trainings, and indicate those also by ID. And so now we match with `=` instead of `like`, because `like` is just for strings. select sum(salary) from people join locations on people.location = locations.id group by location.name; Our data has just been "first form normalized" - we removed a piece of repetition. Repetition is dangerous. We don't repeat city names anymore. We just have a table of cities, and give them by their ID number. Much safer. This also means that later, if we decide to rephrase "engineer III" to "senior engineer," we change the label in one place, every single fucking query gets updated immediately and universally, and no meaningful data is changed at all. it's also hella fast to fix. . &gt; But I don't know what "location 2" is So then, you just ask SQL to ship the string back out with the answer. We used to have: select sum(salary) from people join locations on people.location = locations.id group by location.name; So we'll add a bit to the select on the front asking for the name as well select sum(salary), location.name from people join locations on people.location = locations.id group by location.name; And now our result should be two rows of two each columns: `(chapel hill, 524000), (pittsburgh, all the money)` . &gt; Chaining queries for maximum justice And look, we could keep chaining the query up from there. What if you only want the answer for chemists, and to exclude consultants and administrators? select sum(salary), location.name from people join locations on people.location = locations.id where people.training like 'chemist' group by location.name; Or, you know, maybe you already extracted training out into a table, so it'd be two joins instead. That would be better - your data should be as normal as is practical. Then we would have `training` id label - - 1 chemist 2 physicist 3 administrator 4 accountant 5 consultant 6 kung-fu historian Which means you also have `staff` id name role salary training location - - - - - - 28 mary stevens site manager 181,000 1 2 29 ayombe mulimine head of engineering 178,000 3 2 32 fang-yu tham safety coordinator 193,000 5 1 33 dan merriweather engineer III 165,000 1 2 35 tommy alwinde engineer II 143,000 1 1 And then you'd ask the database something like select sum(salary), location.name from people join locations on people.location = locations.id join training on people.training = training.id where training.label like 'chemist' group by location.name; *Much* nicer. `:)` . &gt; Mind opening meme X-Ray Storing an address as text Brain art Storing an address as several text fields Brain lit up Storing the zip code as a length-limited text field Brain with lines Storing the zip code as an integer Sine waves behind Using the word "whomst" Chakra guy Storing the zip code as a non-negative, size limited integer Union with universe Making a table of zip codes and joining to it, to exclude Centralia Titan reaching to sun Stored trigger that dispatches a human courier to verify on insert Big bang guy Remembering that you have Canadian customers who don't have zip codes . &gt; Who really is Jeff Albertson SQL has four jobs. 1. Keep the data durably. If there's a power outage, hackers, and the datacenter gets hit by a bomb, I expect you to restore gracefully. 1. Refuse to accept any data that doesn't fit the rules we set. (Corrolary: set *extremely* strict rules when you are able to. Postgres is very good at this.) 1. Find fast ways to get answers to really complicated, frequently one-off questions 1. Provide a high security isolation environment that no programmer since the late 80s even knows exists . &gt; In summary First, make sure that the thing you're asking for is something that a person with no context will feel like is answerable. "I have this specific data that I need out of the database. It should be shaped like this. Here are the considerations to keep in mind." No, don't count joins. Focus on what information you're attempting to receive. That's what this tool does. After that, stop trying to do your homework. Start trying to do *the first step* in your homework, instead. SQL doesn't happen all at once. It's the most incremental programming you've ever done. And now you speak SQL.
Hmm. Since you are wanting to do this without joins, maybe you can get your results with a multi-column WHERE NOT IN sub-query. Syntax is like this: SELECT * FROM table WHERE (foo, bar, fizz) NOT IN (SELECT foo, bar, fizz FROM table2) ; Sorry for formatting, I'm on mobile. Give this a try
That's it. The names changed, and the skill set got a little broader. At the same time, I see many jobs for BI developer and Data Analyst that could be called report developer. Often the job title means little. Good thing is, BI developer is a high-paying job and much in demand, even if its really a report developer.
I went through my bachelor degree (in business and IT) with 2 databases courses and one additional project in etl with ssis and mssql... As a student I still did not understand anything about sql whatsoever and wouldn't have been able to join properly Now I'm still a student but had my fair time as an intern and afterwards as a working student in developing dB for about 2 years and just recently I really realize that in database stuff it's really not about joining two tables but joining the logical data behind it, hence asking a 'good question'
Using INTs as PKs is generally faster to join tables on rather than having to evaluate varchars, dates, etc. Creating a PK itself indexes the table obviously, ordering by that key. So it's best practice to use ints for that field. Indexes should, in principle, improve query performance and maintain or degrade insertion rate as little as possible, if done correctly. But if your table is that small, letalone your whole database, performance optimization is not really worth your time. Our database at work is over 1TB and one table alone contains over 2 billion rows. We are just now looking into indexes, partitioning, query optimization, etc. It still is great to know best practices regardless of the application
ya sql seems difficult because it is brutally difficult to write the query until you become comfortable with the idea that the only thing you need to think about is the shape of the result and its sources
I used inner join, thank youüòÅ
Yep I am well aware of your points. Wanted to keep it simple for the OP
Bit late to the party, but just use INT
&gt; 8 bytes (32-bit integer) ‡≤†_‡≤†
Note: Not trying to be pedantic but for ease of people trying to research this, having it as the **Primary Key** should make no difference to performance whatsoever, you probably mean having it as the **Clustered Index**? You can have the clustered index on a different column that the PK (although 99% of the time people put them as the same field(s))
Quick mafs
Absolutely. Databases run the world and for the foreseeable future there will always be a need for people to convert those databases into a more readable and actionable format.
More relevant than ever.
Yep, reporting takes up probably 50% of my time. We have a few other people who do Power BI/SSRS full time. I do a lot with SSRS, SQL, and R for my reporting.
This is the great! Thank you for posting it.
Do as much of the logic as you can in SQL.
Im a BI Analyst and it's basically ali I do. SQL for ad hoc reporting, SSIS for automation, SSRS for self service, and tableau for some extra visual when necessary.
If im understanding correctly, you should for sure keep it in SQL, it will ultimately be more efficient. If you can be a little more specific about the data I can try to help on the script logic if needed.
I guess. I am trying to figure out how to do this efficiently. From what I can tell, i can't use WITH inside a while loop, but could create some views I guess. It just felt less elegant.
Alright so I have a table that when I sum I can get 'ingredient name, total kg' for a given formulation. Now I have another table where I can get the same information but for a single work order. In order to use this, I must get the sum by formula (easy) and the sum by work order (easy) and then: A. Find the quantity when I divide totalworkorder/totalformula. This gives me the number of batches. It won't be an integer because there is always a part batch. Then use this In a while loop from batch 1 to x and thus I'd get: Work order 5 Batch 1, 2, etc. I can then match this against the formula table to list all components and weights per batch (that's an easy part).
Loops and other things of that nature are usually more efficient outside of SQL. SQL is better at set based queries. It‚Äôd be easier to help though if you provided a sample of the data and explained what you wanted to do a little better. If you have to loop through data or use a cursor... well, depending on the query and Size of data that can be really really slow
Cursor?
Yeah. Cursor via SQL. I read iterate so I thought of cursor. A cursor doesn‚Äôt act on a whole set of data, but rather a subset of data. For example, you can run some code on parts 1-4 of the data, then run it or a slight variation on parts 5 - 7 and finally on parts 8-10. You query something... hold that set of data in memory... act on it... then grab more data... act on it... and so on. You don‚Äôt run the statement on all the data at once. It is most often slower but sometimes easier to code and occasionally is exactly what is needed.
Reports and dashboards are applications built using tools more than languages. There is a lot of analysis that goes into the work... much of it done on your own. It‚Äôs such an agile process. You have to get the data. Make sure the customer verifies it. Finally you get to put the UI together. I think it‚Äôs still a relevant job.
Can you elaborate on creating reporting environment? Does it mean creatimg separate tables for pulling put reports instead of directly hitting the main db? Thanks.
My dream job almost. SQL plus Power BI plus Python.
Did you figure this out?
The reason why formal database courses exist isn't just to learn SQL, it's to understand how to approach complex real-world problems (outside of tables composed of salaries, employees and locations) and develop logical and conceptual models that can later be translated into databases from the ground up. Example: &gt; The government of ZIMBALE, a developing nation, desires to establish a system for collecting and reporting seaport statistics. This information would be used to improve port utilization and management, forecast future needs, analyze foreign trade patterns, and support requests for development grants in addition to the traditional function of controlling the flow of goods into and out of the country. &gt; One aspect of the proposed system involves data on the utilization of port and docking facilities. Ships are of various types and come into port for a variety of reasons, including loading, unloading, maintenance and to take shelter from storms. They arrive first into the port area and sometimes stay there without using any docks. If ships have loading or unloading or maintenance to be done, they wait in the port area until a berth [‚Äúa parking spot‚Äù at a dock] becomes available. From there, they are taken by tug boats into docks. Some docks are special purpose docks permitting only one ship of a specialized type to be berthed at a given time, others are general purpose facilities which can berth one large ship or several small ones. &gt; You have been assigned to assist in designing a database for the docking facilities/activities portion of the proposed information system. The following data will need to be stored: &gt; 1. Date and time of ship arrival at the port area. Assume that ships can visit multiple ports. Ports are designated by their name. The total tonnage handled at the port is also of interest. &gt; 2. Date and time of departure from the port area. &gt; 3. Date and time of docking. Ships can go to more than one dock during their visit to a given port. Note that a dock can have many berths. &gt; 4. Which berths were used while in port (dates and times). &gt; 5. Type of ship: passenger, split bulk cargo, tramp cargo, tanker, natural gas, bulk carrier, container, barge carrier, roll on/off. &gt; 6. Physical characteristics of ships: Length, beam (width), draft (how deep in water -- both empty and full), dwt -- deadweight tonnage (empty weight), grt -- gross rate tonnage (maximum loaded weight). &gt; 7. Type of berth used: General cargo, passenger, oil, coal/ore, grain, timber, container, mixed purpose, roll on/off (vehicles drive on or off ship). &gt; 8. Berth characteristics: Location (which dock), length, water depth and age. &gt; 9. Each berth has several material handling equipment such as cranes, conveyor belts, pipelines etc. that are used to load/unload ships. There is also storage equipment such as tanks and silos. It is necessary to know the capacities of these containers. &gt; 10. Type of activity while berthed: Loading, unloading, both, refueling, maintenance, other (such as passengers only, storms, etc.). These are referred to by codes More than one such activity can be carried out. &gt; 11. Empty ships must be distinguished from those with cargo. &gt; **Tasks** &gt; 1. List the entity classes and attributes for Zimbale Seaports. &gt; 2. Develop an ER model. Can't just "learn that shit quick, yo." Formal database design theory takes time and patience to master, as it's an art and a science, just as modeling any other information system.
Did you get your question answered?
Can confirm - my team runs from 90k to 130k (LCOL city too - my mortgage is $620) and literally all we do is write TSQL to pull data and put it out in SSRS or Tableau - which most of my team are literal beginners at. We do no ETL, never touch SSIS. Technically we're "BI Developers" but all we do is write SQL with temp tables/CTEs and the occasional window function. It's madness.
Interesting. I'm gonna have to take a course in SQL...
Preferably I'd set up a new reporting database, often on a new server (using the ssrs server, if it's different, helps with keeping costs down as there's already a sql server license). Here I can then create tables, views, stored procedures and functions needed for reporting. People often underestimate what it takes to write a good, efficient report, it's rarely as simple as joining a few tables and displaying it in a nice dashboard. There's usually a lot of data cleansing and manipulation that needs to happen first and it's better to to that beforehand and have the cleansed data sitting in a table ready for the report to run over that. This way you can have indexes on your tables with reporting in mind and not have to worry about developers creating indexes to suit themselves. Tldr: yes
It‚Äôs really hard to understand because I don‚Äôt speak the language it‚Äôs written in. This is the 3 or more table join blueprint: SELECT what_columns FROM tableA JOIN_TYPE tableB JOIN_CLAUSE JOIN_TYPE tableC JOIN_CLAUSE Ôøº You have to create a virtual table that consists of needed columns of two tables that you need. Then you join the third table to the virtual one. Also, did you convert it to the 3rd normal form? [3 or more table join](https://pasteboard.co/Ib7tSTF.jpg)
I tend to fill a table varaiablr or temp table with numbers 1 to x then join to it on qty &gt;= the number column in my table / temp table. You can use a small recursive query to fill the number table With quantities as (select 1 as qty union all Select qty + 1 from quantities where qty &lt; 1000). Then insert the results into your temp table/table variable. This method avoids any loops or cursors
That's so simple and obvious ... Why the hell didn't I think of that?? THANK YOU!!!
No prob. I often set the 1000 example with a max statement on the quantity field I‚Äôm working with
Oh yeah we're talking like 50 batches tops. Thanks again. I'll implement that on Monday into the prototype
You probably need a server where the database would be.
Not directly, no. You need a server somewhere that hosts the database and an api that accesses it.
Yeah that's a mistake. 8 bytes for a bigint or 64 bit integer, 4 bytes for an int or 32 bit integer.
&gt;This is going into access Please re-flair your post. MS SQL != Access. They behave differently and have different syntax and features.
Not yet, but ive been brainstorming in my spare time on it - I might just use temp tables and case whens to add dates for each member to a table one visit at a time.
Why? Anyone worth their grits recognizes join = inner join I thought?
DBVisualizer
Cross platform is super important to me, so being Windows only is a non-starter. That is second only to advanced code editing functionality like multi-cursors, vim keybindings, etc. Not enough editors out there have actual code editing features and just have glorified text boxes with syntax coloring. Azure Data Studio has been my go-to recently. ADS is essentially a custom build of VSCode for databases (and has nothing to do with Azure, despite what MS‚Äô marketing department wants you to think). The resultset window leaves room for improvement, but the code editing is top notch. It supports SQL Server and Postgres currently, but hopefully soon they‚Äôll support others. Prior to ADS, my go to was Dbeaver. It was heavier weight and a bit clunky, but got the job done. I keep wanting to like Navicat, but it‚Äôs just too anemic and the editor part sucks.
Yeah but there are always newbies who won't and besides I hate it üòÅ I always use left outer join, inner join.
Really appreciate both comments. Will investigate both. DbVis screenshots look good; and Azure being licensed in the MSFT family is an advantage to me as we are heavy throughout their suite (servers to ML to BI)
I‚Äôve been using Dbeaver but good to hear other options.
I have used Azure for Python and R.. I find it interesting..
SQLite is not a server/client DB like you‚Äôre used to using with MS SQL or PostgreSQL. SQLite is just that, super lightweight. It‚Äôs a one trick pony in the sense that you can download it and run it locally on a USB stick, you‚Äôre PC, etc. It‚Äôs not really a server side SQL DB and if you were looking for that then PostgreSQL would be a way better option.
I think what you're looking for is LAG(). Mind giving me some example data? I'll try to write a query off of it.
True!
Yes my old position was a Report Developer, and not like what every other comment on here is like. It was literally managing the SSRS reports and stored procedures that powered them, that was it. No SSIS, DDL, R, or Python. Literally just managing reports based on client needs. Our bread and butter for the company is reports for large healthcare companies
My best friend lives in Dent!
For general use Teradata SQL Assistant. But it lacks an ability to browse DB2 objects, so for that purpose I also use Advanced Query Tool.
DataGrip
So frustrating.
HeidiSQL!!
explain why you can't do it in two queries
We use AquaDataStudio. It's cross platform and has a lot of great tools. We mostly do queries as well, but it has a lot of great tools for DBA stuff too. We like the instant Excel export of result sets, copying result sets back into comma delimited lists in your query. Also has SVN/Git integration too.
Cross platform, versions for macOS and Windows, and has a pretty good support forum... I have multiple platforms I maintain for my team from MySQL, Oracle, DB2, and MSSQL... these all feed into our data pipeline and end up in Hadoop where we have HIVE, KSQL and others. So having support for anything that has a JDBC driver is a must for me. I‚Äôve been using it for a number of years now, and can not recommend it enough.
Somewhat impressed here after some initial research. A few of my analysts are junior and prefer drag/drop Query editor tools. Does Heidi have a friendly GUI for beginners, or is sql coding requisite?
It's a join of two tables with a few conditions, so I was trying to be efficient with my code and not run the joins and queries twice just to run two different limits. I can paste my code if that's helpful!
Oh but the query is being written in t-sql
You could add a row_number column, then specify the row number ranges in the WHERE clause
Regardless of your title, reporting development is still very much in demand. &amp;#x200B; Like others have said, I believe the title and responsibilities have changed, but it still seems very much relevant. My title is Developer, Data Analytics but the prior name before the team was officially formed was SSBI Developer/Analyst (Self-Service Business Intelligence). &amp;#x200B; I'm just going on my second year in the industry and I'm the primary dev/support in our space for our ETL (Informatica, some SSIS, etc.). I also write views/stored procs in SSMS that are the backbone for Spotfire reporting, then use JavaScript and HTML for added visualizations / UI and IronPython for better interaction with the data. &amp;#x200B; I interned in the area before the team was formed and Report building/Scheduling was common in SSRS, which was still a generic "Developer" responsibility. &amp;#x200B; Regardless of your title, reporting development is still very much in demand.
Wrap the whole thing as a subquery and then WHERE ROWID &lt;= 100 OR ROWID IN (10000 - 10500)
&gt; Azure being licensed in the MSFT family is an advantage to me It's not as much of an "advantage" as you might think. Azure Data Studio is 100% free to use for any purpose and open source. There's no license that you have to get as a "ride along" with your other software.
I loved Postico when I was on a Mac because it has a very clean interface. Most other SQL editors I‚Äôve worked with look like they were developed in the early 2000s and never updated their UI. But it doesn‚Äôt have Windows support.
+1 for length
SSMS.
Who is teaching y'all how to write queries? Horrible format. Select blah, blah, blah From sometable s Inner join someothertable i On(i.field = s.field) Where s.blah = blah; ....much easier to read and properly formed. Smh to that elegible syntax up there.
yeah, you pretty much have to know sql to write queries but it's great for admin stuff like dumping tables for backup, etc. "drag/drop Query editor tools" makes me think of MS Access
in what sane universe do you know that the rows you want are rows 10,000 to 10,050??? please read this and get back to me -- https://blog.jooq.org/2016/08/10/why-most-programmers-get-pagination-wrong/
I use psql and vim (via `\e`). It's an incredibly productive setup and I get full completion and excellent syntax highlighting. Back in the day, nothing beat the client that came with SQL Server 2000. `isqlw` was super fast, stable and feature full.
A classic
Visual Studio Code has an official Microsoft-backed SQL extension that works pretty well.
But that's limited to SQL Server.
Yes it does, the first to execute would be your WHERE, FROM, GROUP BY, HAVING, SELECT, ORDER BY. Please correct me if I am wrong?
One area where implentations differ is for example: UPDATE table SET a = b, b = a Are a and b equal to the original value of b or are the values swapped? The standard says they should swap (read all before write) but MySQL processes each one independently. [^1](https://modern-sql.com/blog/2018-08/whats-new-in-mariadb-10.3#3.two-phase-prossing-of-updates-set-clause)
No, but standard SQL has to *act* like it has an execution order. That's what makes it deterministic. Vendors are free to make optimizations, as long as their result is the same as the result would be if they had followed the logical execution order.
QueryExpress is my *go tool* when I'm at a computer/server without SSMS, small, fast, portable and does the essential.
Because I'm working with a large data set and am only supposed to submit a portion of results, specifically those 50 rows, to prove I got the correct answer.
Still, it's an Excellent tool
&gt; to prove I got the correct answer ah, homework run two queries
FROM before WHERE, i think or, at least, WHERE during FROM, but not before
I agree, where during from
That should be more or less correct. So why was sql designed to put SELECT first. As a beginner, I don't see any logic in it. I think the sequence you wrote is much clearer logic wise.
yea FROM should be the first one. From a table&gt;&gt;&gt;&gt;define the condition(where)&gt;&gt;&gt;&gt;group the values&gt;&gt;&gt;&gt;group the aggregation values&gt;&gt;&gt;&gt;select them&gt;&gt;&gt;&gt;&gt;put them in order This is a much clearer logic chain than SELECT&gt;&gt;FROM&gt;&gt;WHERE&gt;&gt;GROUP BY &gt;&gt;HAVING&gt;&gt;ORDER BY
I just feel like everything in pc has to have an order
I am not an expert, but I guess before querying your data, you need to know how your final table would look like? Hence listing down the columns in the select statement first and then the proceeding. As to in what sequence do people write their queries in, I have seen few people writing their FROMs first and then moving up to the SELECT. To each his/her own?
Azure Data Studio has some cool features that I like, but seems to lack a lot of other stuff and seems rather buggy. I'd love to be able to use it primarily, but I keep going back to SSMS
It's close to a verbal statement. Get a beer from a fridge. SELECT beer FROM fridge
you most assuredly have to write the FROM clause first, or at least know what it's going to say... otherwise how do you know which table columns to reference in the SELECT clause?
I very much like SQL Workbench J. Lightweight and multiple platforms.
I've recently downloaded the trial version myself and not sure if it's something that's going to work for me. Was there one specific thing that made it a "must have"? Or just the features you specified?
There‚Äôs a bit of finagling with settings to get it the way you want it. But that‚Äôs the thing, you can customize everything. It‚Äôs got a great object searcher for searching stored procs to see usage of certain fields. It‚Äôs got a query history/ archive for grabbing old queries ran. It has saved me a couple times. The SVN integration was one of the big deals for us. We‚Äôve just scratched the surface of what it can do because we primarily run SELECT queries. But if you were a DBA, I think there are even more goodies. I‚Äôll be honest, I tried it for a week and dropped it. Went back to SSMS. Then I made a concerted effort to really try and it took a week but now I‚Äôd never go back to SSMS.
SSIS is not too difficult to learn. And you'll probably have something already built to work on. Every new employees at my company don't even know what SSIS is and they perform well enough to be kept.
Not if different execution plans produce the same results.
What SQL Query Tool would you use if your on a mac? XD
Take the job. I knew near zero SSIS and ETL before I started my current job and now it runs our entire BI operations (all made by me). I highly recommend the Pluralsight course on into to SSIS. https://www.pluralsight.com/courses/ssis-basic Unless they have updated it, the course is slightly dated, but 90% is the same as what you would use today. While taking the course, you should also go through examples on your own. You will learn a lot in the course, but SSIS is a lot of tweaking little settings or options to get it right. Google whatever gaps or snags you run into. Nothing will beat actually doing stuff in SSIS. So make sure you practice, and not just watch/learn. Thankfully SSIS is free. Good luck!
Thanks for the info! It's reassuring hearing stories like this. I felt like I could learn SSIS but I didn't want to dive in to something like this without knowing it was possible to succeed.
 UPDATE invoices SET Status = 'VOID' WHERE invoiceno IN (1,2,3) Your actual column names may be different.
Hackerrank
If it pays double what you make now you have no reason not to take it! I actually just started a job using SSIS. I'm far from being a pro, but it's pretty easy to pick up and there are lots of tutorials on YouTube. As long as you've been honest with your employer about your experience, I wouldn't sweat it.
Huh? What happened with the debate between submarines vs. space ships? Which one's better?
Subquery
Been doing complex queries and big projects in vs code, random stuff (quick inserts, top 100, etc) in azure data studio, and of course DBA stuff with ssms. I pretty much have all 3 open all day. SQL server only.
Don‚Äôt have much to add but congrats!
SELECT count(*) AS RecCount FROM ( SELECT col1, col2, SUM(col3) as SumCol3 FROM database.schema.table GROUP BY col1, col2 ) AS CTE1
the marketing departments moved on.
You could get a book like TSQ Querying and level up. You could install 2017 or 2019 and start learning newer features. You could start learning how to create reports and dashboards in PowerBI.
Check out Udemy. I've taken a half-dozen or so courses in the past year to improve my knowledge in Power BI and SQL. If you want references for what I've taken, I'll be happy to provide them.
You can test your proficiency with the data-sets loaded on Strata Scratch. Their exercises come from technical interviews taken from companies.
Congratz on passing the MTA! First off, this cert alone may not open up any specific "jobs". When I am looking for jobs, I generally type in SQL into Indeed or LinkedIn. Then goes the fun long process of seeing jobs like Software Dev, Data Engineer etc etc. I think your best bet (if your not in the field) is to get some sort of Analyst (HelpDesk or App Support) to get into a field/company you want. I worked my way from Accounting to Data Warehouse/ETL to DBA. When I was in Accounting, I had zero clue I would end up love being a DBA. But sometimes that is how things work. Keep learning and continue to learn things. If your not learning at work, learn at home. Don't be afraid to switch jobs if you are "Comfortable". There is a lot of advice that can be given, but without knowing your background, too generic to say.
you forgot the GROUP BY clause also, you can only ORDER BY things that are in the SELECT clause
Mock interview. Happy to help.
&gt; It's a pain to troubleshoot and whoever inherits the code will hate you. Usually there's a solution that can be found where you don't need dynamic SQL. This is totally correct. The *need* to do it is symptomatic of poor choices when planning and implementing the database. But we aren't always able to snap our fingers and rectify those mistakes, but I'd still take another route.
Plummer. They ignore my concerns when everything is running well,then call me at 2am when everything is blocked. (I'm a full-time DBA btw :p )
still working on it need to index and use maintenance tool in GoldMIne
If you're using SQL Server, you do not need anything "extra" to access Excel files; there's a driver for them built into the OS. Your vendor's response is confusing at best and misleading at worst. You do not need to use "their ODBC connection" as you don't need to use their software at all. You just need to put the Excel file in a path on your SQL Server that the service account can access. set xact abort on; begin tran; update T set status = 'VOID' from invoices T JOIN openrowset('Microsoft.ACE.OLEDB.12.0', 'EXCEL 12.0;Database=D:\temp\YOURFILE.xlsx;HDR=YES;IMEX=1', 'select * from [Sheet$]') as D where T.status = 'ACTIVE'; where T.invoiceno = D.invoiceno rollback tran;
Knock yourself out! [Newest 'sql-server' Questions - Stack Exchange](https://dba.stackexchange.com/questions/tagged/sql-server)
great, thanks!
Thanks sorry was half asleep typing that. Instead of order by I meant group by in the statement
Trying to answer your questions. 1) With cause - it as a subquery command. You are making some "kind of table" that you can refer to it in the next line. So: WITH SOLD as (column\_a, column\_b)----&gt; you are making "a table" SOLD with specific columns to use it in the same query. It's not a table in the meaning of table because you cannot refer to it later on in your code. Also it's not stored in the memory so it's not temporary table. 2) A.CODFID&lt;&gt;'-1' means every value that is different then '-1'. 3) In here he is reffering to the SOLD statement that has already changed values with the case statement. 4) SQL demands you to put this kind of code after using a subquery to end it . I usually put ") T "at the end. In another example (very stupid one I know ): Select count (client) as number\_of\_clients FROM ( SELECT DISTINCT name as client FROM shop) T &amp;#x200B; Without ") T" or without ") AA" it would scream an error.
Yep. They're focused on "Machine Learning" now.
How do you get to pg support? Just pulled it down and can't seem to find it.
DBA.
SQL Server's isqlw has been superseded by sqlcmd. It's cross platform now too - writen in Python and distributed via Microsoft's GitHub account. It's pretty handy, but the output of sqlcmd does require a bit of sed scrubbing to make it usable in other contexts. ```sqlcmd -h -1 -S $MYSERVER -E -Q "set nocount on; select name from sys.databases order by 1" | sed 's/[ ]*$//g'```
Yeah that would be awesome I would appreciate it!
https://azure.microsoft.com/en-us/blog/azure-data-studio-an-open-source-gui-editor-for-postgres/
The people who claimed that NoSQL was a replacement for all relational databases turned out to be mistaken. Some of them got burned pretty bad. The people who stepped back and said "hey, this subset of data that isn't _really_ relational in the first place, why are we making it harder on ourselves" carefully implemented "NoSQL" databases for portions of their workload and were rewarded. Those who needed to ingest large quantities of data that eventually needed to be processed and turned into relational data, but not immediately, implemented a "NoSQL" database as an initial collection point for their data (because writes were stupid-fast and distributed) and then implemented async processing of that data to eventually get it into relational stores were also rewarded. It's a tool. Not every tool can be applied to every requirement.
I don't think that's possible but often I'll just right click on a table and hit 'Select Top 1000' which generates a SELECT query with all the tables listed out, which I'll just copy and paste into my query.
Actually the 'Select Top 1000' would do exactly what you're asking, you just need to delete the TOP (1000) from your query. &amp;#x200B; I got into the habit of actually starting all queries that way rather than hitting 'New Query'
There are other database languages that use other orders that some people think are more logical, but SQL is the one that caught on.
God. They ignore me until they need something. &amp;#x200B; &amp;#x200B; Got that off Reddit!
I never thought to use this feature. You are right it does accomplish what I am looking for. &amp;#x200B; I was hoping to get some auto-complete benefit, but this also helps me out. Thanks!
SQL: [The Ultimate MySQL Bootcamp: Go From SQL Beginner to Expert](https://www.udemy.com/share/10005gBUoTc14=/). It's by Colt Steele, who does a great job of explaining MySQL. PowerBI: [Microsoft Power BI - Up &amp; Running with Power BI Desktop](https://www.udemy.com/share/1013gIBUoTc14=/). By Chris Dutton, who has a few other Excel/Power BI courses that are great, too.
The point of the question is to use an `OUTER JOIN`. There is some ambiguity of what to do if customers have the same name which is not specified by the question.
SELECT c.name, COUNT(CASE WHEN t.amount = NULL THEN 0 ELSE t.amount END) AS num_transactions FROM customers c left join transactions t on c.id = t.id GROUP BY name
Select c.name , Coalesce (count (t.id),0) as txn_cnt From customers c Left join transactions t on t.customerId = c.id Group by c.id,c.name
You're going to get a value of 1 for every customer with 0 transactions.
Very cool way to introduce someone without too much complexity. Have you posted to /r/learnSQL ?
Well, the spaceships can only withstand between 0 and 1 atmosphere of pressure. The submarines can take more.
Oooh, good catch, thanks.
False. https://www.youtube.com/watch?v=MvQBlk50UMA
Do you technically have to group by ID still since it's already aggregated in txn_cnt?
https://www.youtube.com/watch?v=O4RLOo6bchU
[This is as far as I got before time ran out](https://imgur.com/a/sHa6JDG) &amp;#x200B; Thanks for the answer
What does Coalesce do here? Grab the first non null value of count([t.id](https://t.id)) and if there is no value it returns 0?
Yea, because the group by is referring to the id field in the customers table. If you don‚Äôt have the customer id in the join, any transactions for customers with the same name would be counted as the aggregate of a single customer
Yea, because the group by is referring to the id field in the customers table. If you don‚Äôt have the customer id in the join, any transactions for customers with the same name would be counted as the aggregate of a single customer
&gt; the first non null value of count(t.id) there is only one value of an aggregate function if all values in the column being aggregated are NULL, then most aggregate functions return NULL so the COALESCE here is simply substituting 0 when the count comes back as NULL remember when i said "most" aggregate functions return NULL when all their values are NULL? well, COUNT is the exception -- it returns 0 so the COALESCE here is not needed
sorry, that's wrong it would return 1 only if `COUNT(*)` were being used, but it's `COUNT(t.id)` correctly
No, but there is a corner case where two customers may have the same name. Names are unreliable like that. This would reveal that situation. In reality, such a report would always include c.id for clarity because it's guaranteed to be unique, but that's not what the spec calls for.
If transactions has 0 records for a given customer then count(t.id) is null, not 0, because the count function *ignores* null values. The aggregate function returns null if all values are null. Coalesce just covers that situation, and is precisely why the question calls out that it wants a 0.
Yes
He's not doing count (t.id). He's using a case that returns 0 or the amount and doing a count on the result. If a 0 gets returned it's still getting counted.
i've flunked a sql test in the past too and this stuff drives me nuts. The questions are veiled behind many assumptions and act as if you've been working with their data at your previous job, and should already be aware of all the snags and quirks that come with it. Most of the pieces I did poorly on were very simple after 30 seconds of googling, i just hadn't encountered use cases for them in my previous experience. These basically serve as tests to see if you know all the nooks and crannies that sql has to offer, and I'm sure very few do
 select c.name, nvl(count(distinct t.id), 0) as txn_count from customers c left outer join transactions t on c.id = t.customerId group by c.name;
Thanks! That worked
Hi, would like to ask was this an onsite test?
select name, case when count (amount) is null then '0', else count(amount) from customers a left join transactions b on a.id = b.id
yeah, you're right, i didn't look at goddog's reply very carefully that CASE is awful i mean, COUNT has a built-in feature of returning 0 when all the values are NULL, so why over-think it?
why DISTINCT? would two transactions for a customer have the same transaction id? and why would you NVL the result of the count when COUNT intentionally returns 0 if all the values are NULL?
I was pretty confident that format style of join logic being lumped in one on was deprecated a decade ago. &amp;#x200B; I don't think MSSQL even allows that syntax anymore... granted we're not necessarily in MSSQL (no flag/note) but I think the SQL standard deprecated it, as well.
Meh. I guess you don't need distinct as the order ID is the PK. I added NVL because I didn't test it to see whether count would return 0 or null if no orders were found.
Welcome to 21st century. You'll need to hire a developer to create a web application for your users, OR you'll need to find off-the-shelf software. &amp;#x200B; Could you be a bit more specific about what type of data you are updating?
&gt; I didn't test it to see whether count would return 0 it's in da manual
&gt; then count(t.id) is null no, sorry, this is wrong test it or check da manual
Don't have spare cerebral cycles to waste remembering that stuff.
This stack overflow entry seems to help... I'd rather use a function as it's cleaner, but if you absolutely can't... this _should_ work.. https://stackoverflow.com/a/22385338
If it's not count() then it's sum() that does it. It's really not significant enough to remember.
Thanks man I would definitely hit up that power BI one it looks like the first is for my SQL though. If I did do that does most of that transfer over for a TSQL
[https://use-the-index-luke.com](https://use-the-index-luke.com)
When I've taken SQL tests, I've usually started with a comment block listing my assumptions on said points of ambiguity and a brief mention of a solution if he assumption was not the case (i.e. "I assumed no customers have the same name for this exercise - otherwise I'd group on customer name not id") etc
Don't know how much translates to TSQL; you'd have to do some checking on how they relate as far as syntax and commands go.
Sum can return null, if all values are null. &amp;#x200B; Count will not return null.
Have you ru sp_who2 to see if there is blocking occurring?
&gt; sp_who2 I did, and it returned 2 records. I wasnt really sure what to do with that information however.
He's right. Before restarting, check what's running on the database. Maybe there's something like index rebuild or something else.
In the short term you can kill the process that is causing the block. Look for blocked by which will give you the spid and run kill spid. This is a good place to start to troubleshoot, you may be able to find the code that is causing the blocks and fix that https://www.mssqltips.com/sqlservertip/2429/how-to-identify-blocking-in-sql-server/
Thank you !!!!
No problem. You may find that there are multiple pieces of code causing issues and you will have to fix them all as you find them so it could take a while until it's completely resolved. Good luck.
I'm more looking at it in terms of performance if I'm trying to convert multiple fields (possibly over a hundred), across multiple tables, some of which contain millions of rows, and then run an EXCEPT query across them. It would seem I'd at a minimum want to transform the data with the function or patindex, then run the select, instead of trying to do it all at once.
Get rid of all the NOLOCK's you added and call a DBA before you wreck the application or corrupt the database. SQL 2008 R2 has known bugs involving the use of NOLOCK. Also, support for 2008 R2 is ending in a few months anyway. NOLOCK means you might get: -missing rows -duplicate rows -missing data fields -corrupt data fields -your query mail fail to complete altogether. There are several techniques to getting around blocking problems but you need to have a DBA look at your database and the queries your application executes. I realize small shops might not be able to keep a real DBA on full-time but that's what MSP's are for.
but... but... but it's **ANSI SQL** behaviour!!
There's no ambiguity, they have unique IDs.
So I'm assuming you used to have a bunch of screens on the as400 that were used to read and write to the as400. If the as400 is going to be used still to store the data then what I'd suggest is a web application that calls the same stored procedures the screens previously used to retrieve and store data.
Can somebody tell me how I can do this better? SELECT C.Name,COUNT(T.Id) FROM Customers C JOIN Transactions T on C.Id=T.CustomerId Group by Name Order by 2 DESC
Oddly enough, Access. Access is a great UI tool for a sql server back end. It‚Äôs easy to hook up and simple to administer and maintain. Creating a form or forms that take data and store it in a sql Server Database takes a few hours to set up. For bulk data, SSIS.
INNER JOIN music ON music.user_id = users.id
Thanks for that bugtussleu, it returned what I wanted. Just out of curiosity, I was googling around and apparently NATURAL JOIN can also do this trick, just curious but would it have been "better" to change this query to one using NATURAL JOIN?
&gt; seems to lack a lot of other stuff It's not meant to be a drop-in replacement for SSMS. At least not for a while. It's meant primarily for developers at this point, not DBAs.
You don't even need an MSP for this. A good consultant can probably swoop in for about 20 hours and get OP back on the right track.
Hi ColWaffles, I edited the code to such, but I still see errors thrown randomly at 12.00 am from different pages, stating that at that specific time, the code is not able to grab the data from the database . For example, I have got like 7 pages that runs this SP at every 10 miliseconds, so on 20th April, the error was thrown by Page1.aspx, on 22nd , it was again thrown by Page2.aspx and yesterday, 23rd, it was thrown by Page3.aspx. This time, the error was thrown sharp 12.00am. The error - Exception type : Index out of range Exception, Cannot find Table 0. &amp;#x200B; My modified SQL SP `USE [OMplanner]` `GO` `/****** Object: StoredProcedure [dbo].[TimeStampdetails] Script Date: 23/4/2019 9:38:12 AM ******/` `SET ANSI_NULLS ON` `GO` `SET QUOTED_IDENTIFIER ON` `GO` `ALTER procedure [dbo].[TimeStampdetails]` `@machineid int,` `@currentdate date` &amp;#x200B; `AS` `DECLARE @currrentime datetime` `SET @currrentime = CURRENT_TIMESTAMP` &amp;#x200B; &amp;#x200B; &amp;#x200B; `BEGIN` `IF @currrentime &gt;= concat(@currentdate,' ', '07:00:00.000') and @currrentime &lt; concat(@currentdate,' ', '19:00:00.000')` `BEGIN` `SELECT COUNT(TimeStamp)` `FROM MachineActions where Machine = @machineid and TimeStamp &gt;= concat(@currentdate,' ', '07:00:00.000') and TimeStamp &lt; concat(@currentdate,' ', '19:00:00.000')` `END` &amp;#x200B; `ELSE IF @currrentime &gt;= concat(@currentdate,' ', '19:00:00.000') and @currrentime &lt;= concat(@currentdate,' ', '23:59:59.999')` `BEGIN` `SELECT COUNT(TimeStamp)` `FROM MachineActions where Machine = @machineid and TimeStamp &gt;= concat(@currentdate,' ', '19:00:00.000') and TimeStamp &lt;= concat(@currentdate,' ', '23:59:59.999')` `END` &amp;#x200B; `ELSE IF @currrentime &gt; concat(@currentdate,' ', '00:00:00.000') and @currrentime &lt; concat(@currentdate,' ', '07:00:00.000')` `BEGIN` `SELECT COUNT(TimeStamp)` `FROM MachineActions where Machine = @machineid and TimeStamp &gt;= concat(DATEADD(day, -1, @currentdate),' ', '19:00:00.000') and TimeStamp &lt; concat(@currentdate,' ', '07:00:00.000')` `END` `END`
The Natural Join works only if the two tables have similarly-named columns. In this case, your User ID is not the same because it is ‚Äúuser_id‚Äù in the music table and, simply, ‚Äúid‚Äù in your user table.
DEEP LEARNING! STOCHIOMETRY! K-MEANS oR NeUaL NEtwoRks? &amp;#x200B; DoES yOUr MaChInE LeArN on AwS or gCp?
I get that- it's more the bugginess that bugs me
When writing SQL since 1999. Still can't remember how to spell coalesce. I would fail it on a test if it had to be accurate.
Feels like this should work: select c.name as 'Customer Name' ,count(t.id) as 'Transactions Made' from customers c left join transactions t on c.customerId = t.id group by c.name; Main difference from yours is adding the aggregate-required "group by" clause.
You were super close! I put the answer as a separate reply.
tyy
Thanks! Looks like I have quite the journey to go. I am going to continue learning things and try to switch jobs with a role that can hopefully get me out of my comfort zone and give me a push towards this route
My fastest and surest learning experience was to read up on actual SQL \[specification on the SELECT statement\]([https://www.postgresql.org/docs/current/sql-select.html](https://www.postgresql.org/docs/current/sql-select.html)) (postgresql in my example), and try out every option there with different data where I would try to simulate edge cases. I \["foo"-ed and "bar"-ed\]([https://softwareengineering.stackexchange.com/questions/69788/what-is-the-history-of-the-use-of-foo-and-bar-in-source-code-examples](https://softwareengineering.stackexchange.com/questions/69788/what-is-the-history-of-the-use-of-foo-and-bar-in-source-code-examples)) my way into learning SQL. Eventually I did that with actual workloads, but this problem you linked is a good example of something that you can learn by just reading the documentation. You did actually find the solution, but you stopped short of knowing standard SQL functions. If you're using SQL Server, a good list can be found \[here\]([https://www.techonthenet.com/sql\_server/functions/index\_alpha.php](https://www.techonthenet.com/sql_server/functions/index_alpha.php)). For Postgresql the list is far longer, so it's in a nice page structure \[here\]([https://www.postgresql.org/docs/current/functions.html](https://www.postgresql.org/docs/current/functions.html)). But the gist of it is that you need to know how to cast and convert (string to integer or date, date to string, integer to string, integer to float, float to integer), how to deal with null values, how to work with dates (add dates, get number of months, and many more). It's also good to know all aggregate functions and window functions, in answering questions like "what's the median value".
Lol
Not true about the `ORDER BY`, you can totally order by columns/expressions not in the select clause (at least in Oracle): select name from ( select 'a' as name, 70 as sal from dual union all select 'b' as name, 50 as sal from dual union all select 'c' as name, 20 as sal from dual union all select 'd' as name, 10 as sal from dual ) order by sal &gt;d &gt; &gt;c &gt; &gt;b &gt; &gt;a
Why the hell did you get downvoted for this?
Cause I'm telling people they shouldn't use the magic go-fast button. One of my customers was a international supermarket chain who used NOLOCK by default on everything. Their databases were riddled with corruption and it took weeks to clean up.
Our plan for the as400 is to retire it soon, so we are planning to store all the data on our SQL server
I had the vendor of a major real estate management package tell me that `NOLOCK` was necessary in queries for an archival tool "because other people will be using the system while this is running." `NOLOCK` is dangerous there **precisely because other people will be using the system at that time**.
You've still got those .999's in there. You need to change the '23:59:59.999' to '23:59:59.997'. You're jumping past your intended date because SQL Server can't handle a time component of .999 milliseconds. Go back and read that article I linked, I promise it has the information you need, you just need to kick out your preconceived notions of how a datetime field should work and absorb how it actually does.
In a batch file: :loop SQLCMD ... timeout /t 2 goto loop
You need to make an infinite loop?
It's probably because you're trying to change the table definition of `##Temp` in the middle of the procedure and then immediately use the new definition, which means you're changing the table definition in the middle of the batch. For example, execute this: create table #test (a int); insert into #test (a) values (1); go if object_id('tempdb.dbo.#test', 'U') is not null drop table #test; create table #test (a int, b int); insert into #test (a, b) values (1, 2); go You'll get the same error that column b doesn't exist. The server hasn't had a chance to update it's metadata about the temp table. Make sure `#test` has been dropped and execute this: create table #test (a int); insert into #test (a) values (1); go if object_id('tempdb.dbo.#test', 'U') is not null drop table #test; go create table #test (a int, b int); insert into #test (a, b) values (1, 2); go By putting another batch in there, the statement parser uses the updated metadata. However, since a stored procedure by definition has to take place in a single batch, you can't change the definition of the table at all even if you drop it and recreate it. Personally, I don't understand why you're not just using a VIEW or having the stored procedure just output XML.
I think this will do the job, using SQL Server dialect: SELECT C.name, ISNULL(T.NumTranx, 0) FROM customers AS C WITH(NOLOCK) LEFT JOIN ( SELECT T.customerId, COUNT(T.amount) AS NumTranx FROM transactions AS T WITH(NOLOCK) GROUP BY T.customerId ) AS subq ON C.id = T.icustomerId To improve the speed, if you have a large table, what I learned from this subreddit is that we can put the subquery into a table and index the table and then do the join.
finally!! a tutorial posted to /r/sql that isn't complete garbage this one is actually very good
yeah it seems that the standard has been relaxed since i learned it
I had never thought to do unit testing within SQL -- I have always done unit testing on the application layer but your post had me intrigued so I went down the rabbit hole of reading about it. I typically find Microsoft's documentation to be a decent starting point. [https://docs.microsoft.com/en-us/sql/ssdt/running-sql-server-unit-tests?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/ssdt/running-sql-server-unit-tests?view=sql-server-2017)
I do not know how to efficiently ask this question... hope I am making some sense.
Not sure if you can just use something like WHERE LIKE '\^ %' so you just remove all fields where it starts with a space. LIKE is TSQL though, not sure what the equivalent is for PLSQL.
wow this seems to work, thanks: `where attributes not like ' '` but still, it'd be better if it just skipped any un-parsable records and return null instead
 SELECT t.ColA , t.ColB , s.ColA_count FROM tableA AS t INNER JOIN ( SELECT ColA , COUNT(*) AS ColA_count FROM tableA GROUP BY ColA ) AS s ON s.ColA = t.ColA
Filetables would be better, yes. Google will have instructions on how to implement them, it‚Äôs not crazy hard. Another thing you could do is put the content on a network share and inside your application, provide a button that opens a windows explorer window with that directory. If you were building a web site though I would suggest creating a carrousel. That would be a better way to view this content. But yeah, you shouldn‚Äôt have 30 seconds of lag to grab a picture out of a database unless it‚Äôs gigantic. That is a bit much, I think something is awry.
&gt; But yeah, you shouldn‚Äôt have 30 seconds of lag to grab a picture out of a database unless it‚Äôs gigantic. That is a bit much, I think something is awry. The pics are not in a database right now, they are either on a share on the network or in a local folder (depending on whether the app is run in **Head office** or **Local Store** mode. *This is one of the secondary reasons I can't make it a website, all stores have a local version of the database, the pictures and the app and it needs to stay that way because they have things that interact with the local versions (hand held scanners for instance) and having those local devices be at the mercy of their internet connection or our potential server outages is not something I am willing to put them through.* But as I said in my edit it's not the actual pictures that were the main issue it was the fact that I was running 100 of those objects in sequence, running them in parallel threading has removed 15-20 seconds of the whole process so now I'm just trying to see if I can't squeeze out a few more seconds elsewhere. &gt;If you were building a web site though I would suggest creating a carrousel. That would be a better way to view this content. Not entire sure what you mean by you would suggest creating a carousel though as that's generally just a thing to cycle through a bunch of pictures which is not what I'm doing here.
You could look at REPLACE() and see if it works for you. [https://www.w3schools.com/sql/func\_sqlserver\_replace.asp](https://www.w3schools.com/sql/func_sqlserver_replace.asp) &amp;#x200B; It would be like SELECT REPLACE(' ', ' ', NULL) FROM table; &amp;#x200B; I also think I read that trying to concatenate anything with NULL returns NULL. I haven't tried it out though.
Agreed! This is extremely helpful for someone new to joins that needs an easily readable explanation and example!
I'm fairly certain I use the wrong terminology, but at work I've always called it a sub query. Essentially, instead of counting the class in your main statement, count in a sub query statement and left join this onto the Students table (Which will ensure all students are returned, since any students who don't fall within the class state date will just return NULL). &amp;#x200B; SELECT Students.Name FROM Students LEFT JOIN ( SELECT Classes.Student\_ID, COUNT(Classes.class\_ID) FROM Classes WHERE blah blah blah GROUP BY Classes.Student\_ID) C ON C.Student\_ID= Students.Student\_ID &amp;#x200B; I'm still pretty new to SQL, so this is probably quite a roundabout way to do it, but it should get you the results you need whilst showing all students.
&gt; How can I trick SQL into doing what I want. :) this is not a "trick" but rather just utilizing the syntax correctly SELECT Students.Name , COUNT(Classes.class_ID) FROM Students LEFT OUTER JOIN Classes ON Classes.Student_ID = Students.Student_ID AND Classes.class_start_date BETWEEN '2019-01-01' AND '2019-04-01' GROUP BY Students.Name
Thanks. Moving the date filter from the where condition into the Join "ON" statement did work.
Also, everyone else in my office still writes in ANSI 89 (main reason they were no help to me on this one), so to all of them, this would be considered a "Trick" :) &amp;#x200B; Thanks again!
i just realized that ANSI 89 is **thirty** years old prolly older than some of your co-workers do they also still use Cobol, LOL
Not sure if it'll work in SQLCMD, but T-SQL has a [WAITFOR](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/waitfor-transact-sql?view=sql-server-2017) function and you can supply a number to the batch-ender (GO)[https://docs.microsoft.com/en-us/sql/t-sql/language-elements/sql-server-utilities-statements-go?view=sql-server-2017]. So you could run the query: Select Top (5) QuantityOnHand FROM [WideWorldImporters].[Warehouse].[StockItemHoldings] WAITFOR DELAY '00:02' GO 9999
If it works it isn't stupid. First rule. Have you explored the world of OPENQUERY(), yet? Not sure what you're doing on remote servers, but it can really save you a ton of time in a lot of cases when you're joining data, doing complex calculations, blending data between servers, etc.
My only experience with that is seeing it being used to dupe MSSQL into allowing the output of an stored procedure to be accessible by a view, or by querying Active Directory. I also think I tried to use it to engage with a server that is off-prem, but their security did not allow it. I'll read more about it to see how it compares to just querying the data directly (via a linked server). Thanks!
It sends the query to the remote server to execute there, then sends the results back. Very helpful when you're dealing with large tables, joins between tables on one server, etc.
Wow... I just tried joining a table from my local server to that remote server using OPENQUERY() and it was *unbelievably* faster than joining via some views I had. Maybe I was mistaken about their security or they changed it, but in any case, that is awesome. In my original question, both servers are on-prem, so speed was negligible, but this has opened up all kinds of other possibilities for me. Thanks!
As a matter of fact.....
You can achieve the same thing using ANSI 89 syntax. `SELECT` [`Students.Name`](https://Students.Name) `, COUNT(Classes.class_ID)` `FROM Students` `, Classes` `where Classes.Student_ID (+) = Students.Student_ID` `AND Classes.class_start_date (+) BETWEEN sysdate-30 and sysdate-15` `GROUP` `BY` [`Students.Name`](https://Students.Name) &amp;#x200B; There's very few things you can't do in ANSI89 that you need to use newer syntax for, in fact there's several things that ONLY work using "old" style joins syntax. It's quite frustrating to see it getting bashed so much just because it's been around for a long time. &amp;#x200B; And for the love of all that is holy, please use the DATE data type when dealing with DATEs. Relying on implicit conversion will bite you.
you just need two joins using aliases SELECT m.MerchantName , f.AgentName AS FirstAgentName , c.AgentName AS ClosingAgentName FROM Merchants AS m INNER JOIN Agent AS f ON f.AgentID = m.FirstAgentID INNER JOIN Agent AS c ON c.AgentID = m.ClosingAgentID
I don't quite exactly remember how/what I did, but I remember writing a nested OPENQUERY() once that went to one server, then went to another, and incrementally built the results I wanted before finally coming home to the server I was running the query from to do the joins. The improvement in speed was close to two orders of magnitude than if I had just written it without OPENQUERY(). It's a really cool little feature.
Oh, hell yeah. Thanks
same
Ah cool. That makes it a bit challenging having databases spread around. The filestream should help, or be curious how things turn out.
I use openQuery to my oracle database for all my ETL. It really is awesome.
where do i find such jobs
Message sent.
Eagerness, being a good person and coming across as conscientious and easy to get along with. Higher level interviews can vary widely and the chance of having a 'fit' interview as a final round seems high here. Good luck!
I would start with what you know. People in college will frequently end up making databases that revolve around education with students, teachers, courses, enrollments, etc. You could make a database that would be useful to your hobbies. Perhaps you have an extensive movie or music collection that you could create a database for. Alternately, you could make it a database of movies you have seen or want to see, rather than ones that you own. I had a coworker that created a movie database and implemented a rule that the next movie he watched had to have an actor from the previous movie in it.
Conscientiousness, understanding priorities, and work ethic/fit, maybe problem solving. If you've gotten this far they probably aren't going to ask a lot of technical questions, more will you learn things. You might get technical questions, if so anwser what you know, be careful on terminology if you don't know and ask for what to read to learn if you don't know. This is probably one of those times to have 3-4 questions about the company ready. Be sure you understand the companies basic business model, and if you aren't sure ask, eventually if not early on you'll be making decision aids (how does the money line up) the CTO will be using.
I disagree with the technical question part if you've gotten this far I think they probably will get more technical with you. I've been a data analyst using SQL for the past 7 years
You could create a movie collection database
Wow! So good! Thank you for sharing such a great lesson. I'm still very new to SQL and was just chatting with someone in a different thread about needing to learn more about joining tables. This is super easy to follow and understand.
Depends on the guy. Are you willing to study and lie a bit? If you have only 10 to 20 hours of SQL experience you don‚Äôt have enough for entry level - so I wouldn‚Äôt hire you knowing that. But maybe you‚Äôre really smart and like able? Maybe you‚Äôd learn it quickly? Feel the guy out and come prepared as you studied for it. If you need to embellish do so, but SQL can be an animal. Just make sure you look at SQL job interview questions and be able to answer the ones you find. Understand backups, how to put data in and take data out. Understand schema and such.
Hm, so the job description involves "cleaning up" and verifying data integrity. They use applications like Tableu and Crystal reporting as well. What kind of questions/topics would you anticipate? I appreciate the take my dude.
This is a really good take! Yeah, the 10-20 hour thing was what I told them verbatim, that's why I was surprised when they gave me a call back at all. I'll brush up on those. Thank you my dude
I would guess practical questions related to the business where you would need to verify data accuracy using a program with a gui in relation to the data. What does the first 3 lines of the job description say?
"Create, maintain, and ensure integrity of KPIs from various sources of data. With guidance, utilize Tableau to create dashboards and data insights to help propel the business. Maintain reports and the queries that support them, using Crystal Reports and Tableau"
Yeah. Just understand what it is and how it works and find some juicy nuggets. Read Brent Ozars guides. He has a few I want to be a DBA type guides.
I'm in an entry-level data analyst position and that's roughly the same amount of hours I had before I got the job. SELECT \* FROM WHERE ORDER BY Pretty much most of the queries I run. There are longer queries with Joins that are in the notes which are copy and pasted.
Yikes.
Will suggest you to practice on Strata Scratch before this interview. They have exercises with technical questions from top tech companies. Hope it can help you prepare for this interview.
Haha knowing SQL really wasn‚Äôt a requirement for this position but you‚Äôre expected to learn it.
If all you run is that query I‚Äôm not sure you learned it LOL. (Unless you did and just don‚Äôt use it). Seems pretty pointless to have you run queries that simple though.
Well most of the job is importing records into specific tables, so I‚Äôm just double checking that it imported correctly. It‚Äôs pretty entry level, but there‚Äôs also stored procedures that im expected to know how to read and troubleshoot errors if they go wrong. This is stuff I‚Äôm currently learning
I think this is to see if you align with the business. I get on really well with the CTO for the business I full-time at, he's amazing so just show you're eager to learn and willing to take direction, don't bs, waffle or cheat and you'll probably get on like a house on fire.
I still gotta figure out the delay but the "GO 9999" part was exactly what i was looking for. Thank you so much, have a great week :)
I agree don't lie, just show willingness to learn. My guess is you don't have to be an expert for this position, just capable of learning SQL. I good resource that I would recommend is linkedin learning. There is a lot of content that could help a beginner and it's taught well.
It must be something else. This works perfectly fine for me to test exporting: `DECLARE @t1 TABLE (Col1 INT)` `INSERT INTO @t1 SELECT 5` `SELECT * FROM @t1` &amp;#x200B; Also, table variables can quite often make a query extremely slow. I would recommend #temp tables or other techniques if you are trying to optimize.
No problem! Yeah sorry I set it for 2 minutes not two seconds, WAITFOR DELAY '00:00:02' should do the trick. It at least works in SSMS :D
I assume your using the package that SSMS creates in SSIS? If so create a breakpoint and check the value of the variables passed in at runtime using the locals window
Thanks for the info, that query has the same behaviour though, won't export any data. If i change the query to: SELECT 'Col1' = 5 then it's working fine. Could that be a SQL Server 2008 issue?
Thanks for the hint. I managed to set the breakpoints(OnPreExecute, OnPostExecute, OnError etc.), but i don't see anything happening when starting the debugger. The locals-window doesn't mention any of the used variables, only some system-stuff like ContainerStartTime, CreationName, LocaleID etc.
How are you passing your variable to your SQL code? Is it via snippet of SQL code or are you calling an SP with a parameter? Is this set correctly in SSIS?
I just interviewed with the CIO of a big company last week. If you made it that far, the jobs in the bag. Please prepare some good questions to ask the CIO. Also, research the CIO and company. I was so nervous and prepared to answer questions that I was thrown off guard when he wanted me to ask him questions lol. Also its entry level. You will more than likely be trained for what they have in store for you. Best of luck!
I posted the problem with full code on https://stackoverflow.com/questions/55827772/sql-export-wizard-has-problems-with-table-variables But for the test i just used the code from /u/Eleventhousand DECLARE @t1 TABLE (Col1 INT) INSERT INTO @t1 SELECT 5 SELECT * FROM @t1 So i guess via SQL code.
if the rev code list you are flagging is small you can do a case statement to set the flag for the diag codes you want and then just leave the flag null for diags you arent specifically looking for, that should give you them all with a flag to handle the ones you really want.
i have just created a very simple package. Data flow task containing an OLE DB source using SQL Command, with the code above as the source Connected to a Flat File Destination and it works fine. I would start a new solution, try to create what I have just done in it's simplest form and see if it works. Then work from there.
Isn't entry level exactly where a 10-20 hour experience applicant is supposed to go? What does entry-level mean now? &amp;#x200B; I understand I'm mostly dated, but a job listed as entry-level used to mean "come with the most basic understanding and we'll train you what you need to know." and was more about work ethic/willingness to learn/good fit than current understanding. Oh, and lower pay. Don't forget lower pay.
One question, are you using SQL Server 2008 or later? Because i have the feeling that it has sth. to do with this version of the SQL-Server.
I am, 2016 I think. Do you have access to a later box to test?
Could be - I won‚Äôt argue you‚Äôre wrong or right. 10-20 hour or experience though - and I‚Äôm thinking 10 - is like a long work day. I‚Äôm thinking a person didn‚Äôt get in depth with it but is comfortable with it more or less than only some hours experience with it. I guess maybe to me it‚Äôs about depth not length of time.
I'm stuck with 2008 at work. I guess i'll have to take a different approach and create a view which i then export. Thanks for the help though!
\&gt; replace corresponding entities in colB with the first appearing value in colB &amp;#x200B; could you either rephrase this, or show what you want using the data you provided &amp;#x200B; say, whatever happened to those counts you wanted yesterday? they died?
You want to TRY\_CAST [https://docs.microsoft.com/en-us/sql/t-sql/functions/try-cast-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/try-cast-transact-sql?view=sql-server-2017) SELECT TRY\_CAST(xmlclob AS XML) FROM table;
I guess it depends on the person, but this is literally saying "I got into sql yesterday and now I'm ready for a job" Maybe they are ready, who knows. But I certainly wouldn't hire someone with so little experience, even for entry level.
Yeah, maybe. I think of entry level SQL position as running stored procedures and making sure nothing's broken, and only escalating something if it is. I guess it all depends on what the SQL position title/responsibilities are. You're probably right.
I'm doing this ... `xmltable ('/Attributes/Map'` `PASSING TRY_CAST(mytable.xmldata as XML)` `COLUMNS...` &amp;#x200B; getting : ORA-00904: "TRY\_CAST": invalid identifier &amp;#x200B; does this not work with oracle?
What?
`xmltable ('/Attributes/Map'` `PASSING xmltype(mytable.xmldata)` `COLUMNS...` This is what I was doing, it works... as long as CLOBS are all well formed XMLs (as soon as I get a null or some weird strings this runs into problems) (can't change data at source)
outside of sql, run your list of log files through a text editor strip the leading \\ and then count the remaining \'s and insert as many additional \'s as necessary just before the filename then replace the backslashes with tabs, and upload it as a csv to a landing table
I think you can get round the null issue at least by switching mytable.xmldata for nvl(mytable.xmldata,'&lt;top_xml_path/&gt;')
first, stay well clear of SETs you want a third table -- CREATE TABLE vendor_operators ( vendor_id INTEGER NOT NULL , CONSTRAINT vo_vendor FOREIGN KEY ( vendor_id ) REFERENCES vendors ( id ) , operator_id INTEGER NOT NULL , CONSTRAINT vo_operator FOREIGN KEY ( operator_id ) REFERENCES operators ( id ) , PRIMARY KEY ( vendor_id , operator_id ) ); so there will be one row for each operator for each vendor
I'll tell you how it goes as soon as i can check
I think this works-ish? Select name, count(customerid) from transactions join customers on transactions.customerid = customers.id Group by customerid Union Select name, 0 from customers Where id not in (select customerid from transactions)
I think it's more about checking you're not going to shit in the codebase he sets technical direction for.
What is Play? Is it a key or a table? Is it important to know which individual members of a team played in a game? If not I would recommend a Teams table. This would store the team names and a TeamID. Then in the Player table each Player would have their TeamID as part of their record. Then you can have your Matches table point directly to the Team that played.
&gt; What is Play? Is it a key or a table? So the squares are entities, the diamonds are actions, and the ovals are entity attributes. When learning about entity-relationship diagrams, this is the style I saw being used over and over. "Play" in this case is basically just showing how each entity is associated to each other. &amp;#x200B; &gt; Is it important to know which individual members of a team played in a game? Yes, this is very important as I want to keep track of and display team statistics and standings like the leaderboard does in one of the links I provided. &amp;#x200B; &gt; I would recommend a Teams table. This would store the team names and a TeamID. Then in the Player table each Player would have their TeamID as part of their record. Then you can have your Matches table point directly to the Team that played. (but this might depend on how often team members might sit out). This is a great idea! Team members don't sit out too often so I don't foresee that being an issue.
he's trying to use a `set` to track more than 255 elements, and he thinks the limitation on set members comes from the web editor rather than the database he's randomly changing the datatype of a column hoping to stuff more data into it than it will fit
select case when dbName.table\_name is null then '0'
Sounds like you want a status table with 1 - Installed 2 - Not Installed then do an outer join to it.
Oh. Okay. Your powers of deduction exceed my own. Cheers.
Something like SELECT CASE WHEN EXISTS (SELECT 1 FROM Wherever WHERE {whatever your criteria is to determine if the program is installed}) THEN 'Installed' ELSE 'Not Installed' FROM SomeListOfComputers
Main point is that you must not store multiple data in one column, like playerStats, teams and players in your diagram. All that has no constant amount that will never change must be in separate tables (not "player1, player2, player3" columns, because number of players/teams can be changed easily), and just reference to "container" table. It's called normalization. It has several levels, you can google info about them. &amp;#x200B; Primitive prototype can be smth like that: table Team ([TeamID], Name, ...) table Player ([PlayerID], Tag, TeamID, SomeLifetimePlayerStats, ...) table Tournament ([TournamentID], Name, StartDate, EndDate, ...) table Match ([MatchId], Type, State, TournamentID, ...) table MatchTeam ([MatchID, TeamID], SomeTeamStatsInThisMatch, ...) table MatchPlayer ([MatchID, TeamID, PlayerID], SomePlayerStatsInThisMatch, ...) -- teamID again because players will change teams from time to time, but you need to know about that exact match And more and more tables. Looks like at least separate tables for stats, it's unclear now what you are going to store there. Columns in square brackets are primary keys. Last two have composite keys, but maybe with increasing number of tables it will be easier to introduce surrogate key MathTeamID or smth like that and use it in table MatchPlayer (and future ones) to reduce number of referenced columns to keep foreign keys simple.
offline? sure they're called books
&gt;Say I have a table of People. In this table, I have the following columns: &gt;Record ID (primary key) &gt;Add date (a timestamp field) &gt;Weight (int) &gt;IQ (int) &gt; &gt;How would I delete all records (except the most recent record) that have duplicate weights and IQs? &gt; &gt;For example let's say this was my data &gt; &gt; 1 2018-07-01 158 100 &gt; 2 2018-07-03 140 146 &gt; 3 2018-07-04 140 146 &gt; 4 2018-07-18 151 152 &gt; 5 2018-09-09 151 152 &gt; 6 2018-09-10 151 152 &gt; &gt;After doing the operation I want, the resulting data would be: &gt; &gt; 1 2018-07-01 158 100 (has no duplicate rows) &gt; &gt; 3 2018-07-04 140 146 (record ID 2 was deleted because it was a duplicate of record 3 and record 3 is more recent) &gt; &gt; 6 2018-09-10 151 152 (records 4 and 5 were deleted because they are duplicates of 6 and record 6 is more recent) &gt; &gt;Any help is appreciated! I think this should work: Delete from People p Where AddDate &lt; (select max(AddDate) from People p2 where p.weight = p2.weight and p.IQ = p2.IQ)
wget ‚Äê‚Äêexecute robots=off ‚Äê‚Äêrecursive --convert-links http://www.w3schools.com I guess?
My feeback to you : - no sample data - no desired outcome - no effort in asking the question, just a please fix something I don't understand for me --&gt; all in all, sounds like homework, you can't afford my hourly rate. Hint: If you ask me the question of &gt;The kicker, and the more difficult part is, if the software isn't installed on the machine - there just isn't a record or a null. How in gods name do you expect me to understand your homework of that description? At least make it easy for me, write out some sql, CREATE TABLE something (id, something, something_else) INSERT INTO something (id, something, something_else) VALUES (1, 'test', 'testing test') , (2, 'test2', 'lore ipsum something') give me something I can copy paste into my sql client, to write a query, to give you the given as desired result. Don't make me spend more time trying to understand your question, than you did on trying to solve it yourself.
correlated subquery in a case statement ? brrrrr left join to that table, please dont do shit like that on prod
Man, so much good advice, thank you so much Kant. &amp;#x200B; &gt; Main point is that you must not store multiple data in one column, like playerStats, teams and players in your diagram. All that has no constant amount that will never change must be in separate tables (not "player1, player2, player3" columns, because number of players/teams can be changed easily), and just reference to "container" table. It's called normalization. It has several levels, you can google info about them. &amp;#x200B; I feel like this really filled in some gaps in my knowledge regarding fields (columns). Fields need to have values that are atomic so as to avoid breaking one of the 1NF rules which is that "each table cell should contain 1 value." The attribute playerStats in my Players table would contain multiple values (kills, deaths, damage, etc.) and this breaks 1NF. Instead of a playerStats column in the Players table, I should replace it with a Kills, deaths, damage, .... columns? For example something like this: &amp;#x200B; I see that the same logic applies to the players attribute in my Matches table though I don't understand how it applies to the teams attribute in my Matches table? In theory couldn't my Matches table look something like this: &amp;#x200B; (I left the other columns blank because I realize now those break 1NF and will need separate tables) &amp;#x200B; &gt;Primitive prototype can be smth like that: &gt; &gt;table Team (\[TeamID\], Name, ...) table Player (\[PlayerID\], Tag, TeamID, SomeLifetimePlayerStats, ...) table Tournament (\[TournamentID\], Name, StartDate, EndDate, ...) table Match (\[MatchId\], Type, State, TournamentID, ...) table MatchTeam (\[MatchID, TeamID\], SomeTeamStatsInThisMatch, ...) table MatchPlayer (\[MatchID, TeamID, PlayerID\], SomePlayerStatsInThisMatch, ...) -- teamID again because players will change teams from time to time, but you need to know about that exact match &gt; &gt; &gt; &gt;And more and more tables. Looks like at least separate tables for stats, it's unclear now what you are going to store there. &gt; &gt; &gt; &gt;Columns in square brackets are primary keys. Last two have composite keys, but maybe with increasing number of tables it will be easier to introduce surrogate key MathTeamID or smth like that and use it in table MatchPlayer (and future ones) to reduce number of referenced columns to keep foreign keys simple. &amp;#x200B; Again, this is so helpful. I was really struggling to come up with other potential tables that could belong in this type of DB and this example gives me a better idea of what I should be thinking about in terms of tables. Your concern regarding stats is also well taken as now I see that it will be a challenge to try relate a matches stats to a team/players. I'm hoping I can come up with a table that will allow for the use of a surrogate key as you suggest . &amp;#x200B; Thank you again, this was incredibly helpful. I hope to update this thread with an updated design that takes your suggestions and concerns into consideration.
Would one of the lists be unique to create 1:many cardinality - then do an inner join ? Would avoid the where clause issue
Both lists are unique. One is a list of table names, the other is a list of column values applicable to a column on each table in the table name list. (Apply the whole value list to each instance of the target column).
Just start building your query from your desired granularity: computer * software. And since you need all combinations, do a cross join.
I have code that does this somewhere. I'll see if I can find it.
Awesome
Awesome
Are you able to run SQL something like this, and using your own preferred column names? select table_schema, table_name, column_name from information_schema.columns where column_name in ("alpha", "bravo", "charlie") order by table_schema, table_name, column_name; If yes, then try creating a table something like this, and using your own preferred column names: create table if not exists tmp (name varchar(255) not null); insert into tmp values("alpha"), ("bravo"), ("charlie"); select table_schema, table_name, column_name from information_schema.columns where column_name in (select * from tmp) order by table_schema, table_name, column_name;
Btw. Is this MSSQL?
TSQL. That's what you mean, right?
Does db2 support common table expressions? They are my go-to for this sort of thing. ;with cte as (select [id],[weight],[iq],[timestamp],row_number() over (partition by [weight],[iq] order by [timestam] desc) as rnfrom table) delete from cte where rn &lt;&gt; 1; of course test it with a select first!
Bottom answer might help [https://stackoverflow.com/questions/35945620/in-sql-server-how-do-i-retrieve-data-values-from-table-column-names-identified](https://stackoverflow.com/questions/35945620/in-sql-server-how-do-i-retrieve-data-values-from-table-column-names-identified)
Yes
Look into [Zeal](https://zealdocs.org/). I don't think there are docsets for every SQL flavor but I have used PostgreSQL and SQLite, and there is at least MySQL also available.
Hmm.. so you want the closest date in the second table to join? Select a.*, b.*,abs(a.date-b.date) as datediff from a,b Where a.assetid = b.assetid Is your cross join. Then from that product you select the minimum datediff for each asset +a.date combo. If possible you will benefit greatly from limiting range of difference between a date and b date.
Many people will help you without pay if you just describe the project
I used some of the good platforms when I was practicing and found strata scratch better among them. They have the questions for practicing taken from the top tech companies and universities to help us prepare for interviews.
I believe this is a full solution: -- fill this out DECLARE @columnName VARCHAR(128); IF OBJECT_ID('tempdb..#ValuesOfInterest') IS NOT NULL DROP TABLE #ValuesOfInterest; CREATE TABLE #ValuesOfInterest (value VARCHAR(128)); -- insert values of interest into your temp table DECLARE @sql VARCHAR(MAX); SET @sql = 'SELECT * FROM ('; SELECT @sql += ' SELECT ''' + TABLE_SCHEMA + '.' + TABLE_NAME + ''' AS tableName, CASE WHEN EXISTS (SELECT 1 FROM ' + TABLE_SCHEMA + '.' + TABLE_NAME + ' WHERE ' + @columnName + ' IN (SELECT value FROM #ValuesOfInterest)) THEN 1 ELSE 0 END AS hasValueOfInterest UNION ALL' FROM INFORMATION_SCHEMA.TABLES T WHERE EXISTS ( SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = T.TABLE_SCHEMA AND TABLE_NAME = T.TABLE_NAME AND COLUMN_NAME = @columnName ); SET @sql = REPLACE(@sql + '|', 'UNION ALL|', '') + ') Z;'; PRINT @sql; --EXEC(@sql);
1) What's the error you are getting. 2) That's really old format joins, you'll have better legibility if you do Select First_Name ,Father_Name ,Mother_Name FROM Student as S INNER JOIN Parent_Information as PI ON S.Parent_ID = PI.Parent_ID; 3) When you use table Aliases you should use the Alias to define where each field is supposed to come from S.Firs_Name ,PI.Father_Name ,etc ‚Äã
00955. oh ok ill try that...i thought that initializes it or whatever, but its ok since i define it or initialize it later anyways? i thought there was a way to name the columns in the view though to whatever for legibility
Are you essentially asking "after creating a view, how do I modify it?" If so, change CREATE VIEW to ALTER VIEW followed by your new definition of the view.
00955 is a code, I was asking for the message &gt;i thought there was a way to name the columns in the view though to whatever for legibility You do that by aliasing the fields in the select that makes the view. So Select S.First_Name as Thisisafirstname would make the field be Thisisafirstname in the view
I'm actually curious about that project
i figured it out, i had to drop the view. I guess i was trying to create a second view and that was the error message. For the alias i guess i just had to use as after the column name along with your syntax and it worked like i wanted it. my syntax it worked but not with the column named how i wanted them appreciate it.
yea thats what i was asking, i just decided to drop it in the object browser then retry it since i didint know the views saved until i dropped or edited them.
Please do not suggest people to use double quotes for string delimiting. This is not ANSI standard. It will break half of the databases (those that follow standards). Double = object identifiers (like column name, alias name, table name) Single = strings How to fix: MySQL: `SET GLOBAL SQL_MODE=ANSI_QUOTES` MS SQL Server `SET QUOTED_IDENTIFIER ON`
welcome to the future of america.
Just take it a question at a time, ask for help on here and people will help you understand.
He doesn't want help. He wants to purchase a completed project.
sounds like a type 3 scd &amp;#x200B; for table b create a start and end date for each asset ex: select asset, location from tablea a left join (select asset, b1.relocationdate, b2.relocationdate as \[enddate\] from tableb b1 left join tableb b2 on b1.assetid = b2.assetid and (b1.relocationdate &gt;= b2.relocationdate or isnull(b2.relocationdate))) x on a.assetid = x.assetid where a.utilizationdate &gt;= x.relocationdate and a.utilizationdate &lt;= x.enddate &amp;#x200B; hope that makes sense.
ok thank you
select recordid, max(timestamp), weight, iq from table group by weight, iq
Excellent point. I'll edit the post now to fix it. Thank you!
Excuse my ignorance, but how will double quotes "break" a database? Won't it just not execute the query?
I'm not the person you're replying to but wanted to chime in and say that I agree with their design for the Match tables here. After tackling 1NF as you mentioned above, the next steps in the normalization process would be a) removing redundant data and b) removing unnecessary dependencies between your data. &amp;#x200B; If we take your design for a single Match table, you'll see that there is a mix of match and player information. Using the match type as an example - this will be the same value for every player in the match and therefore, the same value being recorded in your Match table up to 100 times. This is redundant data as you really only need to store that value once per match. Now consider what happens if you need to change that data - the wrong match type was recorded for example. Changing that for a single match in your proposed design would involve updating up to 100 rows. In the example proposed by Kant8, the Match table holds only the info. relating to the match itself, whilst the MatchPlayer table tells us which players were part of that match. We'd only have a single row in the Match table to update in that case, which is much more efficient. This is what is meant by reducing unnecessary dependencies in your data - the match type for a match has no relation to the player involvement, so they should usually be stored in separate tables. &amp;#x200B; It's a tricky thing to get the hang of when you're just starting out, but I hope this helps with your design. Try to think about exactly what you want to do with the data, how best to translate the different elements into tables and how practical your proposed tables are for the way they are going to be used. It will take a few revisions and a lot of sketching out designs but your design should hopefully become clearer the more questions you ask yourself (and the people on this sub!) about it.
You have an aggregate query so either you have to put the field into the group by statement or you have to min, max, avg, sum, etc. the field. I would try selecting all the fields from the table by writing them out rather than using * and the problem would be more apparent. Alternatively, add provider_name to the group by clause
About your Matches table. First problem - you store team name in it. It's better to store teamID. And MAYBE you can also store teamname in separate column, if you want to show teams name during that particular match, but usually people don't care, they do not remember old names. You can store renaming history in additional table. &amp;#x200B; Second, now your table is not Match table. It stores the same match several times. You shouldn't store here any information relared to the match only, because it will be duplicated for every team. Basically if we throw away players column it will be my MatchTeam table, but with teamname instead of teamid. My MatchTeam and MatchPlayer are almost standard Many-to-Many relationship implementation. Each row in MatchTeam references to corresponding match and team and also can store some stats for that team in that match. And MatchPlayer references to the pair of match and team a player played (which is stored in MatchTeam), and the player itself, and then some stats for that player in that match. In general it's enough to leave only 2 columns in primary key here, matchID and playerID, that pair will allways be unique, but you still have to store teamID and apply foreign key to MatchTeam table to be sure that inserted team was participating in that match. Direct reference to MatchTeam table with combined key just does that automatically. &amp;#x200B; More tables for the god of tables! Do not be afraid of them :) Joins in queries will return everything you need in one big non-normalized set of data. It's always better than data duplication. Usually only if you have very high load some denormalized tables can speedup queries. In your case you will just need some indexes for foreign keys columns in tables and thats all. &amp;#x200B; If you need some examples you can try investigating some sample databases like AdventureWorks or anything else [https://docs.microsoft.com/en-us/sql/samples/sql-samples-where-are?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/samples/sql-samples-where-are?view=sql-server-2017)
If you want the sum on every row you need to use window functions. Like this: &amp;#x200B; create table #tmp ( name varchar(10) ,value int ) insert into #tmp values('aaa', 1); insert into #tmp values('aaa', 2); insert into #tmp values('aaa', 3); select name, sum(value) over() as mysum from #tmp; Gives this result: name mysum ---------- ----------- aaa 6 aaa 6 aaa 6 (3 rows affected)
My guess is that OP wants a window function and not an aggregate function. See my other reply in this thread for an example.
I‚Äôd do a window function over the IQ and weight values, order by date descending, and then select only the first row from that. WITH ordered_dupes AS ( SELECT user_id, iq, weight, update_ts, row_number() OVER (PARTITION BY iq, weight ORDER BY update_ts DESC) AS row FROM dupes_table) SELECT * FROM ordered_dupes WHERE row=1;
 so I did this: `xmltable ('/xxx/Attributes/Map'` `PASSING ('&lt;xxx&gt;'||mytable.xmldata||'&lt;/xxx&gt;')` `COLUMNS...` Turns out only invalid XMLs I had in CLOB were either null or " " (whitespaces), and concatenating like above basically turns them into a valid XML (albeit with an extra container element) I do not know how efficient this is but it works for me now. Thanks for sending me down the trail of NVL because that's how I somehow ended making this up.
Found some what of a solution on my other post https://www.reddit.com/r/SQL/comments/bgur94/skip_invalid_xml_when_type_casting_a_clob_using/
Until you have more info about which version of sql or what you will be using sql for, I‚Äôd start with w3schools website, it‚Äôs the best place to start with the basics and you can try and test your sql on the website.
First of all I think it's great that u jumped at the challenge of taking on this new skill for your group. It's this kind of motivation and thirst to learn something new that separates you from the regular dull people sleeping through their day to day. SQL can totally be learned by anyone and the best way to learn is to get in there and get your hands dirty. There are many free tools you can find online where you can practice SQL. One thing to keep in mind is that there are many variations of SQL but if you have a good understanding of the basics and structure...you can work in the various "styles". I've been a Data Analyst for the past 7 years and I'm currently using Oracle SQL at work but at home I downloaded Microsoft SQL server free and created a practice database on my own. I would recommend this as SQL server is easy to use and very robust. When I first started I took some online classes on LinkedIn learning. Highly recommended and there is alot of content. A simple Google search will also yield many good practice resources. Ones I like: https://www.codecademy.com/learn/learn-sql https://www.w3schools.com/sql/ Let me know if you have any questions.
[Server Level Roles](https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/server-level-roles?view=sql-server-2017)
I recommend www.sololearn.com
Awesome response, thanks so much for this. I am always looking for opportunities to grow and expand my skill set. So this seemed like a great opportunity. I'm definitely going to check those links out when I get home. I really just wish they gave us a little info other than just "hey pick someone to learn SQL, they're going to need it starting next year". So the text book they provided was for SQL Server 2005 and 2008 and that's the extent of the direction I got. Also, I'd just like to thank you on a personal level as well. I got a really snide and baseless back handed comment from another IT person the other day that had me feeling pretty down on myself and this actually made me feel really good. So thank you!!!
Thanks, I've checked out a little bit of w3 and I already like it more than this old dusty text book they sent me. Would be great if they told me any details other than"You'll need to know it by sometime next year" ok... but know what specifically? Just one of the many joys of the workplace haha. I'll do my best to get a basic foundation and I'm sure i'll be able to transition that into the specifics of what they need from me, whenever they decide to divulge it.
Thanks for the recommendation, I'll definitely check this one out as well!
Sounds like you have a single DB for all customers and add a new schema for each customer? Have you looked at maybe a DDL Trigger on the CREATE_SCHEMA event to grant a role/group/user(s) select on the schema or whatever specific rights you are adding? If you use separate databases, consider adding the permissions to the model database; new databases should inherit that from the model.
Kant, &amp;#x200B; I can't think you enough. I will be taking all your advise into consideration for the 2nd iteration of this design. Hope to have the 2nd iteration complete by tomorrow (Friday) at which time I will either post the updated design here or maybe make a new thread. &amp;#x200B; Thanks agian.
How would this be applied to individual database schemas?
We have individual DBs for each customer, and then schemas (about 15) applied to each DB individually. &amp;#x200B; The issue with the model database is that we do not create DBs on servers, we have "starter" dbs that we attach and run queries to modify to specific clients.
Thank you for the feedback. My biggest take-a-away from your post and Kant's is that I need to use the normalization rules to vet my entities and ensure that the entity attributes don't break any normalization rules. &amp;#x200B; Before making this post I familiarized myself with normalization rules but didn't really dig into them too deep so when I created my initial design, I didn't realize it was breaking all sorts of rules. I figured I'd just make the design and apply the rules after but it's a much better approach to apply those rules up front, as I'm creating the design, as doing so will better inform the design by letting me know when I will need to create a "bridge tables" to relate tables that would otherwise have M:M relationships. I feel like this is major key in the design process.
&gt; How would this be applied to individual database schemas? `VIEW SERVER STATE` is a Server-level role - you should only have to grant it once to the domain group. &gt;The issue with the model database is that we do not create DBs on servers, we have "starter" dbs that we attach and run queries to modify to specific clients. So add a query that loops through the schemas from `sys.schemas` in the new database and executes `GRANT SELECT ON SCHEMA &lt;blah&gt; TO [&lt;blah&gt;]` ?
That should not be an issue, an attach still is a create database and should still inherit from the model. You may have to test this, but I'm pretty sure that is correct. I'll try on one of my dev databases... https://docs.microsoft.com/en-us/sql/relational-databases/databases/model-database?view=sql-server-2017#model-usage https://docs.microsoft.com/en-us/sql/relational-databases/databases/attach-a-database?view=sql-server-2017#TsqlProcedure
If you‚Äôre trying to grant SELECT permissions on every table in every schema, you could just use the db_datareader built-in database role. Exec sp_addrolemember ‚Äòdb_datareader‚Äô, ‚Äòdomain\staff‚Äô;
Delete from people p Where exists ( Select top 1 1 From people p2 Where p2.weight = p.weight And p2.iq = p.iq And p2.recordid &gt; p.recordid )
I learned from this course on Udemy: [https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/](https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/) It's completely MySQL-based, but it also gives you a great overview of SQL in general along the way. You might be learning a different version than you'll ultimately work in, but you'll at least understand how SQL databases work and the jargon associated with it. And, most likely, the syntax for your version won't be too far off from any other version you learn - and easily searchable if you know the language (spoken, not programming). As for pricing, it's $13 for the next 15ish hours. That's a good deal, though if you can wait a month or so, you might see it drop to $12 and - sometimes - $10. Udemy always runs sales on courses.
Yes, you can only create a view once, after that the view needs to either be altered or dropped and recreated like a table. The main difference being dropping a view does not delete any data. That's why telling the error message is important when something isn't working as that would have been my first answer right off the bat if I had known you weren't dropping/altering the view
Download the free version of the rmdbs and start using them.
I asked a very similar qusetion in this thread: [How to self join without duplicates?](https://www.reddit.com/r/SQL/comments/9ckggq/how_to_self_join_without_duplicates/) Essentially what you want is this: claim1 &lt;= claim2 this isn't going to work correctly because your claim numbers are strings, you need to convert them to ints, so I would suggest using substring instead. If you add something like this to your query it should work: where SUBSTRING(c1.claimNumber, 4, 6) &lt;= SUBSTRING(c2.claimNumber, 4, 6) This is the entire code i used to demonstrait what I mean: create table claims ( claimNumber varchar(100) ) insert into claims values ('KS-00012'), ('KS-00013'), ('KS-00014'), ('KS-00015'), ('KS-00016'), ('KS-00017'), ('KS-00018') select * from claims c1 cross join claims c2 where SUBSTRING(c1.claimNumber, 4, 6) &lt;= SUBSTRING(c2.claimNumber, 4, 6) order by c1.claimNumber asc, c2.claimNumber asc
So that didn't work so I tried a Server DDL trigger and it did, see below: CREATE TRIGGER Add_default_usr_trigger ON ALL SERVER AFTER CREATE_DATABASE AS DECLARE @EventData XML = EVENTDATA(); DECLARE @sql NVARCHAR(MAX), @db SYSNAME = @EventData.value('(/EVENT_INSTANCE/DatabaseName)[1]', 'varchar(256)') set @sql = 'use [' + @db + ']; create user [t] for login [t]; alter role db_datareader add member [t];' exec sp_executesql @sql GO
&gt; GRANT SELECT ON SCHEMA &lt;blah&gt; TO [&lt;blah&gt;] this right here is the answer
What I don't understand is that if a build a query that includes a join onto an OPENQUERY() segment, it completes incredibly fast. Say: SELECT [P].[ID] ,[P].[LAST_NAME] ,[P].[FIRST_NAME] FROM [PEOPLE] [p] JOIN Openquery([LINKEDSERVER], 'SELECT ID FROM PEOPLE') [ls] ON [p].[ID] = [ls].[ID] but as soon as I join certain other tables on my *local* database, it seems to run indefinitely: SELECT [P].[ID] ,[P].[LAST_NAME] ,[P].[FIRST_NAME] FROM [PEOPLE] [p] JOIN Openquery([LINKEDSERVER], 'SELECT ID FROM PEOPLE') [ls] ON [p].[ID] = [ls].[ID] JOIN [ADDRESS] [adr] ON [p].[ID] = [adr].[ID] Joining onto the address table shouldn't introduce that many additional records. It may even reduce the number of records being matched onto the OPENQUERY result set, and yet will run for upwards of 20 minutes.
There is a slack community (sqlcommunity.slack.com)
Beat me to it :)
I'd concentrate on learning ANSI SQL first - the specific syntax of other RDBMS' are usually extensions to the SQL standard, so having a good base understanding of the standard will make it much easier to pick up the specific quirks as you go.
IIRC, the differences are as follow: 1. This query is going to go to the OPENQUERY() and pull all the data from it over to the local machine *before* joining. Meaning that additional rows which aren't necessary in the join will be transferred across the remote connection, then when the join is performed discarded. 2. This query is going to construct the returns you want, and 'compile' the final set as normal, meaning it's going to process the remote table(s) at the same time it processes the local tables. Doesn't really matter if the records in the remote table are 100% being used or not, it matters how the engine tries to execute its order of operations and remote connections are a choke point.
Caveat: That Slack community is _primarily_ target at SQL Server. While folks there will try to help you if you've got questions about other DBs, no guarantees.
Ahh - tbh I didn't even realize that, but I'm mostly using SQL Server so that's probably why :) I've found everyone on there to be helpful and patient.
&gt; I've found everyone on there to be helpful and patient. We try to keep a good atmosphere over there. Our [BDFL](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life) has done a great job setting the tone &amp; maintaining it.
Fantastic! Thanks :)
Yeah you can do this in MSSQL with the lag window function. https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017
Is there one for Oracle SQL &amp; PL/SQL?
Why joining a subset of your data a "performance concern" in any specific way (other than doing anything costs cpu/memory)?
Not sure of the efficiency either, but that looks like a good solution! Nice one.
Ok so what I am trying to do is remove the combinations. What I am trying to do is a little different than what you did. What you did is come up with a permutation(almost) of all the possibilities with the left side being less than or equal to the right side. &amp;#x200B; I actually have the set of left and right side, already. That is predetermined. I just need to clean the data by dropping out the combinations.
Sorry for the latent response. I couldn't find the code so I rewrote it. DISCLAIMER: DO NOT USE THIS AS YOUR DB'S SEARCH FUNCTIONALITY. I have used this method to find values in a db. Many times you'll see a value in an application and you'll be scouring a database trying to find the table it's stored in. This will help you identify tables with certain values. This query is VERY costly. Run it sparingly. USE [YourdbName] GO DECLARE @SearchString varchar(4000) = '%Thriven%' DECLARE @StringLength int = LEN(@SearchString) IF(OBJECT_ID('tempdb..#TableColumnMatching') IS NOT NULL) DROP TABLE #TableColumnMatching CREATE TABLE #TableColumnMatching ([table_schema] varchar(128), [table_name] varchar(128), [column_name] varchar(128)) INSERT INTO #TableColumnMatching ([table_schema],[table_name],[column_name]) SELECT t.TABLE_SCHEMA AS [table_schema] ,t.TABLE_NAME AS [table_name] ,c.COLUMN_NAME AS [column_name] FROM INFORMATION_SCHEMA.COLUMNS c INNER JOIN INFORMATION_SCHEMA.TABLES t on c.TABLE_SCHEMA+c.TABLE_NAME = t.TABLE_SCHEMA+t.TABLE_NAME WHERE c.CHARACTER_MAXIMUM_LENGTH &gt;= @StringLength AND t.TABLE_TYPE = 'BASE TABLE' IF(OBJECT_ID('tempdb..#Results') IS NOT NULL) DROP TABLE #Results CREATE TABLE #Results (Schema_Table_Name varchar(200), Column_Name varchar(128), Found_Result varchar(4000)) DECLARE @Table_Schema varchar(128) ,@Table_Name varchar(128) ,@Column_Name varchar(128) ,@SQL nvarchar(max) DECLARE search_cursor CURSOR FOR SELECT [table_schema] ,[Table_Name] ,[column_name] FROM #TableColumnMatching OPEN search_cursor FETCH NEXT FROM search_cursor INTO @Table_Schema,@Table_Name,@Column_Name WHILE @@FETCH_STATUS = 0 BEGIN SET @SQL = ' INSERT INTO #Results (Schema_Table_Name, Column_Name, Found_Result) SELECT '''+@Table_Schema+'.'+@Table_Name+''' AS Schema_Table_Name , '''+@Column_Name+''' AS Column_Name, ['+@Column_Name+'] AS Found_Result FROM ['+@Table_Schema+'].['+@Table_Name+'] WITH (NOLOCK) WHERE ['+@Column_Name+'] LIKE '''+@SearchString+''' ' PRINT @SQL EXECUTE sp_executesql @statement = @SQL FETCH NEXT FROM search_cursor INTO @Table_Schema,@Table_Name,@Column_Name END CLOSE search_cursor; DEALLOCATE search_cursor; SELECT * FROM #Results
Why don‚Äôt you try to push the results of the OPENQUERY() into a CTE, and join your query to that CTE? Have never used OPENQUERY, but you might give it a shot.
 ORDER BY CASE WHEN period = '' THEN 'humpty' ELSE 'dumpty' END DESC , period DESC
Yes, it won't execute the query. Poor choice of words on my part. I suppose it is safest to just use single quotes for strings and don't use double quotes at all, unless you really need them for something. This way queries will be a bit more portable.
I tried that too - no dice. Good thought, though. I know I could insert it into a temporary table and go from there, but being able to create views out of these kinds of queries was appealing to me.
Oh ok that makes sense.. I thought the error message was the error code. That‚Äôs how it was explained to us, we have a little link that says ‚Äúhere‚Äôs some common error messages‚Äù then lists the numbers and reasons but it doesn‚Äôt go into detail and is kind of ambiguous to me
I don't know what you are using to write/run these statements but for instance if you are using SSMS for MS SQL Server you'll get something like; Msg 2714, Level 16, State 3, Procedure vProducts, Line 13 There is already an object named 'vProducts' in the database.
yea thats true , i was using oracle apex , i think thats very beginners sql like pseudocode or something lol
If this is a one-off data fix, you can join the original table to itself while assigning column2 to column1‚Äôs name, and group by column1,column2. That is: WITH all_combos AS ( SELECT first_case, second_case FROM original_table AS master UNION SELECT second_case AS first_case, first_case AS second_case FROM original_table AS flipped) SELECT first_case, second_case FROM all_combos GROUP BY 1,2; This should give you all the case combos you started with, whether or not they have duplicates, and those that used to have duplicates should lose them.
Why not just use a modicum of string manipulation and compare to create a synthetic key? (Probably subquery) And then use a running count. Iif(col1 &gt;= col2, col1+'-'+col2, col2+'-'+col1) as col3 Count(col3) over (partition by customer_id) as count Where count = 1 Idk what y'all's system looks like that you have two ticket fields like that.
Pluralsight just made their [Introduction to SQL](https://www.pluralsight.com/courses/introduction-to-sql) course free.
&gt; Should the following code work? what happened when you tested it? ‚Ñ¢
Using a partition is currently how we are resolving this. I can think of another way as well, but I'm thinking there must be some other interesting ways to tackle this. As for why we get data like that, we look at millions of claims many times one person can have 10+ claims in a given period of time. We have to analyze that data to understand how claim 1 relates to claim 2.
Are these the only two cases possible? Because there might be many more combinations in your table which do not satisfy your first condition in your case when.
use IS NOT NULL &amp;#x200B; in t-sql, no value is equal to null (or not equal to null) --- not even null = null &amp;#x200B; so you must use a special logical comparer for null and the syntax is AND ftax.AltIDNumber IS NOT NULL
What's your test case?
The possibilities are a string, NULL, or an empty string ''
It's part of a larger query which is joining on another table based on this value. If it doesn't exist, or is null or empty string, I want it to return that string of 'No NPI Provided'
IS NOT NULL gives me the same result, empty row.
I thought the same too
What if altidcategory = 'npi' and altidnumber = null, what would the output be?
The else condition 'No NPI Provided'
It's a "related ticket" field. Gotcha. Ideally it's in two tables, basic ticket data and related ticket data. I think you can probably use recursive sql to build a tree and figure out how many actual active issues they have. Ex: core issue is "can't access client system" All tickets are "request for VMware." "Check on credentials" "can't vpn to client /missing software." TK-10234/TK- 56423/TK-58790/TK-65473 And after completing the tree, you find TK-56289 as unrelated to the others.
Usually the results of a case statement need to be the same data type. Have if the column value is a numeric data type of some sort, have you tried casting it as a string data type? That normally returns an error, though. Not sure it is your issue.
I would narrow your test, and just check each of the two conditions. Isolate it from the join for a moment, and see if the results are what you expected.
As mentioned above, the return type is a string.
It's probably that your syntax is oracle SQL where as Power Query expects T-SQL
&gt; "File" contains many duplicates within colA. From what I understand, this is an outer join. Outer join is when you want records from a table to be present in the result regardless of the join condition (in other words, even if the join condition eliminates/evaluates to false/null for them). "Full" outer join will include all records from both tables even if the join condition "eliminates" them.
I simplified it but no luck. I think i'm just being stupid. If the where conditions made it so it returned no rows, would the ELSE still be 'selected' ?
try adding schema qualifier for your subquery in the FROM clause: SELECT DISTINCT L62T1.pmha FROM L62T1
Are the null values in ftax.AltIdNumber actual nulls or are they empty string values? Try ftax.AltIdNumber &lt;&gt; ''
So, both conditions fail? Or one of them does? Which one doesn't yield the desired result?
Post your full query, and the result.
You can convert your datetime in the query itself to date CONVERT(DATE, @IssueDate)
that would be great!
 WHERE CONVERT(date, IssueDate) between @StartDate and @EndDate Of course, if you have a data warehouse, you can always store a date only version of IssueDate.
Are you joing tables in your openquery that are on your local server, or are all those tables on the remote server?
From the last code snippet: where m.id in (select s.department_id Are we intentionally evaluating Member IDs against Department IDs?
 SELECT DISTINCT pm.NHProviderID , CASE WHEN falt.AltIDCategory = 'NPI' AND ftax.AltIDNumber IS NOT NULL THEN ftax.AltIDNumber ELSE 'No NPI Provided' END AS NPI , ISNULL(ftax.TaxonomyCode, '') AS TaxonomyCode FROM dbo.ProviderMaster pm LEFT JOIN dbo.NHFacilityAltID (nolock) falt ON pm.ProviderID = falt.NHFacilityID AND falt.AltIDCategory = 'NPI' LEFT JOIN dbo.NHFacilityTaxonomy (nolock) ftax ON falt.NHFacilityID = ftax.NHFacilityID WHERE pm.NHProviderID = @nHProviderID AND falt.AltIDCategory = 'NPI' Result
this was my initial thought, also.
i put oracle SQL in vba all the time with no issues.
SQL server is an rdbms product from Microsoft. I assume when you said SQL database, you actually meant rdbms.
Yes, A single database server can hold multiple databases or even multiple instances.
But you said rdbms is an SQL server, no? "I assume when you said SQL database, you actually meant rdbms".
As mentioned already, change the &lt;&gt; NULL for IS NOT NULL. The only values your case statement is allowed to return are ftax.AltIDNumber or ‚ÄòNo NPI provided‚Äô. If it‚Äôs not returning the latter then what it‚Äôs returning must be the value held in ftax.AltIDNumber. I would try adding AltIDNumber and falt.AltIDCategory to your select clause to see what‚Äôs going on row-by-row when the case statement is being evaluated. Have you tried casting AltIDNumber as a varchar after ‚ÄúTHEN‚Äù? I‚Äôm not sure how the case statement would handle trying to fit both numeric (if that‚Äôs what AltIDNumber is) and text data in in its output column..
"SQL Server" is a brand of database server made by Microsoft or Sybase a "SQL server" is a type of database that one can connect to from a client program. For example. PostgreSQL is a SQL database server There are also SQL databases which are not servers, such as SQLite or MS Access
No, he said SQL server is rdbms not the other way around. RDBMS = Relational Database Management System SQL server = Microsoft SQL Server which is one of many RDBMS avaialbe. There are others like MySQL PostgresSQL etc.
Think of SQL Server as Microsoft Word and a SQL database as a Word document.
OH MAN I MISSED A STEP. I need to find members who shop in the departments where the department have over 10,000 in sales and THEN apply the condition where each member must spend over 1000$. THANK YOU.
Perhaps the confusion in terms has to do with the difference between the term server and database. This is an XY problem for you. [SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-2017) (the Microsoft product) is a relational database management system. As others have said "RDBMS". SQL Server [runs as a service](https://en.wikipedia.org/wiki/Windows_service) on the machine. The service manages the database operations and files. The database itself is contained in a set of files called [MDFs](https://stackoverflow.com/questions/1175882/what-is-an-mdf-file) or "master database file". The database can be as small as one of these MDF files. Other RDBMS software use similar, but different ways of managing database files. But they are all held to most or all of the same [principle](https://en.wikipedia.org/wiki/ACID_(computer_science)) of transactional database systems.
Something in your where clause or join, is causing no rows to return. The case that is in the select is mearly formatting the column, but if there are no rows to act on, then nothing will show. Could pm.NHProviderID be null?
Without knowing the data set and without chunking the query out, I'll give it a stab... SELECT fbp.*, bcs.NSC, bcs.col, bcs.col, .. inner join ( select column_id, yyyy_mm_dd, sum(&lt;col&gt;) as NSC, max(col), max(col) from table3 where yyyy_mm_dd &gt;= '2019-04-21' group by column_id, yyyy_mm_dd ) as bcs on bcs.column_id=fbp.column_id If that doesn't give you the desired SUM + other columns I would look to another join (INNER/LEFT/CROSS APPLY/OUTER APPLY) for the SUM only and reference the joined alias in your main query.
When I started learning SQL the hardest concept for me to grasp was joins. I'd recommend looking up join logic. Here's a link to a PDF that I hope helps [https://goalkicker.com/SQLBook/](https://goalkicker.com/SQLBook/)
Ah, so essentially convert the IssueDate field to date-only before the report filters are applied... That sounds like it could be winner. Thanks!
Thanks for the suggestion! I'll give it a shot.
I think this should work! It seems like a more successful version of what I was trying to do. The only thing is, it seems like it's looking not just at tables, but also views (and giving me an error on the views). Do you know how to limit it to tables only?
Never mind, I see it right there in information schema.tables. I'll add the filter then I bet this will work.
YES, this did it. Thank you!
I'm very sorry to bother again but I wrote a query in which I now find all members that shop in departments that have brought in more than 10,000$ but I'm having a bit of trouble combining this with my step 3. Could you give me one last hint? Thank you in advance! &amp;#x200B; 1) Below is a query that now finds all members that shop in departments that have brought in more than 10,000$ select * from members m join sales s on m.id = s.member_id join departments d on d.id = s.department_id where s.department_id in (select s.department_id from sales s join products p on s.product_id = p.id group by s.department_id having sum(price) &gt; '10000') &amp;#x200B; I tried the following: &amp;#x200B; select m.id, m.name, m.email, sum(price) as total_spending from members m join sales s on m.id = s.member_id join products p on p.id = s.product_id where m.id in (select distinct m.id from members m join sales s on m.id = s.member_id join departments d on d.id = s.department_id where s.department_id in (select s.department_id from sales s join products p on s.product_id = p.id group by s.department_id having sum(price) &gt; '10000')) group by m.id having sum(price) &gt; '1000' But I'm still not getting the correct amount of people
It's pretty useless to just learn random dialects, odds are you'll never use most of them anyway. If you can, reach out to the company and find out what DB they use. Then learn that. If you can't, just stick with MySQL.
This looks somewhat heftier than I need for today, but I'm saving it for later since I know I'll be doing similar work in the near future. Thanks!
SQL Database = The actual database housing the data which is comprised of one or many files. SQL Instance = A container which can house multiple SQL databases along with permissions, policies, maintenance tasks, etc. SQL Server = Generally used to refer to the physical or virtual server that the SQL server software runs on, or the SQL server software itself. &amp;#x200B; For example, if I told somebody to restart the SQL server I would be telling them to restart the operating system of the server that is running SQL. If I told them to restart the "WebApp" instance on the SQL server, I would be asking them to find the specific instance named "WebApp" on the SQL server and restart the corresponding service. &amp;#x200B; Hope this helps.
No, if the query returns no rows, then the case statement won't happen so you can't get the ELSE to happen.
I think you want to amend your where clause to AND (falt.AltIDCategory = 'NPI' or falt.AltIDCategory is null) As it stands at the moment, adding falt.AltIDCategory = 'NPI' to the where clause essentially makes your first left join behave as an inner join.
Remove the bottom (lowest) of "having sum(price) &gt;10000". You want to apply that filter at the department level, where you group by apartment_id, but not when you filter by individual customer. That will return ALL customers for departments with more than $10,000.
Thank you for sharing
Would your final query even run? I'd think it would raise an aggregation error because you forgot to also group by m.name and m.email, like "group by m.id, m.name, m.email". I'm having a hard time seeing another problem here. I'd write the query a lot differently, though: with sales as ( select s.department_id, m.id, m.name, m.email, sum(price) as total_spending from members m join sales s on m.id = s.member_id join products p on p.id = s.product_id join departments d on d.id = s.department_id group by s.department_id, m.id, m.name, m.email) select * from sales where total_spending &gt; 1000 and department_id in ( select department_id from sales group by department_id having sum(total_spending)&gt; 10000 );
I recommend checking out Ed Pollack's book [Dynamic SQL: Applications, Performance, and Security in Microsoft SQL Server](https://www.amazon.com/Dynamic-SQL-Applications-Performance-Microsoft/dp/148424317X)
You've got columns and "count(*)" in the SELECT clause. You're mixing individual row data with an aggregate function. That's usually not allowed, and you're not in the special situation where it is allowed. Every database system I know requires the dates to be in single quotes; '01-01-2019'. Make sure your database works with that format ('mm-dd-yyyy') rather than '2019-01-01'. Make sure there are rows in Class, Trainer, and Employee. Make sure some rows in Class have classDate between '01-01-2019' and '02-01-2019'. Make sure there are rows in Class and rows in Trainer that have the same TrainerID. Make sure there are rows in Employee and rows in Trainer that have the same EmployeeID. Build your query little by little. After it works with one table, add the second table. After that works add the third table.
 select table1.column1, table1.column3, table2.column2, count(*) from MyTable table1 inner join MyOtherTable table2 on table1.someIDColumn = table2.someOtherID where table1.column99 = 'Poop' and table1.column98 = 5 and table2.column2 &gt;= 13 group by table1.column1, table1.column3, table2.column2 order by table1.column1 desc; When you use an aggregate function like count(*) or max(amount) or avg(total), all the other columns in your select must be included in the group by clause. You're grouping by those columns, and then finding the aggregate value for each unique set of values.
Helping and doing all is different.
What are the data types of interval_start and interval_end?
What db are you using?
it's not data types.
[literally could have pointed to the msdn article instead or writing a shitty blog post](https://docs.microsoft.com/en-us/sql/relational-databases/logs/database-checkpoints-sql-server?view=sql-server-2017)
Why do you have the columns as strings in the date diff function?
Write it like a true programmer and use a new line each time you nest
If you already have the two columns then you just need: where SUBSTRING(c1.claimNumber, 4, 6) &lt;= SUBSTRING(c2.claimNumber, 4, 6)
Thanks this worked for me!
This also worked for me!
SQL is a standard for a query language, which is used by all relational databases these days. So any relational database is also a "SQL database". A server in the IT world, is a piece of hardware or software that "serves" something: a file server, a web server, a database server and so on. **A** "sql server" is a database server as SQL is used in relational database. Oracle or Postgres are also "SQL servers" as they are database servers that use SQL. Now Microsoft has a habit of naming their products just like things used in every day life ("Word", "Windows", ....) so they consequently chose a name for the relational database product that also confuses people when using it. They named it "Microsoft SQL Server". So if you talk about *a* "SQL server" that would mean "a database server" (for a relational database). However a "SQL **S**erver" (with a capital S because it's a product name) would probably refer to an installation of the Microsoft product. If you want to avoid ambiguity, it's best to explicitly refer to the Microsoft product as "MS SQL Server" or "MSSQL". In my environment "a SQL server" (or "a SQL database") could also mean a Postgres or Oracle server. Of course, if in your environment "a SQL server" is always "a Microsoft SQL Server", then you don't need to bother.
&gt; Incorrect parameter count in the call to native function 'DATEDIFF' DATEDIFF takes two DATE or DATETIME parameters and returns a result which is a number of days try this -- SELECT ( UNIX_TIMESTAMP(interval_end) - UNIX_TIMESTAMP(interval_start) ) / 3600 AS diff_hours FROM interval_breakdown
Show us the code and the exact error message.
This is my code , idk what i'm doing to be honest i just started developing on sql :/ : CREATE PACKAGE FONCPROC AS PROCEDURE PINSERTMEDICAMENT ( CODE_MED IN NUMBER , LIB IN VARCHAR2 ) AS BEGIN INSERT INTO medicament (CODE MED,LIBELLE) VALUES (CODE_MED,LIB) END PINSERTMEDICAMENT; END FONCPROC;
And the error says ( package froncproc is empty (no public membres).
Now I got it. Thanks.
And the message says "empty package foncproc definition (no public members)"
You need a package body: [https://docs.oracle.com/en/database/oracle/oracle-database/12.2/lnpls/plsql-packages.html#GUID-85E86008-3460-4596-B43A-13D54D6E04C7](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/lnpls/plsql-packages.html#GUID-85E86008-3460-4596-B43A-13D54D6E04C7)
Thanks mate
Hmm, when I tried that I got a different error. "Oracle: ORA-00903: invalid table name"
Can you post the Power Query code that includes your oracle query? The problem is likely a misplaced " or not enough " Power Query passes the SQL query as a string and strips single quotes "mytable" so you need to double them up ""mytable""
Check out Max aggregate function.
 select d.ID, d.SUB_ID, sd.STATUS from ID_DETAILS d left join ( select sd.SUB_ID, max(sd.DATE) DATE from SUB_ID_DETAILS sd group by sd.SUB_ID ) lastSD on lastSD.SUB_ID = d.SUB_ID left join SUB_ID_DETAILS sd on sd.SUB_ID = lastSD.SUB_ID and sd.DATE = lastSD.DATE &amp;#x200B; &amp;#x200B; But i'm almost sure this won't work in real life, because some actions can happen in one day. So you need more columns to distinguish what is last in the same day. And make consecutive left join for that column. &amp;#x200B; Or if you have `outer apply` statement or similar, it can be done a bit easier. In TSQL: select d.ID, d.SUB_ID, lastSD.STATUS from ID_DETAILS d outer apply ( select top 1 sd.* from SUB_ID_DETAILS sd where sd.SUB_ID = d.SUB_ID order by sd.DATE desc --, sd.ADDITIONAL_COLUMN_TO_ORDER_ROWS desc ) lastSD
I tried doubling up quotes to no avail. I've had tables named that way previously and they worked. I was trying to roll up all my queries into one since I was asking for almost the exact same information 3 separate times but for different buildings. Originally I had something simpler like: SELECT l30t1.mha, l30t1.rack, l30t1.horcoor, l30t1.vercoor, g08t1.ldct, l30t1.loctype, COUNT (G08T1.carrno) AS "_Pallet Count" Okay, I tried renaming the "Level" stuff to just with underscores. But other than that and trying to add &lt;schema&gt; to the query, it still won't pull. This is how I'm putting it into Power Query: SELECT l30t1.mha, CASE WHEN L30T1.mha IN ('13D1','12R1','12R2') THEN 'AUTO' ELSE 'CONV' END Level_1, CASE WHEN L30T1.mha IN ('13D1') THEN 'Deep' ELSE CASE WHEN L30T1.mha IN ('12R1','12R2') THEN 'Silo' ELSE CASE WHEN L30T1.mha IN ('11B1','11R1') THEN 'Building 1' ELSE CASE WHEN L30T1.mha IN ('13B1','13R1','13R2','13D2','13N1','13N2') THEN 'Building 3' ELSE 'Conventional' END END END END Level_2, l30t1.rack, l30t1.horcoor, l30t1.vercoor, CASE WHEN g08t1.ldct IS NULL THEN 'F' ELSE g08t1.ldct END LdCt, l30t1.loctype, CASE WHEN COUNT (G08T1.carrno) &gt;0 THEN COUNT (G08T1.carrno) ELSE 0 END Pallet_count FROM ast019.G08T1, ast019.L30T1, ast019.L00T1, (SELECT DISTINCT L62T1.pmha FROM L62T1) PMHA WHERE l30t1.mha = g08t1.mha (+) AND l30t1.rack =g08t1.rack (+) AND l30t1.horcoor = g08t1.horcoor (+) AND l30t1.vercoor = g08t1.vercoor (+) AND l30t1.mha = l00t1.mha AND l00t1.mhatype IN ('2','3') AND L30T1.mha=PMHA.pmha (+) AND PMHA.PMHA IS NULL AND L30T1.MHA NOT IN ('11DC','11P10','11P11','11PF','11PP','11PW','11PW9','11S1','12DC','12GR','12O01','12O03','12O05','12O07','12O09','12O11','12O13','12O15','12O17','12O19','12O21','12P01','12P02','12P03','12P04','12P05','12P06','12P07','12P08','12P09','12P10','12P11','12P12','12P13','12P14','12P15','12P16','12P17','12P18','12P19','12P20','12P21','12P22','12P23','12P24','13L','13PHA','13THA','71B1','CART','CYCNT','L49M1','L49M9','L49MB') GROUP BY l30t1.mha, l30t1.rack, l30t1.horcoor, l30t1.vercoor, g08t1.ldct, l30t1.loctype ORDER BY 1, 2, 3, 4
oh first one will work because i have miscroseconds in timestamps as well, thanks a ton
Try this: &amp;#x200B; `SELECT` [`d.id`](https://d.id)`,` `sd.sub_id,` `sd.status,` [`sd.date`](https://sd.date) `FROM id_details d` `LEFT JOIN` `sub_id_details sd ON d.sub_id - sd.sub_id` `WHERE NOT EXISTS (SELECT 1` `FROM sub_id_details sd2` `WHERE sd2.sub_id = sd.sub_id` `AND` [`sd2.date`](https://sd2.date) `&gt;` [`sd.date`](https://sd.date)`)` &amp;#x200B; In almost all cases it's more efficient to use the not exists clause to get the latest/greatest record (or earliest if you use &lt; instead).
As in, you have a stored procedure (or other code) that you need to run, and you need to store the elapsed time? The first question is... what kind of timeframe are we looking at, and what time resolution do you need? If it's going to be fast, you can just store the elapsed number of seconds/milliseconds (depending on the resolution you need) as an integer. If it's going to take days, you might want to store the start/finish times rather than an elapsed time. In most cases, I'd probably store the start/end timestamps or datetime, because that's the most "guaranteed to be accurate" method of storing the information, and then I'd store an elapsed time in seconds (for longer tasks) or milliseconds (for fast jobs) for easier comparison purposes
Thanks this works for me too.
If using SSMS, you could use row\_number to do it as well. select IDD.ID ,IDD.SUB_ID ,SIDD.STATUS from #ID_Details IDD left join ( select row_number() over (partition by SIDD.SUB_ID order by SIDD.date desc) as rownum ,SIDD.* from #sub_id_details SIDD ) SIDD on SIDD.sub_id = IDD.sub_id and SIDD.rownum = 1
Thx
What's should they be? I'm not sure what u mean by strings...
Yeah. MySQL
I think he means the quotemarks around interval_start' and 'interval_end'. If these are the column names they would not usually have quotes.
This makes sense to me. If you want to test it, u/basic_tom try just selecting the join you make, without the case, and see what it returns. If there are no rows with falt.AltIDCategory = 'NPI', then u/fauxmosexual is correct.
Second option is still the better one to do. APPLY is really powerful and is worth looking into
Lets just say, I have an entity called recipes and have an attribute that stores time to make the recipe. So what data type do I use to store the time to make recipes....say 15 mins, 1hr 15 mins etc
The next question is: What time are you interested in? The total execution time? Parse and compile time? Processing time? Disk time? Network transfer time? Different RDBMSs will gather different statistical information about procedures that you run. For very large result sets, often the longest portion of execution is just the time it takes to transfer the data back to the client.
I would use this: SELECT customers.name, COUNT(transactions.id) transactionCount FROM customers LEFT JOIN transactions ON transactions.customerId = customers.id GROUP BY customers.id, customers.name An alternative that would work but may be overkill here is below. Depending on the data set I prefer the CTE / inline view method as it allows for further modularity... it does have limits, though. Makes for good discussion: WITH transCount AS ( SELECT customerId, COUNT(id) transCt FROM transactions GROUP BY customerId ) SELECT customers.name, COALESCE(transCount.transCt, 0) transactionCount FROM customers INNER JOIN transCount ON transCount.customerId = customers.id Also if this was a discussion, I would ask about the value of the transactions and edge cases around it... what are negative values, (are there negatives?) what are 0.00 transactions, etc...
Ah okay, you're storing the time taken for an external task, not a database task? For cooking, it seems most obvious to me to store the number of minutes as an integer, then do a quick conversion for display Eg 15 for 15 minutes, 75 for 1h15 (and then convert it to 1h15 to display)
From OP's reply, it seems they're actually talking about an external "time taken" (eg cooking time) rather than an elapsed time for a stored procedure etc
Where should the COUNT(*) Go in this query?
I would add 2 more tables. One table would have a row for each supplier selected by the client (this would include its own primary key, a foreign key to client, and a foreign key to supplier). The other one would have a row for every answered question (with a foreign key referencing the table above). Columns would be question number and question answer (1-5). &amp;#x200B; If you want normalize even deeper, you could create *another* table that exists solely for the questions and has 5 rows total. This table would have its own key and the question text. &amp;#x200B; If this is a smaller project (just a few hundred/thousand rows) realistically you could just stored it all in one table. But kudos to you for wanting to create a maintainable data structure.
Can you explain what makes this more efficient? How does it compare to the other options like the group by solution by Kant8? Thanks.
Redgate is probably one of the more popular tools: https://www.red-gate.com/products/#sql-server
Thanks :). what about this schema? looks like yours, but one table less &amp;#x200B; https://snag.gy/OyDvf9.jpg &amp;#x200B; I still need to figure out, though, how the form would work and what entries should i pass onto it through PHP.
There are other data types like timestamp, datetime, time .....I wonder what each one does....I simply can't store it in an integer because it is for a project.
You can store it as anything you like, surely? Integers are just as good as other data types if they suit what you‚Äôre trying to achieve A time stamp is the number of milliseconds since an epoch date (usually 1/1/1970), and is a good way to store time in some circumstances but is not a good way to store a time interval. Similarly a datetime is a good way to store a specific date, but not a time interval You do not want to store a specific time, you want to store a time interval - neither timestamps nor datetimes are great for that purpose Think about what you are actually storing: a whole number ofminutes... a whole number is usually best stored as an integer
I used to use Idera‚Äôs SQL Diagnostic manager. I didn‚Äôt have near that large of an environment, but the tool is great! [SQLDiagnosticManager](https://www.idera.com/productssolutions/sqlserver/sqldiagnosticmanager)
Aaa has three records. How does this make sure the greatest one will join?
Thanks I'll check it out. Have you use it before, what would you say the pros\cons would be?
Any reason why you no longer use it?
I work as an Oracle DBA, so I don't use it actively. However, the MSSQL DBAs seem to think highly of it, in my experience
You aren't going to find something that integrates that functionality of SSMS with a monitoring suite. Monitoring/alerting - I'm an SentryOne fan Management of multiple instances - Central Management Server and automation with `dbatools` and maybe a CI/CD tool.
I changed jobs recently and my new company hasn‚Äôt bought it yet. It‚Äôs not a cheap solution but it‚Äôs good. Also, I am now on a different department and not the dba.
DATEDIFF for mysql takes only two arguments, both of which are dates, you want TIMESTAMPDIFF
What about different customers with identical names?
I'm assuming that your revenue table has one line for each month. Do a self join on the table revenue_table rt LEFT JOIN revenue_table rt_hist ON rt.date = (rt_hist.date + 1 year) You'll have to figure out how to add the year on your own Then use COALESCE to pick your budget COALESCE(rt.budget, rt_hist.budget) AS Budget
Yes a 3rd table to control record visibility and permissions is how I would do it. If you plan to do this with multiple objects..could make it a generic table and not user/contact specific. Think down the road if you may have groups or team permissions against a record (any record). Also consider multiple permission levels (read only, write, delete..share, export, etc.) full blown ObjectAccess table. A step further would be to bake the permissions layer into Views for each table, then use Views for filtered user queries.
I'll do it for one million dollars
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/codinghelp] [\[MySQL\] Sproc worked for a few times then received Error Code: 1054](https://www.reddit.com/r/CodingHelp/comments/bhnb5v/mysql_sproc_worked_for_a_few_times_then_received/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
did you literally type &lt;schema&gt;.l62t1 or did you replace &lt;schema&gt; with the proper schema name?
Yup. His reply wasn't there when I was responding.
&gt; I wonder what each one does Have you read the documentation for your RDBMS? &gt;I simply can't store it in an integer because it is for a project. Sure you can. And I agree with /u/audigex , storing the "task time" in your recipe step as an integer seems appropriate.
I had exactly this problem today and eventually found the same solution online like an hour before reading this post! I feel so vindicated.
Either in brackets or in nothing at all usually. Not in apostaphes
Yep. ^I'm a fuckin retard hahaha. I tried ast019 in front of it and it works great. Thank you so much.
Because they want us to use another appropriate data type.
What's the error?
But it _is_ the appropriate type. `datetime`, `time`, `date` and related types are meant to store values as they pertain to a point in the space-time continuum - a point in time on a calendar/clock. A _duration_ can be longer than 24 hours, and you can't represent that with a `time` field. And in the context of your recipe database here, "bake for 1 hour 15 minutes", storing that as the integer 75 (minutes) or 4500 (seconds) is correct. Unless you want to store a text representation of a time/number - which is even more wrong.
do the calculation in SQL?
 SELECT c.CategoryName , p.ProductID , p.ProductName , p.Unit , p.Price FROM categories AS c INNER JOIN products AS p ON p.categoryID = c.ID WHERE c.ID = 3
Sort on date and use FETCH first row only. Or limit 1 for mySQL
You should at least post the question here as well. So we know if we could actually solve it or not, before jumping on and walking you through it.
You are a lifesaver, now I get it.
Thank you! I think this should do the trick I'm looking for. Sucks it's actually a view and not a table so performance is shit, but this gives me good direction.
As /u/wolf2600 wrote, aggregate functions and values from individual rows can only go together when you are using a "group by" clause. Your query is returning single rows; what would the count be for each row? If you want to know how many rows you've selected, that has to be a separate query.
Ideally, I would like to but I ran into some problems because the report is using two data sets that use the same query. &amp;#x200B; If I was to only use one query, how could reference the min/maxDate2 to the second table in the SSRS report? WHERE ITEM_TABLE.item_id IN (@itemid) AND ORDER_TABLE.order_date &gt;= @minDate AND ORDER_TABLE.order_date &lt;= @maxDate AND ORDER_TABLE.order_date &gt;= @minDate2 AND ORDER_TABLE.order_date &lt;= @maxDate2
Tried messing with FETCH first but couldn't figure out a way
So just union them together, use subqueries, #tables, ctes, etc. Try and do like allll of your calculations in SQL, and nothing in SSRS. Makes life way easier.
another completely useless article here are merely the first two garbage claims i found... there are more &gt; Join's combines the data into new Columns no they fucking don't &gt; Types of JOIN are SELF, INNER, LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN SELF JOIN ?? STFU and GTFO
I feel like using Venn diagrams for both joins and unions will confuse SQL beginners.
How much you wanna bet job_id isn't an int?
So what is the difference between Union and Full Outer Join?
&gt;The Output of JOIN is a new horizontal set of rows having same number of Rows but can have different number of Columns. &gt;The Output of UNION is a new vertical set of rows having same number of Columns but can have different number of Rows. You're just mad you don't understand the difference between horizontal and vertical rows in SQL! /s
&gt;select m.id, m.name, m.email, sum(price) as total\_spending from members m join sales s on m.id = s.member\_id join products p on p.id = s.product\_id where m.id in (select distinct m.id from members m join sales s on m.id = s.member\_id join departments d on d.id = s.department\_id where s.department\_id in (select s.department\_id from sales s join products p on s.product\_id = p.id group by s.department\_id having sum(price) &gt; '10000')) group by m.id having sum(price) &gt; '1000' Hey thank you for your help! Unfortunately, yours gets no result :(. Not sure what I'm doing wrong but my output for the one I wrote above is this: [https://imgur.com/a/xL2sQaO](https://imgur.com/a/xL2sQaO)
When I tried changing my final query to adding a department\_id I get no results for some reason.
Weird. Sorry man, I can't help you further without a script that builds the tables and populates them with data. If you send me that I can help you more, though!
Hey! Totally understand I'm going to try to learn how to enter this into SQL Fiddle it's definitely bothering me and I appreciate your help a lot thank you for taking time on a friday to do this!
Very likely lol.
It's deprecated syntax in T-SQL, so it's not recommended using joins as a where clause.
Inexperience would be my first thought. If you don't understand how JOINs work and which one to use, the second code is much easier.
 SELECT @autobatch = max(job_id) FROM syjob WHERE program_id = 'im6010'
The second example is the join syntax prior to SQL-92. The first example is the preferred way to do it for the past 25 years. Both are technically valid but everyone should be using the first.
A LEFT JOIN will return all rows in the LEFT table regardless of if they are found in the RIGHT table. Your LEFT table is the software table.
Hah or syjob isn't in the scoped database.schema.
This is exactly why you shouldn't use Venn Diagrams for describing JOINs.
&gt; A join is used for showing columns with the same or different names from different tables. The output _showed_ will have all the columns shown demonstrated independently. That is, the columns will be aligned besides each other. **What?**
I personally cannot stand idera and would rather work with SolarWinds DPA. Outside of the occasional burdensome requests; its UI is terrible, the web dashboard is borderline useless., and creating/scheduling 'reports' isn't polished. I'd rather stick a fork in my eye than use SQLDm for query history (and overall query metrics).
CTE‚Äôs almost every one of my reports utilize them. They‚Äôre so useful, that I barely use subqueries anymore unless absolutely necessary or it just makes sense.
\+1 for dbatools!
It depends on the two datasets and what is in common between them
Well this is cool! Not terminal but integrates with vscode. Thanks!
It's definitely not the best article in the world, but it is also about very basic fundamentals of relational algebra. BTW -- a self join is a legitimate join.
Agreed.
Rather a "self" join is a technique, but not a join operator.
Parameterized query? Current user is the parameter. Select contact from table where user = @current_user
Saying "self join" is a bad-language-habit making reference to the *practice* of joining a relation to itself.
This article, like many others, including books, and even "teachers", are basically idiots instructing idiots. Woe unto this world...
&gt; a self join is a legitimate join using the well-known `SELF JOIN` syntax as mentioned in the article? yeah, dave, sure
A `union` returns the distinct set of **rows** from two or more queries. select * from ( select top 10 name from sys.objects order by name) o1 union select * from ( select top 10 name from sys.objects order by name desc) o2; Results Notice that there is only one column returned from the above query. name --------------------------------------------------- _trusted_assemblies DF__test__Guid__25A691D2 DF__test2__Guid__2882FE7D EventNotificationErrorsQueue MSreplication_options plan_persist_context_settings plan_persist_plan plan_persist_query plan_persist_query_hints plan_persist_query_template_parameterization sysxmitqueue sysxmlcomponent sysxmlfacet sysxmlplacement sysxprops sysxsrvs test test2 UQ__test__A2B5777DFB8CEA36 UQ__test2__A2B5777DE958A034 (20 rows affected) A full join returns the product of the relationship between the left table and the right table and does not require there to be a match on either side. If there is a match, it's effectively the Cartesian product, i.e. if there is one row in each table that satisfies a join criteria, there will be one resulting row. If there are two in each, then there will be four. select * from ( select top 10 name from sys.objects order by name) o1 full join ( select top 10 name from sys.objects order by name desc) o2 on o1.name = o2.name; Results Notice that there are two columns for name. name name ------------------------------------------------ ------------------------------ NULL UQ__test2__A2B5777DE958A034 NULL UQ__test__A2B5777DFB8CEA36 NULL test2 NULL test NULL sysxsrvs NULL sysxprops NULL sysxmlplacement NULL sysxmlfacet NULL sysxmlcomponent NULL sysxmitqueue plan_persist_query NULL MSreplication_options NULL DF__test__Guid__25A691D2 NULL plan_persist_plan NULL EventNotificationErrorsQueue NULL plan_persist_query_template_parameterization NULL DF__test2__Guid__2882FE7D NULL _trusted_assemblies NULL plan_persist_query_hints NULL plan_persist_context_settings NULL (20 rows affected) Hope this helps.
As you can see above, I corrected myself as it's not an operator, instead it's a technique.
of course it's a technique but imagine you're new to SQL and took the article seriously...
Try sum(sales) instead of sum(extprice) on line 2. You renamed the column in the subquery so the outer query doesn't know the original name
Ah, I think that must have been it. I was doing sum('Sales') and it said I can't use an arithmetic operator with varchar. I didn't realize that I just need to say Sales instead if 'Sales'. Your explanation that I renamed the column really helps! I didn't realize that's what was happening.
Have a look at this stack overflow page: [https://stackoverflow.com/questions/6777910/sql-performance-on-left-outer-join-vs-not-exists](https://stackoverflow.com/questions/6777910/sql-performance-on-left-outer-join-vs-not-exists) . I feel like it explains it better than I would. In addition, Kant8's query joins to a derived table that uses an aggregate function in the select list, while in the not exists statement of my query the select list is ignored as not exists only returns true or false (quote from the MS 70-761 exam reference book, chapter 2 page 133): &gt;As a predicate, EXISTS doesn‚Äôt need to return the result set of the subquery; rather, it &gt; &gt;returns only true or false, depending on whether the subquery returns any rows. For this reason, &gt; &gt;the query optimizer ignores the SELECT list of the subquery, and therefore, whatever you &gt; &gt;specify there will not affect optimization choices like index selection.
Out of curiosity, why reuse Transaction type in your case statement? It looks like you already have a column named Transaction Type in your existing data, and if someone else were trying to read your query it might get confusing.
This logic is done in the where clause (NOT EXISTS). The logic works as follows: * Select the defined columns * From id\_details * Left Join to sub\_id\_details * Where there does NOT EXIST * a record in sub\_id\_details (alias sd2) that has the same sub\_id as the LEFT JOIN sub\_id\_details (alias sd) * AND a greater date
Additionally i'll add that in the context of the data shown by OP (very very small data sets) the efficiency gain of my query would not be noticeable, if there was one at all. However, when dealing with larger data sets (hundreds of thousands of rows or more) the performance gain made by not having to perform an aggregate function on every row would likely be considerable.
Ohk, I though there would be another type to do it üòÖ. Thanks man!! Helped a lot!
I'm "resetting" the transaction type to what it would/should have been before someone else made a change to the logic. Also, I had a bigger use case in mind when I started the query but ended up paring it down. Finally, I am a bad sql person and would like to get better. Can you give me an example of what a better usage would be?
Just something like "Transaction Category" instead. It's a minor thing, but lets someone know there's a distinction.
Yeah, but where is there a venn diagram demonstrating a union..?
At the very top of the article?
I do not suggest using row\_number in such way. This makes SqlServer calculate rownum for all rows in subquery and only then pick first one. If simple, group by is usually best for performance. Cross apply does sorting, but engine knows that you need only the first row, so it can optimize the query. And if you have proper index for cross apply filter and order by columns, SqlServer even omits sorting in query plan at all, because index already sorted in that way. &amp;#x200B; row\_number() = 1 was always awful after 100k+ rows for me
What about, if you want to add data-columns, use a join If you want to add data-rows, use a union. Simpler..
It doesn't say, annoyingly.
Primary keys can consist of multiple columns. Can't you just define the key a grouping of all the columns?
How much data gets loaded from the flat file? Is it ordered? What's the total width of the columns? The problem with a hashing algorithm is A) time it takes to create the hash when doing CRUD operations and B) the fact that no hash algorithm guarantees 100% unique hashes. If the entirety of the columns is the only way to guarantee uniqueness and you don't want to use a surrogate key, then your only option is to use all of the columns in the primary key.
&gt; is that a ridiculous approach to creating uniqueness no on the other hand, an auto_increment integer would work just fine as a PK however, you did say "I'd like ... to be sure that no duplicate data is inserted" this requires that either you make all columns a concatenated PK, or else you use an auto_increment as PK **and also** declare a UNIQUE constraint on all columns if this table has any potential child tables, use the auto_increment
Just a thought: but have you thought about hashing the data to get a true primary key which would also enforce uniqueness? This would involve a bit on the application side as case sensitivity and order would come into play but you would remove the chance of indexing every column on the table for essentially a computed column that is a unique representation of the row.
MCSE: Data Management and Analytics https://www.microsoft.com/en-us/learning/mcse-data-management-analytics.aspx
I taught myself SQL 18 years ago and am a senior data engineer for Xbox now. So yes, you can. I recommend learning other skills as well though.
Three approaches that I can think of. 1. Make your primary key a combination of every column. Won't work if columns exceed the max primary key length. 2. Hash the whole row as you suggest. Then checking whether the hash already exists is cheap. The tradeoffs here is the hashing algorithm and the chance of a hash collision. 3. Ensure no duplicate data is inserted in some other way.
How the hell did you pull that off?
He had 18 years
Lots and lots of personal training. It's a life of constant education. Also I found something I truly love doing and have a passion for. That's key in my opinion.
He didn‚Äôt always have 18 years of experience.
Small data, &lt; 2500 rows per "load", about 30 columns with an average length of about 30 characters. A mix of dates, floats, and strings. I figure if anything it's just doubling the data in the table to have a unique key, but I could be off base. Disk space is not an issue with something like that.
Have you been through this? I am coming to the realization that I need something structured like a class to pull this off.
Thank you for your comment. I'll use your experience as motivation and I imagine it will motivate others that read it. One thing is that you did this 18 years ago, would the same apply today? I'm thinking there could be a glut of those with entry level database skills today. Also did you have a degree in IT or CS at the time?
My BA is in Ancient and Medieval History and Latin. Yes, I think you can do the same today. You'll start at the bottom, but hard work and persistence will pay off if you put in the time.
Thought about that, but my front end is MS Access and VBA to call up Excel with a .visible=false and import data behind the scenes. But like you said it seems to be that I don't have to exactly duplicate the entire row, concatenated, to represent uniqueness if perhaps a hashing value can represent it uniquely with less information. Question is, where do I invoke hashing and does something like that even exist within MS/VBA? Or, could I somehow make my own? I'm an OK developer but not "senior level" or anything.
I have and it‚Äôs not easy. The practice test and reading material would give you a good indication of what you need to learn. Don‚Äôt expect to pass it with just book knowledge. It‚Äôs meant show hands on experience.
In regard to 1), I know that you can have multiple columns represent a primary key, but do you know if it is better or worse to create a 30-column primary key, or string all the data together in one column, doubling my data, but only having one primary key?
I‚Äôm joining this community today in hopes of doing what you did. I‚Äôm a hs math teacher but it‚Äôs bad for teachers. Could you give me some recommendations on where to start?
Perhaps I can... it would be like 30 columns... I feel like that's a lot, but it's probably weaksauce compared to what some people set up.
Another question: If I define a composite primary key that includes every column, what happens if one of the columns has a null value? I have at least one column where there is only a value about half of the time.
Absolutely. Whether you decide to go for Data and Analytics (dev) Or Database administration, you‚Äôll definitely land something with enough practice. https://www.brentozar.com/sql/picking-a-dba-career-path/ Invest in yourself. Get a a dev server box so you can install the engine of your choice BOU a√±ade a few articles on this subject. https://www.brentozar.com/archive/2016/12/build-me-a-build-what-would-you-do/amp/ Get a Sample dataset from stackoverflow or any other source. Practice and run real scenarios. Disaster recovery, reporting, etc. Get your network game going. PASS has sql Saturday events all over the world, there are Oracle User Groups, and all other platforms (including those used for BI) do as well. Certifications are not something I look for when I hire, but they definitely help. I mostly do practical tests with real data. My latest is some FEMA data I got from /r/Datasets. Links: https://www.sqlbi.com https://www.powerbidays.com https://erikdarlingdata.com https://dba.stackexchange.com
Good to know! Thank you. I'm going to go for it!
Thanks.. you rock!
Thanks for your input. The data I'm loading is a monthly recurring invoice in xlsx format. My fear with using a sequence or some kind of number incrementer is that should the same file be loaded twice, it could be archived in Oracle twice. I need to read about "auto_increment" though because I'm unfamiliar at the moment. In your opinion, if I were to create a composite PK consisting of all columns (about 30), is that better than concatenating all the columns together and having a single primary key? This data isn't too large, about 2500 records per month, columns averaging 5-50 characters. Things like dates, item numbers, item descriptions, amounts, sales tax amount, and some numeric IDs. Disk space and processing is not an issue.
Welcome! My fellow DBA has no degree but she is a damn rockstar. So don‚Äôt ever feel like education will hold you back. Just make up for it elsewhere.
With 30 columns I don't think either one is good, but it really depends on how the data is used, or is likely to be used in the future. How about tracking each monthly import in a separate table? Add ImportId column to the main table with a foreign key to Imports table. In the imports table the primary key will be an auto incrementing surrogate key. You can add columns such as date of import into staging table and the file name of the flat file. That allows you to later audit the imports and investigate/remove duplicates if needed. Without additional info I'd say the hash-the-row approach is the best for you.
Well isn‚Äôt it up to you figure out what‚Äôs true or not? If you‚Äôre the kind of person that can‚Äôt figure that shit out on your own then maybe you shouldn‚Äôt be writing sql professionally anyways.
Good luck! I hope you succeed!
Yes. Read as much as you can and practice. Learn the basics well and be able to demonstrate them and someone will give you a chance. Reporting Services is a good place to start and plenty of companies are in need of report writers. Experience here will give you good foundation in SQL and put you within arms reach of many things data related that you may find yourself interested in.
You can get an oracle certification
Yes, I did and now I'm a SR ETL developer but I've done front end and modeling work too. Started learning about 15 years ago, have been in real database development for 9 years (I went down management trail first in customer service custom reporting.) My degree is in education and I didn't take any CS courses in college. So you can do it too. Read and fiddle with code, get a job in a company that uses SQL (I started in customer service on the phones and taught myself during down times playing with existing queries) and find people that can help you. It took me having a brave/scary conversation with my boss where I told him I was bored and wanted more challenge and growth... Thankfully he did that (hooked me up with some people, access and watched for projects I could do) and really started my career. It took a few years working at it, but it's paid off tremendously. Try to learn and understand data modeling as well, will really help to understand the logic and bigger picture of it all.
I‚Äôm self taught, I moved up within my company from an entry level call center job to a senior business data analyst.
I wouldn‚Äôt, you‚Äôre just asking to be fired if you do. Not sure what country you‚Äôre in (I‚Äôm in the UK), but here I think that‚Äôs a breach of the data protection act and could land you (and your employer) in trouble with the information Commissioner‚Äôs Office. Why not use the AdventureWorks database that MS provide for learning ?
Do you want to be a database developer or admin? There are some key differences. Learning SQL would be key for a developer as well as database design principles and best practices. An admin position requires more knowledge about backing up, restoring, hosting, and security details regarding the database itself. Smaller companies may combine these roles but larger ones tend to separate them. I just got a DBA position with pretty minimal SQL experience. Anyways for good resources check out Microsoft virtual academy. They have awesome SQL and database lessons. Lots of businesses are going to Azure now as well so you should probably check it out at least.
I was more so trying to learn the way these specific tables work. It's quite a messy situation (over 100 tables) and I don't have any schematics or anything to go off, so just figured this would be the better option. I'm in the US, but yeah, I could see some issues, thought I would just copy it onto my work computer.
Let me ask you this...is your import process done via code or something else? If it's via code where you can catch dupes, I'd let that worry about the duplication data rather than add the overhead of a 30 column PK.
Depending where you live and your other professional experiences it might be hard. Ok Australia (where I am from) I was able to work my way into a job where I was sent to a SQL course and used it on the job for three years. I think it would be doable there. In the USA (where I am now) my professional experience didn‚Äôt translate so well; for many people a stumbling block was my lack of degree (although I‚Äôm set to graduate this year) in a saturated market. It took me a couple of months to get a job in the biggest tech market and even then I am for sure underpaid.
You can just take a backup of the database then copy the backup over to a flash drive, then restore the Backup on your computer. If it‚Äôs sensitive data then I would delete the data out of the tables and insert some fake data so you can still play with it.
SQL and Python can make you damn near deadly. Learn both. SQL can take you like a month of dedicated study to do 90% of the querys you'll write on a regular basis. There's a good udemy course by Colt Steele on MySQL, I highly recommend that
He also had the foresight to buy into to an emerging industry so I imagine it was even more Wild West then it is now. OP could say, but I bet part of the reason he is self taught was by circumstance. I doubt there were very many institutions teaching CS or sql for that matter.
The one that sends emails.
create a table consisting of `article_id` and `tag`, where `article_id` is a Foreign Key to your articles table, and `tag` is a VARCHAR one row per tag per article if you want to restrict the tags which are allowed, make `tag` a Foreign Key to a `tags` table -- note you do ~not~ need an integer id for this
Thank you!
sp_whoIsActive by Adam Machanic If you have a server and you want to know what's happening right now, this is the go to tool. Brent Ozar's first responder kit These stored procedures will check your server for configuration errors, health concerns, etc. dbatools.io PowerShell modules. These are not stored procedures but as a DBA, I use these on a daily basis. Redgate SQL Search ApexSQL plugins These are plugins for SSMS that provide some very handy features that are missing from SSMS. They are not all good or worth the price but some of them are pretty good. For maintenance: Olla Hallengren's maintenance solution. These stored procedures and jobs can automate your database maintenance. For monitoring SQL Sentry is a good tool but kinda expensive.
Yeah I don't have a degree either. Although I'm 23 hours short for a degree in mathematics. My plan is to finish after getting a job. It's going to be about 12k to finish and I would like some income.
Good advice. Thank you!
This bar FAR.
I suggest not experimenting on a live production database. Even read-only access can cause blocking or hang the server if you run the wrong queries. Better to take a copy and put it on another server. Make sure that you don't break the log chain when you make your backup. (Tick the copy-only check box) And like others have suggested, best not to take the data home. Most companies setup a VPN so you can remote connect and then you RDP to a jump server which has all the tools you need like SSMS. Ideally the database servers shouldn't even be reachable through the VPN, only the jump servers. Vendors usually don't have the best security practices. Their job is done when the application is up and running. Plugging the security holes is left to the customer. :P
Thank you Tim! It's good motivation.
Then that will not work. No nulls allowed.
Came here to say Adam‚Äôs sp_WhoIsActive, was not disappointed. ;-)
I would strongly advise you to never try and do this. Getting you fired is the least of the potential problems here - you would be creating an unsecured copy of confidential data, which could easily be compromised or stolen. No doubt people will tell you how you can do this, but don't. Just don't.
Just a suggestion, but why not build your own schema? You could do this by compiling a list of tables in the database: &gt;SELECT \* FROM information\_schema.tables And then viewing the table definitions to see the datatypes and sizes of columns, as well as Primary/Foreign key constraints and indexes. You can view this information by highlighting a table name in SSMS and using the default keyboard shortcut: &gt;alt+f1 Using a list of the table names and foreign key constraints you can build your own schema. Not only would this be helpful for you and other developers/analysts at your company, but it would be a very good way of learning the database.
I am an engineering tech electronics and work as shift SA. Trying to change fully into IT and have been doing self study in sql, bash, and python. Almost done with my ccna r/s certs. Thanks for the feedback, makes me redouble my efforts in my studies. Hate to trouble you, but I have a question, would you recommend a ccnp together with the sql, bash, py. Or would I be wasting my time. Originally my goals were networking, but have been leaning towards sql lately.
For Python you just want Python's own documentation, no online course can beat it: https://www.python.org/about/gettingstarted/
Thanks. BTW: I attended one of your workshops in the Netherlands. Was well worth it!
Hi, thanks for your reply ! Sorry for this but I just wanna be sure : do you mean that if I have an article with two tags like "#politics #elections" for an article with a id that is "xxxx1", I should do this : ID - Tags xxxx1 - politics xxxx1 - elections &amp;#x200B; Or should I do this : ID - Tags xxxx1 - politics, elections
I would say yes. I was an Admin Assistant and got my Undergrad with a focus in database management. Honestly it was a lot of Oracle, and a lot that I have never used. I got a job at an email marketing company and now at two school districts. There are jobs out there for SQL and honestly from what I can see, and I can be wrong, but it is more of a keystone now. As Python and R, along with Azure and AWS; the whole data anytime and deep learning. I have seen more opportunities recently grow. Even six years ago it was SQL, SSRS, and SSAS. Otherwise the big focus was full stack developers. I don‚Äôt get a huge salary, but I have a decent retirement system, and three extra weeks time off including my regular days off. Plus it is not as demanding and pressure like private. So the jobs are out there. I would suggest doing some searches, and keep those job requirements. You start seeing patterns and know what to work on.
Personal training. I guarantee you have forgotten so much from 18 years of experience and knowledge. That personal training of learning, but researching and making things work is huge. A lot of what I see as a developer is Google and research. Someone did it, or has an idea, how to make it work. I write everything down and I still go back to code and go oh. Sometimes a good oh and sometimes a bad oh. I know even me writing stuff down, if I used it once it is forgotten till I need it again.
I'm not sayin' he's a gold digger.
I went to a school for my Under grad of Computer Information Systems. I had a focus on Database Management, was only myself. There was two Cyber Security guys. The other fifty were web development. There was also the big group of game developers. This was 2006 to 2009. As I said in my other post there is more to know of SQL now I think. This is due to the huge cost reduction and shift in cloud computing. I am not sure on real DBA work for the future. Data storage space is so cheap now, I had a whole class on mo I toting and auditing tables sizes. You can buy that space now and scale it like never before. Also, with HTML 5, and looking at Office 365, there is a shift to online websites and apps sharing the same code, and less about desktop programs. Won‚Äôt say it will die, think there will always be a need, but the market seems to be moving in that direction. This link is a company that did a space shooter game for Facebook Messenger: https://venturebeat.com/2016/11/28/html5-is-the-next-100-billion-game-platform/amp/ A little off course, but with all of the front end needs a back end. And as others say there is Python and this huge data science push. I see it on Reddit, LinkedIn, and Facebook and that field seems to have an influx that web developers had when I went to school.
SELECT s.* FROM student s INNER JOIN STUDENT_CLASS sc ON s.STUDENT_ID = sc.STUDENT_ID INNER JOIN CLASS c ON sc.CLASS_NUM = c.CLASS_NUM WHERE c.COURSE_NAME = ‚ÄòIT‚Äô;
I wrote a loop in basic on an Apple II in grade school to repetitively print "f*** you" or something dumb and now I make near a quarter mil (not in sf/ny) as a lead dev. It might not be as easy these days, idk, and I did pour my life into a wide variety of tech and programming in particular. Spent plenty of time underpaid. Never have had trouble finding work, though, even as a multiple felon (drugs).
&gt; I doubt there were very many institutions teaching CS or sql for that matter. 18 years.. that's 2001. There were *plenty* of places teaching CS and, yes, SQL then. A decade long tech bubble was popping at the time. It was a different landscape than today, but it was not even remotely a new thing. Maybe if poster had said 38 years ago..
Thank you. I've been using Google Cloud. Do you think the Azure and AWS are more marketable?
Yes. But it will be a junior roll. SQL is part science part art and part gut feel. SQL is still one of the most useful languages out there and it‚Äôs only going to get more widely used, not less like some of the naysayers.
No prob with a junior roll. You have to start somewhere. Thanks man
The most important thing is passion, drive, and attitude.
I just did a search for the Microsoft Virtual Academy and there is a banner on the top of the page saying it's being retired in phases. They said to be sure to finish anything in progress by April 30 and to check out [Microsoft Learn](https://docs.microsoft.com/en-us/learn/). just an FYI
From the Oracle side of things I love having an assert procedure to use in my code create or replace procedure assert ( p_condition in boolean, p_message in varchar2 ) is begin if not p_condition or p_condition is null then raise_application_error(-20001, p_message); end if; end assert; This is mostly a helper procedure for other procedure. I try to fail fast if something is going to not behave correctly. Why load a temp table with a million records just to explode later because some variable is null? Make sure to assert the necessary requirements for the procedure to be successful as soon as possible so the database is not doing a bunch of unnecessary work!
The first one
The front end application is MS Access, and its a form that lets the user call an open dialog box to select an excel file. Then it uses excel (hidden) to do the import into a local Access table. Where Oracle comes in is that the historical table is contained there. To get it there, I'm going through a linked table within Access. After I get the table stored locally, I perform some DML, then send insert the records to the linked table. I'm not sure in what way I'd tell the app through VBA "try to insert into history but not if every single column is a duplicate". There's probably way to do that, maybe by joining every alike column?
Thanks!
u/EngineEngine is right but Microsoft Learn doesn't have anything up on SQL or database fundamentals that I've seen yet. They probably will when the transition is complete.
It'll be ugly, but you should be able to do compares.
Whatever you are into, you can do it. Keep learning and keep improving.
Why you lie
Thanks bro
Post the SQL that you've got so far.
https://imgur.com/a/VAdzp63
You want WHERE last_name LIKE 'J%' or last_name LIKE 'A%' or last_name LIKE 'M%'
I wish you could put LIKE in an IN function :(
where substr(lastname, 1, 1) in ('A', ...)
thanks man! its working now! appreciate it. been stuck on it since last night.
SQL is straightforward and not enough for a job. Do you want to become a DBA? Software engineer!
&gt; Is there a better/correct/simpler way to do this? MySQL has this functionality for MYISAM tables, although you will have to test it for 3 columns, as the manual here shows the example only for a secondary auto_increment -- https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html
 DevOps said no MYISAM tables due to some replication concerns with MyISAM using composite PKs.
&gt; the easy thing would be to iterate through the days u need to learn to think in sets, not iteration.
u must be a really nice guy at work. u have job but can't do it very well. i assume i am unemployed because i am too smart to work with ppl who r nice and stupid.
Sorry, my bad guys....I just used numeric as my data type
Wow. Thanks for that list. I will definitely give those a look. Have a great day.
Nice. Thanks for sharing that. Hopefully no haters chime in. =]
Wow, thanks some interesting stuff here.
&gt; For monitoring SQL Sentry is a good tool but kinda expensive. And if you have a boat load of servers to manage check out Minionware. Again, also expensive. * What's your time worth, as a DBA? * How much does downtime or poor performance cost your company? My guess is that the price of any monitoring suite is much lower than either of these.
I work as a long-term consultant. My current client is skimping on everything: Memory, CPU, disk IO. They will not approve the purchase of any extra software. Yet they are paying through their nose on personnel costs because half their IT department consists of external consultants. They rather have me re-invent the wheel than look for existing and proven solutions. No problem with me, it's their money and I get to have fun scripting and automating everything with SSIS and SSRS. I guess somewhere in management they separate HR and IT costs and they are coming out of different department budgets.
What kinda databases do you deal with at xbox? What about the tech? Would love to know these things if possible?
You are going to want to use a BI tool to create the dashboard... something like Tableau, PowerBI, or Google DataStudio. You can normally have the BI tool connect straight to your database and then its pulling the data and making the dashboard. Im not sure how well postgreSQL works with connectivity, I have only used DB2 for I and SQLServer. Real time can be a bit challenging as it can be a constant strain for little change. You should be able to adjust the frequency of updating though based off needs. 15 min updates would probably be fine to get pretty close to real time, if not you can reduce further.
OP use Qlik Sense. Trust me. Then become a power user of it and master Set Analysis. Qlik Sense and it‚Äôs predecessor QlikView have a long history in the banking/finance sector.
Yes, and honestly, this is probably the best way. It makes more sense when you have to figure stuff out rather than someone dictating to you what to do and it seeming like black magic.
In one of my previous jobs there was a setup based on the Elastic stack which worked quite nicely (also real time).
This is really cool! Good job
Thank You!
Nice try, Qlik Sense salesman.
Thst would be a nice gig. We‚Äôve done a lot of testing on a lot of BI viz tools. Qlik worked well for our env.
&gt; where the age is greater than or equal to 13 OR where the age is equal to 44 duh also, how come when i run a query it always says "No rows found"??
This looks cool. FYI I'm not able to type more than 1 character in the code box from my phone. When I try to type, it replaces the first letter I type with the next letter and so on, only keeping the last character typed. I'm using Chrome for Android if that helps. Definitely will give it a go when I'm on a PC.
Yeah I wish my company wasn't so deadset on Tableau...everyone and their mothers uses it, including my team. It's a good tool but it has its problems and I feel like there are better options out there.
It looks really nice, one thing I have always wondered is if there are any explain type websites for SQL where the elements just get highlighted in the tables as you type the SQL. Do you know any projects like that?
thsnks
It's worse to concatenate all your data and duplicate it in an additional column. Either of those options might be entirely unviable, depending on what makes up a row for you. A hash can work, but collisions do happen. You need to be able to differentiate duplicate hashes. Any way to add a UUID to the Excel sources to uniquely identify rows? You would want it to be a seqential UUID, like Version 1 or 2 or COMB. Since you're in VBA, you may be on your own for hash and UUID algorithms, but it's not to difficult to copy a reference implementation.
sudo pacman -Sy postgresql Further note would be atleast google the issue before asking on a forum
Well engineered. Your proposed solution to Level 9 is wrong: SELECT MAX(age), name FROM person; You can *either* select values for each row (like column `name` above) *or* compute an aggregate (or multiple aggregates) of all rows (like `MAX(age)`). Give it a thought. One possible solution for Level 9 might be: SELECT age, name FROM person WHERE age = (SELECT MAX(age) FROM person) I feel, though, that such usage of a subquery should occur only later in your tutorial.
What i meant was how to proceed beyond that, I know how to install it but how do I make it work, nothing seems to work, tried copy pasting a lot of stuff and still did not work.
https://github.com/malnvenshorn/OctoPrint-FilamentManager/wiki/Setup-PostgreSQL-on-Arch-Linux
Yup same here Chrome for android When I input something it deletes the most recent input somehow So if I wanted to type select I had to type in S s e e l l e e c c t t
Thank You!
Again this was the first result on google
"Question" 4 isn't a question. Looks more like a series of statements/commands.
Personal question, do you have a college degree?
Just an Associates.
I would concatenation the column data then hash it. I don‚Äôt know Oracle well, but I know the sqlServer function is HASHBYTES Try to use an SHA_256 hash - that will give you a pretty short hash size and good collision resistance
i'm on level 11 level, so far so so good! just out of curiosity, what's the final boss level?
No, because the field is a very relevant aspect of the tuple. Imagine the implications of this. What if one field was Price and one was Quantity: Row 1: Price = 5, Quantity = 1 Row 2: Price = 1, Quantity = 5. Would you want these treated as non-distinct? Absolutely not. You would have to create some logic that generates an ID for that combination of cities, then apply distinct to the ID.
Distinct is looking for entirely distinct entries. I just did the following in SAS: data temp; infile datalines dsd; input dest1 $ dest2 $ number; cards; "Seattle", "Denver", 12 "Denver", "Seattle", 12 run; proc sql; select distinct dest1, dest2, number from temp; quit; &amp;#x200B; The results were: dest1 dest2 number Denver Seattle 12 Seattle Denver 12
Not trying to be rude but I use duckduckgo and I tried almost every result. Thanks anyway.
Thanks a lot !
Good point - I noticed that as well. Hope you're right about them adding SQL after completing the transition. I've been really interested in learning some fundamentals recently.
you didn't mention which database platform you're on, so in case it's MySQL, you can do this -- SELECT DISTINCT GREATEST(origin\_city, dest\_city), LEAST(origin\_city, dest\_city), `time` FROM t
Thank you for this, the editor is actually an open source React component. I'll write up an issue on Github for this. Thanks again!
None that I know of. Maybe you could make it? It would be cool.
What are you confused about? It‚Äôs the same idea as question 3, it‚Äôs just asking you to join another table to your query from question 3.
So cool! Thanks! You are gonna have to find out!! &amp;#x200B; &amp;#x200B; ... it's nothing too crazy, just chaining together a few things. If you have ideas for more levels, I'm definitely open to pull requests!
Well that was one problem that I just figured out. Had no idea you could join more than two tables! &amp;#x200B; This was my attempt that I just finished. I'm not sure if I did it correctly or not: &amp;#x200B; SELECT Top 10 BusinessEntityID, FirstName, LastName, Sales.Customer.CustomerID, Sales.Customer.AccountNumber, SalesOrderID, OrderDate, SubTotal, TotalDue FROM Person.Person INNER JOIN Sales.Customer ON Person.Person.BusinessEntityID = Sales.Customer.PersonID INNER JOIN Sales.SalesOrderHeader ON Sales.Customer.CustomerID = Sales.Customer.CustomerID
I did finish it but there was no explanation about having clause. I think you should add a difference between like and having in one of your slides. Good job otherwise! GGWP.
I think you made a typo in your join condition because you have the same table twice. It should be: ON Sales.Customer.CustomerID = Sales.SalesOrderHeader.CustomerID
Thank you. I keep making that same mistake because of the way the text completion in SSMS works :( Thanks for the catch!
Nice! I like this. :) Wish I would have found something like this back when I was learning SQL.
As others have said, DISTINCT will not work with your design. select distinct (origin_city) as o ,max(dest_city) as d, city_time from t group by city_time order by &lt;city or dest&gt; You could also left join to the T as well, but, to each their own.
MySQL is CLEEEEEAN! :P
Good idea!
It‚Äôs the best way to organize large amounts of data into a easy to analyze package.
GREATEST and LEAST are supported in Oracle too, and for the life of me I can't work out why they aren't universal.
This is definitely a Python problem. You'd have more luck on learnpython subreddit. It's hard to read the stack trace on my phone but it's probably a path issue. Either where you're impirting your flask app or something changed with flask between when the video was recorded and 3.7 Python or whatever the most recent version of Flask is.
That might be the problem. Duckduckgo is worse than Bing in terms of relevant search results.
Will DM you.
I'll make sure to Google first :)
You could change the count on order date to be a count(distinct order_date) but that assumes that customers can only place one order per day. If you have the order id available that is really the column you‚Äôd want to base your count distinct on.
I second what u/mkingsbu said that this is more of a python issue than a sql/mysql issue and you may find better assistance on a python board. That error however is due to a miss-linked or missing core package that the python module is looking for. When you import a package from a script the module must be installed (e.g. pip install mysql) and the requisite dependencies must also be installed. What you‚Äôre likely missing is the OS package (e.g. apt-get python-mysql, or whatever the OSX variant is, I‚Äôm not a Max person). ImportError is a bit vague as it will be thrown if the code point cannot be resolved in some way. The stacktrace will indicate where in the dependency chain the load failed. In your error, the .so cannot be found which indicates a operating system level package cannot be found. To identify which package, you may want to seek assistance from the package maintainers or on a python specific board. Best of luck to you.
I am 10 years removed from college and just got a job as a DBA. I am more of an operational DBA which means I review a lot of scripts, manage user accounts, manage active directory, review replication and other tasks. I am part of a team that manages 60 SQL Server instances. I found taking the Windows Server MTA was useful for what I am doing. I do not mind doing 50 to 60 tickets a week from small tasks to updating millions of records. If you want it, build DBs yourself and if possible, in the cloud. Use AdventureWorks DB. Work on these tasks and demonstrate you can master basics. For example, what is a deadlock.....and how does it get resolved. I was in IT support, ETL and data warehousing before I got a job as a DBA.
Experience and training are worth more than Certs in my experience hiring developers and DBAs. Also, knowing just SQL (while the essential part of the job) won‚Äôt set you apart, learn ETL and the products used (DataStage, Oracle Data Pump, SSIS, SSAS, SSRS, etc.) in order to have something that sets you apart. Look into Reporting with SSRS or PowerBI (both free) to see if it‚Äôs something you like. Things like training from BrentOzar will help tremendously: https://www.brentozar.com/training/training-plans/ GroupBy.org (Free) has tons of sessions as well. Here are a few links that will help IMO. Production vs Development DBA:Career paths (Brent Ozar) https://www.brentozar.com/sql/picking-a-dba-career-path/ The Jr DBA workout plan (Erik Darling) https://youtu.be/5LOv5uq4ofU Microsoft Hands on Labs https://www.microsoft.com/handsonlabs
That is simply not true.
I'm also actually learning SQL myself, and I've got through the basic stuff, and now I'm told to get adventure works db and work on it. But I'm not sure what I'm supposed to do with it? Are there any guides out there I can look up?
 SELECT user_id , COUNT(order_date) AS total_orders FROM mytable WHERE order_date &gt; CURRENT_DATE - INTERVAL 29 DAY GROUP BY user_id
I tried some good resources like leetcode, sqlzoo, and strata scratch. I found stratascratch most helpful as there are questions from technical interviews from other companies so I found it best resource for my career to use for interview practice. You can also check leetcode the best thing is they have a discussion board to get help from others.
Thanks for creating and sharing. Well done. One thing, whenever I run the Select query, the Query Result section always says "No Rows Found". Just putting it out there as a "you may want to look into it". =\]
I think colleges do a poor job of informing people that there are 2 distinct career paths with SQL. One is a developer, the other is the DBA. I'm a developer, I deal with data inside the database. There DBAs deal with the databases in the server. At very small companies the same person will often do both jobs. Certs may be a thing for DBAs, but they have no value to any developers. Getting an intro job at the help desk won't prepare you for anything. It is it's own career path. Don't take that path as you deal with crabby people who are often tech ignorant.
There have been lots of attempts at this sort of thing over the years. The trouble is that once you get beyond trivial queries, it becomes either very difficult to graphically build a query, or the query that gets generated ends up running like crap.
&gt;Certs may be a thing for DBAs, but they have no value to any developers. Wow, what a sweeping statement. I don't agree with that at all. I'm a SQL dev with \~10 years experience and the certifications I'm studying for now, the MCSA, will certainly be of value. Not only will they prove to me that I know my subject they will also prove to potential employers that I know my subject.
 I think you‚Äôre getting squishy results because you haven‚Äôt actually assigned a floor for each user. You‚Äôre currently asking each order date to be within 29 days of itself, rather than within 30 days of the user‚Äôs first order, so it brings them all back! In order to set the floor, I‚Äôd create a CTE (that first blob that defines an interstitial search first, ‚ÄúWITH cte AS (SELECT somestuff FROM somewhere)‚Äù gives me a table I can call on later that‚Äôs ephemeral). Specifically, I‚Äôd make one that gets me the first date of purchase first, then use that info to set the window of interest.(You could also do this with a windowing function, but I find CTEs more legible and easier to write on my phone, lol). Something like this: `WITH first_dates AS ( SELECT user_id, MIN(order_date) as first_order FROM mytable GROUP BY user_id) SELECT user_id, COUNT(mytable.order_date) FROM first_dates JOIN mytable ON first_dates.user_id= mytable.user_id WHERE mytable.order_date&lt;(first_order::date+‚Äô29 days‚Äô) GROUP BY user_id` This would ideally be accomplished with a table that has unique IDs for orders, not by counting dates. I‚Äôd feel more confident with a COUNT(DISTINCT...), myself, even on the query right here of dates, but that really *will* be only a count of how many days a user placed any number of orders in the first month of customership. (Being clear about caveats is hugely helpful to any business, so go with what you have confidence in and communicate clearly about the limitations of the number. If you‚Äôre sure, having gone over the data, that there are no erroneous duplicates, counting dates can be safe enough for government work, I‚Äôm just super paranoid and want distinct UUIDs on eeeeevrything.)
OP, clearly ymmv. I've been doing this for 6 or 7 years. None of my coworkers have had certs. That includes data warehousing stints at 2 Fortune 500 companies. Depends on your employer. When I interview I get asked technical questions. I would agree that certs prove to a small employer (who doesn't have a pool of other skilled developers to quiz applicants) who is competent. My lack of certs has never held me back.
[Hmmmmmmmmmm.... ](https://www.reddit.com/r/SQL/comments/bikzbv/sql_certification_for_aspiring_dba/)
No, nor me. I've never had an issue without certs. Gaining them is more for my own peace of mind than anything else. But I would argue that going through the process can be an invaluable learning experience for those who may just be starting out.
To be honest, SQL isn't that hard to learn. And the stuf that is difficult, you would have no way or making it user friendly.
Outer Join and Full Outer Join are the same thing. You can join two tables on as many fields as you'd like. Just be aware that it will impact performance because the db needs to make all those comparisons, so don't use any more than you have to to get the job done.
It can be done, but the queries can't be too complicated. And even on fairly simple queries you can end up with an inefficient query. I've toyed around with the idea of building something like this for myself to generate boilerplate that I can modify as needed, but I don't know that it would save me enough work over just copy/paste/edit from our canned queries to justify all the work of building it.
No, they are not exactly the same thing. A "full outer join" is a specific type of "outer join". But, in addition to FULL, there are also LEFT and RIGHT outer joins. A FULL OUTER JOIN has all records from both tables being joined with NULLs for columns on the missing side. Likewise, a LEFT OUTER JOIN has all the records from the first table, and NULLs for columns on the second (right) table where the join condition isn't met and a RIGHT OUTER JOIN has all the records on the second table, and NULLs for columns on the first (left) table where the join condition isn't met. While there are multiple outer join types, there is only one inner join, which filters out records that don't meet the join condition. In answer to your other question, yes you can run joins on multiple columns with the same SQL, and the join conditions do not have to actually be defined as primary/foreign keys.
pgexercises.com
I always recommend granularity-driven query design: 1. What grain (relevant) the input tables/datasets are? 2. What is the desired output granularity? 3. What is the granularity of other aggregations? In a very simplistic manner, you can think of "group by" as "reduce/set granularity at", i.e. dataset with granularity (A,B,C,D) group by (A,C) gives you granularity (A,C). You can think of joins as "adding granularity" - (A,B) &lt;any&gt; join (C,D) gives granularity of 4 (A,B,C,D) unless your condition culled or made sure some granularities match up, i.e. (A,B) &lt;join&gt; (C,D) on A = C would give 3-member granularity (A,B,D)
Nice approach and best practise. * fbp is one row per object, no partitions. * bcs is also one row per object but is partitioned. One partition = one day of data. * st has many rows per object as it deals with their current state. Current state is identified by having a null entry for the updated at field. This column also isn't partitioned. * cspd is also one row per object. One day of data per partition.
they aren't the same thing i forget where i read this analogy, but it's perfect imagine a dance, where there are boys on one side and girls on the other side and the music starts and many couples start dancing but there are still some boys left over on one side who aren't dancing and some girls left over on the other side who aren't dancing a left outer join gives you all boys' names, whether or not they are dancing with a girl, and you get the girl's name if they are a right outer join gives you all girls' names, whether or not they are dancing with a boy, and you get the boy's name if they are a full outer join gives you all boys' names and all girls' names and shows you which ones are paired up dancing
fyi a python certification is probably not worth the cost either. Take some free courses online or get one of the books that r/learnpython recommends and then start coding and building projects. A github is more useful to most employers than a certification. If you want to do a course make sure it‚Äôs project-based and that the course will help you build a portfolio of work. Just having a certification without something to show for it won‚Äôt get you far.
You kind of need to get to the real level of detail (specific column names) there tho. "one row per object" _should_ be a definition of all of your tables (as long as they have a PK).
Thank you, random stranger. The thing is why I wanna do a python certification is that I already know python and have a few projects on github too. I switched from CSE major and have been coding for a solid 6 years now but this one time I was applying for an entry level job and the recruiter didn‚Äôt even look at my profile and projects and straight up asked for a certification as I didn‚Äôt have a degree. Missed a good opportunity that day sooooo...
Open your resume, and under your "skills" section, add: SQL. Done.
I cant say I agree that there is a clear line of separation between developer and DBA. https://www.brentozar.com/sql/picking-a-dba-career-path/ I am a Production/Development/DevOps DBA for SQL Server, but just DevOps and Development DBA for Oracle (I don‚Äôt handle Oracle backups or hardware). Developers don‚Äôt normally deploy code, don‚Äôt do much tuning, and don‚Äôt handle operational tasks. Development DBAs are becoming way more common now, especially with the adoption of Cloud based Database platforms like Azure and Oracle 18c+. There is no more hardware to manage.
I don't know why you got downvoted, this answer is correct. Unless you specify a specific type of join (LEFT or RIGHT), an OUTER JOIN on its own is by default a FULL OUTER JOIN (at least, in SQL Server).
What do you mean by "statistical normalization"? You want to remove the outliers from a data set?
Because it's the internet. And if you don't specify everything to the n^th detail, somebody is gonna get pissy about it.
In order to answer this question I need to know what you mean by "only 100 rows are produced" and what flavor of SQL you are using. If you are using SQL Server and if by that statement you mean to say: is there a way that I can write a SQL statement that can utilize the first 100 rows from the source table as the total amount of rows in the created table? Then yes this can be done through using a CTE or temp table then use that limited data write your joins on. As such the code would look something like this: WITH CTE\_EXAMPLENAME (ColName1, ColName2, ColName3) AS ( SELECT (\[ColName1\], \[ColName2\], \[ColName3\]) FROM \[schema\].\[SOURCE\] LIMIT 100; //write your joins on this select statement as it pertains to the source table information that you want. ) SELECT \* INTO \[schema\].\[NEW\_TABLE\] FROM CTE\_EXAMPLENAME
ah. Never heard of that happening, sounds like you had a terrible recruiter. Certificates don‚Äôt matter if you have work experience using coding. Any company that knows what they‚Äôre doing would know there is no standard python certification. I would suggest just getting a cheap coursera certificate and then directing them to your github for further evidence.
I know I was pretty bummed even though I had a couple certificates from online courses he wasn‚Äôt listening and ended the interview when I tried to argue my point. Anyways, he was pretty old (probably late60s) and someone told me that he has never ever written a single line of code.
Can you use another table for that?
I guess I have to, indexing with the same primary key?
I‚Äôm actually working on something like this now. It‚Äôs a tool that allows you to create ‚Äútables‚Äù via GUI and make basic links between them. It then reads the template and generates views/sprocs. Then a second tool interfaces with the compiled structures and provides CRUD access
It is good enough. A lot of people slap SQL on resume just because they learned to do SELECT \* FROM table; I wouldn't worry about no professional experience, since everyone at some point had none of it and yet it seems good idea to inform potential employer that you have some knowledge of it. Remember, that you are trying to sell your skills, just make sure you have them.
Easiest way is to try and get the person to stop thinking about SQL as though it were a programming language, and instead to think of it like a command prompt. SELECT * FROM Table Is analogous to `cd &lt;DirectoryName&gt;` and then `cd..`and lets you see what's there. Then you might only want to see the *txt files... then you want to "join"... etc. Makes the entire process a lot simpler.
Statistical normalization refers to scaling down the data set (usually to the continuous set between 0 and 1). &amp;#x200B; So he wants to convert a dataset of (0,10,200,3000) to (0, .001, 0.2, 1).
Thanks yes, of course, it seems so obvious now. [RESOLVED]
How about a Coursera certificate in Database Engineering/ Database management? Also take some of the LinkedIn learning courses on time management/ consulting/ business writing etc. To bulk up your certificates within soft skills.
How does 200 and up .2 and 3000 as 1? I statistically normalize data all the time but have never heard of the term as you're using it.
To normalize any set of numbers to be between 0 and 1, subtract the minimum and divide by the range.
Spitballing min/max values of a dataset. Though, what definition of normalization are we referring to? I'm referring to this one https://en.m.wikipedia.org/wiki/Normalization_(statistics)
Why are you using the square brackets unnecessarily? Unless you are using a keyword in an object name, the brackets are optional and actually really annoying to look at and read. If you stop using the brackets, the intelligence behavior should be what you expect. I kinda like the way it works because it punishes you for using square brackets where they aren‚Äôt needed.
Not sure what others think of these but this is what I‚Äôve been looking at https://www.coursera.org/specializations/excel-mysql https://www.coursera.org/learn/sql-data-science
Slap it on there and spend a weekend on HackerRank doing SQL problems. You'll be just as prepared for an interview as I was to work in T-SQL shop.
So what‚Äôs the difference between outer join and full outer join?
No, it does not default to FULL OUTER. In fact, "OUTER" is optional and the join type (left/right/full) implies the "outer". If the join type is not specified, "inner" is the default.
There's no reason to blame internets if both of your statements are patently wrong.
i hate intellisense. ive always turned it off.
Check out SQL Complete by db forge. I use the free version and think it‚Äôs a hundred times better than intellisense.
Just go to vscode for query writing. I keep ssms open for dba or seeing seeing table columns etc, but I do the bulk of my work with vscode now
/u/haribofiend explained it well. So, I have a function that reads as the following: DROP FUNCTION NORMALIZATION; &amp;#x200B; CREATE FUNCTION NORMALIZATION(Age NUMBER) RETURN NUMBER AS BEGIN RETURN ((Age-MIN(Age))/(MAX(Age)-Min(Age))); END NORMALIZATION; But I'm running into an error that says "function or pseudo-column 'MIN' may be used inside an SQL statement only"
Making a UNION ALL views for the tables is one common way to handle this. Then you only query the views. ETL-ing the data into combined tables or a star scheme is another method. One final option would be to push the legacy data into the new CRM so that it‚Äôs all in one system.
REDGATE SQL PROMPT!!!!!!!!!
Data will be in combined tables. Eg the customer name column will have all the customer names, but for customers initiated in the old CRM I'd use one customer ID and for those initiated in the new I'll use the new ID. When reporting on all customer orders, I'll gave to join old customers to orders via old custid and again to orders via newid.
We're talking the same thing, calculating standard deviations and normalizing the data to remove outliers. I've never had a use for converting the data between 0 and 1 though, but I can see how that might be useful for a variety of purposes.
Got it. So the data is already in combined tables. Are the IDs in 2 different fields? If so, would it work to make a derived column that unifies the IDs into one and ensures uniqueness, and then you just join on it?
I discussed that with the DW team but they said that in some cases both IDs will be necessary, such as where (ex) a customer is created on old system but modified on new system, or created on old system but has an order created on new system, they'll have to be joined on both IDs.
But what if in 50 years Contact becomes a keyword and results in a syntax error? Have you thought about that??? Gotta think ahead, man.
sorry don't know where to post, I'm new to sql
The In statement on customerstate has HI‚Äô in it which caused the fluke. It should have been ‚ÄòHI‚Äô
How can I get rid of it, do I need to type the statement again??
I don‚Äôt know based on this info. Clear Screen or restart I am guessing.
But let's remind everyone that there's never a good reason for a right join
Usually, by convention, people put the tables (or views/subqueries) they want _all_ records from first and then use left [outer] join (the "outer" keyword is optional -- so in my experience people say it or write it in comments more than it is actually written in SQL) to get records _if any_ from other tables/views/subqueries. Left join means you get all the records from the first table and might get nulls if there are no matching records in the second, but don't return rows from the second unless there is a matching record in the first. This is the most common scenario, so left joins are the most common type of outer join. Right [outer] join is pretty rare by style convention. There could be places where it makes sense, but because join order matters, it would usually be avoided unless you need something like: FROM A LEFT JOIN B ON ... RIGHT JOIN C ON COALESCE (B.x, A.x) = C.x Full outer join is more likely to be spelled out because it is rare and you want to make the strange case really obvious. Full outer join means all rows from both tables/views/subqueries are returned whether or not the other has a match. So, there could be nulls on _any_ given column. For this reason, I don't think I have ever had a use for a full outer join without the select clause having a COALESCE or ISNULL or something similar to give at least one column we can rely on to give the row an "identity".
Stack Overflow, my friend, Stack Overflow. ;)
Thank you for posting that! Looks like a great resource.
... and I went to the University of Washington who is one of the leading institutions on the west coast for CS departments... yet I chose History and the Classics.
We have internal data storage called Cosmos (not to be confused with CosmosDb in Azure) Cosmos uses the SCOPE language which is a C# hybrid looking very similar to T-SQL, but with the ability to use custom C# to aggregate, manipulate data as needed. We also use a lightweight Azure tool called Kusto which is uses a language like reverse SQL... Then there's a lot of Python, R, Spark, and we always have lots of MS SQL servers around... well, actually most is moving to SQL Azure or an IaaS Azure SQL server... most tech revolves around Azure.
As a data analyst do you have access to their database? I just started a junior data analyst position and use SQL server everyday, although pretty simple queries
Interesting thanks for the reply. I'm intrigued to learn about SCOPE so will check it out later.
I think what you're looking for is the UNION function.
```SQL SELECT * FROM (QUERY 1) as A LEFT JOIN (QUERY 2) as B ON A.id = B.id ```
This. Avoid using brackets unless necessary and that should help cut down on the ridiculous failures in intellisense.