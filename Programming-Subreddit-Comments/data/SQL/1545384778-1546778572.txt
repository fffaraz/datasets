Start looking for another job and send this to your boss's superiors. 
Just indent every line of your query by 4 spaces or a tab. I usually write it in a text editor then indent it and paste to reddit
It would be really nice with a [code] [/code] tag here, similar to BB boards.
https://twitter.com/pokalope/status/1075757828680368128
Great, t-sql wants to have Exec sp_columns TABLE
Amazing 
 select a.Date from ( select curdate() - INTERVAL (a.a + (10 * b.a) + (100 * c.a) + (1000 * d.a) ) DAY as Date from (select 0 as a union all select 1 union all select 2 union all select 3 union all select 4 union all select 5 union all select 6 union all select 7 union all select 8 union all select 9) as a cross join (select 0 as a union all select 1 union all select 2 union all select 3 union all select 4 union all select 5 union all select 6 union all select 7 union all select 8 union all select 9) as b cross join (select 0 as a union all select 1 union all select 2 union all select 3 union all select 4 union all select 5 union all select 6 union all select 7 union all select 8 union all select 9) as c cross join (select 0 as a union all select 1 union all select 2 union all select 3 union all select 4 union all select 5 union all select 6 union all select 7 union all select 8 union all select 9) as d ) a where a.Date between '2010-01-20' and '2010-01-24' 
I want to get the length ob the column. To get that, I need to join somehow... üòÇ
I want to get the length ob the column. To get that, I need to join somehow... üòÇ
Like select column1 + max length From table1
But not Select max(datalength (column)) I need the real max of database column possible
that's an ~insane~ query to produce just five date values
TL;DR -- you cannot reset the auto_increment number to a value that is less than the maximum value of the column currently in use in the table of course, the "workaround" is to TRUNCATE the table!!! i wish all my database problems were this simple
if it's Oracle: select * from all_tab_columns; MySql: Describe tablename; or Explain tablename;
Gauge charts are awful. Anyone using them or suggesting them needs to be ignored. Just use a bullet chart to convey the same information. 
That is just horrible advice. Procedural SQL extensions are much slower than traditional procedural languages. It's not the focus of the developers of the data engine. It's meant for trivial stuff only, which is why the syntax is horrible, debugging is hard, features are poor, and there isn't a single editor that approaches the ease of use and efficiency of industry leaders in editing. And no proper testing, and no proper version control. It's good to know it, but it's definitely not good to code in it too much.
Ty
Maybe do a left join
Look into using `ROW_NUMBER() OVER(Partition By JobNo Order By DateFinished DESC) AS RN` and then in an outer subquery you select `WHERE RN = 1`
WITH d AS ( SELECT JobNo, ROW\_NUMBER() OVER (PARTITION BY JobNo ORDER BY whateverYourTableIdIs) AS Rown FROM FROM APEX.dbo.BillingDet ) DELETE FROM d WHERE Rown &gt; 1
Your solution is what I use. For someone new to SQL, I gave him an easy way to do it using a CTE.
I stole it from my colleague, sorry!
Since you're new to SQL, let me ask you a question Would you rather delete this data, or just not fetch it when you ask? Generally you don't delete from SQL very often at all
won't this delete every row with a duplicate, rather than just the duplicates, leaving one copy?
I don‚Äôt want to delete it just exclude it
use left joins.
The row number function assigns each duplicate row a number, and the 'WHERE Rown &gt; 1' deletes everything except the first row. 
&gt;WITH d AS &gt; &gt;( &gt; &gt;SELECT JobNo, &gt; &gt;ROW\_NUMBER() OVER (PARTITION BY JobNo ORDER BY whateverYourTableIdIs) AS Rown &gt; &gt;FROM FROM APEX.dbo.BillingDet &gt; &gt;) &gt; &gt;DELETE FROM d WHERE Rown &gt; 1 Sorry for being such a noob, where would this fit in my current query
The original entry is rown = 1.
You're wanting to remove your dupes, right? 
Yes, not delete, just remove them
you could use http://docs.sqlite-sync.com/ to sync between the mobile db and the remote server db
Create a REST API for the registration / authentication https://stackoverflow.com/questions/27911244/new-user-registration-authentication-on-android-app-with-django-backend
Orrrrrrrr, I could continue to use what charts I want to convey information. I like them, they are simple and display an instant value/understanding to what‚Äôs going on in my department. Irrespective of what tableau, or other people think...I like them and I continue to use them. But now I have to make them by hand instead of natural integration on tableau 
 WITH d AS ( SELECT whateverYourTableIdIs, JobNo, ROW\_NUMBER() OVER (PARTITION BY JobNo ORDER BY whateverYourTableIdIs) AS Rown FROM APEX.dbo.BillingDet ) SELECT whateverYouWant FROM APEX.dbo.BillingDet b JOIN d on d.whateverYourTableIdIs = b.whateverYourTableIdIs WHERE Rown = 1
I really appreciate the help, thank you!
*A relevant comment in this thread was deleted. You can read it below.* ---- So your boss is asking you to write an RBAC-based front end reporting system from scratch that interacts with the Internet and also supports local protocols for LAN distribution? You just went from reporting analyst to full stack developer. [[Continued...]](https://www.resavr.com/comment/sql-project-dont-where-11673377) ---- *^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&amp;subject=remove&amp;message=11673377)*
select distinct a.id from table1 a where a.id not in (select id from table2) and a.id not in (select id from table3) and a.id in (select id from table4)
Hi, I'm not great at SQL but use bits in my day to day job. What does the "OVER(Partition By" part of the code actually do? I've seen both in code that I've nicked and played around with til it does what I want, but never actually understood that part. Thanks
I am not exactly sure, good question.
&gt; If this were organized as a table with each ClassID and SchoolID in separate rows, that‚Äôd be a simple left join, but I‚Äôm not sure how to approach this problem. i would try a simple three-table inner join SELECT c.class_name , s.school_name FROM school_classes AS sc INNER JOIN schools AS s ON s.id = sc.school_id INNER JOIN classes AS c ON c.id = sc.class_id WHERE sc.school_id IN ( 1,2,3 ) AND sc.class_id IN ( 4,5,6 ) 
You should look into how dbo.BillingDept relates to the other tables you want and find a better join condition as it looks like right now you are basically doing a crossapply which probably isn't what you want. Also, look into using the join keywords as it makes your joins easier for yourself and other people to read. The source.of your problem would also be a little more clear... You aren't actually using any relationships for your Billing Dept data so it's basically just filtering on your LIKE statement then being applied to every row.
It‚Äôs called a window function. The Over clause defines the window it works on. Partition by indicates to reset the window every time the defined values change, followed by the Order By clause which specifies the order the window is processed in.
Never thought I'd see a gender identification question on something database related, but okay. I guess some progressive companies already have a data structure with genders slightly more advanced than my company's single BIT column, haha.
Very interesting; at the moment SQL Server is *by far* the most commonly used database.
I‚Äôm interested in knowing how the Apache Helicopter learned to code. I didn‚Äôt think SkyNet would assimilate this fast. 
To me this makes sense as Ozar mostly caters his website to the SQL Server folks so those will be the majority of survey takers. 
https://www.brentozar.com/archive/2017/12/female-dbas-make-less-money/amp/
Are there any results from previous years?
https://www.brentozar.com/archive/2018/01/2018-data-professionals-salary-survey-results/ If you do some googling, there's some interesting blog posts on the results of these surveys. 
Thanks! I've only been SQLing in a major way at my job for about a year. This will help me understand how much I'm worth.
`(SELECT min(salary) FROM employees as e2 WHERE e2.department = e1.department) FROM employees as e1 )` Table alias **e1** is visible only within this subquery. This is like with variable scope in programming language functions and so on.
I had look this up. So, e1 has only limited scope - it is available only within the subquery Q1? Is this correct? In order to access that subquery (SELECE max(...etc...) outside the subquery Q1, I must give it an alias? 
Yes. But that column you used outside Q1 could actually be prefixed with Q1 and it would work, since you are SELECTing department in Q1 subquery and Q1 is visible in WHERE.
You are right. I changed e1 to Q1 in the outer query and it worked. I still don't understand why giving it an alias allows it to work. The alias gets created in subquery Q1 but is available outside in the outer query as well? What about e2? Am I redefining e2 whenever I use it outside of subquery Q1 and is that why it does not need to be renamed? 
 I still don't understand why giving it an alias allows it to work. The alias gets created in subquery Q1 but is available outside in the outer query as well? Q1 is what you're selecting from in your main query, and using aliases there is really just naming the columns of the Q1 dataset, allowing you to use those aliased column names in your main query. What about e2? Am I redefining e2 whenever I use it outside of subquery Q1 and is that why it does not need to be renamed? Yes, and that is why you can also use it in both subqueries in Q1 too; e2 is your alias of the employees table in the subqueries of e1, and e1 is your alias of the employees table in Q1 
As others have pointed you need a self-join. Assumming the table is called `family` the following will give you the results you want: ```sql SELECT f1.child, f2.child FROM family f1 INNER JOIN family f2 ON f1.parent = f2.parent ```
Any reason not to use FILTER or CTEs? WITH range_salary as ( SELECT department , max(salary) as max_salary , min(salary) as min_salary from employees group by department ) SELECT department , first_name , last_name , salary , case when salary = max_salary then 'Highest' when salary = min_salary then 'Lowest' end as salary_type from employees inner join range_salary using (department); 
I just finished my first course and I'm reviewing the problems that I found difficult with only the knowledge that was taught up to that point in the course. I'm trying to make sure I understand the fundamentals. 
BTW, unrelated to this post, do you know of a resource that shows the appropriate "grammar" conventions for SQL. For example, in your example above, your commas start on a new line and your parentheses start on the line above. 
They don't convey information that's why they aren't included in any credible visualization tool nor will they ever be included. https://stats.stackexchange.com/questions/30476/if-gauge-charts-are-bad-why-do-cars-have-gauges
They are called style guides. There are many out there. Use whatever works for you. I picked the commas in front from somewhere and just stuck with it. It works for me because I can comment out lines quickly and I don't forget commas.
On a site where the owner/author is a SQL Server DBA, not very surprising. In the real world, Oracle is still the most used. 
my mistake, sorry
my mistake, sorry
Yeah I didn't realize that when I mentioned it --- that makes more sense!!
upvote for "leading comma convention"
Haha true. I did a lot more than 5 values in the actual query but it was more just trying to figure out how to do the equivalent of generate_series in Postgres. 
I wish companies would make offers instead of waiting for you to propose a rate. I believe many issues is just how men vs women are wired and their self-worth for income.
Brent didn't santiize his inputs! &amp;#x200B;
I agree. I have a team member who doesn‚Äôt value herself very highly even though we do basically the same thing. Not just Salary-wise, but when responding to requests, making decisions, or explaining concepts that we architect together. People look to me because I‚Äôm the male and I just turn around and defer to her (in-person and in emails). We have been working on her confidence for about a year now and I think we‚Äôre finally getting to a point where she has enough to represent/make decisions but I definitely have to work on getting her a higher salary. She‚Äôs just staring her DBA track and I want to make sure she doesn‚Äôt start on the low end of the scale (salary-wise).
No worries. Like any language, there's lots of learning involved. 
 select store_id ,count(purch_id) as "purchase_count" ,avg(item_count) as "avg_items" from (select hd.store_id, hd.purch_id, count(*) as "item_count" from Purchases hd inner join Purchase_Items dtl on hd.purch_id = dtl.purch_id group by hd.store_id, hd.purch_id) nest_query group by store_id order by store_id; Maybe something like this?? 
can i offer you some advice? fire your audio guy that video is useless because the audio sucks also, making us stare for an extended time at a slide does not make for a very compelling video -- perhaps you might consider powerpoint instead
Thanks. We will take care of that part. 
Could you post the table layout?
Appreciate the effort, but would have rather read it
Thanks. Here's the blog post : [https://sqlultra.com/data-warehousing-overview/](https://sqlultra.com/data-warehousing-overview/)
http://lmgtfy.com/?q=Oracle+sql+interview+questions
Also, never use acronyms without explaining what they mean, unless you are certain the audience knows them.
How well versed do you feel you are in PL/SQL?
I know group by, order by, inner join, outer join, left/right join, most aggregate functions, update, alter, create, truncate, partition by, over, foreign keys, primary keys, constraints. I sometimes struggle with knowing when to use what functions but I am practicing. It‚Äôs kind of hard to get complex or real world sql query problems to really understand how to use certain queries to their max potential
I will more careful from next time. I have just started making videos after working in SQL Data Warehousing For 10 Years. Please read my blog &gt;[www.sqlultra.com](https://www.sqlultra.com)
As someone who interviews for this position, definitely know the differences between the joins and be able to explain them. I can't count the number of interviews where someone *pretended* to know the difference between the joins but I could tell they were bullshitting. Try to learn the basics of PL/SQL and creating stored procedures. That's normally what I look for in interviewees. It shows that you know enough, and you'll be a helpful asset in product automation, eliminating manual labor. No one expects you to know all the PL/SQL functions, even strong DBAs/Database Analysts don't know all of them by heart usually. That's where *knowing how to Google* comes in hard. It shows that you're willing to find an answer if you don't know off the top of your head. 
Thanks man, I‚Äôll definitely look into that. I‚Äôve been really focused on normalization, memorizing functions, views, and indexing. I‚Äôm just looking for a junior position and definitely up to continue learning. I‚Äôm also looking to get the oracle 1z0-071 certification (even though many people have told me that certification doesn‚Äôt mean much). You have any more advice or fields I should be focusing on? Thanks again 
Normalization is a great thing to understand, it helps you build *not* shit. If you're Junior, chances are someone will throw you a bone at least with the hopes you'll learn. In the real world you'll learn more than any class can teach you. I can't speak to certification. I don't know one guy that actually knows what he's talking about that has a certification. They're a nice plus, but you'll still be inquired on your skills. Do you want to be an Oracle DBA?
I was looking more towards being an oracle sql developer. I‚Äôm finishing up two online classes I took for understanding sql. My biggest fear is going to a job site and not knowing what to do, being under-qualified, and not prepared. I‚Äôd much rather over prepare than under
Ahhh, nice field, same thing I do. I can tell you that hardly happens in the real world. It's more common for someone to be like "Oh, you don't know how to do that? Let me help you understand it." Sincerely, Someone that's also taught PL/T-SQL to several beginners. 
What does this video explain that the other 500 intro to data warehousing videos dont?
Haha thank you. That has honestly cleared up a lot of anxiety I had for the real world environment. And do you still teach oracle sql? I will look into what you told me then start applying to jobs. Thank you 
Yes, still teach it to new individuals. PM me if you have any questions. 
It great so see people making content. Please take the comments as constructive. Thanks.
Nailed it my dude. Agree 100%
Thanks, bro. 
Know very common Oracle errors. Know what deadlock is and what you'd do if you were troubleshooting a production database that was experiencing deadlock errors.
Know what *all* the deadlocks are. There can be different kinds and they're all unique. Examples: table locking, record locking, transaction locking, schema locking (less known), index locking. 
Not a very serious question, but I like to ask "what is the difference between a left join and a left outer join"
Absolutely! Thanks for commenting. I just started sharing my 11+ years of data and data warehouse experience.
It's an introductory video. More to come.
Aren‚Äôt these the same? 
Yeah
They are. The candidate often chuckles once it clicks and we joke about it. 
is ok i solved it. thanks anyway!
Without knowing what the role requirements are, it's not really possible to be particularly helpful here. What I suggest is that you look at the person/role spec and go through it line by line. Work out what you know, what you don't know, and what you kinda know. Take the stuff you don't know, and learn what it is. Even if you don't know how it works or anything else, make sure you know what it is and what it is for. Take the stuff you do know, and brush up on it. Make sure you have it firmly embedded. The the stuff you kinda know, and get better at it. Read more about it, learn more about it. It's a bit vague but TBH, so is your post!
It‚Äôs a trick question ;) 
Deadlock is not the same as simple lock.
Fixed. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sysadmin] [SQL Crash Course](https://www.reddit.com/r/sysadmin/comments/a94nz1/sql_crash_course/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I‚Äôm in a similar boat - I got up to speed thru w3 schools and taking on an individual contributor role on some small projects. I find (and ymmv) that I need to be more familiar with the DB than with the ins and outs of SQL; my value comes in helping with the 10,000‚Äô view of solving a problem instead of the nitty gritty. I.e. - being able to suggest that my team can do what they need to by grabbing X field from Y table, which they can relate to the primary data source by doing A/B/C is more useful than being able to coach on how to use window functions. I also find (and this may be peculiar to my role) that fluency in the business processes that result in the underlying data is super critical. 
What exactly are you trying to learn? Writing SQL? OR maintaining SQL servers and managing a DBA type team?
I'd like to start with a 10k ft view of SQL overall. Basic concepts for best practices. I'll pick up details of our implementation as I take over. I'll not be expected to write queries or maintain a system but should be able to manage and strategize for future projects. Any best tips for managing a dba team would be amazing!
I'm working with SQL for just over 15 years. Check out my course on Udemy, which I created this summer for "business and data analysis with SQL". I go discuss fundamentals of running large scale data projects, working with SQL and project management. Discount link below. If you have any questions, I setup a group on Facebook. www.facebook.com/whatdata https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHATDATA10
https://www.brentozar.com/ His team knows sql very well. join their webinars, or someone on your staff. use the tools they provide to get an understanding or audit your servers and setups. Download the tool kit and read through the documents. https://www.brentozar.com/first-aid/ 
It‚Äôs not totally related, but I wrote [this](https://www.reddit.com/r/SQL/comments/6c7g24/comment/dhsiab3?st=JQ2HET63&amp;sh=9767a5cf) a while back and I still believe it‚Äôs a good intro. I worked as a SQL Dev I, II, and team lead for a few years before becoming a DBA with a focus on the more coding heavy aspect of it (automation and optimization over server configs). Speaking from basically every side of it, the best advice I have is to learn who you can trust to give honest opinions and believe them if they‚Äôre fighting for something, like preventing bad design. The database often gets blamed for things when something breaks, but more often it‚Äôs how other people‚Äôs code is interacting with legacy systems. Learn enough to know when you should be fighting back against other grumpy managers for your people and you‚Äôll be a solid manager.
SQL PASS is the go-to in my mind. Join your local chapter or a virtual chapter. Find the basic webinars or events and enjoy.
it's worthwhile to understand the relational model. If you're managing the team building your data model, making decisions based on sound normalization principles will help your company in the long run. https://en.wikipedia.org/wiki/Relational_model 
Khan Academy has a good crash course. 
One time as a Developer DBA at a company: (They are OOB now). &amp;#x200B; we had massive income growth and a bonus program. But the payout for the bonus stopped at 12%. &amp;#x200B; i asked Hey, what happens if we exceed the numbers tied to the 12% payout? does it continue to go up? "No" i forgot the exact amount, but we smashed the program two years in a row. I literally missed out on approximately 40,000 USD due to that policy. I think we were supposed to hit 12mil and hit 76million. something crazy. HR had their HR database on-premise servers. I tweaked a scheduled report that was delivered to all employees with everyone's salary instead of just HR. It was an "accident" due to the fact SSRS was accessible by "Domain Admin" and anyone in IT was domain Admin. it was a real CF. and... that's when it was reported that all the female developers, BAs and programmers were making 15 to 20% more than males. All of HR were woman. awkward..... &amp;#x200B; &amp;#x200B;
alt + f1
Good one. For anyone who doesn‚Äôt want to google - shows the table structure (data types, keys etc)
Ctrl-R to make the results list appear and disappear so I can see all my code. 
Shift + F5 will execute the sql statement the cursor is on. Super nice not having to highlight the entire query to run it. I have also mapped Ctrl+k+O to be ‚Äúchange connection‚Äù which is handy
Wow one guy has 1,050 years working with a database and only makes 22,000. What a shame.
alt + shift to write on multiple lines. You can use up and down arrows while holding altshift, or place cursor at bottom, scroll to top, hold alt shift, press cursor at top; voila. You'll see a vertical line appear on the left.
There's a button on the top right for a web browser. Ta-da! Reddit at work without switching windows.
Not a shortcut, but I frequently use object explorer details to edit multiple jobs in one script. I didn‚Äôt used to do that because I never knew that existed. It‚Äôs neat. 
Not sure if this is a shortcut but I like to set different environments to different colors so that I can easily see if I‚Äôm in dev vs uat vs prod by the color of the bar at the bottom
7/7 stars
More general than an "SSMS" trick, but if I'm doing a bunch of ad-hoc statements while researching an issue or whatever, I like to put a ``RETURN`` at the very top of window. That way if I accidentally hit F5 without having anything selected, nothing happens.
The dark theme https://www.sqlshack.com/setting-up-the-dark-theme-in-sql-server-management-studio/ 
This is definitely one of the best tips anyone ever gave me for SQL. This works in Visual Studio too for all those python kids out there.
I put the change connection icon on the menu bar, then set it to text only with the G underlined, so I just have to press Alt-G to get the change connection dialog 
Other than CTRL+R that was already mentioned, I end up using the shortcuts for commenting/uncommenting all the time. Comment lines: CTRL+K, CTRL+C Uncomment lines: CTRL+K, CTRL+U
Column select, Column copy, Column paste. Best tricks. Period. Full stop.
If you press ctrl + c on a line, it will copy it. You don't have to have anything highlighted
Shortcut: Ctrl + shift + r It refreshes the intellisense* cache so any newly added/removed fields, tables, stored procedures etc will show up in your intellisense and inline errors (*is it still intellisense in SSMS? Whatever the equivalent is called)
I first began learning SQL because an application I was supporting used it. It proved to be very useful/portable, as I‚Äôve moved through 3 different industries since then. 
Great! Thanks for sharing!
Ctrl + Shift + f5 will run the whole query to the point where the cursor is
Cause it‚Äôs fucking awesome 
Kinda by accident. Didn't know it at all, learned it in a previous job position, really enjoyed it, never looked back. 
Game. Changer.
I like it, similar to putting 1=1 after the where or the ON so every statement after starts with AND and you can easily comment them out
I found the idea of linking data and then querying or finding patterns in it quite amazing.
Awesome!
Necessity, I knew our reporting guy was just lazy and the (relatively simple) stuff we were requesting wasn't "impossible"
I learned that knowing SQL paid $30+/hr starting in a different department at my company. I was making $17/hr as a call center supervisor and the job (and pay) sucked. Did some SQLBolt and got a position. 
Good one!
Workplace was using Access to interact with SQL Server. I thought that was stupid and install SSMS. 
Because answering phone calls on tech support was getting old and we had a need for someone with SQL knowledge. Picked up a few books and decided to try to make a career of it. Has worked out pretty decently so far. No more tech support and my salary is a lot better than it used to be.
Only all the text stays black against the content menus making them difficult to read. 
that is a good idea! 
Part of learning web dev but to get a job doing data 
Pretty obscure one, but gets a surprising amount of use from me: Press CTRL+W to highlight the word your cursor is on. Some other ones (works in Visual Studio as well) Without highlighting anything: Press CTRL+C to copy the current line. Press CTRL+V to paste that line above the cursor. Press CTRL+X to delete the current line and copy it to the clipboard. CTRL+V works with CTRL+X in the same way as it does CTRL+C. CTRL+E to execute the current query (works on the highlighted query). I find this easier than F5, as I don't have to move from the home row. 
Not a direct tip to SSMS, but I use Sublime Text to format and mass fill queries. Then I copy it back to SSMS to run. The tricks in Sublime makes it much easier to produce queries that otherwise would take forever to type in SSMS.
Made it easier to troubleshoot issues with the application when on helpdesk. After a while of sending queries to the 2nd line and working up some trust and a baseline level of competency, I was given access. Now my job also includes writing stored procedures for custom exports, and it's magical.
Would you prefer Administration over development?
I don't think I would. Development in SQL is kind of creative and it's a rabbit hole that never seems to stop going down. If I'm stuck on an issue, I'll ask the tech director who has been with the product since inception and he'll blow my mind with some new method. Administration probably has its perks, but it seems a little pedestrian to me in comparison.
I started with Administration, then sql developer, now data warehouse developer! 
You can color code your windows by connection so that Dev/ACC etc are easily identified.
So I could stop wiping butts for $8.50 an hour.
Well you can change that individually too. üòÅ
This is incredibly useful, particularly on a large schema
I have a blank template with a comment block able to be filled in with control shift m, and then begin tran and rollback tran that auto pre-fills on every new/blank query window. 
Check out SQL Operations Studio (now azure data something or other) - has all the VSCode style text editor features with built in SQL Connectivity. 
Because the website needed a way to communicate with the database. 
ALT + Mouse Drag for column edit mode. Especially useful for setting , and ' for long IN (...) lists copy pasted from Excel.
How tho
If ctrl-r gets you gold here, the standards are lower than I thought.
I'm a nurse who got really sick of our EHR's reporting interface and wanted to really explore the richness of our data.
This! You can also hold alt and highlight with your mouse for the same effect.
Connection properties in the ‚ÄúRegistered Servers‚Äù tab.
Not necessarily part of the Original SAMS, but redgate SQL search is a plug-in that helps you search object names and code easily (like syscomments, but enhanced). Very useful for finding a weird column used, or checking if a table is used anywhere before dropping it. https://www.red-gate.com/products/sql-development/sql-search/index
Nice. I'll check it out. Thanks!
I‚Äôm the admin of the data inventory system for our restaurant group. I started to learn SQL so I could ask the software company for custom reporting and know how to ask questions correcrly
I use this literally every day. Incredibly fast and saves me hours of searching esp with funky DBs 
How?
Yes! Agreed!
When I block select around people in my office they think I am doing sorcery. Especially when I am writing prefix to table names all at once.
It's still intellisense
Well it's a huge time saver if you didn't know about it before. No joke I used ssms for years before I learned about that trick. I used to resize the results list over and over again to see more code then see more results. If I had saw this post years ago then I would have given it a gold as well. 
Was told that I needed to learn it. Pro/con of being the IT guy for your family's business.
GIS and it seems more fun than other languages I've played with. Alas, here I'm self teaching.
Then drop some knowledge and collect some gold
Even i thought everyone would know this. Pretty basic stuff. I can only imagine the pain of resizing that results pane everytime. 
I've gone through enough shit to know that I'll never take this risk. For me it will always be select text &gt; verify &gt; f5 Data is precious. 
Had to learn MYSQL when I got into web dev and running forums in the early 2000s. Got into TSQL and then PL/SQL at work years later for call center reporting and development.
Anytime you‚Äôre running update queries with prod rights you‚Äôre going to have a bad time. Standardized deployment procedures can help with that. 
I got tired of 1-2 week turn times on data requests from my boss. Also wanted to make myself more valuable for my current organization and future. 
Great!!
That way, you don't have to pay an external IT guy 
https://solutioncenter.apexsql.com/how-to-set-different-connection-colors-in-ssms/
 SELECT s.student_no , s.student_name FROM students AS s LEFT OUTER JOIN student_enrollment AS se ON se.student_no = s.student_no AND se.course_no = 'CS180' WHERE se.student_no IS NULL 
oh, shit, wait... you said a hint, not a solution... ah well... merry christmas
There is a function called EXIST (you can use NOT I'm front kg an exist as well). Just google that some and you should be able to figure that out. 
My hint is that you can in fact achieve this with a WHERE statement AND you should know that. :)
Are you using sqlplus?
When I run your query without the last line, I get the following: https://i.imgur.com/YcYdG25.png Does the AND function like a condition here? And the last line, you used se.student_no IS NULL. Why not se.course_no IS NULL? 
&gt; When I run your query without the last line, I get the following: that's perfect by leaving that off, you got all students, and whether or not they have taken CS180 that's how LEFT OUTER JOIN works adding `WHERE ... IS NULL` filters out the ones where it's not null, leaving you with students 2, 4, 6, 7, 8... &gt; Does the AND function like a condition here? the AND joins two conditions together, both have to be true &gt; And the last line, you used se.student_no IS NULL. Why not se.course_no IS NULL? i prefer to use the same column as the column in the ON clause join condition 
Pl/sql developer
I don‚Äôt think you can use a bind variable starting with a number.
I'll take a whack at this over the next few days. Happy Holidays!
This feels a but like code golf. Except SQL only. Super fun! This should be a frequent thing!
&gt;the AND joins two conditions together, both have to be true The two conditions being? 1. s.student_no = se.student_no 2. se.course_no = 'CS180' 
Does anyone know of a dictionary in a easily digestable dataset form? Like CSV or JSON? How am I supposed to know what a 'word' is? 
Free free to create a sample dictionary, but I found [this](https://raw.githubusercontent.com/dwyl/english-words/master/words_dictionary.json) which might fit your needs.
This is a good thing to think about. I am learning Python but use SQL more often at work. Is developing this in SQL likely to be much more difficult than Python? My SQLs at work rend to be SELECT queries, rather than algorithmic stuff.
I would suspect it is easier to do in Python honestly.
1 - Insert every word in the dictionary stored in a table and index it. If I remember right this is a couple hundred thousand words for English so not too crazy. 2 - Create a procedure that creates every possibly n-character word where n = @param. Assuming only lowercase letters this would be 26^n. Exponential growth, n = 8 would be 208,827,064,576. 3 - Create a procedure that takes a VARCHAR param and creates a VARCHAR for both every permutation of both the characters of the param and the unique characters of the param (like "inp" would have "inp","ipn","nip","npi","pin","pni"). You would then have to store these in their own table with the original value of the parm and both permutations in their own columns with indexes. The number of permutations grows really fast (factorial on the number of characters in the word) so this table will be monstrous. There's a couple different algorithms for generating permutations, recursive and iterative. They all have exponential running time obviously though. There would also obviously be edge cases where the original word and the permutation are the same but those are 1 out of n! so not worth worrying about really. 4 - Feed every word created by step 2 into step 3. There would be duplicates but that can be handled easily by the keys/indexing. 5 - Inner joining the permutations on the dictionary table would give you a link to all the random n-character words which are actual dictionary words. You'd have to group by the original random word value to get a count for the most. All of this of course assumes we don't care about case and are just using upper/lowercase for everything. 
Good luck :) Give me some framework if you want that sweet gold. Also I think you're over thinking the problem from the approach I currently have in mind. Not saying it wouldn't work, but if I did want to use `@param = 10` then your example would be very unwieldy. If I wanted to use a value of `20` it would be disastrous. 
I think we're stuck with exponential/factorial growth for this one, since you want to look at every possibility to see if it's a word. Every possibility would be n! possibilities for n-character words no matter what you do. That's what makes problems like this impractical in the general case. Even if you don't have n! space you still have n! time just to run through them all.
I think you're stuck thinking about this from the wrong approach. Look at your first point in the list. You made a very good observation there before you took a wrong turn at Albuquerque. 
If you are on an UNIX-like platform, the following generates insert statements from all of the words in /usr/share/dict/words. cat /usr/share/dict/words | awk '{ print "INSERT INTO WORDS VALUES('\''" $1"'\'');" }' generates: INSERT INTO WORDS VALUES('Zythia'); INSERT INTO WORDS VALUES('zythum'); INSERT INTO WORDS VALUES('Zyzomys'); INSERT INTO WORDS VALUES('Zyzzogeton'); For me on MacOS.
üòé
I'll give it a try in Python. The first issue will be iterating through all the possible alpha strings of length n. Counting in base 26 might be the way.
Use &amp;&amp;1 (ampersand ampersand). 
Just tried and it worked in both Toad and SQL Developer.
OP I just tried your exact query (replacing table/column names). and it worked fine. What data type are you putting your where clause on?
inp allows for the same words as pni. So for candidates we don't need to consider any variant where Letter(N+1) &lt; Letter(N) (i.e. AB we consider, BA we don't). So when we create our dictionary we should also already sort all letters in the word. Words: ID | Word | Len | Letters --|----|---|------- 1 | Bee | 3 | bee 2 | Bag | 3 | abg 3 | Bar | 3 | abr 4 | Be | 2 | be If we create that information for every word in our list the problem nearly solves itself. Syntax might be wrong but this is the idea: SELECT TOP(1) relevantWords.Letters FROM Words INNER JOIN ( SELECT DISTINCT w.Letters FROM Words w WHERE Len &lt;= X ) relevantWords ON Words.Letters = substring(relevantWords,1,Words.Len) GROUP BY relevantWords.Letters ORDER BY COUNT(relevantWords.Letters) DESC For the example table this should output "bee" since it matches the sorted letters for "Be" and "Bee". This spares us the storage to check combinations that are irrelevant since they don't have any fitting words.
That‚Äôs fair. I couldn‚Äôt remember if you could or not, but that was the only thing I could find that might have been an issue.
I agree with you which is why I had to double check. I think it‚Äôs object names that can‚Äôt start with numbers. Columns, tables, etc.
Clever observation. I was starting to wander down this road mentally myself in terms of creating some "statistics" on the dictionary table.
I hadn't thought to sort the letters. I was thinking of taking a dictionary and running distributions on the words for combinations of words with multiple occurrences of any given letter. So for example if `n = 6` you would isolate the words that are 6, 5, 4, 3, 2, and 1 letters long and then look at the distribution across that set to come up with the set of permutations --&gt; but to your point, we consider AB, but not BA. Your approach is much more elegant. The code to generation the ordered lettering could get interesting, but I think you solved this much more simply than I was prepared to think was possible. I'm going to give you Platinum so you can give others a few Golds. Really nice work.
I'm running my query in the SQL Window. I replaced :1 with &amp;1: and that seems to work. I get a prompt to name the value of my specified variablename. I'm still trying to wrap my head around bind variables so maybe I'm not truly understanding how they work.
That‚Äôs weird. I literally tried your query and it went without a hitch. Glad you‚Äôre up and running though.
I think F7 is the shortcut.
Thanks m8
Hey I've been curious about how this turned out for you. Did you get it solved?
correct those conditions define which `se` row the LEFT OUTER JOIN is trying to find when it doesn't find one, the `se` columns in the joined row are set to NULL and `WHERE ... IS NULL` specifies which joined rows you want to keep
Thanks you very much for this challenge, your kind words and the gold and platinum :) I had fully expected to wake up to someone having found a critical flaw in my thinking so this absolutely made my 2nd christmas day! I'll try to spread the joy!
&amp;&amp; isn‚Äôt the same thing as a bind variable. It‚Äôs substitution done at the client side, so the server still sees different string literals in the query, and will have to parse each query. 
Love this idea!
Yes, please
Great point. But I‚Äôm just curious, why would you want to alter an index anyway? 
Adding or removing column(s) from PK or correcting fragmentation comes to mind. 
Thank you. 
Thanks. 
You‚Äôre correct. Didn‚Äôt think of that.
Few years ago, I was asked a question about altering index. Someone just wanted to add a column in an index and wrote an alter statement. I though it would be better to share with the SQL community so that no one tries to write this statement again! 
Correct!
What books helped with your journey? Thanks!
I'm not sure your approach will totally work, I'm playing with things now, but nevertheless it's a nice big step in a direction.
DB2 disagrees :P https://www.ibm.com/support/knowledgecenter/en/SSEPEK_11.0.0/admin/src/tpc/db2z_addindexcolumn.html
Got a job doing tech support, then they had me start doing stuff in Access, which morphed into SQL Server work. Discovered I was decent at writing queries and 10 years later, I'm making good money doing it. I still hate it, tho...
Thanks for the tips! Would love to see these every day. 
I like the idea of your post, but keep in mind this subreddit is to discuss SQL - not the SQL Server DBMS. Not every DBMS is the same.
Thanks.
The SQL Tips and Tricks will for SQL not SQL RDBMS
Thanks! I look forward to the upcoming knowledge. 
Thanks.
You can do Alter Index Rebuild though. 
He asked why you'd want to alter an index. I said fragmentation. 
Is there a large disadvantage to putting an index on every column? I assume so and that is why it‚Äôs only on some. 
Yet, the tip you just posted isn't relevant to SQL.
Yet, the first tip you posted isn't relevant to SQL.
Happy about this üéä
Performance and storage space. When you update a table, the associated indexes also get updated. Also, indexes take up space in addition to the space the table already takes up.
Do you know of a more suitable subreddit you can suggest OP does this on? 
Space issue. Also, not always indexes increase performance. They help to decrease in some cases. 
Sure. I'm just saying that on both SQL server and db2 op is incorrect. You CAN alter to rebuild. 
Make sure you read the article and ask me questions. 
I'm confused. You replied to my comment on another comment asking why someone would want to alter an index. Yes OP said you can't, but the comment was not related. 
sorry to disappoint you, but this is another very low quality article according to you, the star schema is "not a great model of analytical purposes since it‚Äôs needs normalized data model" utter bullshit
Could it be that some entries are pulling multiple valid "EventDate"s falling between '2018-10-01' AND '2018-10-31' corresponding to an "AttendingAS=1" AND "Attended = 1" duping the entry? &amp;#x200B; It would be a distinct record if the record pulled multiple dates with matching ClientID's. 
What exactly are you trying to return in your query? Without knowing anything about your schema, I would assume that clients might attend more than 1 event and if that is the case, you wouldn‚Äôt want to filter out duplicates. It raises the question about why you have duplicates in the first place also. Just a blind guess, but you could try a group by clause instead of using distinct. It may not be what you are after though. Hope that helps.
Thanks.
building a dynamic union Cartesian. could have removed @Word loop and replaced with another stringsplit csv. could have removed last comma from exec sql table hack... could have done a lot of things. &amp;#x200B; DECLARE @Len BIGINT = 3 DECLARE @Count BIGINT DECLARE @Count\_Sub BIGINT &amp;#x200B; DECLARE @Word NVARCHAR(100) = 'input' &amp;#x200B; &amp;#x200B; &amp;#x200B; DECLARE @csv\_Word\_NoUserDefinedTableType NVARCHAR(MAX) = '' &amp;#x200B; DECLARE @Script\_Select NVARCHAR(MAX) DECLARE @Script\_From NVARCHAR(MAX) DECLARE @Script\_Main NVARCHAR(MAX) = '' DECLARE @Script\_execute NVARCHAR(MAX) = '' DECLARE @Script\_Parameters NVARCHAR(MAX) = '@Word nvarchar(100), @csv\_Word\_NoUserDefinedTableType NVARCHAR(MAX) OUTPUT' &amp;#x200B; &amp;#x200B; SELECT @count = 1 &amp;#x200B; WHILE @Count&lt;= @Len BEGIN \-- max length of word returned loop SELECT @Script\_Select = 'SELECT ' SELECT @Script\_From = 'FROM ' SELECT @Count\_sub = 1 WHILE @Count\_Sub &lt;= @Count BEGIN \-- dynamic cartesian with union loop SELECT @Script\_Select = @Script\_Select \+ REPLACE('t&lt;n&gt;.word','&lt;n&gt;',@Count\_Sub) + '+' SELECT @Script\_From = @Script\_From \+ REPLACE('@tblMain t&lt;n&gt;','&lt;n&gt;',@Count\_Sub) + ',' SELECT @Count\_Sub = @Count\_Sub + 1 END &amp;#x200B; SELECT @Count = @Count + 1 \-- remove trailing , and +... add alias and union SELECT @Script\_From = LEFT(@Script\_From,LEN(@Script\_From)-1) SELECT @Script\_Select = LEFT(@Script\_select,LEN(@Script\_select)-1)+ ' val' SELECT @Script\_Main = @Script\_Main + ' '+ @Script\_Select + ' ' + @Script\_From + ' ' + 'Union' END &amp;#x200B; SELECT @Script\_execute = ' \-- parse word into single characters DECLARE @tblMain TABLE ( word NVARCHAR(100) ) &amp;#x200B; WHILE @Word &lt;&gt; '''' BEGIN INSERT INTO @tblMain (word) SELECT LEFT(@word,1) SELECT @word = RIGHT(@word, LEN(@word)-1) END &amp;#x200B; SELECT @csv\_Word\_NoUserDefinedTableType = @csv\_Word\_NoUserDefinedTableType + val + '','' FROM (' +LEFT(@Script\_Main,LEN(@Script\_Main)-5)+') x' &amp;#x200B; EXEC sys.sp\_executesql @Script\_execute , @Script\_Parameters , @Word = @Word , @csv\_Word\_NoUserDefinedTableType = @csv\_Word\_NoUserDefinedTableType OUTPUT \-- compare this query with dictionary. SELECT \* FROM STRING\_SPLIT(@csv\_Word\_NoUserDefinedTableType,',')
No time to check this out tonight, but I will. Did you have fun?
&gt; Can anyone point out what I'm doing wrong? misunderstanding how DISTINCT works -- it applies to **all columns in the SELECT clause** a client attends more than one event, and you will get more than one result for that client
len 6, 5460 returns, less than a second. len 7, 21845 returns, less than a second. len 8,... stopped at 60 seconds.
All 10 minutes were fun. Abstraction is always fun.
Is this a joke? I can't believe this low-effort post with zero depth or explanation has attracted so many upvotes. Here's a tip: Tables can be created with the CREATE TABLE statement, wow. Go advertise elsewhere, SQLULTRA DOT COM.
I don't understand the confusion? Your point lists the reasons one might want to alter an index. I said one of them is actually doable. If you want to go on at lengths about this feel free - I covered my point 2 comments back.
That is a question for OP right? If I tell you not to drive 50 above the speed limit, it is not on me to find you a racing track.
Only 10? Impressive? I thought about it for a few hours in my head and am not fully solved, but I'd say it took a good 30 minutes of coding to get to.
Thanks for your tip. üòÅ
You forget where we are: this is the SQL subreddit, where homework questions and low-tier garbage get double-figure upvotes.
been doing this for 7 years. do or do not and all that. stuff just comes naturally.
Frankly I think there are lots of very good questions and answers here (along with some bad, but that's how all forums / subreddits are), but this post takes the cake. 
I've been coding for over 20, it's still impressive.
i have a youtube channel that goes over this abstracted development. im starting from 001 again so i can make a visual guide and a SQL dev video instead of trying to combine the two. if i ever finish it.. and you make it to the end, anything in sql will become trivial at best. 
I don't generally think in terms of code, I think in terms of concepts and then I bash code until it does what I want. 
every concept fits into 1 of 4 table shapes (a shape usually consists of 4-5 tables and each table has identical columns). If you knew the design concepts, the code would make itself. You wouldnt have to bash anything. A well designed system has you thinking about what the data management system requires to operate, not what you require of the data that the data management system organizes. The DBMS doesnt care about what you need. So by designing a data management system, something that just manages data... all business reqs/needs are met. Keep those challenges coming and ill take all your monies =).
I'm intrigued, but generally my job is totally opposite to what you describe. We are given a set of data, we generally cannot make any decisions regarding how it is structured that do not take months of discussions, and then we are tasked with extracting valuable pieces of information from that data in order to write algorithms to do predictive models, analytics, machine learning, etc. Once we figure out how to do something valuable, then we talk about how to improve the structure of the data, and start talking about what the data *should* look like. My own role operates in the middle of that.
Anti-patterns are a b--- to get out of.
Original Comment: "Great point. But I‚Äôm just curious, why would you want to alter an index anyway?". I said: To correct fragmentation. You said: You can use alter index rebuild though. I have no idea why you said "though" as a reply to my comment as if I wasn't already answering the original comment as to why they might want to use alter on their index. Maybe you meant to reply to the original comment and not me? 
r/sqlserver 
Thanks for the response. You're spot on here when you state that clients might attend more than one event which for my programs is not a bad thing. I am, however, trying to create two different charts in Tableu. One that tracks unduplicated attendees and one that counts all attendees. I'm having trouble sorting distinct client IDs to give me the two data sets. Does this make sense? 
code can... but SSIS benefits from an ecosystem of support - the catalog with configurations for variable/parameter mappings as well as logging (both storage of logs as well as reports to view), and support within agent jobs... how much exactly do you want to recreate in code?
Thank you. I'm very new to working with SQL (less than a month) and I'm struggling. How would I write a query that selects only distinct clientID? 
Unfortunately no. I've inherited a database that was very poorly designed. AttendingAS = 1 is a field that denotes if the attendee is a client or a mentor and Attended = 1 is how the database captures if the client attended an event. 
Exactly. I‚Äôm primarily using Spark SQL. Indexing isn‚Äôt even a thing in that world as far as I know. That being said this sub has been a great resource for me to learn. 
Use the existing query but remove all columns from the select with exception of clientID.
If all the data is the same per output line then the duplication is happening from another field and as a result of the join. You could just select distinct * from (your query) then to get a single row if that is the case. Otherwise perhaps tweak the columns you are outputting if you aren‚Äôt focused on keeping any duplicates.
Thank you! Do I need to create a new table with the results from this query and join it with another table to filter by date? I have to be able to filter by a set month too. 
If the table is having frequent inserts/updates/deletes, a large number of indexes can slow down performance. If the table receives infrequent updates and is mainly used for queries, there is no issue with having tons of indexes.
This is one of my biggest things when I read other people's code! Next one should be that no one should not be allowed to use an Union instead of Union all UNLESS they can explain exactly why they need a Union. I have found so many issues from people using an Union, drives me crazy. 
I was reading a query full of unions and that was written by a data scientist!
Oh man I would tear that apart. It drives me insane to read. I found that a hosting company used an Union in a production application simply from the results. It joined two similar objects together but only broke once and a blue moon (had like 7 fields that could match). I told them the exact issue of using an Union and they did not believe me. Found it was fixed "magically".
Preach!
No, it was directed at the person who said this was the wrong subreddit. It was a legit question, not an arsey rhetorical one. You're right, the onus is on OP to find a more suitable subreddit but if anyone else knew of one, why hold back that information? 
IMO any decent SQL engine would optimize the first query as join, so there shouldn't be any noticeable performance difference. Also these queries are not equivalent: the second query will produce a different result set if some authors do not have any posts, which is possible.
In that case then r/SQLserver would probably be a better fit.
Schedule a cron job to run a (Python) script in which you'll execute the query.
Such a blanket statement is wrong. Oracle allows indexes to be altered. 
you can easily filter by date in the WHERE clause but if you want only one result per client, and there are multiple events attended for that client, you will want to use something like `MAX(tblEventAttendance.EventDate)` in the SELECT clause of your GROUP BY
This rewrite is not valid in general. 
I don't know python, nor do I know what a cron job is. Do you have any resources for that? My idea was to set up a cte, but I can't get it to work. 
Mods, look at banning u/sqlultra please? Looks like spam.
What's your problem? 
Unions have their place though. I've used them in production for getting multiple very specific records from a table programmatically
I say leave it, he's not linking to some shitty site full of ads.
Oh I agree but the average person just uses them without knowing the consequences. It needs to be used for a specific reason. 
I wouldn‚Äôt classify that as spam. Helpful, though? Not so much until now. 
I guess a more suitable sub would depend on which DBMS OP wants to talk about. I like OP's idea - I would like to see daily tips for SQL. I find it handy to save quick things like that for future reference.
Put this at the top of your query, before SELECT DECLARE @theDate Date = DateAdd(m, DateDiff(m, 0, GetDate()), 0) That will give you the first day of the current month. Then replace all of your '1/1/2017's with @theDate To schedule it, you can do that through SSMS: https://itknowledgeexchange.techtarget.com/itanswers/how-to-schedule-sql-query-to-run-in-microsoft-sql-server/ To schedule the job, that really depends on what rights you have and where you want the data to land. Talk to your Tableau folks about how to get it setup.They probably already have a data refresh system setup, they'll just need to add your job to it.
You have posted two tips, and neither are entirely accurate and relevant to SQL. Unfortunately that's quickly earning you a bad rep. I like that you want to contribute, but most of us don't want misleading information floating around, particularly for less experienced users who may not be able to spot the flawed logic. Some DBMS optimizers will actually perform a join when a subquery is used. There are also times when a subquery is more practical. Perhaps your posts would be better phrased as "points for discussion" instead of "here is a tip - do this".
In Oracle, you could try RTRIM, LTRIM and TRIM. You'd select the same column a few times. 
I am the Tableau person at my work. I am trying to run this code through Tableau because I have no access to our SQL server. I only have read only access. I'm trying to find a work around since I can't actually create a table.
The trouble here is that you're performing an ETL process but trying to skip the *T*ransform part. I often find that native SQL statements aren't the best suited in these kinds of cases and instead standardising the data into a consistent format through the use of other tools (such as AWK and SED if Linux based) is the only way to consistently produce valid data.
On windows, though I'd be very glad to learn these tools. Getting data from pdf is a pain. 
1. Replace 2 spaces with 1 space 2. Use substring and locate functions with the single space as the delimiter 3. Profit! 
Regular expressions www.regexr.com
Read the replies to your post. Accuracy, and topic quality criticism. If you're a contributor with advice that's a big help. If you're just posting fluff, which may be wrong, to push your site then it's not.
You can still create a bad enough time for yourself in a non-production environment with the right (or wrong?) query. :)
Hah, fair enough.
The intentionally do this on PDFs to make it as difficult as possible for you to parse the data. So it's not really easily doable. You aren't intended to scrape data from a PDF (they want you to pay for that). You could check the LEN first but that is not really a guarantee. Does your data also contain spaces?
How manual of a process do you want or don't want? You can always use Alt + Highlight for vertical blocks and there is Excel. 
Less Manual work as possible..
Easiest approach I see here is to include a step in anything but sql. I'd use notepad++ to just replace every " +?" with a single space. Then you can create your insert statement with a little more search+replace magic, like replacing line endings with '),(' and spaces with ',' etc.
I was looking for someone to say something like Notepad++. I use that pretty often, and while it's not great for automation or anything like that, if I have a big chunk of data that I need to clean up a little bit, and it's too messy for typical functions, notepad++ is great. The replace function (F8) and a combination of regular expressions to find and edit certain parts of data is great. [Here's](https://regexr.com/)a great spot for more about regex stuff. 
After curiosity got the best of me I dug a little deeper and assuming that this is what you are looking for: https://labor.alabama.gov/wc/FeeSchedules/2018ClinicalLabFeeSchedule(revision1).pdf Then this is the sanitised data, with tab separation: https://pastebin.com/wGuj1J1E The process: Copy paste into np++, manually remove headers, replace "(.+?)\r\n(.+?)\r\n(.+?)\r\n" with "\1\t\2\t\3\r\n", check regular expressions.
I had a feeling something was getting lost in the chained 'AND' statements so I isolated the ClientIDs from tblEventAttendance with a CTE. I also changed the inner join to a standard right side join. With CTE1 AS ( Select tblEventAttendance.ClientID, tblEventAttendance.Attended, tblEventAttendance.EventDate, tblEventAttendance.AttendingAS From tblEventAttendance Where EventDate Between '2018-10-01' AND '2018-10-31') &amp;#x200B; SELECT DISTINCT tblClient.ClientID, CTE1.ClientID, tblClient.FirstName, tblClient.LastName, CTE1.Attended, CTE1.EventDate, CTE1.AttendingAs FROM tblClient JOIN CTE1 ON tblClient.ClientID = CTE1.ClientID WHERE Attended = 1 AND AttendingAS = 1 &amp;#x200B; &amp;#x200B; &amp;#x200B;
Yup, I don't know why they don't provide csv files.
For that amount of pages you should also create a regex just to remove the headers. It should be something like "[0-9]+?\r\nPage [0-9]+?\r\nTYPE \r\n2018\r\nTYPE \r\n2018\r\nSERVICE\r\nCODE\r\nWC FEE\r\nSERVICE\r\nCODE\r\nWC FEE\r\n" replaced with nothing.
800 records? Just use excel...
Thank you, this is exceptionally helpful. Can I ask why you did created a right join? 
I've been getting some weird data when I open them in Excel. Such as 0001U√¢‚Ç¨¬ê8 instead of 0001U‚Äê8 
Working on repricing on all states, and Alabama's the hardest since they provide pdfs instead of csv and txt on all of their pricing rules. I have more problems extracting data from pdf to csv. Only if there was a magical way to do it.
hey, this is MySQL, yeah? SUBSTRING_INDEX will do that almost trivially 
I'm confused.... where do your Tableau dashboards read their data? From the SQL Server, or somewhere else?
How would this differ from substring that I'm using?
Mind sending me the pdf if its not confidential? I dig those types of puzzles
We were in a tech-centric area where anybody who could do IT full time could walk off and work for a dozen other companies for well over what my dad could offer. He snagged my brother for a summer before my brother got an Oracle internship. It's so bad now his best candidate for a shipping and delivery position didn't have a driver's license. 
I'm sorry I ended up not actually using the right side join in that statement. I was going to use it but I couldn't be sure of any NULL values. If the code above doesn't work try "Right Outer join" instead. 
It's publicly available on Alabama https://labor.alabama.gov/wc/2018FeeSchedules.aspx I would love to learn better process in ETL.
There are no Null values within the date range, but there are 1,000s of NULL values in the table itself. The person who designed this database dumped almost all of the data into three large tables instead of using relationships to connect data. I am trying to pull data for now while I work on redesigning the database itself. I've worked with access a lot in the past but never needed to know sql, but this database is so bad I've been forced to learn sql. 
Would be great having a source to learn all the reasons 
well, for starters, [SUBSTRING_INDEX](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_substring-index) is a different function altogether from SUBSTRING CREATE TABLE test_substring_index ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT , str VARCHAR(199) ); INSERT INTO test_substring_index (str) VALUES ('one two three four five six') ,('this is a short string') ,('this sentance has three errrors') ,('this very long string has seven words') ; SELECT id, SUBSTRING_INDEX(str,' ',1) AS substr1 , SUBSTRING_INDEX(SUBSTRING_INDEX(str,' ',2),' ',-1) AS substr2 , SUBSTRING_INDEX(SUBSTRING_INDEX(str,' ',3),' ',-1) AS substr3 , SUBSTRING_INDEX(SUBSTRING_INDEX(str,' ',4),' ',-1) AS substr4 , SUBSTRING_INDEX(SUBSTRING_INDEX(str,' ',5),' ',-1) AS substr5 , SUBSTRING_INDEX(SUBSTRING_INDEX(str,' ',6),' ',-1) AS substr6 FROM test_substring_index ORDER BY id; id substr1 substr2 substr3 substr4 substr5 substr6 1 one two three four five six 2 this is a short string string 3 this sentance has three errrors errrors 4 this very long string has seven 
I was working in a call center and wanted a change and also better pay. A reporting analyst job opened up that I interviewed for and got. First started with simply executing other people's queries to retrieve data. Thought I'd learn how to write my own queries so I can retrieve data on my own. Still learning everyday. Now I'm interested in learning SQL tuning because we have some queries that take 45 minutes to run which slows down my reporting. I now feel underpaid as I'm the only one on my reporting team who can write SQL queries....
What if there are not 1, but many spaces between words? Like "one _two __three ______four" Imagine the underlines are spaces. 
Seems to be that lots of people are pulling that data right now :) I'm getting timeouts trying to open the link.
A Union should be used when you want to combine duplicate rows between the different queries. This could be two different tables that have similar data ( like two different types of nvoices for example). It is best practice to use Union All to start and if the problem is best solved by using a Union, then you need an explanation of why it is best. This explanation should include an example of what happens when both an Union and an Union All is used and why the Union is the correct result for the query.
I connect to our Microsoft SQL server and I write my SQL query in the "initial SQL" of Tableau, so the data is pulled directly from our SQL Server.
OK. You don't want a CTE for this, that's not what they're for. CTE's are meant to be very temporary things that exist for the life of the query/stored procedure. You need a table created. You can point your dashboard at that table. Then, once a month, a job runs that repopulates the table. Once the data query is written, it will take about 5 minutes for the DBA's to set that job up. You need to talk to your SQL Server folks and tell them what you need and why. If they say "No", go to your boss and say "Hey boss, you know that big pile of money we're spending on Tableau? Our SQL Server team won't help me maximize that investment".
apply `REPLACE(str,' ',' ') a few times, i guess i imagine if too many "what ifs" like that are introduced, you might want to actually write some external code first
I would suggest using trim and then split_part
Haha, yeah, that was going to be my last resort. I just assumed this would be something I could set up myself. I am relatively new to SQL, so I just assumed that I am missing something. I got a cte to run, but it's not running properly. It is showing the same number of long stayers as thise with cases closed in a year, which seems unlikely. The cte I tried to set up is: WITH cte AS (Select Distinct case_id, person_id,aa_start_date, county_id, involvement_start_date,Involvement_End_Date, CONVERT(VARCHAR(7) ,CAST(Involvement_End_Date AS DATE), 120) ASReportDate, CAST(DATENAME(mm,Involvement_End_Date) ASVARCHAR(10)) + ' ' +CAST(DATEPART(yyyy,Involvement_End_Date) ASVARCHAR(10)) AS RDate From Involvement_Periods_All), cte_A AS (SELECT RDate AS CasesAsOf, Region_Id,County_Name, count(person_id) AS LongStayers,SUM(ClosedinYear) AS ClosedinOneYear FROM ( SELECT Region_Id, County_Name, case_id, person_id,ReportDate, RDate, start_date, involvement_end_date,Caseage, TimetoClose, CASE WHENTimetoClose&lt;=365 THEN 1 ELSE 0 END ASClosedinYear FROM ( SELECT Region_Id, County_Name, case_id, person_id,ReportDate, RDate, start_date, involvement_end_date,datediff(d, start_date, RDate) AS Caseage, datediff(d,RDate, involvement_end_date) AS TimetoClose FROM ( SELECT DISTINCT co.Region_Id, co.County_Name,ip.case_id, ip.person_id, ReportDate, RDate, CASE WHEN ip.AA_start_date is null THENip.involvement_start_date ELSE ip.AA_start_date ENDAS start_date, ip.Involvement_end_date FROM cte AS ip INNER JOIN Counties_New co ON ip.county_id =co.Magik_County WHERE (ip.involvement_end_date&gt;=RDate OR ip.involvement_end_date is null))s GROUP BY Region_Id, County_Name, case_id,person_id, ReportDate, RDate, start_date,Involvement_end_date) r WHERE datediff(d, start_date, RDate) &gt;= 730 GROUP BY Region_id, County_Name, case_id,person_id, ReportDate, RDate, Caseage, TimetoClose,start_date, Involvement_end_date)p GROUP BY Region_Id, County_Name, ReportDate,RDate) Select * FROM cte_A 
Can I see what output your code was producing? Unless you are no longer in need of assistance. I'm not the best by any means but I have experience navigating inherited funky databases. 
Unfortunately, I can't put the output online given that the client data is protected by HIPPA. I can say that it was producing all clients who attended all events during the time frame in the code. Instead, I want to count unique attendance. 
One of our SQL developers looked over my original code and whipped this up for me. He got it working in about 15 minutes... CREATE TABLE #templong (CasesAsOf date,Region_Id INT, LongStayers INT, ClosedinOneYearINT); DECLARE @RC int declare @MonthIndx int; declare @StartMonth date='01-01-2017' declare @WorkMonth date select @MonthIndx=(select 0); while @MonthIndx &lt; 12 begin; set @WorkMonth = DATEADD(MONTH,DATEDIFF(MONTH, 0, @StartMonth) + @MonthIndx,0); DECLARE @BeginDateOfReportMonthCurWkDtmdatetime=DATEADD(dd, -DAY(DATEADD(mm, -1,@WorkMonth)) + 1, DATEADD(mm, 0, @WorkMonth)) DECLARE @BeginDateOfReportMonthCurText varchar(10)=CONVERT(VARCHAR(10),@BeginDateOfReportMonthCurWkDtm,101) insert into #templong(CasesAsOf,Region_Id,LongStayers,ClosedinOneYear) SELECT @BeginDateOfReportMonthCurText ASCasesAsOf, Region_Id, count(person_id) ASLongStayers, SUM(ClosedinYear) AS ClosedinOneYear FROM ( SELECT Region_Id, County_Name, case_id, person_id,start_date, involvement_end_date, Caseage,TimetoClose, CASE WHEN TimetoClose&lt;=365 THEN 1ELSE 0 END AS ClosedinYear FROM ( SELECT Region_Id, County_Name, case_id, person_id,start_date, involvement_end_date, datediff(d,start_date, @BeginDateOfReportMonthCurText) ASCaseage, datediff(d,@BeginDateOfReportMonthCurText,involvement_end_date) AS TimetoClose FROM ( SELECT DISTINCT co.Region_Id, co.County_Name,ip.case_id, ip.person_id, CASE WHEN ip.AA_start_date is null THENip.involvement_start_date ELSE ip.AA_start_date ENDAS start_date, ip.Involvement_end_date FROM Involvement_Periods_All ip INNER JOIN Counties_New co ON ip.county_id =co.Magik_County WHERE ip.involvement_type_id = 3 AND (ip.involvement_end_date&gt;=@BeginDateOfReportMonthCurText OR ip.involvement_end_date is null) )s GROUP BY Region_Id, County_Name, case_id,person_id, start_date, Involvement_end_date ) r WHERE datediff(d, start_date,@BeginDateOfReportMonthCurText) &gt;= 730 )p GROUP BY Region_Id ORDER BY Region_Id select @MonthIndx=(select @MonthIndx+1); end; select * from #templong 
Try this With CTE1 AS ( Select Distinct tblEventAttendance.ClientID From tblEventAttendance Where EventDate Between '2018-10-01' AND '2018-10-31') SELECT DISTINCT CTE1.ClientID, tblClient.FirstName, tblClient.LastName, tblEventAttendance.Attended, CTE1.EventDate, tblEventAttendance.AttendingAs FROM tblClient JOIN CTE1 ON tblClient.ClientID = CTE1.ClientID Join tblEventAttendance ON tblEventAttendance.clientID = CTE1.ClientID WHERE Attended = 1 AND AttendingAS = 1
How's ETL on those PDF to text or csv?
Didnt load last time i tried. Not at the computer anymore
this is such a fun question /u/notasqlstar thanks for posting! I filled in the gaps between the sorted letter table /u/cybertier proposed and the dictionary /u/undeuxtroiskid suggested. [Here's a gist of the steps](https://gist.github.com/dankleiman/2e00b967697e22836a76ad0e75023a9e) I used to create a table in postgres with each dictionary word, its length, and the sorted letters. Right off the bat you can find interesting stuff, like the most common set of sorted letters: words=# select letters, count(1) from words group by 1 order by 2 desc limit 10; letters | count ---------+------- agnor | 11 aelps | 10 aeerst | 10 eerst | 9 acert | 9 antu | 9 aacinr | 9 aemrt | 9 aelpt | 8 aeginst | 8 (10 rows) Where the top ones include words like: words=# select * from words where letters = 'agnor'; word | len | letters -------+-----+--------- Orang | 5 | agnor Ronga | 5 | agnor angor | 5 | agnor argon | 5 | agnor goran | 5 | agnor grano | 5 | agnor groan | 5 | agnor nagor | 5 | agnor orang | 5 | agnor organ | 5 | agnor rogan | 5 | agnor (11 rows) Or: words=# select * from words where letters = 'aeerst'; word | len | letters --------+-----+--------- Easter | 6 | aeerst Eastre | 6 | aeerst Teresa | 6 | aeerst asteer | 6 | aeerst easter | 6 | aeerst reseat | 6 | aeerst saeter | 6 | aeerst seater | 6 | aeerst staree | 6 | aeerst teaser | 6 | aeerst (10 rows) I'm still a little unsure of the overall approach though, even though I was also blown away by the initial simplicity of organizing the data like this. I think the substring join ends up costing us a lot of time and complexity when you start to find e.g the 5,4,3,2,and 1 letter solutions when n=6. Need to poke at that more to see how it works!
Sadly I've had to deal with some production fires since I posted this and have not had a chance to start it. Nice work.
Yes they are, but they're also fun. My current role deals with data from clients across the globe, so each of them have their own architects and I have to often find a way to normalize and rationalize different systems, that track different things, and find commonalities which would allow for meaningful analytics. Often this involves creative methods which are not ideal, but that are more ideal than asking 15 major companies to restructure their data in the same way.
Oh man. Good luck putting out the fires!
I don't know how long it's been around in SSMS, but I just recently noticed that Ctrl+Shift+C in the results pane does a "copy with headers".
No offense, but I completely agree. However I would comment that a better "Tip/Trick" would be to strive to completely eliminate sub-queries entirely, and replace them with #tables. While not always true, you can and will quickly over complicate a set of sub-queries until the optimizer just blows up and the query performs like shit. You can take complex queries like that and reduce them to a series of joins along #tables, and it will massively improve performance. I don't, per se, see a problem with this being irrelevant.
No offense taken! But to say that subqueries should never be used is too broad of a statement for me to agree with, with no knowledge of what the query is doing, how the data is structured, and what the DBMS is. I do agree that they are often overused and are often better replaced with JOINs.
He didn't say never, he said as a tip/trick, use a join where possible. I don't see the harm in that statement. It's generally good advice.
No - it doesn't say "when possible". 
It doesn't say never, it says "this is a tip/trick" which in simple English means, "when possible." 
Inaccurate and poorly written. " star schema is very normalized which is good for reporting. " What? 
Compared to a transactional schema, the star is normalized. Why don't you read the full line.
Shout out to Sublime Text as well. 
I believe you should be able to get this done with a CTE. You can use the recursion to loop through until the "connectedToVertex" is null. Here is a link to a simple example (I like the SQL Authority guy): [SQL Authority Example on CTE](https://blog.sqlauthority.com/2008/07/28/sql-server-simple-example-of-recursive-cte/) &amp;#x200B; &amp;#x200B;
unfortunatly only Microsofts SQL Server supports CTEs imho
Mysql supports it as well: https://dev.mysql.com/doc/refman/8.0/en/with.html
Downvoted, no wonder people don't offer help here. Apparently there's RTRIM in MySQL as well. [https://dev.mysql.com/doc/refman/5.7/en/string-functions.html](https://dev.mysql.com/doc/refman/5.7/en/string-functions.html)
Sybase supports it as well
Only barely started it, but I got *[SQL Antipatterns](https://www.amazon.com/SQL-Antipatterns-Programming-Pragmatic-Programmers/dp/1934356557)* for Christmas. Every single chapter has the same format. * Objective * Antipattern (the wrong way to do it) * How to recognize the antipattern * Legitimate uses of antipattern * Solution (right way to do it) It's not a book to teach you the basics. Instead, it's how to avoid common mistakes. 
In SSIS you can have a OleDB query in the Excel Source to select what columns you need from it. That query can be stored in a variable which can be build as an expression, using the proj params. Then you could have an OleDB command Component to handle the update based on that source columns. Look into SSIS variables using expressions, oledb queries for Excel, the Excel source and the OleDB Command component.
The excel sheet name from which you take the data should be always the same. You can work around that with Script Component (using c# to get the sheet name in a variable used in the Excel Source query variable), but it's a lil' but trickier.
A lot of that is easy to replicate. SSIS logs are terrible, the report features as well. The ui gets in the way more than it helps. SSIS is fine for ppl who struggle with code, but it's just honestly way past due for an update. 
Check out the head first series. They do a great job of going through material in a fun and informative way. 
PowerShell would work for this as well. There's an \`Import-Excel\` module which doesn't require Excel / COM Objects. &amp;#x200B; Or convert the Excel doc to a .CSV and you can use TSQL to import it into a temp table.
It sounds like database design and complex SQL queries are what you are interested in. Have you looked into different types of jobs out there that use SQL? &amp;#x200B; Some ideas that come to mind would be Database Engineer, DBA, SQL Developer, ETL, and Developer, Business Intelligence (Analyst, developer, reporter, etc.). Most titles are usually ambiguous, I think I can create a firm outline of what each title should entail but most companies / HR cannot. Always look to the description and during the interview process to find out what they really need and find out what the position really is. &amp;#x200B; As far as SQL goes, SQL for Mere Mortals has been a highly recommended book and so is the Data Warehouse Toolkit by Kimball. You could probably throw Database Design for Mere Mortals into the mix too. If you had a particular flavor of SQL you wanted to dive into, people could make other suggestions based on that. I really only know SQL Server books at the moment, but I know others can chime in with other suggestions.
If this is a one-time thing that you'll never have to do again, I'd suggest skipping the use of other tools or adding layers of new code that you'll need to test. Just add a third column to your spreadsheet that concatenates the first two into an update statement. Something like "update mytab set folderdesc = " column2 " where folderid = " column1 Drag that down the whole spreadsheet and you've got a whole mess of update statements. Copy paste them into your SQL window and run them. It's dirty, but it's quick and it's also super easy at a glance to see if it's going to work, so the risk of bugs is low. 
It seems you are trying to replace existing relationshipsetids with new values without changing the sets. If this is the case, you can create a temp table with old relationshipsetids and newid() column, then insert all the newid_column values in your relathionshipset table, then update the relationship table using your temp table as a mapping old-&gt;new value of relationshipsetid.
Any way to script the insert on the groups? That's where I'm stuck
This. I‚Äôve used PowerShell for bulk upload in the past &amp; found it pretty neat. The Regex functionality also comes in handy.
You shouldn't need the "and year(order_date)&lt;&gt; 2018" part as you are already limiting the results to ones that satisfy the first where clause. The query might perform better if you changed your where clause to a BETWEEN. Something like this : where order_date BETWEEN '2017-01-01 00:00:00' and '2017-12-31 23:59:59'
A simpler way would be with cte.. With yr_cte as( Select cus_id from Tab Where Year(Business_year)=2017 ) Select * From tab Left join yr_cte on Tab. Cus_id =yr_cte. Cus_id Where yr_cte. Cus_id is null And Year(Business_year) in (2016,2018) This would ensure that all client appear only both years.. On mobile so can't be of much help.. 
Your overall design here is pretty questionable. You're breaking [first normal form](https://en.wikipedia.org/wiki/First_normal_form). That's a very fundamental set of rules in the relational model, and you'll find that SQL will not work well with data that ignores first normal form. PostgreSQL does support [array data types](https://www.postgresql.org/docs/current/arrays.html), and while those are now ANSI SQL, many systems that will try to access the database may not support them (reporting software is what I'm thinking of primarily). And it may bring it's own problems with it. I'd use one of the solutions with the external table. As to your actual question, any way you do your parsing it's probably going to be a very manual process. There's simply no way to guarantee that you won't have a problem like "Thomas Jefferson" and "Jefferson Sarah" both existing as valid names. If you've got a manageable number of sales persons, like 50 or so, this will probably be easy enough to just do one at a time. It's tedious, but poor designs create a lot of tedious work. That's part of why they're poor. Ideally, you also shouldn't be storing names in the system twice. You should use the primary key from the SalesPeople table (probably an integer ID) so that when your sales people get married and change their name you don't have a whole new nightmare of data integrity to deal with. 
Questionable is putting it quite.. nicely. 
Thanks. How would `order_date BETWEEN '2017-01-01 00:00:00' and '2017-12-31 23:59:59'` show the clients who returned back in 2018?
Thanks for your input! Absolutely agree with your points... I felt that no good solution exists and that the design needs to be addressed. Cheers!
In all reality, while not the best solution, you could make this work if whatever is writing to the table did something as simple as adding a delineation when writing out multiple salespeople. You can then use an array operator quite easily. If you're going that route though, you're best off doing a complete fix and adding a normalizing table as well.
 with max_order_date as ( select distinct client_id , max (order_date) as 'max_order_date' from mytable ) --derives latest order by client select mt.client_id , mt.client_name , mod.max_order_date as 'latest_order' from mytable as mt inner join max_order_date as mod on mt.client_id = mod.client_id and mt.order_date = mod_order_date where year (max_order_date) != year (getdate ()) --latest order not within this year
Wait, do I need to setup an odbc connection first?
Your domain account probably doesn't have the access rights or the VM itself is not domain joined, log into it inside the VM using the admin account then check the account security settings or create an actual user with permissions instead of using domain account.
Are you trying to connect via SSMS(SQL Server Management Studio)? If so, you have two options for login, domain user (assuming they've been setup with the neccessary permissions during installation) and so (default SQL system administrator) also setup during install. Next question, are you trying to login remotely or on the machine that's running the SQL Server Engine (check services)?
Ah sorry.. missed the fact you need r to link back in that way. Func_you's post further down looks like a good route to take.
So, the SQL VM is domain joined. The account I am trying to test from, is also the same domain account. I also created an SQL user with database rights inside of the Managment studio using my Domain account.
I can connect to SQL Management studio fine, using any of the accounts I created in SQL Security (domain accounts added). The issue, is that when trying to test the connection when not logged into the VM I can't access. So for example, If I use my domain account to sign into the VM that hosts the SQL server, I can connect fine. However, on my office computer (domain joined) I get those errors trying to test the connection. Both are same accounts, one is just not hosting the SQL server.
More things that you should check: - is your database server running - is SQL browser running - in SQL browser - are protocols like tcp and named pipes enabled
I didn't realize you could be in SQL management studio without it running.. this is a sad question, but how do I verify it's running? 
I'm not as familiar with the Express edition, but see if a SQL Server Configuration Manager app was installed. That's one place where you can verify the SQL Server service (and SQL Browser service) are running (you could also just check all services to see if they're running). The configuration manager is also where you'd check if TCP or named pipes are enabled. 
Why make statements like this if you don't actually know. 
SQL server is running, but the Browser isn't and starting it is grayed out. Both TCI and named pipes are enabled.
So a UDL will work provided you have the SQL protocols enabled on the Server and the MSODBC installed. However, if you have SSMS installed on your workstation you've got what you need. SQL Configuration Manager is what you need to check. Ensure that TCP/IP is enabled on the SQL Instance on your VM. Check to make sure that the assigned Ports are not being blocked. There are several. There is a table of ports posted here https://docs.microsoft.com/en-us/sql/sql-server/install/configure-the-windows-firewall-to-allow-sql-server-access?view=sql-server-2017 Check to make sure remote access is enabled on the SQL Server Instance because it is not always by default. Honestly, in a case like this use your Google-Fu. Tom's of great articles and helpers out there when searching with error messages.
&gt; MSODBC installed I have installed, but I don't think I have it configured. (is there configuration?). I will make sure the ports aren't being blocked, but TCP is enabled and I made sure Remote Access is working. 
UDL is a super basic test. Try it from the server too just to check on the settings. You can also try just doing a UNC path to the server as another basic test.
The connection works on the VM that is hosting the SQL server, well.. that's progress I guess! I can ping and remote into the server with RDP, maybe the ports are blocked. Hmm
Since this is an Instance you need to ensure that the SQL Browser Service is running and is set to automatic start. No browser service will hamper your ability to connect remotely because no port is being made available. You can also set the TCP port number if you do not wish to have one do dynamically assigned at startup. You're getting close and learning lots for next time. One other thing occurred to me. It sounded like you intended to upgrade from SQL Express to something else. As far as I know you can only move from Developer to Standard or Enterprise Editions with the appropriate licensing. Nothing wrong with Express as long as you don't need the SQL Agent.
No
So I think I figured this out... but I'm not sure if it's the right way to do this... what I've been able to do is run the following: &amp;#x200B; SELECT SUM(high_bid) AS high_bid_total, high_bid_team FROM players WHERE bid_days_left &lt;= 0 GROUP BY high_bid_team &amp;#x200B; With this query I create a temporary table. With this temporary table I then remove high\_bid\_total from user\_gold, then drop the temporary table. It works as intended.. I'm just not sure if it's the 'proper' method.
So running SSMS on the VM that is running SQL Server works? This is likely because it is able to connect via Shared Memory. Remote Network resources cannot connect via Shared Memory. You're running a named instance, which means when the SQL Instance starts it will pick a random high numbered TCP port. The SQL Browser instance runs on UDP 1434 and must be running to automatically connect to this random TCP port in use by the Instance. If the option to start the service is greyed out, that usually means the service is set to "Disabled". Set it to Manual or Automatic and start it. Also, make sure the VM's firewall is either off or has been configured properly. A final option - if you don't want to run SQL Browser, you can launch SQL Server Configuration Manager and configure the TCP settings to a port, and if you set it to the default port of 1433, then you will be able to connect without passing in a port or an instance name - just the server name. The path is "SQL Server Configuration Manager -&gt; SQL Server Network Configuration -&lt; Protocols for SQLEXPRESS -&gt; TCP/IP -&gt; Scroll down to IPALL and set the Dynamic ports to Blank and TCP Port to what you want it to use.
Seriously. I only started using SSMS six months ago or so and immediately I felt like I was peering at my code through a tiny window. I'll definitely be using this now. 
I can't figure out what you're doing, but I suggest creating a new table to track bids where each row contains a big on a player by a team. The idea being record bids as transactions, then use those transactions to update your tables.
Keep this post up for when I have this question later!
I discovered recently that if I tried a basic test with a udl file from a brand new Server 2016 server that I had TLS locked down to only TLS 1.2 support, the udl file test failed. Whatever ships with Server 2016 doesn‚Äôt seem to natively support TLS 1.2. I can‚Äôt recall if the error was the same as what you were getting though.
You can use a subquery instead of a temp table if you would like. Functionally it's going to work the same bit it's a bit cleaner than dropping and creating tables 
Yes. Why on earth do you need 22,000 tables for?
i'm going to guess there is one table per customer, or one table per location, or one table per something... and there are a lot of those somethings
Not bad. Bet it would be funny to see the relational model. Loads of foreign keys? How many indexes on the larger tables? 
Roughly 181k rows per table, I'd imagine an event log of some sort for 20k users over an extended period of time?
I work in healthcare. This is about as many as your average hospital EMR will have.
Did you open port 1433 in the windows firewall on the server?
Thank you for you answer! Those books look perfect to me. I think database design and complex SQL queries are indeed what I need ATM. I'm currently working as a quant and I believe we have to reconstruct our database at some time which is nothing but a chuck of data divided into two clunky tables. Our DBA will be busy doing something else (I'm working in a small startup) for a while so I wanted to have at least good backgrounds about SQL before working with him (there's no one understanding the importance of database design here...). Also I wanted to know how the very logic of relational database in general. BTW, our company is using SQL Server and MySQL ATM. I guess I'll start by having a look at the books you've recommended, possibly something about the relational database along the way :)
Average? Nah. Source: same sector
That wouldn't work in MS SQL though, missing a GROUP BY in the CTE. But if you're just looking to get a list of client ID's and their last order, excluding current year, much easier to do it with a HAVING and save the extra joining: SELECT mt.client_id , mt.client_name , max (order_date) as latest_order_date FROM mytable mt GROUP BY mt.client_id, mt.client_name HAVING year(max(order_date) ) = 2017 
I was at a Navision instance once and it created all base tables per organisation entity, of which they had about 60. After every takeover 1000 or so tables were created. It adds up pretty quickly that way. 4 billion rows I wouldn't find that large, but it depends on the table width of course. How large are your dbs?
what's EMR?
Electronic medical record
Without knowing anything else, yes. I'd be curious what the actual size is in Gb, how "wide" the most frequently used tables are, why there are so many tables (as opposed to multiple databases), what the schema structure looks like, what the normalization looks like...
22k tables... I'd call that bad database design.
[removed]
Yes - same here, Navision with numerous entities from company acquisitions. Why couldn‚Äôt the Nav devs have used a field inside the tables to represent the entity instead of an entirely extra set of tables per entity? It meant queries and reports all had to be repeated with a bunch of union statements.
Maybe a boxed solution database, where they have built tables for every every business component known to man, even if your organization doesn‚Äôt use them. I would doubt all 22k tables are utilized. 
Well fair enough, I've only worked with Epic and Meditech.
My mistake, hard to write code when you can't execute and see errors! I've amended the cte to have the group by to avoid confusion. I've never really utilised HAVING, something I'll look at.
Electronic Medical Record. They have all the clinical and billing data for the hospital. The ones I've worked with have several modules, each with their own data model and are sold as a packaged product
I'd say maybe 25-50 tables are actually utilized 
I'd say about 10-15 indexes on the larger tables
I'd be interested in this answer also. Is this a transactional database or an analytical data warehouse? 
Definitely not average. But EMRs tend to have table counts in the hundreds - thousands.
I work on a medical records database that has more. It's a very complex business with a lot of government regulation.
Epic probably takes cake for largest. Especially when considering Chronicles, Clarity, and Kaboodle
I would remain hesitant to call it "big data" tho. My impression from this past year's SQL Saturday is that big data has more to do with automated device data collection and can/will result in orders of magnitude more records over time.
I guess that's what happens when you take several hierarchical databases that have been added to for 30 years and translate then into a relational database.
Rows, no, but I can't see how you could have that many tables without a serious design fuckup.
4 billion rows is reasonably large though not at all unusual in many industries. That many tables is absurd though, and almost certainly points to either questionable design involving making copies of tables per customer or some packaged product that is trying to do too many things. Fwiw, I'm in healthcare as well and our records platform has about 20x that number of rows and about 300 or so tables, many of which are just used as references for various code values to ensure data integrity. 
Maybe you've not dealt with that kind/volume of data? 
They often updated? We do index usage audits every few months. Never had an audit that didn't find unused indexes. End of year/month/quarter jobs need to be looked out for though. 
They finally added that? Oh man I can't wait for my clients to upgrade to 2016! They are still on 2012 R2 but they should finally get to 2016 version by 2020....hopefully. Do you have an example?
Create or alter Procedure Create or alter view. The only thing is that it doesn‚Äôt stick. So the next time you script it out, it doesn‚Äôt hold the create or alter. Just the create, or the alter. They should add a ‚ÄúCreate or alter‚Äù when you right click.
That's nice and simple... Using cte's has kind of spoiled my original methodology of keeping code as simple as possible.. The extra joins do add up if data sets are large 
20k tables is not unusual for ERP systems.
[Applies to: Azure SQL Database, SQL Server (starting with SQL Server 2016 (13.x) SP1).](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-procedure-transact-sql?view=sql-server-2017) Good to see you are enjoying it. One of the things I missed when moving from Oracle Database (CREATE OR REPLACE) FYI; If on earlier versions I use IF NOT EXISTS CREATE instead of DROP - just personal preference and seems cleaner. IF OBJECT_ID('dbo.sp_break_stuff', 'P') IS NULL EXEC('CREATE PROCEDURE dbo.sp_break_stuff AS SET NOCOUNT ON') ALTER PROCEDURE dbo.sp_break_stuff AS BEGIN -- real code go here END 2016 SP1 or greater CREATE OR ALTER PROCEDURE dbo.sp_break_stuff AS BEGIN -- real code go here END
I'd love to know what kind of data necessitates 22k tables. It's unmanageable in my opinion and probably suggests that there's a problem in the design of the database.
I queried a column from all\_tables and found a 'last analyzed' column. Used a max aggregate function to find the most recent 'last analyzed' and it was a few months back. I'd say it's updated every quarter or so but that's my guess.
I focused on reporting early on so it was mostly T-SQL and Microsoft Press Step By Step books for me (SSRS and SSIS). Later I got into data warehouse stuff so Ralph Kimball‚Äôs Data Warehouse Toolkit and Chris Adamson‚Äôs Star Schema have proved invaluable. In regards to learning just SQL, you can learn a really good foundation for free on W3Schools.com.
I think you've nailed it on the head. The database has maybe 15-20 different 'owners' and each 'owner' has thousands of tables. 
I'm not sure but that is a great question. I will have to look into that as I'm now curious.
Just a heads up that it's not in the base 2016. You need to have service pack 1 at least. Found out the hard way...
`CREATE OR ALTER` is great, but that wasn‚Äôt as big of a win as `DROP TABLE IF EXISTS #tmp`. That is 1000x better than `IF OBJECT_ID('tempdb..#tmp') IS NOT NULL DROP TABLE #tmp`. Why it took until 2016, I‚Äôll never know. Now just waiting on RegEx support so I can throw away my custom CLR UDFs.
You could probably use the R integration in 2016 to help with REGEX. All the code would be written in SQL procedures/functions.
I wouldn't call it a VLDB just yet. Complex? Certainly. I guess the question is why are you asking?
I want to learn as much about the database I work with in my current roll. I want to know if I'm working with a large and complex database and also want to know if it is setup good.
1k actual tables, 1k staging tables. 1k archiving tables, 19k lookups relating to a severely overburdened front end. I'm really teasing. 
Every time I go to use it, it gives me a result that doesn't make sense (usually poor data structure is the underlying issue). So it's quicker to rewrite how I know rather than learning EXISTS. Maybe I'll make an effort for it in 2019
I tend to lean towards joins before these situations. 
Fucking hell. 
I use it all the time in my job, but many of the tables in our primary system have composite primary keys. You *have* to use a correlated EXISTS or NOT EXISTS subquery. In general, though, most people tend to prefer non-correlated IN subqueries. Most advanced RDBMSs have query planners that are smart enough to re-write IN subqueries to EXISTS where possible. Also, EXISTS is not best described as a function. It's variously classified as an operator (MS SQL, SQLite), a condition (Oracle), a predicate (DB2), an expression (PostgreSQL), or just a subquery (MySQL). Either way, it's a core element of SQL syntax. 
EXISTS/NOT EXISTS is slightly better than IN/NOT IN performance wise. In the case where the column can be NULL then IN/NOT IN can provide wrong results depending on what you‚Äôre expecting. This article seems to have a great explanation: https://stackoverflow.com/questions/173041/not-in-vs-not-exists 
it's just you
My team uses it all the time and we set it as a preferred approach long ago. It frequently outperforms other equivalent structures (though not always). However, more importantly for us, it's almost always a better indication of intent than something like a left outer Jon with a check for null would be. When you see a not exists in a where clause, you know immediately what the query is trying to check. Anecdotally, it does seem a little underutilized in this forums suggestions and given that my team typically needs to train new people in why that's our standard.
&gt; They are still on 2012 R2 There is no SQL Server 2012 R2. There's 2008 R2 and 2012. If they're still on 2008 R2, you need to get them migrated off it soon.
&gt; So the next time you script it out, it doesn‚Äôt hold the create or alter. Just the create, or the alter. That's SMO dropping the ball there, not SQL Server. 
The R "integration" shells out to an external process to run your R script. For something small like doing regex searches/matches, the overhead isn't worth it.
People are still using RTM?
My last gig provided developers with laptops with that installed. Thankfully we were able to update it. 
Derp that's what I meant. I wish I could but they are in control of it. It is a bank so they are regulated like crazy. 
I'm not sure how to find out the actual size in Gb. Can I write a query against the database to know? When you say "wide" do you mean how many columns in the table? The largest tables have maybe 15-20 columns
I'm not sure of which database you are using, but of all the ones I've used (SQL Server, PostgreSQL, MySQL, Oracle) there are queries you can run against the database itself that can tell you table sizes in rows, Gb, etc., plus lots of other diagnostic tools. Super helpful in figuring out where you can make adjustments.
Always someone to jump in with the performance argument. I read enough contradictory claims to figure the only way to know is to run some test queries. I will say `exists` / `not exists` are both perfectly valid idioms.
SQL For Mere Mortals and Database Design For Mere Mortals
I always forget these - I wish comment/i comment was on the right-Click menu when you highlight lines in the editor.
&gt; It is a bank so they are regulated like crazy. Their auditors will probably have a conniption when they find out the company's running on unsupported software in August :)
You need to group by all the fields you want in the select that are non-aggreates. 
You can't select a value that isnt wrapped by an agregate function(like sum) or part of the group by clause cause the column might have different value. In your example if you group by only employeeid the system wouldn't know what to do with the different rate values for example.
GROUP BY is essentially "collapsing" or "grouping" all the rows in a result set that have matching value. If I rewrite your requirements, I'd say, "I need to group all the data by employid first, then by rate, then sum up the total hours for each of those groups." Let's start simple: SELECT rate, sum(hours) as sum_hours FROM temp GROUP BY rate That will get you something like: RATE SUM_HOURS 9 59 10 41 ... So the query said, "Go to the temp table, GROUP all the rows by rate (e.g., 9s, 10s, 11s), then SUM all the hours for each one of those groups. But that's not quite what you want. You want the hours summed by rate _and_ employid. So you would need to adjust your GROUP BY to include both of those columns. You'd also add the new column to your select statement so you can see that employid in your results. I don't think you need to bother with the code column at all, it's not in any of your requirements. In your second SQL Statement, you included hourss in your grouping, which is _not_ what you want, as you're trying to SUM that column.
`GROUP BY` is making explicit what's implicit when we use human language. If I ask you "How many hours each of your employees work last week", I'm really asking something like "take all the time sheet records for all your employees, group them by the employee identifier (name, ID, whatever) and sum up the number of hours they worked". The query engine doesn't know how to make these assumptions about what to group by, so you need to be explicit by saying "I want to group by employee ID and employee name". In your case you're saying "take all these rows and throw them into unique buckets identified by the combination of rate, hourss, code and employeeid, then for each of those buckets give me the sum of their hours."
Hi sounds like a CTE would be good here - I had a quick look and this seems like a good example. &amp;#x200B; [https://www.youtube.com/watch?v=5KGjqnMss7g](https://www.youtube.com/watch?v=5KGjqnMss7g) &amp;#x200B; Cheers, &amp;#x200B; &amp;#x200B;
Yeah that sounds about right. SysComments/sysobjects doesn‚Äôt even store the actual Create or Alter statement when deployed (even from SQLCMD). The closest thing management studio does is allow you to script something as DROP AND CREATE. Not really a fan though as I have audits for dropping/creating objects that would go crazy from this.
group**ed** by
Remove hourss from the group by because you are performing the sum on that column. Group by tells it that if all of the the values for the specified columns are the same then perform the sum over the matching rows. Which is probably confusing and someone else could explain better but here's an example. This returns all of the hours for each employee: Select employeeid, sum(hours) as hours, count(*) as rows_summed From temp Group by employeeid This shows by employee for each unique rate Select employeeid, rate, sum(hours) as hours, count(*) as rows_summed From temp Group by employeeid, rate
If your select includes an aggregate function (ie: sum(), avg(), count(), min(), max(), etc...) you have to explicitly tell the query to group by all the non-aggregate columns in the select. SELECT RATE, SUM(HOURSS) as HOURS, CODE, EMPLOYID FROM temp GROUP BY EMPLOYID; In this example, you can't only group by the employee ID... what would happen if there is an employee ID record with more than one RATE? If you're only grouping on the employee ID and taking the sum() of hours, how do you handle cases where there are multiple RATEs or CODEs? SQL says you have to group by all the columns which are not aggregates... so your GROUP BY either needs to be: `GROUP BY EMPLOYID, RATE, CODE` or you can wrap RATE and CODE in aggregate functions to ensure that only a single value for each of those fields is returned. Something like this: SELECT max(RATE) as RATE ,sum(HOURSS) as HOURS ,max(CODE) as CODE ,EMPLOYID FROM temp GROUP BY EMPLOYID;
This solves so many problems for me and the departments I'm helping. I'll be coming back to your post several times in the next week. Can't thank you enough. One coworker is about in tears thinking she'll have to hand-key 1,311 items (in this example). This reduces it to 84 that I can import into the payroll engine. 
Well said, very well said. If I had two upvotes, I just might give ‚Äòem to you.
 SELECT id , "order" , "date" FROM something ORDER BY "date" DESC , "order" DESC LIMIT 1 
That did it. Thank you very much!
As an accountant im not the most efficent with sql imo but i would use a sub select in the where clause to get max date and then simply do top 1 in the select... Something like this... Select top 1 order from something where date = (select max(date) from something)
Man so simple compared to my version a few posts below!
It's a set-based approach. You would need to think about the minimum set it represents.
TOP? you're thinking of MS SQL
Let's say on Monday you gave Billy 2 cookies, and Tom 1. Then on Wednesday you gave Tom 3 cookies, and Billy 0. No one got cookies on Thursday cause they were being little shits on Friday they both got 2. If you wanted to know how many cookies each not got for the entire week, you count the total cookies for each child between Monday and Friday. Billy got 4, and Tom got 5. Conceptually that's what Group By is doing 
Yep! Didnt see it said postgre
Your right! Didnt see the postgre in the aubject
great. not OP. I am learning SQL and reading TSQL book in the past few months. Found it very useful. Also, went through w3schools on SQL. I'd like to ask where I could find practice questions with an online playground for SQL. I'd like to start with programming interface on SQL after I achieve good knowledge through direct practice. Thanks
Could you provide the architecture of the tables and example data?
do you know how to use excel pivot tables? dragging a field into the "Rows" box is like grouping by that field in SQL. 
Some dbms might be more tolerate, essentially all fields following select that are not in aggregate functions need to be in group by. 
The error message is pretty clear to me: The field 'temp.RATE' is not in the group by statement clause
I was originally hired with basically 0 experience/knowledge. While in that position we hired (3) new analysts and none of them had any experience or knowledge. Questions like this are open ended and all about how you sell yourself, and how well you interview. Do you *really* want to transition into this field? I did when I did it, and I think that came off in how I presented myself. I had enough ancillary skills / experience that reassured them that I could pick up and learn SQL, and I would like to think I did to their satisfaction based on my reviews, promotions, and career trajectory since then.
Couldn‚Äôt hurt to start learning on your own but it should be an easy translation. You‚Äôll find that most business analyst teams already have most of the queries written for day to day tasks. So what I mean to say is a lot of the learning can be done on the job. Unless you‚Äôre aiming for a BI type role where you are actually building the databases. If that‚Äôs the case, you‚Äôll need experience 
I got a data analysis job without knowing any sql. I did spend a ton of time on the sql tech test questions though. You pick it up quick. I also did have other desirable skills though so I don‚Äôt think they thought sql was a deal breaker. 
Thanks for all the answers guys. So it sounds like a transition should be fairly "easy" in terms of not needing to be exactly proficient in SQL before landing a job in that field. I still plan to finish that SQL bootcamp course and probably the python course before having the guts to apply to some entry level SQL/Data analysts type roles but its re-assuring to know that if I sell myself well enough in interviews, I can possibly land a job even if my knowledge isn't where I'd like it to be. &amp;#x200B; Thanks again!
Depends. I was passed over for one gig because I couldn't answer how to separate records of varying length by name (apparently dumping the csv into excel to delimit was 'a cheap workaround'), or create a (hypothetical) table with a primary key based on incomplete information (no employee ID, no unique email addresses, can't use F/L name+DOB because HIPPA restrictions, etc). Got another job where they told me nbd, I'd learn SQL so they wouldn't have to keep writing my scripts. Depends on how much they like you, I reckon.
Was it a data analyst job? 
It's a plus to be as good as possible, but I don't think there's a minimum. SQL is easy. One can easily learn it on-the-go. One full day with proper lessons and practice is enough to learn what you will use in 90% of the queries in your career as an analyst. To do it, IMO, the best way is to do this: 1. Install a database server. I recommend Postgresql. It works on all OS, is fast, configurable and likely the most widely used DB engine for data analysis. Works well with DBeaver as a GUI. 2. Download a sample database, say, from here [http://pgfoundry.org/projects/dbsamples/](http://pgfoundry.org/projects/dbsamples/) 3. Read documentation on SELECT: [https://www.postgresql.org/docs/current/sql-select.html](https://www.postgresql.org/docs/current/sql-select.html) While it might seem overwhelming, most of it is "advanced". So read and learn this: 1. General, standard SELECT syntax, that's the same for all SQL databases (except MySQL. It doesn't deserve the 'SQL' in its name). The order of the statements (SELECT .... FROM ... JOIN ... WHERE ... GROUP BY ... HAVING .... ORDER BY ...) 2. JOIN clauses (LEFT JOIN, JOIN, maybe FULL OUTER JOIN, but you probably will never need a RIGHT JOIN) 3. WHERE conditions (pre-filter the data) 4. [Aggregate functions](https://www.postgresql.org/docs/current/functions-aggregate.html) and GROUP BY (just know that when you use aggregate functions, everything that's not aggregated but listed in SELECT, must also be listed in GROUP BY. Simple rule of thumb. If you want a longer explanation, read [this](https://www.reddit.com/r/SQL/comments/aavxu3/can_someone_help_me_understand_group_by_really/ecvey77) 5. HAVING conditions (post-filter the processed, aggregated data) 6. ORDER BY (how to order stuff) 7. LIMIT (if you want only X amount of rows) That's it. That's your everyday SQL. The hardest part will be JOIN and GROUP BY, but a couple of examples is enough to learn them, and then you're set. Later, you'll learn CTEs and Window Functions and other stuff, IF and WHEN you need it.
Thanks for the reply. It appears the course i'm currently working on covers all of that stuff and its all done using Postgres. So it looks like I'm on the right track! :)
For analyst role you just need to know basic joins, selecting the right data and being able to navigate schema. understand that schema isn't always provided as a visual for you to reference. &amp;#x200B; Any SQL job outside standard analyst, you're at risk of being caught out for not knowing relational database best practice. Maybe at least, read up on many to many relationships etc. 
Seems like something they could/should add as a `ScriptingOptions` flag in SMO, doesn't it?
Agreed. 
You can use inner join with or without foreign key so long as data is of same type that you're joining on. 
If I understand your question correctly, no, an inner join is not the same as a table constraint. 
/u/zettabyte saved my ass with number 4. Still can't get my head around for-each loops. Maybe I'm thinking wrongly?
Look into the MCSA certs for SQL Database Dev. 
Thank you, that works for me.
Thank you, you have cleared that blocker for me.
\[serious\] People land a job with just SQL?
honestly, yes. it is amazing the lack of capabilities regarding data. ‚Äújust‚Äù knowing how a relational structure is set up and the ablity to reliably query and aggregate the data could get you a 6 figure job. 
They'd be using complex software tools to aggregate the data, not hand-written SQL queries. If there are any jobs that are truly just "write simple SQL and get paid $100k+ without any other DBA duties" then they are very rare.
If you want to, you can also look into NoSQL. At least just understand what it entails. Will look good mentioning it in your interview. 
You need a user to image linking table: - UserId - ImageId
&gt;They'd be using complex and often proprietary software tools to aggregate the data you'd be surprised
Does each image have its own unique ID?
You have three tables: - Image table - User table - Image to User linking table Images and users need unique ids. The linking table doesn't need a surrogate key. The combination of a UserId and ImageId is the unique id.
Awesome, thanks
I upvoted your comment as I was going to say something very similar. I would think that looking for a SQL Analyst role might be tougher than a Data Analyst position, because the latter is less well-defined, while an SQL Analyst feels more like a DBA position. I work with folks who don't know much SQL at all - their specialty is SPL (Splunk's language). But while data analysis requires some 'tool' knowledge, it is much more about how you think about and talk about the data. If you can draw insights from the data, it doesn't matter much which tool(s) you use - though clearly it would be helpful to know some some SQL. &amp;#x200B; And as another commenter mentioned, you can get started with SQL quickly because it's pretty easy. The Udemy course that OP mentioned looks fine (though only at their 95% discount price of $9.99 - I can't imagine paying almost $200 for something that you can get from W3C and YouTube for free).
yes
You are mixing 2 things here: 1. **DDL (Data Definition Language):** where you create tables and establish relations b/w them by using FK constraints. 2. **DML (Data Manipulation Language):** here you are querying your data by using SELECT statement and using JOINS to query from multiple table. You don't need to create FK constraints to query data from multiple tables, the JOINs will do. FK provides you referential integrity so that your related data is not unnecessarily deleted updated. Check this: [https://sqlwithmanoj.com/2009/12/27/sql-basics-working-with-foreign-key-fk-constraints/](https://sqlwithmanoj.com/2009/12/27/sql-basics-working-with-foreign-key-fk-constraints/) Hope this helps.
I generally prefer to use either of these functions is following scenarios: 1. **JOINS:** when I have to retrieve columns in the SELECT clause from multiple tables. 2. **EXISTS():** when I've to just check existence of data in a particular table against a base table. EXISTS() performance better in 2nd scenario as it quits immediately as soon as it gets a match of 1 row, whereas JOIN will find a match for each &amp; every row.
&gt; Maybe you've not dealt with that kind/volume of data? I've dealt with 15TB databases (large enough for you?), I'd also call it a red flag.
burn it with fire....
please go ahead and define "Big Data".
not OP. Is there a free resource of practice questions + solutions to get familiar with various SQL clauses?
You probably shouldn't be using loops to begin with. SQL languages excel in set-based thinking. Using loops and going RBAR is typically a terrible idea unless it is something you can't avoid.
I'm in the wrong line of work if that's the case.
BI Analyst for the first, Data Analyst for the second. Odd reversal, imo.
Code Academy actually offers a free course that is useful for beginners. I transitioned from a finance role to to data as well and it gives a decent starting point. 
All the ones that I've seen so far suck. They're trivial, but still don't illustrate the topic correctly. I've made a habit for a while, going to khanacademy forums and helping out people who didn't get SQL tutorials that were in videos. A simple alternative explanation was much more efficient at making people understand. I don't know why tutorials suck that much. If you want interesting problems, then do the following: 1. Learn the basics of SQL as shown above 2. Learn to use a database engine. How to get data in or out (for postgresql the best way is `psql` command line interface and the `COPY` command, but can also work with just `COPY` if the destination is a file and the database engine has permissions to write to that file [https://www.postgresql.org/docs/current/sql-copy.html](https://www.postgresql.org/docs/current/sql-copy.html)). Like I said, learning the database engine. Not too in-depth, just enough to install and import stuff, that's like half a day of learning. 3. Download a real life dataset. See options below this list 4. Load the data in the database. Google it if you feel lost, maybe people have already done it and have shared their code via github or on blogs. 5. Once you have the data, start asking questions. If you downloaded the NY cab rides, ask where people go more often to on Valentine's Day, where people leave most often at 1AM (good night clubs maybe?) use your imagination. If you downloaded the NOAA dataset, or a piece of it, try to see records for locations, or trends. Try to identify droughts (it's relative for different regions), and maybe get their frequency. Same for floods. Weather is complex, there are tons of questions. If you got the StackOverflow survey, try to find out whether it's really worth switching from tabs to spaces when coding, or whether the averages are tainted by other dynamics (maybe pay is a racist issue and more latinos or blacks using tabs rather than spaces). 6. All of these are flat datasets. Try to normalize them. Learn about the normal forms, try to extract attributes into separate tables and use foreign keys. Try to query the new structures, QA them if they show the same results when queried. If not - why. I promise you, after you do these 6 points, you'l be a qualified data analyst / engineer / whatever. Just without too much experience and no idea about performance, stability, and pretty code. All you need for all of it is at most a couple of weeks of work. The rest comes later. Some options for datasets: * The NOAA weather stations measurements. Normally you could just google it, get to a page on NOAA website, and commence your downloads, but because Trump wants his wall with Mexico, NOAA ran out of funding a few days ago and the website, as the whole agency, is down. Wait a while, maybe someone decides to impeach that rubber duck and data will become available. Sorry for the political rant here, I just found out that the data is no longer available and that made me extremely angry. Let me breathe a bit before continuing. * ... still though... it's sad how an agency can simply disappear because of a big baby. Ok ok I promise I'm done. * [New York cab rides data](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/#taxi-data). * [StackOverflow annual developer survey](https://insights.stackoverflow.com/survey/2018/) (a much smaller dataset if you have weaker hardware) * Head over to /r/datasets and see what else people are sharing 
very helpful! thanks
Regular expressions?
I managed to land a job where it was SQL based and I'd never even opened SQL in my life.
"We don't need indexes. We have primary keys"
Really cool to hear this. I am doing desktop support AND DB stuff. I am strong in support but I really like DBs but still pretty green. I have a growing family and need the financial boost that comes with being a BA or SQL dev. A mentor would be helpful about now. LOL
&gt; You don't need to create FK constraints to query data from multiple tables, the JOINs will do. FK provides you referential integrity so that your related data is not unnecessarily deleted updated. Foreign keys _can_ also aid in query optimization by enabling [join elimination](https://sqlperformance.com/2018/06/sql-performance/join-elimination-unnecessary-tables)
Thinking and talking about data and gleaning valuable insights from it seems like it's very important especially as you move up the latter in data analytics. That's why once I'm done with this SQL course I am planning to take a Probability and Stats class (from the same teacher of this SQL course) I bought from Udemy (for $10) to be better equipped to make those insights. &amp;#x200B; And, yeah, I would never pay more than $15 for any Udemy course for the same reasons you listed. 
So did you just see a data analyst job posting and applied with zero knowledge? That to be me is pretty ballsy but it paid off so that's awesome. I would feel super anxious in an interview for a job posting I know I have zero experience or knowledge in. 
Are there only 3 positions or are the number of positions infinitely possible?
max 3
Thanks man, this is some dirty trick. If I could give more upvotes, I would.
So, there's a couple situations going on with your dataset and I'm not really sure how to handle them... TEACHER\_ID 1 has a one-to-one ratio of each of the aides having a position. TEACHER\_ID 2 has two aides, both in the first position. TEACHER\_ID 3 only has one aide, but they're listed as position 2. &amp;#x200B;
sorry, got my tables mixed up when editing, I fixed the last 2 tables
This should do it: https://www.mediafire.com/file/vckx9lza2ankayj/PISTONIAN.sql/file 
So you need to bring through 3 different fields which lookup data from the same table. But if you join the two tables, it's just going to bring through everything. Maybe you could join the table you're looking up more than once to define which name gets brought through to which field?
I got rejected from a data analyst job for not knowing off the top of my head the difference between UNION and UNION ALL, but was then offered a job as a BI Engineer. Seems like some places are more finicky than others with how much you know vs how well you can quickly google. 
None. I knew enough to talk about it in the interview but had no experience. I learned it on the fly. Fake it till you make it. 
wow, way more complicated that I would have expected. I will have to take a look and convert to my actual tables, but thanks for the starting point! 
There may be an easier way, but that's just the fastest way I know how to write it off the top of my head. 
THIS. 
 SELECT t.teacher_id , t.name , MAX(CASE WHEN ta.position = 1 THEN a.name ELSE NULL END) AS primary , MAX(CASE WHEN ta.position = 2 THEN a.name ELSE NULL END) AS secondary , MAX(CASE WHEN ta.position = 3 THEN a.name ELSE NULL END) AS tertiary FROM teacher AS t LEFT OUTER JOIN teacher_aide AS ta ON ta.teacher_id = t.teacher_id AND LEFT OUTER JOIN aide AS a ON a.aide_id = ta.aide_id GROUP BY t.teacher_id , t.name 
In my last job we worked with a lot of census and credit data, so we had some tables that were well into the hundreds of millions, up into the billions of rows. That's big data. At that job the largest table that was "mine" was only about 80 million, but growing quickly. That's about where I think the cutoff is. Some order of magnitude around 10 million.
IMO once you‚Äôve grasped the core concepts, and even understand the advanced functions some, the level of difficulty with SQL stems from the question you are trying to answer with the data available to you. Some of the most sophisticated and elaborate queries I‚Äôve written don‚Äôt necessarily include advanced functions or knowledge of SQL. I might not really understand PIVOTs well, but I can learn if I think the problem I‚Äôm trying to answer requires a pivot. Mastering problem solving skills is really the final step. I once wrote a 150 line query as a Data Analyst that produced the correct data set, turned it over to a Developer and they decided to rewrite it. They wrote it in 1200 lines and preformed like garbage, but produced the correct result. Our knowledge of sql was similar, but our problem solving skills were on complete different levels. Probably not the answer you were looking for, just my two cents.
&gt;the difference between UNION and UNION ALL TIL UNION filters out duplicates. Guess I have some DISTINCT logic to factor out.
No this does help. The practical side I have learnt by ‚Äòdoing‚Äô projects I already had in mind. For example, one project started as a SELECT query ended up spanning 3db‚Äôs, multiple joins, an api call and import, some t-sql cleansing, a few ranks, a trigger, loop and then some other stuff as the project evolved....each improvement yielded a growth in knowledge for me. I‚Äôd like to understand where possible what I may be missing and if possible test my level of skill. I believe I can actually be ‚Äòcertified‚Äô in sql, but not sure if I‚Äôm there yet. I‚Äôm not fussed in DBA as much as extracting/changing and analysing data, just not sure where to go from my current level to the next. Thanks for the info though!
I'd suggest getting rid of the trim functions in your where clause as they aren't sargable. Is there a particular reason that you're concatenating those columns rather than doing them individually? https://datatellblog.wordpress.com/2016/11/24/sql-server-query-optimization-sargable-functions/
Do you have indices on these tables coming into those joins? How many rows are you bringing back? 
You can be certified in SQL through the database engine. I‚Äôve been certified as a SQL Developer through Oracle and SQL Server. That might be a good start to seeing what areas you are missing. The certification pages usually have an outline or some reading material to prep for the exam. If you look through those or even start ‚Äústudying‚Äù for the exam you can learn a lot. I would recommend the SQL Server one though, if I remember right there were videos on different topics that were really useful, such as how to properly utilize indexes for optimal performance. DBA level is much much much more technical in understanding the database engine as a whole. Most of which is not required knowledge for a DA. 
Awesome! Thank you, I‚Äôll take a look :) happy ny 
We don't know anywhere near enough to respond. Please give us an explain plan, the relevant indices, and tell us about the cardinality of the data
More data on your tables would help, especially regarding what indexes do and do not exist. Also &gt; case when p.PickTicket_Number is null then i.PickTicket_Number else p.PickTicket_Number end PickTicket_Number, is doing the same as &gt;IsNull(p.PickTicket, i.PickTicket) And the IsNull() function is likely a bit faster since it's built-in functionality to the engine.
&gt;So I could create a bunch of new tables from my database tables, selecting for only the columns I need. Then I can use those tables instead. I tried to do something like that but it only made the query run never-ending (or atleast over a minute). You didn't create any indexes on your temp tables, which is going to tank you if the original tables do have indexes. Also, why no `WHERE` clause there if you have one on the original query? If you're going to break things down into temp tables (which is a valid approach), work in such a way that you're as selective about the data as possible, as early as possible. Depending on how much data you're dealing with, 10 seconds may not be that bad. Not having any information about your indexes, the IO stats, or the query plan, there isn't a lot that we can do here except guess. /u/InelegantQuip is correct that having the `rtrim(ltrim())` and concatenation in your `WHERE` clause will be a performance hit. At a minimum, having indexes on the fields used for the `JOIN`s should give you a boost.
I looked up the difference afterward and immediately forgot because I never use unions in my queries. 
Thank you very much, it‚Äôs been a few years since I first studied the fundamentals of sql. Cheers mate!
An eleven-way join is "bare bones"? InelegantQuip is right; you've got a non-sargeable expression in your filter that's going to result in a table or index scan on the "orders" table. You've got some odd CASE statements in joins on the SHIPHIST and SHIPMSTR2 table, too, and those could be a bit oogy. If you truly want perscriptive advice for this, you'll have to post your schema -- table creates, indexes, the whole bit. Actual row counts and selectivity would help, too. Without that, it's challenging to offer any solid advice. Posting the execution plan might be an alternative path. 
Can you explain what you mean by indexes and how to create them? I'm quite new to SQL
I don't understand your questions to be honest. The code I posted is pretty much the full code. 
An index is a set of keys that allow for more efficient sorting of your data. Your tables probably have primary keys, which are by nature indexed. You can create non-unique indexes on columns that have non-unique information so that the queries are more efficient. You can search for indexes to find out how to create them. Choose the columns that you will use for filtering and joining data most often that contain information. 
The first code I posted is pretty much the entire code. The thing I tried with recreating the tables didn't really work so I don't know how usefull that is but this was the attempt: SELECT Company_Code, Division_Code, Control_Number, Customer_Purchase_Order_Number, Date_Entered, Order_Status, Order_Value, Warehouse_Code, Ship_Via_Code into #o from [JMNYC-AMTDB].[AMTPLUS].[dbo].Orders OT (nolock) SELECT Company_Code, Division_Code, Control_Number, Odet_Line_Number, Item_Number, Color_Code, Price, Discount_Value, Discount_Percentage, Tax_Value, Tax_Percentage, Quantity_Ordered, Quantity_Allocated, Quantity_Invoiced, Line_Status, Freight_Charges into #od from [JMNYC-AMTDB].[AMTPLUS].[dbo].Order_Detail ODT (nolock) SELECT Company_Code, Division_Code, Control_Number, PickTicket_Number into #p from [JMNYC-AMTDB].[AMTPLUS].[dbo].PickTickets PTT (nolock) SELECT Company_Code, Division_Code, Control_Number, PickTicket_Number, Invoice_Number into #i from [JMNYC-AMTDB].[AMTPLUS].[dbo].Invoices IT (nolock) SELECT Company_Code, Division_Code, PickTicket_Number, UPS_Tracking_Number into #b from [JMNYC-AMTDB].[AMTPLUS].[dbo].Box BT (nolock) SELECT Color_Code, Color_Description into #c from [JMNYC-AMTDB].[AMTPLUS].[dbo].Color CT (nolock) SELECT Company_Code, Division_Code, Item_Number, Color_Code, Description into #st from [JMNYC-AMTDB].[AMTPLUS].[dbo].Style ST (nolock) SELECT Ship_Via_Code, Description_1 into #sv from [JMNYC-AMTDB].[AMTPLUS].[dbo].Ship_Via_File SVFT (nolock) SELECT Packslip, Procstep into #ph from pickhead PHT (nolock) SELECT Procstep, Status into #z from Z_Status ZST (nolock) SELECT Packslip, Date_Modfy, SHIP_NAME, SHIP_ADD1,SHIP_ADD2, SHIP_CITY, SHIP_PROV, SHIP_ZIP, SHIP_VIA, SHIP_SERVC, SERVICE, SHIP_NUM, COST_SHIP, SHIP_WGHT into #sh from SHIPHIST SHT (nolock) SELECT Packslip, TRACKTRACE into #sm from SHIPMSTR2 SMT (nolock) I wrote that before the full body of code. 
After a closer look, it seems like me joining this table left join SHIPMSTR2 sm (nolock) on case when p.PickTicket_Number is null then i.PickTicket_Number else p.PickTicket_Number end=sm.packslip is causing the query to take from 3 seconds to 10 seconds or more. This is a very large table but I only need like two columns from it. What is the best way to optimize for this?
I wrote this response to another post, but that post got deleted. I'll post it anyway, here: We don't have definitions of the tables you're using; we also don't know what kind of indexes the tables have, or on what columns. We also don't know how many rows each table has, or what cardinally or distribution of those row counts have around the different keys, and which keys might be involved in this query's execution. All of that information is requisite to providing prescriptive analysis about the performance of a query. But you're not providing that information ... so how can we help?
&gt; I want tests sqlzoo.net has quizzes, not really too advanced, some are tricky though
I think the best way is to review the execution plan. That'll show what's really happening. Then, make incremental changes based on information provided in the execution plan to improve the generated plan. Your changes might be to the query, or they might be to the physical schema, or they might be to the logical schema. 
Don't store your images in a database. Just save them as files somewhere and save the file locations in the database. 
Well this particular data analyst job isn‚Äôt just running sql queries for people. It‚Äôs more of an in house researcher job. I also have advanced R programming knowledge. 
How much data are you querying? Do you have the tables indexed on the columns you‚Äôre joining on? Making sure your tables are properly indexed can change how the data is queried. 
Ah, well then that makes a lot more sense. Would you say you use a lot of R in your day-to-day? I'm planning to learn python in the near future since my end goal is a data analyst position.
When you say server, do you mean the host or the guest VM or both?
I use R constantly. I also know Python but I come from an academic background so I‚Äôm more comfortable with R. I would also argue that for most data analysis projects, R is better. The real virtue to Python is that it I a bit easier to put ML models into production or do general programming tasks. But doing statistics or graphs are much more straight forward in R. In general my workflow is like this: I write data import functions to get data into R and do any of the exploratory analysis I want to do. Sometimes I import that data and clean it and then feed that to a python script if I need to have it hooked up to an API or something. Generally if the project is a big deal project, I‚Äôll rewrite code in Python because it is easier to build a website around it (R has plumber now ‚Äî which looks good but I haven‚Äôt learned it really yet.) But I usually work in R. I have an internal r package I built which includes a bunch of those import functions that run sql for me that makes it super easy and fast to get data into R and starting to work on it. It is also nice because after I get the query figured out, I can pass parameters to it via an R function and not have to think in SQL. 
How would you automate that without having to go in automatically inputting the location and image file name
Alright, I started SQL Browser and learned something new. So, using my .udl file test on the actual VM (and domain account) running the SQL instance I have a successful connection test. However, even running that same test on the host (bare metal) machine, the test fails for the same reason. So, no one except the VM can the connection be made.. so strange. Firewall is off on the VM and the host.
What is your salary? Did you ever find good resources for tuning? Thanks!
What are some good resources for someone wanting to become an SQL developer? Thanks!
I'm so foolish (Ôºç‚Ä∏·Éö), I was checking the wrong named pipes were started. I was checking SQL Native Client.. ahh
It's not terribly different from uploading the file to the db. You just need to add a step to save the file somewhere, then add the location and file name to your INSERT statement. btw, you want to store the location separate from the file name. That way if you need to move things, it's super easy to update the db to the new location... you don't want to have to parse the location\name string if you don't have to.
So you have resolution?
Yes! I'm not there yet. I still need to link Access to the SQL database (the whole point of this) and upgrade Express to Standard to fully utilize all of our resources (my VAR hasn't gotten back to me yet sadly). Small victories though! I'm learning a lot.
Thank you so much for your help! Appreciate it.
Think about a residential phone book. It‚Äôs basically the list of phone numbers in an area, indexed by name. However, if you are instead interested in a reverse lookup (finding the owner of a specific telephone number, the current index is useless, and you‚Äôll have to go through all numbers one by one. If you had another index were the information is sorted by phone number, then you‚Äôll be able to perform a reverse lookup much faster.
Find out what port the sql instance is listening on - via the error log or the sql server configuration manager and try connecting with that... so it would be &lt;vmhostname,portnumber&gt;. Or you could try to telnet to that port... if you get a black screen you have connected. If you don't then either a firewall is blocking that traffic, or the host name is not resolving, or sql server is not listening via TCP.
If using management studios, click the button that shows the execution plan. It will also suggest some basic indexes for you. Rough and ready but will help. One issue you have is with you rtrim in the WHERE clause, this will force a full table scan and fetch and ignore indexes. Can you get around not needing to use a function in the where? A like statement? Or cross apply to create your trimmed order number in the from clause
Indexing is one of the most important things you can learn for query performance. I won't get too into the gory implementation details but rows in tables aren't in order, or at least aren't in an order that is meaningful to our queries. If the database wants to find some records WHERE SOME_TABLE.SOME_COLUMN = X then it's going to have to run through every value of SOME_COLUMN and compare each value to X to find the matches. This is very inefficient as a tables will be joined in a manner that combines one full linear look at every column value with another full linear look at every value for some other column and you just get a very slow query overall. That's where the index is key. An index is a data structure used by the database to store what is essentially a sorted list of the column value(s). It's a lot easier to find something in a sorted list. Formally it is the difference between n and the logarithm of n. If n = 1000000000 then log(n) is still very small, say 30-ish for the standard base-2 logarithm. So you have the difference between a billion lookups vs. 30 lookups at most. Now there's a lot more going on and indexes are super complex data structures that also incorporate hashing etc. but this is the gist of it. The downside is that you have to store a separate sorted list for each indexed column(s) and (more space is used) and you have to insert into the correct spot in the sorted list (more time involved in inserting records). It's a tradeoff between super fast lookups and moderately slower inserts and more space. 
&gt; I once wrote a 150 line query as a Data Analyst that produced the correct data set, turned it over to a Developer and they decided to rewrite it. They wrote it in 1200 lines and preformed like garbage, but produced the correct result. This is why I don't let my developers write their own pure SQL for use in the application. Any time they need to write something that's more complicated than SELECT PK_COLUMN FROM TABLE (and even then they usually use SELECT *) I really have to insist that they get it checked by someone that can actually check it for performance. So sick of seeing ODBC stuff where there are a bunch of loops through result sets of one table calling a separate query to get data from another table. You get like 10 layers of nested loops doing the same thing LEFT JOIN should be doing in a fraction of the time but since the devs can't be arsed to actually learn SQL fundamentals we'll have a mess if they are left unchecked.
I have, in both data warehousing and large scale application architecting. No where close to 22,000 separate tables. Just say it out loud: "twenty two *thousand* tables" WTF 
4 billion rows isn't an insane amount. It's a lot but for healthcare stuff or financial transaction stuff it's normal. 22,000 tables though is next-level crazy complex. You're sure it's really 22k distinct tables? That's unreal. 
The JOIN has nothing to do with they keys directly. You can JOIN on any columns provided the data can be compared. They keys enforce data integrity within the database. When you have a foreign key relationship you're effectively saying that TABLE2.FOREIGN_KEY_COLUMN is a direct reference TABLE1.PRIMARY_KEY_COLUMN, and manipulating one may has consequences regarding the other. Example: trying to insert a student into course where the value for the student key doesn't exist in student. If you have a foreign key then the database is going to figure out instantly that the student key you're trying to insert doesn't exist and isn't going to allow it. It forces you to have a real student before it's added to a course. Aside point: having a course table and a student table isn't enough. You have to have a table which is basically just two foreign keys from course and student to indicate which students belong to which courses and which course enroll which students. Like you have COURSE, STUDENTS, and STUDENT_COURSE to represent the student-course relationship and it will just be comprised of the primary keys from both COURSE and STUDENT.
Yes, SSIS is just code, so obviously its functionality can be recreated... the core SqlBulkCopy code is all of like 15 lines... add logging and you add another 15 lines (granted half of it is in the app.config)... add control flow operations (like sproc calls, which are fairly common) and you're adding say 5 lines per operation (using loggingContext, connection, command, maybe a parameter or two)... now it's in the database but still no reports, so write a few RDLs with drillthrough... still not difficult, right? or just use SSIS because it's all built in. not saying that SSIS couldn't benefit from some love, both in terms of enhancements and new features... but the ROI for "just write some code" is first and foremost unlikely (only possible if you're writing a replacement tool and not just solving each problem individually), and secondarily past the duration that most businesses care to justify (2 years or less is almost effortless to justify, 3-5 only with justification, 5+ isn't worth pursuing).
http://lmgtfy.com/?q=how+to+optimize+sql+queries
This is interesting, thanks for sharing. The article recommends creating an index of LTRIM(RTRIM(ProductName)). What do you do if creating an index isn't an option?
Cheers I‚Äôll take a look!
The article suggested using LIKE instead. Apparently any leading spaces will be ignored.
And don‚Äôt forget half the battle is just knowing how to use whatever software you‚Äôll have available at the job. Tableau, R etc. are all very powerful once you wrangle the basics 
It suggested using LIKE if there were only trailing spaces. % is a wildcard operator. I guess LIKE '%Product 1%' is the best option? I don't know how that impacts performance, but it's how I'd do it. 
True. Personally, I'd try to push to clean the data and remove any leading spaces so you don't have to worry about it going forward. Maybe it's a failure of imagination on my part, but I can't see what the value would be in keeping it.
Just curious... What is 'normal' 
10 million is "good sized". I don't know that I'd call it big, though. Heck, Access can handle one million rows without issue. I'd say big starts around a billion. But I guess I'd have to qualify that by saying it depends on what you're joining up.
I said to the order of magnitude of 10, so anywhere from 10-100M, take your pick where you want to start calling it "big."
*yawn* If SQL can't handle your data, you just need more horespower. Same is true with Hadoop or NoSQL or whatever else. Teradata with a couple hundred AMPS is more power than 95% of the companies in existence will ever need. Or be able to afford.
Agree with mikeblas. You can't really generalize here, especially with a 10 second query and this number of joins. You need an explain plan output and an intimate knowledge of how the data is structured in each table along with what indexes exist and what the optimizer is currently choosing. Outside of restructuring your filters or the joins, I would recheck your required output. You have a lot of data here and reducing the requirement by even a couple columns could significantly reduce runtime if it allows you to drop a join or two.
Did you change the default database from master in the odbc setup?
Try replacing this: left join SHIPMSTR2 sm (nolock) on case when p.PickTicket_Number is null then i.PickTicket_Number else p.PickTicket_Number end=sm.packslip with this left join SHIPMSTR2 sm1 (nolock) on p.PickTicket_Number = sm.packslip left join SHIPMSTR2 sm2 (nolock) on i.PickTicket_Number = sm.packslip Then up in the select statement you would use COALESCE(sm1.TRACKTRACE, sm2.TRACKTRACE) or IsNull(sm1.TRACKTRACE, sm2.TRACKTRACE)
I agree with all posted and will add a suggestion. Assuming that a table that provides the name to match the order_status numerical value does not exist, create one. You could eliminate that CASE logic, but it would add another JOIN even if it is an equi-join. Not sure what flavor of RDBMS is being used, but ISNULL can be used in place of a CASE statement that does largely the same thing. Regarding Indexes, think of a database like a textbook. If you wanted to search for a specific term you'd use the index to find all occurrences of that word. Without it you'd have to scan each page to find what you're looking for which will take far longer. SQL can use an Index to do the same. Provided the column indexed has unique values, or is a composite of several columns that when combined make a unique value, these can help to make your query run more efficiently. However, there is no guarantee that the Query Engine will use the index if it doesn't make sense to do so. It has to be useable by the WHERE clause and if you have logic built into that clause it's less likely to be used. Similarly, if the OR, IN, and LIKE operands are used the column or table will need to be scanned for the value. SQL Server has a tool to help determine if an Index would be helpful in your query. I believe there are 3rd Party tools that do the same for Oracle. No idea on NoSQL.
Try scrolling down further
This smells like homework, but it's New Years Eve. You need to do something like a `ROW_NUMBER() OVER(PARTITION BY workout_id ORDER BY log_id) AS 'RN'` inside of a subquery, and then in the outer query do something like `WHERE RN = 1`. Basically you need to "flag" what the "maximum" value is and tell your query how to remove all the other rows that don't meet that criteria (which are "duplicates")
Add a join to Select exercise_id, max(date) max_date From logs Group by exercise_id On exercise_id and logs.date = max_date. If there's a chance the same exercise can be logged more than once on a day, the field should be a timestamp. Ideally, you wouldn't rely on a surrogate key like log_id to determine if one event happened before or after another. 
There's no reason not to have additional tables, but if you insist, you can use a field containing semi-structured data like json or xml. Query performance will be trash, though. 
It's not a SSRS-specific feature, it works in all apps that use that text editing control. &amp;#x200B; Holy shit, though- it changed my life when editing sql.
Dragging the the names of things from the object explorer to the editing pane. &amp;#x200B; Especially helpful for ERP systems with absurdly long names and occasional typos. &amp;#x200B; "select \* from AchivedInvoicesCust\_0045\_Legacy" &amp;#x200B;
This is correct, but you would want to `ORDER BY date DESC`, not `ORDER BY log_id`. In theory you may want to specify `ORDER BY date DESC, log_id` just to be deterministic in your ordering. If your query needs to account for ties (such as the example data will have to), use `RANK()` or `DENSE_RANK()` instead. 
I would suggest adding exercise_id to the partition by clause there based on the statement that he wants log_id 5&amp;6 Row_number() over(partition by workout_id, exercise_id order by log_id desc) as RN All else same
Thanks - definitely not homework :P I'm trying to create an android app for myself to track my workouts - it's my first time using SQL in an app so I'm still new to it. I haven't heard of PARTITION BY before, I'll have a read up on it and try it out.
log\_id is the primary key which auto increments. So the biggest log\_id will always be the latest log. Is this not the best practice thing to do? I can swap it to a date/timestamp if that's the better way.
Could you append a string of x spaces then just take the first x character substring? Like for a 30 char column, concat the value with 30 spaces then take the first 30 characters of the result?
Personally, i would have consodered transfering to NoSql after seeing that monstrousity of a query. If you want to stay with sql you have to create certain indexes very carefully to minimize the time but since there are like a milion joins it is inevitable that when the data grows the slower the query will be. In my expirience you should reengineer a database.
[Kaggle's datasets are great](https://www.kaggle.com/datasets). They're chosen more for their machine learning use, but still plenty there you could do some BI with. 
Thank you! 
I would expect the optimizer to handle these identically. 
I would comment out all joins and add them one by one, then measure which join introduces the largest performance hit. (Same goes for where expressions; the TRIM bit is obviously the most suspicious). 
The Stack Overflow dataset is available via BitTorrent. https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/
While they're logically the same, optimizers are weird beasts. It wouldn't surprise me if the results are different.
I always see these posts. It‚Äôs not going anywhere soon. 
A leading wildcard on a `LIKE` is also not SARGable.
I'll believe it when I see it. I'm a developer. People will always want data from the db. GUIs and other things will allow them access to simple queries, but the complex ones will always have a person involved. Who is going to set up that autonomous db? In my experience the index suggestions SQL server makes in complex joins are always terrible. It is good when you've got a simple join, but when you've got a bunch of columns in the join it's worthless. You'll still have a job, maybe they'll need fewer of you, but as the job is in high demand now you'll just make less. That's years away though.
I don‚Äôt think you should learn more T SQL if you just maintain databases. From my perspective, the Ola scripts can automate maintenance, or maybe some drone in India, but what will always be around is data, like extracting, transforming, querying and reporting against that data.
Yeah nice. Im pretty good with t-sql. Thats a good point. I'm mainly a dba but I learned a lot of programming because I'm really curious. I love olas scripts. Thank you guys
most business users are clueless. maybe at really tech orientated companies these things will happen but if you're like most tech workers..you dont work in tech. 
Can you link me to posts or papers about autonomous databases? Machine learning tools can assist in certain aspects of designing a db: identifying functional dependencies/keys, anomaly detection, constraint checking, etc. But many decisions call for a human being. I‚Äôm a DBA as well, and when I structure a db, my priority is to make sure that data correctly represents reality (example: nobody has a heart rate of 0). This kind of intuition won‚Äôt be programmed into Oracle/Microsoft software any time soon to my knowledge.
Can you link me to posts/papers about autonomous databases? I‚Äôd appreciate it.
Data is MORE critical than ever, and there is more of it. SQL skills in the broad sense are not going to go out of fashion for a long time. What is as risk these days is the tedious parts of DB management and server building are getting simplified/abstracted away. If you think that the specific skills you have as an MSSQL DBA are not viable for landing jobs then you just need more skills, sometimes a wider range of skills, sometimes deeper in something you already know. That has always has been the game in IT. I would say the places to go from an insecure DBA position would be first of all to make sure you have some familiarity with spinning up database instances in various cloud platforms and getting a handle on the various offerings and their pro's/con's. This is the way the wind is blowing, and honestly it is the kind of thing you can become at least conversational in over the course of a few days. That way when your boss comes at you with a "we're thinking about moving this workload into Azure" you can bring some useful knowledge to the conversation and are still valuable. You also want to become an expert in your database engine (or eventually a couple engines) so that you are not one of the lower skill grunts that gets made "redundant" by more efficient tools. You have to be certain that you know how to implement the newer features and what they are doing, it worries me when I still work with MSSQL DBA's who have never set up AG's and don't seem to understand the difference between the MS Windows Cluster IP and a listener address. You need to stay ahead of your company in terms of database technologies. SQL 2017 had been out a while now, many companies still aren't using it in prod but you definitely need to understand what changed between whatever you run and the newer releases so you can explain the risks your data may face in transitioning to a new version, as well as the potential benefits the business can reap from the new features. Another place where I see a lot of DBA's drop the ball is that they don't seem to think their processes through for programmability and standardization. If you are making a lot of tweaks to the instances by hand in SSMS then you need to start transitioning those things into some kind of workflow that is more consistent and reliable. Instances should be able to be deployed, tested, documented, and finally handed over to the application owner with as little interaction from you as possible. Being able to set up that kind of process is golden. If all you do all day is install simple instances by hand, or a good chunk of your day to day could be replaced by some scripts from Ola Hallengren or Bret Ozar then your job is at risk and you need to step up your game. If you are an expert in your tools, have at least a passable understanding of what other tools are out there and what they have to offer, and understand how your business actually uses their data and how to use those tools to help them with that then you will have job offers to keep you busy the rest of your career.
They've been tossing that around the past decade or so. "Autonomous DBA blah blah." As others have said, I'll believe it when I see it. I have not seen any progression on it and if anything, we're being given more and more tools, add ons, and responsibilities that require even more human intervention.
DBAs might need to diversify a bit and be able to administer a smattering of Aurora, MariaDB, Mongo, Redis, etc along with the traditional MSSQL/Oracle, but the DBA job isn't going to go away. Silo'ed DBAs that can only work in one engine might eventually struggle to make their jobs worth the expense to companies, though.
In general with SQL, you shouldn't assume that anything will work the way you want to even though it logically should. In this case, yes, an auto-incrementing unique ID should also indicate that row 2 is "later" than row 1, but what happens if you modify the table and have to rebuild it? Or move to another system? (Ok, that's unlikely in your case, but as an example, Teradata has increment fields that 100% do not indicate the order rows were added to the table.) It's better to treat surrogate keys like LOG_ID as serving exactly one purpose: identifying a unique workout. If you need to determine if one event happened later than another, use a date or time attached to the event, not something that reflects how the database engine decided to label a row. Two more things: I was on mobile earlier, and missed the actual design of the tables. The query you want is something like this (using log_id rather than date) SELECT exercises.workout_id,logs.exercise_id, log_id, exercise, set1, set2, set3, set4, set5, weight FROM EXERCISES LEFT OUTER JOIN LOGS ON exercises.exercise_id = logs.exercise_id LEFT OUTER JOIN (SELECT workout_id, exercise_id, max(log_id) AS max_log from LOGS GROUP BY workout_id, exercise_id) AS last_log ON logs.workout_id = last_log.workout_id AND logs.exercise_id = last_log.exercise_id AND logs.log_id = last_log.max_log WHERE exercises.workout_id = 2 ORDER BY log_id DESC; If the DBMS supports analytical functions, you can do it without the subquery, although you might need to turn your query into one; it depends on the platform. As outlined elsewhere, that would be something along the lines of SELECT exercises.workout_id,logs.exercise_id, log_id, exercise, set1, set2, set3, set4, set5, weight FROM (SELECT MAX(log_id) OVER (PARTITION BY workout_id, exercise_id) AS max_log, exercises.workout_id,logs.exercise_id, log_id, exercise, set1, set2, set3, set4, set5, weight FROM EXERCISES LEFT OUTER JOIN LOGS ON exercises.exercise_id = logs.exercise_id WHERE exercises.workout_id = 2) AS all_logs WHERE log_id = max_log; But you'll have to play around with that to get the right syntax for your system and work out how you want to handle exercises with no logs. The other thing is to think more about what you're modeling. You can skin cats many ways with SQL, and what you have can work for your application, but there might be some better ways. It seems to me that you're really tracking three things - exercises, programs (collections of exercises intended to be performed together), and workouts (the actual performing of programs or individual exercises). It's almost certainly overkill if you're building a personal app, but if you want to use this as a learning exercise, think about what tables you'd use if you wanted to track 1. Exercises. Each gets an ID, and you have fields for name, muscle group, bar type, etc.. 1. Programs. An ID and name for each, then maybe comments, day of week to perform, a flag to indicate if you shouldn't work out the next day, and so on. 1. Workouts. One workout for each time you go down to the basement, gym, wherever. There's a workout_id, but also a start and end time, maybe a location, fields for how you felt before and after, perceived effort, you get the idea. 1. Workout details. This is more or less your current logs table. It links the exercises and program to each workout, and is where you would store reps per set and weights. You wouldn't need the times (since they're in the workout table) unless you wanted actual time per exercise. (You could go even further and have a set details table rather than having fields for sets 1-5. That has the advantage of supporting varying sets and/or weights per exercise within a workout, but also requires more SQL and front-end work to manage. I'd say that 1-4 would give you a solid introduction to SQL and thinking about relational data, probably more than I've seen with some people at work.) The fun thing is that you can get to the same place multiple ways, so if you want a working app now, you can make it happen with what you've done so far - pretty cool for starting from zero! If you want to go deeper, you can. Happy new year!
This is spot on, but there's another important component here: there are millions of databases out there that need to be maintained, modified, and supported. Autonomous databases are OK when you are creating something ex nihilo, but it takes a human to pore through the code to confirm that **course.id** created by Jim is a FK to **class.courseID** created by Bob or that the dates in that one table are all stored as VARCHARs for some stupid corner case downstream and that you also need a CAST() or DATE\_FORMAT() in your WHERE clauses when dealing with them. (A couple of years ago, we purchased a scheduling tool made by a company in France. And while the table names were in English, all of the fields were in French. Even with Google translate, it was still a bunch of hours sorting out how things hung together.) Every day somebody asks me for some new slice of data. But that only works because I *know* the data and the history behind it. I provide read-only DB access to a small group of people in my org (people that aren't going to run a CROSS JOIN - and more importantly - have a clue about what that is) and it always requires a lengthy email about how things actually work. This field doesn't mean what you think it means sort of thing. Maybe someday machines will be able to look at a legacy database and the associated PHP/Python/ASP/CFML or whatever and know what's actually going on in that custom-made professional services time-keeping/utilization-reporting/forecasting app. But we won't care because we'll either be taking up arms against Skynet's armies or we'll be deciding between red and blue pills in the Matrix.
Sprinkle a little machine learning on top and you're golden for this lifetime.
I think you're defending a poor product because you can't code and thus rely on SSIS like a crutch. I'm through with this
Clearly as we are gathering less data every day, databases will go the way of the dinosaur and DBAs will no longer be needed. Oh, hang on a minute, today is not opposite day.... I think u will be ok. More data means more databases means more dba jobs. Get more dba skills. Its a great career to earn very high salaries but you have to work at being very good and knowing best practices for many tasks relating to data storage both local and cloud based. 
I would learn Postgres. After 15+ years on Oracle, we recently moved to Postgres. I wish we had done it years earlier. 
There are still businesses that legitimately use access databases as a prod solution so just like anything... Just because the elite tier something it could easily be 40+ years for others to do so. Tech producers love to espouse their latest greatest tech that will disrupt industry in short form but for many businesses there's not much motivation to switch. I.e. you could pay numerous db's for a hundred years for what some of the really high end db's cost to purchase and maintain. Remember, they aren't generally buy it once products - there's a pricing scheme. 
The Row_Number() Over Partition By "trick" to order records and get Max / Min records is pretty legit. There's a few other ways to accomplish the same outcome such as joining the dataset to itself with a where clause that compares your ordering field. But I find the Row_Number() Over Partition By method to be the most efficient, usually. 
Snowflake Computing is pretty close to NoOps. I've been working with it a while, it's very impressive.
dude. relax. everything will be OK.
True. One trick around this is to use a persisted computed column in the table that reverses the string. That way you can do a trailing LIKE and keep your query sargable. :)
Hey just in case you don't get a chance to see my comment reply to alinroc further down. If you have to use the LIKE operator, one trick around the performance bottleneck of a leading wildcard is to use a persisted computed column in the table that reverses the string of the original column. That way you can do a trailing wildcard and keep your query sargable. :) Note this slightly affects your write operations performance into the table, but depending on if you read from the table more often than write to it, and how many records you're reading vs writing, etc, this is a fine option.
Aws Aurora with Postgres is pretty slick.
&gt;millions of databases out there Also a great point. The number of databases will keep growing at a huge pace. More and more metrics are being tracked. Big Data is just starting. Database people should feel secure in their careers.
Expand your base. Get into data mining, analytics, and business intelligence.
What resources have you used to get to the level you are today, especially where you're working with JSON? Thanks for your reply!
* Stack Overflow * Google * LOTS of mistakes and head banging * Reading, reading, reading. I had to complete two key data projects for my company - the data comes from an API, 3 internal DB's, and Manually compiled data from an outside partner - so i dug in and just started doing it piece by piece - and reached out whenever i hit a roadblock. &amp;#x200B; &amp;#x200B;
So I'm guessing there wasn't any online tutorials or books to help you for reference? Thanks again!
There's rudimentary stuff on Code Academy, Khan Academy etc...but it doesn't really get out of the very basics. I think the best option is to think about a project/goal you need to achieve, and work on it bit by bit....when you learn a new piece of code/technique...google the \*\*\*\* out it till you understand WHY it's in your query and HOW it helps :) &amp;#x200B; Bit by bit, it will help! &amp;#x200B;
Honestly, as a SQL Server developer for the past 12 years at a major Bank in the United States, the future of our industry belongs to one word: India. 
I'd argue that server building is not going away. It's VM provisioning now, but it's the same. And Microsoft setups are basically unchanged since 2005. We have a heavy azure footprint as well (mix of vms, paas database, managed instance, elastic pools, etc) and the dba role has not lessened at all. I don't have to deal with low disk space alerts now I guess, but I also get the fun of setting up index jobs on a database with no SQL agent and things like that.
i think this site does a good job at explaining it. https://community.modeanalytics.com/sql/tutorial/sql-subqueries/
ok thanks. I'd check it out.
What DB ?
MS SQL
If you're just starting out with subqueries, you may want to see if your Sql supports common table expressions (CTEs), which "allow" you to "write" "temporary tables" before writing the final query. 
It will either be completely blank with no FKs defined (if they fucked up this badly on design already I wouldn't be surprised) OR A resemblance to what would be the result of a truck full of spaghetti hitting a string factory at mach 1.
Here's what you want to do instead: 1. Get the required rows and columns 2. Label each row with an ascending number, by USER_ID 3. Choose each row that is labeled row_number 1 You've got #1 already You need to use a Window Function to do the rest. Here's the PostgreSQL version: with numbered as ( select *, row_number() over (partition by user_id order by visit_time desc) as row_num from already_joined ) select user_id, visit_time from numbered where row_num = 1 
Try adding tables to the columns, like table1.user_id and so on. My guess you‚Äôre using mysql, mssql would give you an error for not defining them.
&gt; When I execute this what you posted here will **not** execute because `GROUP BY user_id` contains an ambiguous column reference, since `user_id` is in both tables
!RemindMe 1 day
I will be messaging you on [**2019-01-03 17:47:12 UTC**](http://www.wolframalpha.com/input/?i=2019-01-03 17:47:12 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/abteiw/online_training_resource_by_a_rsql_member/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/abteiw/online_training_resource_by_a_rsql_member/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
HankerRank has been great for me.
I will double check. However, I am trying to link the Access tables to SQL back end (which ideally should be empty). Even if I was linking to the wrong database, wouldn't I still see the Access tables?
Second this. Sub queries have their place but CTEs are easy way reorganize your tables on the fly and reference data in more refined ways. 
Reference the table before the field. such as "already\_joined.user\_id"
Just write the max and group by as a sub query
No, in the window in your screen shot you'd see a list of the tables from the ODBC linked DB, not the access db.
Yes, totally correct. I misunderstood. One issue is that I can only import the older access database using MS SQL 2017, by default it doesn't accept the newer .accdb files.
I wonder whether it's possible to do an export from Access. If you right click your table in Access and choose Export, is there an option for export to ODBC? If so, you should be able to use the ODBC connection you've already got set up to export the table.
Ah yes, your detail is perfect, if I analysed the dB to 3NF I would have pulled out the two foreign keys into their own table. Thank you for your reply.
I'm re-running the conversion on the VM (much more resources than my local machine (which isn't bad at all) and it seems to actually be running this time. So guess that answers that part of the question, assuming it doesn't fail.
Use the row_number function
Trying a new link works, this string works: Description=ODBC connection for SQL Order database;DRIVER=ODBC Driver 11 for SQL Server;SERVER=DL-VMSQL-1\SQLEXPRESS;Trusted_Connection=Yes;APP=Microsoft¬Æ Windows¬Æ Operating System;DATABASE=ORDERTEST; And this one doesn't: DRIVER=SQL Server Native Client RDA 11.0;SERVER=DL-VMSQL-1\SQLEXPRESS;Trusted_Connection=Yes;app=SSMA;DATABASE=ordertest; Can I just update the string?
Because your ELSE could contain non-integer values. How do you plan on comparing the non-integers to cs.PolicyNumber? If you are trying to add leading zeros I find the easiest way is something like this: `SELECT LEFT('0000000000000000000' + PolicyNumber, 19)` 
Is the problem that you're trying to compare a nvarchar cs.[PolicyNumber] with a bigint in the sub query and sql is trying to convert PolicyNumber to bigint?
&gt; How do you plan on comparing the non-integers to cs.PolicyNumber? That's my question. Thanks for the response. I want to REMOVE the leading zeroes for one specific company, but leave all other [PolicyNumber]s the same for other companies, and then join that list to a different table. However, the other table has the policy numbers stored as numbers. I know it's stupid, but it is what it is. It's less than 100 records overall and I'm trying to avoid manual combinations. Why won't my subquery CASE statements seem to work in this situation since the same query works as a stand-alone SELECT statement? Below shows what I want to use in a subquery - the 'PolicyNumberLink.' SOLEMPolicyNum PolicyNumberLink 003439148 3439148 003431428 3431428 9178992060 9178992060 9178736040 9178736040 003440757 3440757 003440685 3440685
Not quite - the cs.[PolicyNumber] is a true number while the SOLEM.[PolicyNumber] is a nvarchar. How would I convert the long nvarchar to a number without the error? I get errors on CAST to int and bigint.
I'd try using linked table manager in Access to force a refresh of the connection parameters for the linked tables. 
Isolate the rows that give am error and looks at them.
You should cast the bigint column to varchar then pad the column to be the same length. Right('000000000'+ Cast(policynumberlink as varchar(10)),9)
It might be the same problem, but the conversion is happening with the cs.PolicyNumber and the SOLEM policy number that isn't cast to an int. Take out that case statement and just pull records where the CedingCompany like '%country%'. Sorry for the guessing game. It's always tough for me to try and diagnose stuff like this without running it myself.
&gt; but the conversion is happening with the cs.PolicyNumber and the SOLEM policy number that isn't cast to an int. I think you're exactly right. This isn't a life or death thing, but it was driving me crazy. Thank you again for bearing with me.
[TRY_CAST](https://docs.microsoft.com/en-us/sql/t-sql/functions/try-cast-transact-sql) will return `NULL` if it fails to cast. You can also use this to find which rows are failing to cast. SELECT [PolicyNumber] FROM [Everest].[Facultative].[FacSOLEM] WHERE TRY_CAST([PolicyNumber] AS bigint) IS NULL
Why are you converting it to bigint? Do you *need* it to be a form of a numeric type? My personal opinion without context of your domain, is to keep both sides as a string and handle them appropriately. Unless you're doing math of some kind with these policy numbers, there shouldn't be a need to convert. I also suspect that particular policy number record you find is visually all numeric, but might have an extra space or something, blowing the conversion up.
Add a rtrim to the field and you're good to go!
the word you are looking for is pivot which db?
yes, thank you! Legend
Your only option may be to restore from backup. 
How much are these jobs paying? I‚Äôve been writing queries in sql for a few years now on a very regular basis for work (not as my main job but as a way to accomplish the job). Some of them are reasonably advanced. Almost all of them have multiple joins. I also understand some (shallow) DB basics, have a grasp on stored procedures and keeping an eye on our jobs activity monitor etc. Can I just get a job (changing from a very unrelated field) without learning anything else? Learning more was on my list...but if I should just be applying and brushing up on the stuff I already know I mean I guess I should get started 
Not every database is happy with dynamic columns for pivot though. You might first need to identify X,Y and Z and then programatically create the necessary query.
Why don't you want to use the bit data type? 
While I don't know your use case, I'm pretty sure this is a bad idea as you get poor performance with both reads and writes. You can do it with a VARBINARY field. Converting VARBINARY to VARCHAR gives you Hex. Then hex -&gt; bit is pretty simple. This is not a good idea though.
&gt; SSMSBoost ApexSQL just reguarly crashes SSMS regardless... CBA using ApexSQL no more.
The easiest method by far would be to restore the old database from backup from immediately before they used the old software, and compare it with the modified old database that they have now. That will give you the actual changes made that need to be merged into the new database. If you don't have that, then it's going to be essentially a 100% manual process. 
Would you save that output as a text file to use locally, or build something into your code to access it online when needed? I find when I right-click and save-as that it saves the json code, rather than the content. I am not very familiar with it, you might be able to tell.
Thankfully we do have backups and I think that will limit a lot of the manual processing. 
Pivot table using dynamic sql. 
While it's technically possible, it seems counter to the normal usage of a relational database. Why can't you have a table for the user activity? |UserId|ActivityYear|ActivityWeek| ----|-----|-----| 123|2018|3 123|2017|4 123|2018|40 456|2016|5 456|2017|3 456|2018|27
How would that work to represent a full year? 52 columns, one per week?
You are creating a 'shortened field' by storing multiple pieces of data into a single field? Ok, you are trying to break the first normal form. There is a reason why "not doing this" is the basic norm - solutions based on "doing this" usually are targeted to a very specific set of use cases with specific optimizations, don't work well for generic use case and sometimes fail to scale even for the original use cases.
So I reviewed the execution plan but I don't fully understand the results. My execution plan says, "Missing Index (Impact 99.8849): CREATE NONCLUSTERED INDEX [] ON [dbo].[Container] ([TrackingNumber]) INCLUDE ([ResolvedCarrierTrackingLink]) The table it is talking about, [dbo].[Container], adding that made my query go from taking 2 seconds to taking 7 seconds. So obviously, it is right and there is some sort of problem with this table. How do I create this index? Is this something I can do in my query or do I have to go into the actual table and mess around there? Is this a temporary change for the specific query I'm running or is this a permanent change? Is there a good resource guide for understanding the results of the the execution plan and what they mean?
How many millions|billions of users do you have? Storing binary as Hex is the normal way. 4 binary digits =1 hex character. Humans don‚Äôt read binary very well. Find an older programmer who did COBOL on a mainframe and they‚Äôll explain it all. Easy peasy. 
Binary\_Checksum might work for you?
Yes. I don't think that would look worse than constantly packing and unpacking bits and bitwise testing, etc. and you won't have to debug that code. SQL will take care of minimizing the RAM and disk used to store that data. Plus, there would be a change that you could get some indexing to work, if it comes to that.
I was thinking 53 columns to match your 53 weeks. Yes. I don't think that would look worse than constantly packing and unpacking bits and bitwise testing, etc. and you won't have to debug that code. SQL will take care of minimizing the RAM and disk used to store that data. Plus, there would be a change that you could get some indexing to work, if it comes to that.
Select Right(field name, len(fieldname) - 1)
 SELECT RIGHT([your_column], LEN([your_column]) - 1) FROM [your_table]
Move each tab (right click tab then click 'Move or Copy') into a new workbook and convert individually? I'm no expert but that would be my approach. 
I could do this, but I'd like to come up with a better method in case I need to do this 5-10 times a week.
PowerShell is probably going to be the best way to automate this: https://www.mssqltips.com/sqlservertip/3223/extract-and-convert-all-excel-worksheets-into-csv-files-using-powershell/
ah ok yea makes sense. That's the extent of my knowledge, sorry I can't be more helpful. 
With SQL? No. With PowerShell? Yep. Check out Doug Finke's `ImportExcel` module (available from the gallery), it's probably got something in there.
You can also use VBA to save each worksheet as a CSV
Use CSVKit[https://csvkit.readthedocs.io/en/1.0.3/] (Python): $ in2csv --write-sheets '-' foo.xls
What you quote from the execution plan isn't part of the execution plan. It's the output of a feature called "missing index analysis". The query optimizer inside SQL Server sees what you're asking to do, and is telling you that it could do it faster if you'd created the index described in the message you quote. Funny thing is, the table ```Container``` isn't in the query you've shown us but it appears here in the missing index suggestion that you get from the optimizer. That means you're not telling us the whole story. Either you're now trying to optimize a different query than the one you've presented in your original post, or that query involves a couple more layers ... like views, for example. Either way, it's not possible to give you good and responsible specific advice about what to do. To answer your specific questions: &gt; How do I create this index? Indexes are created using the [```CREATE INDEX``` command](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-index-transact-sql?view=sql-server-2017). &gt; Is this something I can do in my query or do I have to go into the actual table and mess around there? Indexes are persistent objects and created separately from queries. They're not really a part of a table, but they exist on a table. You shouldn't be "messing around"; you should be analyzing and acting thoughtfully based on your analysis. Other people in this thread have explained indexes to you. Have you read those explanations? &gt; or is this a permanent change? Nothing in a database is permanent. An index is persistent -- it exists until someone changes it or deletes it. &gt; resource guide for understanding the results of the the execution plan Yes, but I don't think you'll have much luck understanding an execution plan if you don't understand fundamental database concepts like indexes. If you really want to jump in the deep end, maybe [this blog post](https://sqlserverfast.com/blog/hugo/2018/02/execution-plans-101-back-basics/) is a good start. This [Kendra Little post has a video](https://littlekendra.com/2017/09/22/how-do-i-analyze-a-sql-server-execution-plan/) too, and she's a really good trainer. If you prefer books, [*SQL Server 2017 Query Performance Tuning*](https://www.amazon.com/Server-2017-Query-Performance-Tuning/dp/1484238877/) is pretty good. 
Your SQL looks backwards. T1.SupplementType is actually in T2 and vice versa.
Oof, sorry. I was kinda hacking this together. Anyway, I think I figured it out anywhow. I'm just going to delete this.
Rather than try to discourage you from perusing this solution, I'll provide an example of how it can be done... not that I'm endorsing this solution for this use case. (sqlfiddle is down so I'm using rextester) https://rextester.com/MPZ94512 Some comments on the script: * Using a physical table (tally table) for `dbo.number ` instead of a view will be more performant * The `make_active` procedure uses a bitwise OR `|` operator to flip the bit. This is just an example of to manipulate the data. Look into bitwise operations in SQL for more info. *The `SELECT` at the end of the script uses a bitwise AND `&amp;` in the join to "unpack" the binary Please don't bother telling me what a bad idea this is. I'm just answering the original question!
Many-to-many relationships (Source/Contact, for example) need to be modeled via associative/link tables.
I can't speak to the rest of the questions, but in terms of hardware, that's **well** beyond what you need for a 1.5GB database.
I also put the SQL VM on it's own SSD raid partition, as we want to do this right and have it be as quick as possible. It was also a 1.5GB Access DB, I am not sure if it's larger now that it's been converted to MS SQL.
The size of the DB might have actually gone down, but you'll likely have some more supporting indexes or other features you'll want to build in now that you have more flexibility. There's really no 'right' answer to if 6 cores will be enough - you mention three people editing the database - are these the only ones accessing it at all? You'll need to monitor the CPU/Mem usage and adjust accordingly, but again, that seems like plenty for the criteria you've laid out. Also note that you have a significant number of optimization type things to look into if you aren't getting the performance you would expect. Managing a MSSQL DB is significantly more involved than an Access DB.
Have any links to articles on optimization/maintenance that you agree with. I'm interested in learning all I can right now.
Wouldn't this better if done with a CTE?
I actually just took this exam! Passed with 92 which I was pretty pleased about. I found that the linkedin/lynda.com material covering the exam was a great help, it's about 6 hours worth and definitely made passing a lot easier.
That hardware is way more than it needs to be for that size of database. As for backup solution; either one of those would work. You could also create a maintenance plan that backs up the database/logs and set it to a schedule. The only other thing I‚Äôd recommend is possibly setting up an index on the most used and large sized tables. Also make sure you set your database‚Äôs file size auto-growth to what matches how much new data you will be storing. 
It's hard to say. Most people prefer CTE's for the readability, which I have come round to agreeing with. The only real way to tell which is more effecitient is to try it both ways and see what works best in your specific scenario. 
Aren't you using the express version? iirc it will only use one core and 1 gb of ram, which is probably all you really need.
I'm really not a MSSQL expert from that side of the house, unfortunately. I'm sure someone will chime in here with some resources.
Don't forget to do a compact and repair on the access front end - it may still have the space allocated for all the data that you've migrated away, which could have performance impacts.
Just wanted to let you know your suggestion helped me solve my issue. Both tables had nvarchar values. So I did a Cast on the field with leading zeroes to bigint. Then I Casted the bigint back to nvarchar. Worked perfectly. Thanks. 
Start by breaking it down - the first piece displays customers where the birthdate matches. The second piece displays where the parameter passed in is null. Create two separate queries and get them working. Then, work on combining them. It sounds like you've at least got a start. Give us what you have thus far.
When you create a procedure, you specify the parameters and data types after the name. I'm pretty sure this is the syntax for SQL Server, but I'm not 100% on that. &amp;#x200B; CREATE PROCEDURE DisplayBirthDays @birthdate DATE AS BEGIN SELECT * FROM BirthDays WHERE CASE WHEN @birthdate IS NOT NULL THEN Birthdate ELSE 1 END = ISNULL(@birthDate, 1) END The user executes DisplayBirthDays, and if they don't specify a NULL @birthdate, then the query will be equivalent to &amp;#x200B; SELECT * FROM BirthDays WHERE Birthdate = @birthdate &amp;#x200B; If @birthdate is NULL, the query will be equivalent to &amp;#x200B; SELECT * FROM BirthDays WHERE 1 = 1 And since 1 always equals 1, it will display all results.
Glad to hear it. In SQL there are many ways to solve problems. Some work better than others. Joining two varchar fields that store numbers is usually messy.
Good call - it looks like it uses a max of 4 cores with no explicit overall memory limitation, but some very tight cache limitations. https://www.microsoft.com/en-us/sql-server/sql-server-2017-editions
Yes, I am purchasing the standard License tomorrow. We don't want any limiting factors.
I am purchasing a SQL Standard license tomorrow through a VAR. I am running SQL 2017.
Thank you! Another question, if I want users to be able to add or delete data, but basically that's it. What role should I assign to them? Or is there another role close to that definition. 
Thanks for the info! I will go back to the drafting board and work on this. 
SELECT * FROM users WHERE ISNULL(@birthdate, birthdate) = birthdate
You can find good information on that [here](https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/database-level-roles?view=sql-server-2017). Basically you would want to use the database role 'db\_datawriter', which allows the user/login to add, delete, or change data in all user tables in that database. You would probably want to add them to the db\_datareader role as well. That role just gives them access to read the database. 
how's the interview going? ;)
I actually don't see those roles by default https://i.imgur.com/u7vctLi.png, do I need to create them? Thank you!
I'm just going to put this out there as my opinion, but I really think a full MSSQL installation is overkill for this, unless you expecting **tremendous** growth in the near future. This is like buying a 16-year-old an F1 race-car as their first car. While I understand you're learning, it doesn't sound like you have a proper DBA in your organization, and MSSQL is an extremely pricey database platform to just 'wing it' with. At the size and complexity you're currently at, MySQL would be more than adequate, and Postgres would take you light years into the future for zero cost. If you truly want to stick in the MS ecosystem - stick with express until you actually see yourself hitting the limitations of that server. You are no where near them at this point.
What does Standard Edition have in it that Express Edition doesn't and you need? Your database fits within the limits of Express Edition, at least by size. Don't spend the money on Standard if you don't need it.
Don't buy Standard unless you're actually bumping up against one of those limitations. It's a lot of money you may not need to spend.
So go into security and find the user you want to give permission to. Then right click and go to properties. Then click on the user mapping tab, then select the database by clicking the box on it, then you should be able to select those roles from the permission list. 
Haha thanks for asking! it was a written test of 10 questions, pretty straightforward except this question, my SQL server knowledge is pretty low compared to Oracle db. 
just messing with ya. good luck.
I have to agree with other posters that Express makes more sense. Deploy your workload to Express, evaluate performance, and then appropriately size the environment. SQL Native backup will be more than sufficient. Ola's script is overkill based on the use case you describe. Have you considered deploying this workload to Azure? 6 cores of standard edition will require software assurance which is $$$. Start small and save a ton of money. 
Homework question eh 
http://www.letmegooglethat.com/?q=What+is+sql
SQL is a language that is used in conjunction with a relational database. The database is like a bunch of spread sheets that are all interconnected. SQL is used to make queries to that database to retrieve, alter, or store some data. There are some pretty great tutorials on the Postgresql and MySQL websites to get you further. However Most of the time you use a frame work, like Django or React, to interact with a database. Those frameworks are built to interact with databases so we don't have to use SQL (mind you sometimes it can be more dynamic and powerful to make SQL queries). I suggest learning a frame work like the above mentioned. And then spend sometime with SQL (imh)
nope
Thank you so much for the query and the explanation! Superb logic!
You can add an additional statement to your join as well. AND table2.visit_time = (SELECT MAX(visit_time) FROM table2 AS t2_ed WHERE table2.User_ID = t2_ed.User_ID
What's the problem?
We only have four users. A standard license and four CALs is $1600. It's not a huge deal. The express engine is limited to 1GB of memory, and I think 10GB cache. That's too small! 
The record with ID 2
You just want WHERE birthDate = @birthdate or @birthdate is NULL
Are you *sure* it's too small? Give it a try.
I would use group by and having to filter this.
You could add AND gh&gt;=2 and rn=1 If they're string cast(gh as int)&gt;=2 likewise with rn, cast (rn as int) =1
Thanks for the feedback! The problem is when I add that to the last WHERE clause gh and rn are not recognized. So I‚Äôm not sure where I have to put it. 
Thanks for this. I was struggling with the syntax a bit, as I've only ever joined "ON" values. Was hoping you could answer these to help clarify. 1. Do you not need an "ON" statement in your JOIN? Or does the "WHERE table2.User_ID = t2_ed.User_ID" replace the typical "ON" command in the JOIN? 2. Is the WHERE you listed part of the JOIN? Meaning, would this query possibly have multiple "WHERE" Clauses? (One in the JOIN, one at the end of the script?)? I guess answering question 1 might answer this. 
Don't you have another column name after gh and/or rn? Could be corrected by using the alias T. 
You should express what you want in simple terms. Like, I want the highest paid contribution for any member with more than one contribution. Right now we don't necessarily know what you want. 
If it wasn't clear, the date and A# is generated automatically by the system so those two portions are always formatted the same, so I could always ask it to just grab information 15 or 20 characters back. That part can be hardcoded. But the text between each date and A# is variable and changes according to what the employee entered.
That's all in a single string instead of being split up into multiple columns when inserted into the DB? Ew. Also, which rdbms?
Is this all in a single string? Any delimiter besides the letter A?
All in one single string. It's a mess. D:
If this is T-SQL (MS SQL Server) you could use CHARINDEX inside of a SUBSTRING function. However, with something like this, it's going to end up being a nested SUBSTRING. Starting with SQL 2016 there is a new function called STRING\_SPLIT which seems handy to me, but does not help you if you're not at that version or not using SQL Server. &amp;#x200B; I guess I should ask, what flavor of RDMS are we talking about here?
Yeah. SQL Server Magement Studio
Using SQL Server. I've never heard of string split. Maybe I'll have to look into that. This sounds like it's gonna be a mess either way, huh?
The T alias worked like a charm. Not sure why I didn't think to do it earlier. Thanks much!
Seems so. Found this nugget about using a WHILE LOOP that might work. `DECLARE @str varchar(100), @offset int;` `SET @str='Another brown fox jumps over something';` `WHILE (@str!='') BEGIN;` `SET @offset=CHARINDEX(' ', @str+' ');` `SELECT LEFT(@str, @offset-1);` `SET @str=SUBSTRING(@str, @offset+1, LEN(@str));` `END;` I take no credit for this. Found it here: [https://sqlsunday.com/2013/02/17/basic-string-parsing/](https://sqlsunday.com/2013/02/17/basic-string-parsing/)
Thanks. Might give me something to go on. Man... what a headache! lol
Yeah, I would either need more information or a better formatted/more meaningful code-block. But it is refreshing to have somebody asking for actual SQL feedback instead of just the answer to his homework assignment. OP I see you got what you need, keep up the good work.
I found this resource page to be pretty useful: [https://www.essentialsql.com/70-761-resource-page/](https://www.essentialsql.com/70-761-resource-page/)
Maybe a dumb suggestion, but have you thought about nesting the query and selecting from this result set? Select \* from (your query here) as sq where sq.gh&gt;=2 and sq.rn=1 
This seems like a lot of string manipulations. IMO, SQL is not the best place for this. I feel that exporting your data, pre-processing it with some custom code (perl, etc.) and re-loading it back to your DB might be a more robust solution.
On the Chance you are on SQL Server..... Stuff(column,1,1,'')
Unfortunately, that's not really an option. I'm sort of in a position where management expects me to figure this out to the best of my ability, if at all possible, without giving me any extra resources.
Did you try anything? 
I'm not even going to yell you what I run my 2 TB dB on. This is very overkill. 
I can't believe how much money you're spending for three people to edit a 1.5gb database. It's crazy. 
Don't use this solution, rarely you'll have two rows with the same date time. Use a row number. 
This is super basic though. 
Mad man
We're a multi-million dollar company, I understand that it's a lot of money personally, but when it's something important like this not really. I'm thankful the IT department is given the resources we need.
Here is one example, when clicking the button to go to the "Last Record" it takes a good thirty seconds. That's not an everyday usecase, but that's incredibly slow. Maybe access front end plays a part, but having more memory cache available should make a difference. 
Just wanted to let you know I'm making some sort of progress using this method. Not 100% sure it'll work in the end but it's helping me divide stuff up in a string. Just need more time to figure out how to end the loop properly and adjust what it pulls. So thanks for this!
Why not enclose your query in a derived table, and filter on that ? SELECT * FROM( SELECT T.customer_no , ref_no , cont_dt , cont_amt , cust_type , rn , gh , Max(rn) over(partition by cont_amt) as 'recdmax' FROM( SELECT customer_no , ref_no , cont_dt , cont_amt , cont_type , rank () over(partition by customer_no order by T_contribution.cont_amt desc) as gh , row_number() over(partition by customer_no order by T_contribution.cont_amt desc) as rn FROM T_contribution WHERE cont_dt &lt;= '2015-12-31' ) T INNER JOIN ( SELECT customer_no FROM T_CONTRIBUTION GROUP BY customer_no having count(cont_dt) &gt; 1 ) grouped ON T.customer_no = grouped.customer_no INNER JOIN T_CUSTOMER on T.customer_no = T_CUSTOMER.customer_no WHERE (cust_type='1' or cust_type='3' or cust_type='7') )o WHERE o.rn=1 AND o.gh &gt;= 2 ORDER BY o.customer_no &amp;#x200B;
What else did they ask as far as SQL Queries goes? Can you post the questions so that others can learn more about what level they should know? Thanks!
Hi, can you recommend any resources for learning more advanced SQL querying? I am trying to improve my skills to be ready in professional setting. Thanks!
What are you trying to relate? Languages spoken to a particular person? 
Yes and have a table ‚ÄúLanguages‚Äù that would be filled with the languages a user can choose from
So it seems like you might have two different tables in mind. One that is "people" and one that is "languages" that you can connect with a one to many relationship. 
It's your money to waste my friend.
"Correct" depends on a lot more details than provided, it might be a "working" design. But I am not sure one-to-many is the right relationship type to be modelling, IMO, this sounds like a many-to-many situation. One-to-many would imply that only one user spoke a given language. A one-to-many doesn't really require a join table (what you called Spoken) and can just be captured using a foreign key from the Language table to the User table. A join table is needed for many-to-many. Additionally, I personally, sometimes decide not to capture these kinds of trivial relationships, when I am very sure I will never take advantage of it. For instance, do you ever need to find all users that speak a given language? If not it might not make sense to introduce 2 new tables, a few FK constraints, and the extra work on inserting a user. Rather it might be easier to use something like SQL arrays, or even JSON/XML if you are using PostgreSQL
This would actually be a many-to-many relationship (one user can be assigned many languages and one language can be assigned to many users), and would be best represented with 3 tables - User table, Languages table, then a cross-reference table with the user id and the language id (plus an optional identity column primary key if you want to reference the data in other tables and you don't want to deal with composite primary keys). Unless you intend for a user to only be able to have a single language assigned to it, in which case a language id column in the user table that refers back to a language table would be sufficient. 
 Users (userid, name) Languages (languageid, name) Now create a mapping table... UserLanguages (userid, languageid) So Bob (userid 5) speaks French (Language 4) and Italian (Language 17) 
&gt; A one-to-many doesn't really require a join table How do you mean? If one User speaks many Languages, how else would you model it other than via a join table?
For a one-to-many relationship, the Language table would have a column for the id of the User it is tied to. As I said one-to-many implies only one User may speak a Language. Your comment seems to mostly describe a many-to-many situation, which is what I also noted
This is what the post described, where UserLanguages=Spoken, but it is not a one-to-many relationship, it is many-to-many
do you need to know net at a transaction grain or user grain?
The usual method would be to use a subquery and a CASE expression: select userid, case when net &gt; 0 then 'profit' else 'loss' end as output from ( select userid, coalesce(sum(soldPrice),0) - coalesce(sum(expenses),0) as net from unnamedTransactionTable group by userid ) t The two coalesce() calls are to cover for the possibility that the soldPrice or expenses may not have any values. Also note that this query calls a net of exactly 0 a loss when it would probably be called breaking even in reality. 
A few options come to mind: SELECT a.userid, case when a.net &gt; 0 then a."profit" else a."loss" end as output FROM ( select userid, sum(soldPrice - expenses) as net from table group by userid ) a This will option uses an inline-view (or inline subquery, I can't remember) to calculate "net" for each userid. Then, you query that inline view using a CASE statement to assign profit or loss. A CASE statement is the SQL version of IF() in Excel. CASE WHEN ELSE END. Option B: SELECT userid, CASE when (soldPrice - Expenses) &gt; 0 then 'Profit' else 'Loss' end as output FROM table If you want to see the number for net then add another column (SoldPrice - Expenses) as net 
I like the use of coalesce, but it may not be ideal if you're also interested in finding missing values.
Hmm...well I guess that‚Äôs good to know. Many of our tables are effective dated rows and require you to join to the most recent for the current info. It appears (only been on the team for 2 months) the precedent is using MAX in the join. Most of the queries are joining to 10+ tables where half could need the effective date clause. Would it perhaps just be due to keeping the logic centralized in the join? Genuinely curious. It‚Äôs dated due to record changes so for example one client couldn‚Äôt be updated twice at the same time so you wouldn‚Äôt have duplicate datetime in that case.
Thanks for the explanation. I see what you mean now, and agree you're right :) 
Sql server can run regular expressions. Write a regular expression that captures date and A#s. If there is more than one in a single line then you'll have to make multiple columns to catch each one. 
yeah i tried solving with IF-ELSE statements but obviously was ending up with 2 select statements. It didn't strike to me that a case statement could be used in the where part.. that was neat.
That's true, but I'd favor of an explicit query if missing values needed to be corrected rather than making a rather deeply aggregated query do that.
I've seen it happen in so many systems where these edits have the same datetime magically. 
Seems the general idea of my query matches matches yours. If this were a coding test for a video skype interview, would my answer be enough reason for the interviewer to reject me?
Seems the general idea of my query matches matches yours. If this were a coding test for a video skype interview, would my answer be enough reason for the interviewer to reject me?
If they're grading on this question alone? Yeah, you'd fail. You've got the basic idea, but your code wouldn't work.
I would bet cash money that the issue there is not resources and cache size is a negligible factor. 
If it's a question to test basic SQL literacy, then I don't think your answer would do very well. I would probably grade it a D. You show you understand the concepts, but not how to connect them together into something that works. Would it be enough to reject you as a candidate completely? That depends on the requirements of the job in question. 
Even trying to edit the tables directly yield the same results. 
I think this might be it, how would I go about doing this? &gt; The table being modified is a linked table without a primary key. For certain backend databases (e.g. Microsoft SQL Server), Access/Jet requires the table to be keyed to make any changes. This makes sense since Access wants to issue a SQL query for modifications but can't uniquely identify the record.
I tried running this (CREATE INDEX PrimKey ON Addresses (index) WITH PRIMARY) DDL statement following the instructions [here](https://stackoverflow.com/questions/38079722/how-to-run-ddl-in-access-2013) but I get "Syntax Error in CREATE INDEX statement"
You'd want a constraint, not an index. [https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-primary-keys?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-primary-keys?view=sql-server-2017)
Wouldn't I need to create the primary key in Access, rather than SQL? Or should I create the primary key in Access and then re-convert to SQL.
If I'm understanding your setup correctly, creating the PK in your converted SQL DB should work fine. I've not used Access for years though, I don't quite understand why Access queries which previously worked fine would stop working now the backend is a SQL DB - possibly a perfectly good reason. The error description (?) you posted before seems pretty straightforward in its complaint though.
Should I create a new column for the primary key, or can any column besides Index work? It's not so much that they don't work, as nothing can be added/edited.
It's honestly been so long since I used Access as a frontend that I can't remember, but creating a new column would presumably require altering the Access application to reference it and shouldn't be necessary. Given its name, index might be the column you should use - a primary key is a column which has a different value for every row in the table, i.e. can uniquely identify any row in the table. To check its' uniquity you can issue a query like &amp;#x200B; `SELECT` `Index,` `COUNT(*)` `FROM` `myTable` `GROUP BY` `Index` `HAVING` `COUNT(*) &gt; 1`
I was under the impression that MSSQL doesn't evalute what is commonly known as regex - only a subset of it. Regardless, that's where I'd start looking. [https://regexr.com/](https://regexr.com/) has been very helpful to me in the past. [https://www.mssqltips.com/sqlservertutorial/9106/using-regular-expressions-with-tsql-from-beginner-to-advanced/](https://www.mssqltips.com/sqlservertutorial/9106/using-regular-expressions-with-tsql-from-beginner-to-advanced/) might be helpful too
Though talking to someone else, apparently the Access application already had primary keys by combining two different columns.. but I didn't see it. Hmm
The column named index can be your primary key. After creating the primary key in your SQL DB, you might possibly need to use the linked table manager in Access to refresh so that the stored table definition will include your new PK.
Just recreate the same compound primary key in your SQL db, and refresh (relink) the access DB using linked table manager. To create a PK: https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-primary-keys?view=sql-server-2017
Maybe. This query will not execute if the column ‚Äònet‚Äô doesn‚Äôt already exist in the table. In my SQL experience it doesn‚Äôt matter that you aliased a column as ‚Äònet‚Äô later in your select statement. Additionally you didn‚Äôt include the from clause or table name. 
Yes. However, in Teradata you can use aliases in other fields so your query may work after some tweaking, 
You're on the right track here. It's a little confusing, but assuming you're doing this in Access, your actually using Jet DDL to tell Access that the linked table has a primary let in SQL server. I've had access's linked table manager lose this setting. The easiest solution is to just replace your existing linked table with a new one. That is, delete/rename your existing linked table and make a new one with the same name. 
Assuming you already have a primary key. Had this issue once, try adding an access timestamp to the table.
all of the sql.
Learn how to read execution plans and obtain a basic knowledge of how the optimizer works in SQL server. How your code performs (and scales) is of key importance. 
stored procedures, triggers and functions are not SQL - they are PLSQL totally different thing though sounding the same. creating and deleting stuff on the DB is child's play, dont put too much effort there. SQL is complicated if you want to be expert in it. SQL is not a complex language per se. the syntax consists of maybe 7 words you need to learn and use daily. SQL is complicated because you need to think in tables and joins. focus on that. focus on tables, relation between tables, star schemas, snow flake schemas, facts, dims, connection tables, etc... after you know that. focus on performance and how to develop SQLs that not only return the required data but also do it efficiently and fast. PLSQL is just a programming language like any other that also incorporates SQL statements within it. if you know programming you wont have trouble there.
Thanks.
Lol yes!
Ill give some examples of where you should be on the learning curve when you interview: given the tables: orders, customers, countries, items, items_in_orders * list of all customers that do not have any orders (write one using joins and one using subqueries, state the PROs and CONs of each version) * show the name and country of the 4th highest customer top paying customer. * what needs to be done and how would the plan of the previous query needs to look in order for it to the most efficient it can be (given there are 120 million Customers that make on average 10 orders with 250 items per order on average) * write a query that returns the name of the second most highest customer by amount of orders. Write the query to comply ANSI SQL and can be executed on at least 3 different DBs (Oracle, MySQL, PostgreSQL) 
Thanks very much, I'll try these out.
Data hygiene is the most important part for a server side developer. Everything else you can mess up, but you can't mess up data that's no longer there. 
Inventory table with dealership ID and vehicle ID Dealership table with dealership details (state, name, etc) and dealership id Vehicles table with year/make/model/trim/etc. and a series of Boolean fields for each category (I.e isluxury issport iscompact) 
Where do you practice SQL? Bc that‚Äôs more than I know. I dev with Laravel so I use their ORM a lot. I don‚Äôt think I‚Äôve worked on a project big enough to write more complex queries? 
Oh, I literally have only done as far as some OOP examples with PHP and so far havn't integrated SQL with code. The reason I started SQL about five days ago was because I'm learning PHP and I wanted to grasp a firm understanding of SQL before building my first full stack site. Since I started Mongo and Node as my first ever server-side web programming experience, I am only recently realising that I only know the basics of Mongo and Node. Now that I have the knowledge of what I'm *meant* to know, I will be revisiting Node and Mongo in depth after PHP and SQL. So the reason I'm going pretty hard at SQL, is because I wan't to learn this stack really well, not just enough to make a good site, enough to really understand the more advanced principles and be quite proficient in my work. &gt;Where do you practice SQL? I just made a dummy database that holds a load of fake data about a school. Like classes, tests, students, scores etc. I practice my commands by just executing them either directly from the terminal or from MySQL Workbench.
&gt; stored procedures, triggers and functions are not SQL - they are PLSQL totally different thing though sounding the same. This isn't exactly true. SQL/PSM has been apart of the core SQL ISO standard since SQL:1999. By way of standards definition, the core SQL language has evolved from just being a domain-specific declarative language to also including procedural extensions to the main language. It is a part of the SQL core language itself now. 
Sweet I use to use MySQL Workbench until I switched to an IDE where the database is integrated into the environment. So you haven‚Äôt gotten into frameworks yet? Laravel‚Äôs Eloquent is pretty sweet and it generates sql queries for you. I like it bc I can focus more on the relationships and the data in my project and not so much on how to write the SQL to do this one thing. 
Nice, I'm going to go straight into laravel after I do some projects from scratch. I noticed laravel had some handy features like eloquent and is quite popular.
I tried setting the Primary Key in SQL, but when exiting it asks to save, I say yes and it says "Saving changes it not permitted. The changes you have made require the following tables to be dropped and re-created. You have either made changes to a table that can't be re-created or enabled the option Prevent saving changes that require the table to be re-created". I didn't select anything besides default settings during conversion I think.. Any thoughts?
I tried setting the Primary Key in SQL, but when exiting it asks to save, I say yes and it says "Saving changes it not permitted. The changes you have made require the following tables to be dropped and re-created. You have either made changes to a table that can't be re-created or enabled the option Prevent saving changes that require the table to be re-created". I didn't select anything besides default settings during conversion I think.. Any thoughts? 
That's a pretty solid line up. I think you have done a pretty thorough study of SQL once you finish up indexes and privileges. Indexes are extremely important! I would suggest to read a book like "Database Design for Mere Mortals: A Hands-on Guide to Relational Database Design." This book teaches a methodology for designing a database, without deep diving into the theory behind relational design. I personally read it and found it helpful.
That's an annoying default in SQL Server Management Studio - go to Tools &gt; Options &gt; Designers and untick the prevent saving changes that require table recreation option.
Yeah, google gave me the answer. I don't know why I asked here first, considering half my job is googling. I tried setting the Primary Key in SQL, but when exiting it asks to save, I say yes. It runs for a while and then I get "Errors were encountered during save process" "Unable to modify Table, Execution Timeout Expired. The timeout period elapsed prior to competition of the operation" 
If it's a big table you might need to change the default timeout. iirc the option is in the same place as the one you've just changed.
I dont agree with you. You might be right with the history and declarations. But one is a structured querying language that and one is procedural. One needs a parser and stuff happen not as you plan it but as the optimizer wants it. The other happens exaclty as you want it and line by line - procedural. It also compiles. Two different set of skills. 
B R E N T O Z A R
no
No? :( 
Thanks
Now facing the error "Unable to modify table. Cannot insert the value NULL into column "index". Column does not allow nulls. INSERT fails. I'm facing challenges setting the Primary key on pre-converted Access database as well. Ahh! 
https://www.reddit.com/r/learnsql
You're my man 
The column index must have some null values in it, which can't be used as a PK. I think you mentioned that your coworker told you that the PK should be a combination of two fields? Possibly using those two as a PK should work, provided that every record has a unique combination of those fields. Another clue might be to have a look at how that table is being used by the application. If you can get into the design view of an access form or subform that is using those tables, viewing the dataset properties should give you an SQL query that shows how the tables are being joined together. This might give you a clue as to what fields are used by the application to uniquely identify records, which might help determining an appropriate field for use as a PK. I have to say I'm jealous of the awesome learning opportunity you're getting - this project of yours must be a great crash course in relational databases, and to be doing it in an organisation that hasn't tied your hands with restrictive DBA policies and doesn't think twice about licencing a full SQL instance for three users is pretty unique!
https://www.google.com search for beginners sql courses and pick one that looks good. Part of learning any form of coding language is learning that google can pretty much tell you how to do anything if you search the write thing.
You would think that. The problem is I haven't been able to find what I am looking for. 
I'm not sure I completely follow. You can force SQL to execute procedurally. Can you give me an example of what a PLSQL environment would look like and how it would differ from a SQL environment, or what you can do in one that you can't do in the other? Legitimate curiousity.
No... not really - that's 100% a symptom of the Access front-end, or improper PK/Index setup of your DB. The most a cache is ever going to use is 1.5GB (your entire DB loaded into memory).
What are you looking for? That‚Äôs how I learned the basics then once I got more comfortable found more advanced courses the same way depending on my insterests
Is there an easy way to insert unique values into the index field? Looking at the backup of the pre-converted Access database, our other table has a primary key listed, however the main table does not. Do they both need Primary keys, or should I just use the other table? Also, while he said there was a combination for the primary key, It's definitely not showing up in the main table, and unfortunately he isn't here so I cant ask him for clarification, otherwise I am guessing at combinations. Looking at them in design view doesn't make it much clearer. Though the other table uses AutoNumber. I agree, I get to learn abundances of things here. I love my job and the company I work for, and my knowledge from servers (both physical and VM), networking, active directory, AV protection of an organization and so much more has grown huge amounts. I'm just now foraying into databases. 
Yes, SQL was initially a declarative language that had nothing to do with database programmability, control structures, or the ability to act as a procedural language.s But that hasn‚Äôt been the case for nearly 20 years. It‚Äôs time to move on from the archaic misrepresentation and misunderstanding that SQL just queries information. The core SQL standard defines its procedural nature as well. It‚Äôs still a domain-specific language, but it‚Äôs a programming language nonetheless and you‚Äôd be hard pressed to have a production database environment that didn‚Äôt make use of these constructs this day and age. To compare, JavaScript has went through an astounding number of changes due to its increase in popularity - initially starting out as a domain-specific language and growing to a full-featured, multi-paradigm, almost general-purpose programming language. It‚Äôs standards reflect these changes. 
I'll second that, I find it hard to believe Google doesn't have the answer you're looking for lol. What are you trying to learn?
I'm struggling to understand how you're getting your numbers in the top table from the bottom table. Could you explain a little further? Thanks.
The data isn't equal. from Bank to Mike = minus money for mike, from Mike to Bank = plus money for Mike. It's a deposit vs a withdrawal. He wants to come up with an output that lists the account status it looks like. 
That's what I thought, but I still couldn't figure out how Mike's(100 + 150 - 25) = -112.
I don't have much time today but something like this should get you started. You calculate the total sum's and then subtract one from the other. with cte as ( select from, to, currency, sum(amount) as samt from table group by from, to, currency ) select * from cte a join cte b on a.from = b.to and a.to = b.from 
Yes you'll very likely need to have a primary key on both tables. I'm hesitant to offer advice here: I'd suggest talking to the owner of the application to find out whether there is something existing you can use as the primary key. BUT, you could create a column for the purpose of using it as PK, just to get the thing working. An identity column is essentially the SQL version of the Autonumber field. The following would add a field called ID to MyTable, which starts at 1 and increments by 1: ALTER TABLE MyTable ADD ID INT IDENTITY(1,1) NOT NULL If you did something similar that would be unique and could be used as a PK, but the risk there is that you might be creating a key where there is something more appropriate to use instead.
I'm trying all the other combinations and coming up with one issue or another. Most of the other columns aren't unique, we have some duplicates. The index column would be perfect, but it allows null values and there are some empty spaces, if I could insert a number into those empty fields in index it would be ideal. How would I run that query (like, where do I enter it!) Sorry, totally new to SQL!
Just use AND or am I missing something
The main view in sql server management studio should be a big text area, but if you don't have that Ctrl + N should open a new query window. At the top there's a drop down that lists the databases on the server, it probably defaults to 'Master' so change it to your database. Put the query in there and the green play arrow will run it.
For each user, sum all Bank-&gt;User based on currency. For all users sum all User-&gt;Bank. Subtract them to get the net transfers(+ or -). So for example if Mike made the following Sent 100 Sent 50 Received 75 He would be at +75 provided they were all the same currency. "Bank" is the common account that is sent from or to with all the other accounts.
Mike never sent 50 though, from what I can tell. * Mike to Bank 100 * Mike to Bank 150 * Bank to Mike 25 Am I missing something?
So, if I put a WHERE clause where I say: &gt; WHERE VAR1 = X AND VAR2 = X I still get back ID #3 because it has that combination under its ID. I don‚Äôt want ID #3 to come back because it has the XY combination. 
Oh I understand now. Try this, there is maybe a better solution but for now it will do. SELECT \* FROM TABLE A WHERE A.VAR1 = 'X' AND VAR2 = 'X' AND NOT EXISTS (SELECT 1 FROM TABLE B WHERE [B.ID](https://B.ID) = [A.ID](https://A.ID) AND (B.VAR1 &lt;&gt; 'X' OR B.VAR2 &lt;&gt; 'X')) 
Sorry, I was using two examples. In the post I made a table, and yes you are right. Mike would be -225. In my comment to you, I gave a second example which adds confusion (sorry). `Sent 100 Sent 50 Received 75` So he sent 150, received 75, he would be +75 in the example I gave in my comment. 
I‚Äôll give it a run through in a few. Thanks!
Select 0 or 1 into a helper field based on the variables matching. 0 if OK , 1 if NOK. Then group by on the Ids, and sum up the values of the helper field, selecting ids only when the sum is 0.
So, disregard everything other than Mike is supposed to have -225, right?
That worked! Thank you! Finally, now with this new column, When they create a new entry how we were, will it automatically add a unique value to that column or would that query have to be ran again? 
This sounds like it would work but I can‚Äôt visualize how it would look in query. Can you provide an example?
Not gonna lie, I'm still extremely confused on the calculations, but give this a shot. Worst case scenario, you just need to switch up the debit/credit values so it recalculates correctly. https://www.mediafire.com/file/t1ixst8fp7khz31/MIDNIGHTPROGRAMMER.sql/file Let me know if it works.
Not gonna lie, I'm still extremely confused on the calculations, but give this a shot. Worst case scenario, you just need to switch up the debit/credit values so it recalculates correctly. https://www.mediafire.com/file/t1ixst8fp7khz31/MIDNIGHTPROGRAMMER.sql/file Let me know if it works. &amp;#x200B;
That column will automatically get a new value every time a record is added, incrementing by 1 each time.
Maybe you should learn how to query Google first
 SELECT fromcolumn as user, sum(VALUE) as net, currency as Currency FROM (SELECT fromcolumn, currency, case WHEN to = "Bank" THEN amount * -1 ELSE amount END AS VALUE FROM transfers) Group by fromcolumn,currency; &amp;#x200B;
I would kill to have a junior developer with your SQL skills as one of my employees. That's a good base on knowledge that some of my mid level devs don't have at my company
you may have to convert "amount" in the case statement using something like the below, if it's not already as the datatype you need. TRY_CAST(amount AS DECIMAL) &amp;#x200B;
Maybe see if you can write two smaller queries to calculate all deposits (where "to" = 'Bank') and all withdrawls (where "from" = 'Bank'). Then you could use those as CTE's and join them together like this: WITH deposits AS ( SELECT "timestamp", "from", currency, sum(amount) as deposit, 0 as withdrawl FROM transfers WHERE "to" = 'Bank' GROUP BY 1,2,3 ), withdrawls AS ( SELECT "timestamp", "to", currency, 0 as deposit, sum(amount) as withdrawl FROM transfers WHERE "from" = 'Bank' GROUP BY 1,2,3 ) SELECT... I left off the rest because there are some thorny assumptions in there, but at least separating out the two queries would give you a solid start understanding the data (also, shameless plug, but [I just wrote about this strategy](https://dankleiman.com/2019/01/02/stop-writing-sql-backwards/)).
I'm probably not that good, but it's surprising how few days it can take to learn a decent amount of SQL if you put in a lot of hours.
You got me there 
https://www.db-fiddle.com/f/fwFogc3Zs8qSnmT3K4TpDS/0 this fiddle shows how it works. It's in mysql because the site doesn't have an mssql host, but if you change the specific syntaxes it should work in mssql as well.
Good article. 
I get the error 'cannot add foreign key constraint'
Yes all I care about is how much + or - a user is from their transactions with the Bank account. 
Then that script I sent should do it. 
why is `customer_id` in `customerFeedback` declared as `UNIQUE`? that would suggest you could only ever get one feedback per customer 
Well, assuming you're here to un-wrap your head not to get the ready statement, here's one way you can reason to your answer: 1. What is your desired output granularity? (User x Currency). 2. Is your original data at the output granularity? (No, it is not, so you'll need to 'group by User,Currency' somewhere along the way) 3. How can you get elements of your output granularity from the original data? (Currency is a direct mapping, User comes from 'To' field if 'from' field has 'bank' value or it is equal 'from' field if 'to' field contains 'bank'. Writing this as an expression: case when "from" = 'Bank' then "to" when "to" = 'Bank' then "from" end) 4. What are aggregate (non-granularity) fields? (Net) 5. How do you calculate the "Net" aggregate? ( You add the "amount" when the "from" field has 'Bank', you subtract when "to" field contains 'Bank'. Writing this as an expression: sum( case when "from" = 'Bank' then "amount" when "to" = 'Bank' then -1 * amount end) Ok now you just need to put these pieces together. select &lt;granularity fields&gt;, &lt;aggregates&gt; from &lt;original table&gt; group by &lt;granularity fields&gt;
ah yeah it shouldn't be unique, ive removed it but i still get the same error
try adding NOT NULL to both `customer_id` columns 
Real life example: Every day we get credit card transactions from various sources. Every source sends a JSON with all the transactions made yesterday. The files come via ftp to the db server and sit there. A plsql code scans this folder for new files. Every time a new file comes in it opens a new thread with a relevant plsql code that loads that JSON into the transactions fact. Because each file is different in nature with more or less data and is structured differently there is a full plsql package for each source. So far so good. From the card schemes we get fraud data via rest calls (there is a plsql package waiting for these rest calls). There is a package that runs every minute and joins the data of the fraud and the transactions. This package invokes a rest call to a machine learning system that learns fraud patterns. If that rest call returns something different than 200 it will try to send it again in a few minutes depending on what error code was received. After a few attempts it will send an email to devops that the machine learning system is not responding. I wanna see how you do all that in SQL. All this is done within oracle 12 db. And it is super fast because it happens on the db level. The jsons can sometimes be 2gb of data, one such file will take roughly 40 minutes to fully load and populate the whole snowflake schema with many table.
When you say a psql code scans... are you talking about SSIS, or what? &gt;There is a package that runs every minute and joins the data of the fraud and the transactions. This package invokes a rest call to a machine learning system that learns fraud patterns. What is the framework or software being used here?
Yeah my customers table is declared first, and ive added not null and still get the error
PURE PL/SQL within Oracle DB. Nothing else.
Not SSIS, that is microsoft‚Äôs ETL platform. Im talking about Oracle here, no microsoft products at all, and no ETL platforms at all.
 SELECT * FROM MyTable t1 WHERE NOT EXISTS ( SELECT 1 FROM MyTable t2 WHERE t1.ID = t2.ID AND t2.Var1 &lt;&gt; X AND t2.Var1 IS NOT NULL AND t2.Var2 &lt;&gt; X AND t2.Var2 IS NOT NULL ) You can ignore the IS NOT NULL conditions if Var1 and Var2 are not nullable.
I mean... OK, I can wrap my head around that. I've never used Oracle. But what you're describing can all be done with SSIS, correct?
I would recommend that be one of the last things to learn. What's the point of attempting to learn that if you can't build code that does something in the first place?
Of course it can. We dont use microsoft. So we have informatica as ETL. But because the files are so huge and they need to be joined to many tables and do different logic with related to many other tables it was much faster to be done directly within the DB. All that back and forth from DB to ETL costs a lot of traffic, memory, CPU and just slows stuff down. I don‚Äôt know sql server much nor SSIS, but I assume you would have that same back and forth stuff going on as well and I would assume that doing it within the sql server only with stored procedures would be much faster. We could also do that with Python by the way, but python has problems selecting and working with large data sets and adds a lot of overhead. We also had the option to write it in bash over the linux server of the DB, but thats just like... write it in aseembly already lol.
So we are migrating to Informatica, but I fail to see how Microsoft is relevant here. It isn't as if you are just logging into a server to do all this, you are using a tool and then feeding it SQL to accomplish a task, correct?
You joke, but you really should know a vast amount of sql to make it beyond the ‚Äújunior‚Äù role in a sql heavy environment. 
&gt; There is a package that runs every minute and joins the data of the fraud and the transactions. This package invokes a rest call to a machine learning system that learns fraud patterns. &gt; I wanna see how you do all that in SQL. Please know that Packages in Oracle DB is added flavor that no other database (that I know of) uses. Packages are really cool and I wish that other databases would implement them as the added layer of encapsulation for database objects works really well, but CREATE OR REPLACE PACKAGE is an Oracle specific PL/SQL language extension and isn't something you find in any of the other RDBMS' implementations of SQL/PSM (T-SQL, plpgsql, etc.) &gt; one needs a parser and stuff happen not as you plan it but as the optimizer wants it. On this though, and since we're talking about packages - along with programming constructs in PL/SQL - when you "compile" PL/SQL all it's really doing is pre-determining the Execution Plan for the PL/SQL code. It's kind of like how .NET will interpret the code as MSIL and then JIT compile it. You can also do this type of compilation with T-SQL in SQL Server for sprocs. Irregardless of the specific database's implementation, though - and irregardless of whether a piece of SQL has pre-determined Execution Plans saved or it does the Execution Plan translation "JIT" - The SQL standard itself defines the SQL/PSM guidelines for SQL being procedural. 
&gt; Irregardless I'm interested in what you're saying, but this isn't a word. Can you, in a simple nutshell, tell me what the difference is between PLSQL and TSQL? And, dumb it way down because I'm not that smart.
Oracle has this idea of a "Package". A package is an encapsulation of PL/SQL code that "goes together". A similar implementation in MSSQL would be to create separate schemas with database objects for those separate schemas in each. This essentially would encapsulate the database objects and T-SQL code at the schema level instead of at the "global" dbo schema. Since, in Oracle a schema and a database is really the same thing - Oracle implemented a "Package" to handle this sort of encapsulation for database objects. [Also Irregardless is defined by Oxford ED](https://en.oxforddictionaries.com/definition/irregardless) it's not formal, but after 300 years of use, I'd say it's common enough to type in a comment. 
SSIS has this idea of a "package" as well, which is what I'm trying to parallelize. &gt;A similar implementation in MSSQL would be to create separate schemas with database objects for those separate schemas in each. This essentially would encapsulate the database objects and T-SQL code at the schema level instead of at the "global" dbo schema. So you can do the same with either, it's just a matter of flavor? &gt;schema What does this word mean to you? &gt;Also Irregardless is defined by Oxford ED it's not formal, but after 300 years of use, I'd say it's common enough to type in a comment. I taught English for a few years before I got into the SQL game and I'd straight slap your hands with a ruler, son. 
&gt; SSIS has this idea of a "package" as well, which is what I'm trying to parallelize. SSIS is a separate service apart from the main SQL Server installation though, which is what I believe the other person was arguing about as "not pure SQL". &gt; What does this word mean to you? A logical representation of the database objects in the database. In Oracle you cannot login with the same user and see multiple different schemas. In MSSQL you can, because users and schemas are separate. Which is why the implementation of multiple different schemas works to encapsulate database objects in MSSQL. &gt; I taught English for a few years before I got into the SQL game and I'd straight slap your hands with a ruler, son. I'll holla at you then if I ever publish. 
&gt;SSIS is a separate service apart Right, but you just talked about Informaticca [sp?], and I can conceptually think of how to accomplish everything that SSIS can do in just straight pure SQL using sprocs, etc. &gt;A logical representation of the database objects in the database. In Oracle you cannot login with the same user and see multiple different schemas. In MSSQL you can, because users and schemas are separate. Which is why the implementation of multiple different schemas works to encapsulate database objects in MSSQL - and why the equivalent is Packages in Oracle. This makes no sense to me. It is simply an area that I have apparently no knowledge and cannot even begin to follow what you're talking about. &gt;I'll holla at you then if I ever publish. Lots of authors break rules when they publish. 
&gt; Right, but you just talked about Informaticca [sp?], and I can conceptually think of how to accomplish everything that SSIS can do in just straight pure SQL using sprocs, etc. You're correct. I wasn't disagreeing with you - in fact, I was doing the opposite. All of this could be implemented in T-SQL. &gt; This makes no sense to me. It is simply an area that I have apparently no knowledge and cannot even begin to follow what you're talking about. So, I usually leave roles/users security to my DBAs because the DBA side of Databases isn't really my thing, but I'll try and explain. You can have multiple databases on a single server for both MSSQL and Oracle. Where they differ is how users work in comparison to what schemas they actually have access to. In Oracle, a user is defined at the schema level - in MSSQL a user may have access to multiple different schemas. 
Oh, I see, that would actually be really useful.
Where is the Postgres server running? On your computer, or is it hosted somewhere and you need to connect to it? Based on the screenshot, it's assuming you're running it locally.
yeah, I have the postgres server running locally. 
So you recently installed it, I assume? Make sure it's actually running.
got it to connect! but now I'm getting another error trying to follow these exercises on Udemy... [not sure if you're able to help me with this, but any idea with this error?](https://imgur.com/a/xy3uNzp)
I've never used Postgres before, so unfortunately, I'm unable to help with that particular error. Sorry.
nothing to be sorry about, thank you!
I agree. These bullet points listed are the nuts and bolts and slightly more. Also agree that indexes are extremely important. Damn, depending on the field of work you do (dataset) if you can answer the questions for these topics, you'd be a shoe in for a Junior Dev job. Good luck.
I don't use postgres either, but it sounds like the importer is expecting the database and tables to already exist (and its just attempting to import data). Could the --section=data be involved maybe? You would need to import the schema to create the table structure (and maybe the database itself) if its not already there
I tried! I was going to hire a VAR group that has DBAs to do all this, but they kept ghosting me (and I know them personally, was weird!) so I ultimately decided just to do it myself, and I don't regret it. I've learned so much and I'm so thankful to this community. 
 SELECT DATEPART(year, TIMESTAMP) AS 'YEAR' ,store ,count(store) AS total_purchases ,sum(Coupons_Used) AS total_coupons FROM `supermarket.table` WHERE STATE = "PA" GROUP BY TIMESTAMP, store ORDER BY TIMESTAMP, STORE This may work for you. 
An MS cert certainly wont harm your CV, but they are multiple choice exams and there are websites just giving you the answers to memorise, so they don't hold a lot of weight in terms of expertise. Its an entry level position though, so I doubt they are expecting your CV to show much experience. Being passionate in the interview and showing you are keen to learn, will no doubt go a long way. If you are really serious about getting into this, set up a local dev server and just throw yourself into it, delving into every aspect and get to know it inside out. Theres just no substitute for knowledge and experience. 
I had to add a postscript to all of this, just out of responsibility to your learnings: All of this, *you absolutely never go randomly poking about with the structure of an in-production database without some really robust discussion with the owner of the thing*. At which point you take the emails and copy them into your CYA folder. I realise you're in an odd situation at the moment and just need to get things working, but what you did was grabbing an application and then in a very rough and ready way fix a problem your business had - which is impressive. But if you approached an issue like this in an enterprise environment you'd be met with stares of abject horror by the DBA.
Shouldn't this be GROUP BY DATEPART(year, TIMESTAMP), store ??? Otherwise you will have an aggregate row for both 2017-07-04 and 2017-12-25 for example?
If your goal is to get into project management wouldn't being a business analyst first be a better path? Not saying that technical knowledge isn't beneficial but PM work is the furthest from technical in most companies.
In the company I‚Äôm attempting to get into, the database maintenance position is actually the position that you do to get promoted into a Business Analyst, which can then work up to a Project Management role! I‚Äôve heard from a few in the dept that they hardly ever hire BAs externally since the software/database is proprietary to the company.
Cool. I added an edit to my original response. I didn't think of it at first from a "get a foot in the door" perspective. When I got my foot in the door I took community college courses, learned SQL, Java, and C#. I started in an operations position that gave me just enough experience with technologies to use it as a stepping stone to the next job and climbed from there.
DOH, you are correct. Will edit. Thanks
Depends on the house. Some development houses strictly separate the developers from the database side. You'd have SQL developers that would expose functions for you that you'd then utilize in the code you're working with. Smaller companies though don't work like that, having as much SQL knowledge as possible is a good thing in general. It helps to have the knowledge when it comes to troubleshooting issues. You can speak to it or offer up solutions. The list you wrote out is scratching the surface, user privilege / knowing how to look at access rights is great, so definitely do that. Also know how to communicate with databases via ip/port. Advanced SQL is all about efficiencies, you may not need to get that deep. The reason why I started with it depending on the house, is some jobs/companies you won't even be able to touch the databases.
I'd add a category table with a cross reference table to vehicle rather than multiple boolean fields. Other than that, looks like you covered everything. Views next.
Oh definitely, and we aren't lacking backups. Currently have someone more familiar with databases looking over everything to see if there is a better way to achieve all this, or what changes can be made. I'm more of a sys admin and developer than DBA but when you're "the guy" of a small shop, you have to wear many hats. I fully planned to contract this job out but they kept going non responsive and decided to do it myself, though I would have been lost without this community. The infrastructure we have now compared to when I was hired is already ridiculous. Thank you for everything!
What's wrong with `SELECT DISTINCT ID FROM T WHERE Var1='X' AND Var2='X';`?
SELECT ID FROM T WHERE VAR1='X' AND VAR2='X' AND NOT EXISTS ( SELECT 1 FROM T AS T1 WHERE VAR1!='X' OR VAR2!='X' )
I really dint understand why you are complicating it so much PLSQL or any equivalent TSQL language uses loops, whiles, variables and functions. SQL is a querying language with tables and joins with none if the above. Its that simple. Yes so what Oracle has packages and other dbs dont. I couldve done that in stored procedures only. Since oracle 9 you could define types applicable only in packages. Not globals or anything.
Can I pm you to ask a few questions about career path/choices?
I‚Äôm not trying to complicate it. You‚Äôre stating that SQL doesn‚Äôt have these procedural programming constructs. I‚Äôm telling you that you‚Äôre flat out wrong and it‚Äôs defined in the ISO standard. 
Ok, thanks.
Definitely would go with an options tables with a cross reference in this case. Otherwise, the scalability of the table and code is pretty limited on the basis that when a new option is added to cars, the underlying table structure would have to change to accommodate. 
Really? this is what you argue? that the ISO standard classifies PLSQL as SQL? I choose to pick my battles and let you win the argument.
interesting. Im curious what were the arguments that lead to the decision to go to Informatica, because I always thought both are good... any way good luck with informatica, it has its pros and cons (as with everything)
Do you need that field in entities 9 or 11? Is it a way of ensuring the records are distinct in entities 9 and 11? 
&gt;Do you need that field in entities 9 or 11? I don't and i actually don't want it there but the program automatically makes it when i make a relation with a table inbetween wizyta and choroby. And when i try to delete it it deletes the whole relationship between the tables, don't know if thats just how SQL works or the program being dumb (or me)
&gt; DROP CONTRAINT it is spelled CONSTRAINT 
Hi there, fellow Oracle person. I just wanted to pop in to let you know that, at least in my market, you have the edge over about 90% of candidates. PL/SQL is a whole other beast and far more complex that T-SQL to learn/write, and that makes you much more valuable than someone who knows SQL Server/T-SQL. My team interviewed candidates for a contract position for a solid 8 -10 months and we never found someone who had PL/SQL experience (even though it was like the #1 thing we specified that we wanted). I‚Äôd highly suggest getting with a IT Resource/Staffing agency to check your options in consulting and contract work. IMO you‚Äôd make a killing as a contract Reporting/BI analyst with PL/SQL experience. If you want to break into DW and ETL, the easiest way for you to start is to: 1) set up an Oracle express database, fill it with data from /r/Datasets. 2) set up a SQL Server Developer edition Instance and a linked server (sql server‚Äôs DBLink) to the sql server instance 3) move the data over to SQL server via you own home brew ETL process (just be sure to follow proper data warehousing techniques like Fact/dimension, Star/snowflake, etc.) 4) once the data is in the SQL server, take it to the next level by reporting on it (with PowerBI, for example, which is free). You‚Äôll have a start-finish understanding of the concepts this way. You can also just download the StackOverflow database for your SQL server, then make some ETL to migrate it over to ORACLE if you‚Äôd like. https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/amp/ I‚Äôd highly recommend BrentOzar.com and GroupBy.org if you want to really get into the SQL server world. Both are great (free) resources. Best of luck!