OP is on 2008, so I think it was still allowed then. 
Outside of my area of knowledge I'm afraid. In terms of checking tables to see if there are any changes at all you could query the INFORMATION_SCHEMA tables (e.g., http://dev.mysql.com/doc/refman/5.7/en/tables-table.html), which I imagine would be less resource intensive than doing a count(*) on each table. To identify changed rows you might want to take a look at: http://weblogs.sqlteam.com/jeffs/archive/2004/11/10/2737.aspx But that looks pretty resource intensive.
I meant doing SELECT * or selecting all columns of a table.
Yes doing a count will be less resource intensive that fetching all of the rows.
dummy join not necessary -- SELECT s.member_id , s.club_id , c.club_fee , ( SELECT AVG(club_fee) FROM club ) AS avgincome FROM subscription s INNER JOIN club c ON c.club_id = s.club_id
I started my first job in Sql three months ago. I was promoted from within the company as other people in our IT department left. Before those people left I made friends with some people in IT and the manager gave me SQL access. Since he left 2 YEARS ago I volunteered time and work... Now all that time is paying off! Goodluck with your job hunting.. . I guess my advice is to just find a job where they will let you transition in time.. In my case they had to learn to trust me. 
Reports developers need real skills to do their thing. Reports are (usually) on the lighter side since it is just getting data out in whatever flavor (crystal, excel, ...) The business needs. Spend a year or two doing that and you're set for bigger buzzwords like BI, or big data on the same platform. 
It's a pretty important concept about how you build and design your database. I recommend reading up on it a bit - https://en.wikipedia.org/wiki/Database_normalization
Would also like to know!
I don't know the filepaths or even proper webpages where game-level NBA data is kept, but once you discover them, and if they are uniform, then you should be able to come up with some form of batch file system to automatically pull all of the data for you which you can then import into a SQL DB. something like this http://www.fangraphs.com/techgraphs/building-a-retrosheet-database-part-2/
MySQL isn't as popular in the enterprise world as MS SQL Server or Oracle. Both have free/educational versions if you want to mess around with. Oracle has a ton of business products out there. You may want to check out Oracle Business Intelligence. Maybe even download a VM from Oracle. http://www.oracle.com/technetwork/community/developer-vm/index.html 
Any job leads from your women's coders group?
It can happen that you have multiple orderlines per order. Every line has a distinct product (donut) and a quantity (7 strawberry donuts and 3 chocolate donuts). Every line is also uniquely identifiable with a key.
 WITH CTE AS ( SELECT * , ROW_NUMBER() OVER (PARTITION BY Id1, Id2 ORDER BY Score ASC) AS Sequence FROM Table ) SELECT * FROM CTE WHERE Sequence = 1
Managed to figure this out myself. Table names are irrelevant but required in a format file. All that matters when doing a bulk import is the column order number. Do I delete the post or just leave it? 
No, although I haven't expressed any interest to them about my job goals yet either. I am definitely going to let it be known in our next meeting, hoping something comes of it! 
Thanks! I have seen SQL server many times as a required skill on job postings. Looking into this now! 
Thank you! Free is certainly attractive :) so I'll be looking into this. 
Please don't tell me how to answer questions.
such as this? WHERE CAST([ID] AS varchar(255)) = CAST(@ID AS varchar(255))) I got the same results as if its not matching
i'm pretty sure the optimizer is smart enough not to do it over and over...
xidel is good for scraping as long as the site doesn't require javascript. 
1. regular price goes w product, but actual price needs to go into order lines table 2. order lines table needs a "line number" column 3. if customer has many phone numbers, then phone numbers should be its own table
SQL = "sales qualified leads" lol. What a fucking idiot
I'll second the BI track. I've coached a lot of young professionals and there are many easy way to demonstrate competency without actual 'experience'. My usual recommendation is to: - Start a blog - Download Tableau Public: https://public.tableau.com/s/ - Find some data sets that interest you and make tableau reports with them - Embed tableau public reports on your blog - Link your blog posts to linked in as articles and projects That will get you a bunch of requests from recruiters and you can point to all your tableau projects as experience. That should get you an entry level position in no time! 
Yes, for a self referential table to work it needs to be a 1:M, you are correct. The query would be something like this: SELECT A.ID, A.Name, B.Friend, B.Name FROM Person as A JOIN Person as B on A.ID = B.Friend The field Friend needs to be the ID of the person he is friend with.
Awesome, thanks! And it can be self referential multiple times as well?
That's similar to what I had in mind, yes. Upon second look I'm guessing that it is getting confused somewhere with the multiple fields named ID. It looks to me like your @tAudit table has one ID field as does Mydata and CBdata. I'd try renaming one of those throughout the sp and see what happens.
Awesome, thanks for the help!
True, but counting on the optimizer is lazy coding.
Not generally. It's dependent upon the number of rows in the tables, whether they are indexed in a way that helps your query, and then just how well your database optimizes the work. Identical SQL against identical tables in SQL Server and in Oracle can still have totally different performance.
That's why you should have a DataType table. It then definea the valid domain values and RI will prevent any errors occurring. You can then get rid of the CASE altogether.
So, SELECT Id1, Id2 FROM SimRateTable INNER JOIN ( SELECT Id1, MIN( lowScore.Score) FROM SimRateTable lowScore GROUP BY Id1 ) Id1Low ON SimRateTable.Id1 = Id1Low.Id1 AND SimRateTable.Score = Id1Low.Score though that does give you too many rows if more than one row has that same low score.
FirstName + ' ' + LastName is going to be really damn quick. It's really no different than asking for FirstName, ' ', LastName. Seriously, it's really quick.
The fact that it shows both options with the same score might be even a plus point, thanks!
SQL Server *can* give you an idea if you view the estimated execution plan, but there are a ton of caveats to that, and you need to know how to read an execution plan. For other flavors of SQL, I couldn't tell you. 
You're saying insert a numeric value into the table so i can order on it, or you're saying to have a small params table that has each data type and a value to sort? Both seem undesirable.
Thanks! This appears to be what I was looking for.
 select E.Event_ID, E.Event_Type, E.Event_Name, M.Member_ID, M.Member_Firstname, M.Member_LastName, R.Result from EVENT E inner join RANK R on E.Event_ID = R.Event_ID inner join MEMBER M on M.Member_ID = R.Member_ID where E.Event_ID = 'EVENT001' order by R.Result fetch first 1 row only ; There seems to be no reason to do a subquery column. But I really need to see the schema here. If you are using an Oracle version earlier than 12c you can use the old rownum way of limiting results.
&gt; fetch first 1 row only Cool, looks like some pretty nice (way overdue) [new options in 12c](https://oracle-base.com/articles/12c/row-limiting-clause-for-top-n-queries-12cr1). Here's the pre-12c method using `ROWNUM = 1`: SELECT * FROM ( --orig query --order by.... ) where ROWNUM = 1;
You can connect Access to a SQL db, right? I'll probably just add a line that updates the Access db every time the code runs (only once a week). It's just a quick fix since I'm leaving the company in a couple weeks and don't have time to convert it into VB .Net + explain it to the guys who would be left in charge of it.
Ive heard so many stories like ours once I started looking into it. Seems thats pretty norm... Even has a official name as the 'accidental DBA' 
&gt;it broke compliance That may be true, what data access governance policies exist in your company? &gt;it was dangerous as I could brick the server for the whole company. That may also be true. A poorly-written query can negatively impact performance on the whole instance, unless you're using Resource Governor (and that's not foolproof). One bad join or a WHERE clause missing a critical predicate and you're doing a cartesian product and spewing out billions of records in your result set. Ask them for a day-old *copy* of the data (or subset of the data) on a non-production server. But that may not be sufficient to satisfy data governance concerns.
I think the reasons they're giving you for lack of direct access could very well be true. Aside from the compliance parts, running ad hoc sql in production can be risky. It might be a more useful path to work with them to improve the portal (if it's something you guys wrote in house) or give them the types of queries, with examples, you'd need to run and see waht they can do to work with you on that.
I think the reasons they're giving you for lack of direct access could very well be true. Aside from the compliance parts, running ad hoc sql in production can be risky. It might be a more useful path to work with them to improve the portal (if it's something you guys wrote in house) or give them the types of queries, with examples, you'd need to run and see waht they can do to work with you on that.
I'm a business systems analyst. I'm not an IT guy but I'll take a stab at a couple areas. I don't know how familiar you are with SQL but if you write a bad query it could eat up processing power, and while it may not brick the server it could bring things down to a crawl. Having worked in IT support, your IT people are probably gun shy about people 1) doing things while not fully understanding the implications of what they are doing and 2) repeatedly trying things over and over again without taking a step back to think about what was going on. One instance I saw where someone, faced with the prospect of her interface hanging when doing an intensive task, aborted the task, retried it, and then did a rinse/repeat creating 50 orphaned processes that brought the whole server down. IT *probably* is not denying your request solely because they don't want you in their area. They have a responsibility to *all* users of the system, not just you. If you're an analyst, you're probably not generating revenue like the primary users of the system and therefore you aren't a priority. Make the case to your leadership about why you need this data and work with your IT partners to get the need fulfilled. Depending on resources there are several ways to get this done, from making database copies to setting up a virtual server to performance test the queries to partnering you with a developer to understand how better to structure your query.
Hmm. Unless you are allowing a person to be a friend to only one other person, you would need a many-to-many relationship and therefore, a link table.
can you get IT to build you a replicated reporting/bi sandbox?
Some background aboutmyself: I studied Aero Engnieering and Commerce at uni. Plenty of experience coding in Matlab. My Thesis used heaps of programs written by MIT which I had to teach myself. I also taught myself Powerpivot to get around the copious amounts of Vlookups I need to use to make reports when the datasets come from multiple extracts. Basically I know it will take time but I am very confident I can learn the skills as I have faced challenges like learning new programs/syntax before. I actually understand the implications of bricking a system. as part of my research I looked into the basics of SQL, had a look at the QA server we have for tests to see what data were in tables I currently cant extract from the portal, and also drew up some ways in which i plan to use SQL (i.e drawing multiple customers data at once for a specified time period as opposed to one at a time or every customer) The issue I have is our IT department is understaffed. Any request I make on behalf of the company for improvements to data avaliable gets prioritiesed as low importance and is promptly forgotten. My idea then is to empower myself to not rely on this department. Like I said, we are a smaller part of a larger business and are repeatedly given no love. I raised a big fuss 2 weeks ago, they sat down, listened to my request. gave me time with some IT developers, I told them what I needed. then, never heard from them again.
THIS
Fixed?
If your case statements are looking for "Forecsat" instead of "Forecast" you'll end up with nulls. It's misspelled in bother the group by and order by clauses.
Seems like that would be the answer. How long would that take to do? as I said we have a very low priority on resources
the answer is it depends. i dont know your enviro or available resources so i cant say. took me about a week to figure out how to setup a replication server when i did it the 1st time many moons ago.
the plan would be to generate Queries that are functional on a simplistic level, generic standard data for the most part, and I would just drop in the customers, date range's or other factors I want to filter for. The existing portal only allows for one customer to be selected, or all of them when it comes to data sets. Ok you can work with that but its annoying. Another issue: some of the data in there exists in the tables but isnt extracted into any output in the portal. Here is an example: There is no place on my portal that lists customers addresses or information in bulk. Say I want a regional analysis for revenue (a request my boss gave me). I would need to individually look up all our customers one at a time and write down which state they are in. I know it exists in a table on the server but I cannot see it in bulk Imagine telling ur boss that our system is so bad that I cannot look at the addresses of all our clients. Sorry this is turning into a rant but I have always been a believer in learning skills, empowering yourself to create the things you need but it feels like i am getting hamstrung by the very division that created the problem in the first place.
The latter. You should not be rendering data into code if you can avoid it. Just normalise DataType as an entity with a sort order. This is then both compliant and extensible, and will automatically guard you from the mistakes you made in that code. You will also gain a small amount of performance. What is undesirable about it? I can only lead the horse to water, you must choose to drink.
What's "very large"? How big are the tables? How many servers? Is it just SQL?
Sounds about right in terms of needing a redesign. Unfortunately, RDBMSs are notorious for being difficult to modify once the final design has been implemented. It's difficult for companies to keep up with data requirements and modify the database appropriately while upholding proper database design and integrity of the data. It takes a skilled person who knows what they are doing to be able to modify correctly over time. 
&gt; Any request I make on behalf of the company for improvements to data avaliable gets prioritiesed as low importance and is promptly forgotten. Then you aren't making an adequate business case for gaining the access. &gt; Plenty of experience coding in Matlab. That's nice. **LOTS** of people who have "plenty of experience coding in X" fail with SQL because it works completely differently from every language/environment that they're accustomed to. They use the same approach/technique as they did in other languages and then the DBAs cringe as the server slows to a crawl because they're doing a loop over 100K records instead of using a set operation. Point being, you should not be "figuring this out" on the production servers. You may *think* you won't hurt anything, but I'm guessing you're wrong. If you've never *really* done work in SQL, you don't know what you don't know, and that's going to end up breaking production. I had one analyst who would run queries, then copy/paste her results into Excel and filter the results there. The queries took 15-30 seconds to execute, put heavy load on the server's I/O system and slammed the cache. After I showed her how to do the filtering in the query which brought the runtime down to sub-second and reduced the I/O load by a factor of 10 or more. Her response: "My way is fast enough for me, so I'll keep doing it" - completely disregarding every other user of the system. **This** is who your gatekeepers are trying to protect against.
As other's have noted from a technical perspective it is easy to give you direct access to the database (which is a horrible idea for all the reason's previously mentioned) or to set to up a read only copy or backup of the database. The problem with this is it would require a separate server and it's own licensing costs. Is your cost center going to cover these costs? SQL licensing isn't cheap and dont' forget to factor in the the cost of hardware. If this is a large company they may have hundreds or thousands of servers and databases and they don't really have time to be coming up with custom solutions for every individual that needs access to data contained within a database. This is not something that is scalable regardless of IT staffing levels. This problem is more likely a business process/compliance issue. Perhaps they have the portal as they need to audit every customer record you look at. This isn't something that can be easily done with direct SQL access. If direct SQL access is the "best" solution then maybe see if they have a test environment with sample data where you could write queries (views, stored procedures, etc.) which could be evaluated by the DBA's to ensure they are efficient and to validate that you only have access to the data that is required to do your job. If the current portal is not sufficient for you to be able to do you job then you should probably go through your boss and up the chain that way to get them on-board in addressing your concerns rather that going to IT directly. 
You are not the president of this company, head of IT, or head of your own department. You don't get to call the shots. Having an attitude isn't going to make your company's IT department want to help you when they clearly don't trust you. You're telling them how to do things that you know you don't understand. Handle this appropriately. Tell your boss that you need better access to the data or reports in order to do your job. Work with them to develop a memo with your needs that they agree to. Get a meeting scheduled with the boss, IT, and maybe other parties. Don't be accusatory. Tell IT your business requirements and deadline. Get them on board, not demanding server access but saying you need the data exported in a specific format. Be open to IT writing the query or hiring someone trained in SQL to do it. Don't make enemies.
Hmmm..this is intriguing. Sounds like a solid approach that would help me build my network at the same time. Thanks for the public tableau as well..this is awesome! 
I feel your pain, but the business needs to figure out how to get this data to you outside of a production SQL server if it is important enough to them. Push for a data warehouse or day old data on another server. 
It could break security compliance. If the database stores sensitive information (like credit card numbers, addresses, passwords), adding a new user isn't a small deal. We get audited annually for this stuff.
&lt;snort&gt; I have 20 years of database development in Oracle and Sql Server including query optimization. But I have a MSM, Mgt of Science in Mgt. And I still run into this attitude from IT. There is a bias in IT that has nothing to do with knowledge or compliance. &gt;Be open to IT writing the query or hiring someone trained in SQL to do it . It is the year of the Fruitbat, Please come join us. BI can not afford the two week around time that IT written SQL requires - not when it is a 20 minute ad hoc question. And hiring someone to run the scripts that I just wrote in really not cost efficient. No one wants to run against PROD. But restrictions that exist on a reporting database that treat it like PROD are nonsensical. 
&gt; the business needs to figure out how to get this data to you This is the TL;DR of this whole thread. It sounds like OP needs to get with his management to come up with requirements for what he needs to do whatever, build a business use case, and then he and his management should submit the request via the proper channels to do so. It probably won't be an overnight resolution either.
Use this until it becomes a problem. If I am building a lookup for employees then this should be fine given there are only so many employees that can exist in that dataset. If I am building a lookup for every person on the planet including those who are deceased, then I will start to look at excluding/adding predicates as necessary. Here is the tried and true approach I use as an Oracle developer. https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::p11_question_id:1669972300346534908 **edit** Here is a valid link to the article Tom mentions in his answer. http://www.oracle.com/technetwork/issue-archive/2009/09-jul/o49asktom-090487.html
Stuff like this just reduces readability. What is wrong with Where (foo.bar=@bar OR @bar IS NULL)
I missed that @bar is a variable and not a column. Yeah, then forget what I said. In this case dynamic SQL would allow index use (if present) because you don't get the equivalent OR pattern: http://use-the-index-luke.com/sql/where-clause/obfuscation/smart-logic
Ah ok. I was ready for you to blow my mind.
In my experience, using an OR to compare two conditions like that where one is null, is actually a lot slower than COALESCE/ISNULL. 
BODS utterly kicks SSIS to the kerb. It is also very expensive. The primary advantage of an ETL tool is your logic is organised into a pipeline, so the outputs of one operation become the inputs of the next. You can achieve the same thing in SQL code but there is little but your eyes and comments to keep your path straight. If you are all MSSQL then SSIS is a fine choice. Surrogate keys shoukd be used in preference to natural keys in a few circumstances. When the natural key is very compound When you are likely to get key collisions from multiple sources When you need to track history (type 2 slowly changing dimension) Don't just use them everywhere because someone tells you that it is some kind of special rule for dwh. Btw did you know there are several other DWH architectures other than star schema? Do you know your choice is the right fit for your needs? Are you going to do it realtime or batch? You need to know these things BEFORE you choose your methods and tech.
Really? Show me a query plan that proves this please?
you need to combine your logical clauses by logical operations, not commas, so 'WHERE field1 = x AND field2 = y'
In the future, please clarify what your seeing that you are not expecting since the only obvious issue was the syntax which you fixed.
No load balancing there. The princess is in another castle. No, really. The only type of "load balancing" SQL Server has is a AlwaysOn Availability Group, which can provide you with a read-only copy of your databases on the secondary servers, but it is available only on SQL Server Enterprise Edition. And yes, you can set up a cluster environment which would support a rolling update (patch passive node, failover, patch other node), but it would still require a short outage.
&gt; There's so much risk giving people access to the data directly. What damage can be done to a mirror used for reporting? 
I can't test this and not do I use Oracle on a regular basis but this should work. I made up table and column names since you didn't provide them. SELECT SUBSTR(VIN,10,1) AS VIN_YEAR , COUNT(*) FROM Vehicle_Table GROUP BY SUBSTR(VIN,10,1) ORDER BY 1
I think you replied to the wrong person. : )
When I first started my current job I was tasked to master tables that generally did not exceed 500K rows, while other members of my team worked on tables in the tens of millions. They *used* to make jokes about me and the efficiency of my queries and frustrations because their environment was so much larger. Fast forward a year and I came up with an analytic process that resulted in a potential for 1.25 trillion rows given the way MS2008R2 works the execution plan. It ran for 19 hours and crashed the server. Another analyst saw it, and modified the code from using `or`'s to `left join`'s and it ran in a few seconds. In short, the answer is no.
I doubt we really need 8 cores either. We are not a high usage business. If we start running into performance issues, I can offer them the option of more licenses, less redundancy, or hire an actual DBA who can tune this better than myself.
The 2 queries are not equivalent. The second query filter should be Where pi.gender_code = IsNull(@gender_code, /**/pi.gender_code/**/) That's why the 'IsNull' is performing faster.
&gt;This is a question in my sql class and I can't seem to find any examples of this type online. Could someone please help me. http://lmgtfy.com/?q=Sql+subqueries&amp;l=1 SELECT B.A FROM ( SELECT C.A FROM ( SELECT 'A' AS A ) AS C ) AS B;
How big are the tables? Do you need the data in realtime? Is it feasible to copy the needed data to another SQL server daily and then work from there?
I really need to dust off my knowledge about query planning :S What i get is this (for the last query): explain SELECT end_device_id, MAX(id) AS id FROM data where gateway_id = 'b8:27:eb:11:22:40' GROUP BY gateway_id, end_device_id; +----+-------------+--------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+---------+-------+---------+-----------------------------------------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+--------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+---------+-------+---------+-----------------------------------------------------------+ | 1 | SIMPLE | data | ref | PRIMARY,received_index,measures_data_end_device_id_fk,measures_data_gateway_id_fk,measures_data_measured_index,measures_data_gateway_and_end_device_id_fk | measures_data_gateway_id_fk | 146 | const | 6829326 | Using where; Using index; Using temporary; Using filesort | +----+-------------+--------------------------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+---------+-------+---------+-----------------------------------------------------------+ So i'm using one index for the gateway_id but you are correct. There is another index for the end_device_id to. I'll try forcing the use of that index and see what comes of it. Regarding the GROUP BY clause, i don't think i need it, however i haven´t found another to get the results that i want (for each device of the selected gateway, get the most recent measurement). Also there is the problem of the massive number of rows... In the explain above you can see that it has to search more then 6 million rows and the complete table has over 30 million.
Honestly, with that code, you can not test anything. IsNull(@gender_code, @gender_code) is so redundent I would tripple check that it is not optimized out. You need to look at the execution plan (in this case, the xml), not the IO statistics.
Does your query need to be for all end_device_id's for the gateway_id, or just the list in your examples? I'm no expert with MySQL tuning or plan reading, but if this is saying it can pluck 6m out of 30m and then operate on them, that isn't a bad thing. It is using measures_data_gateway_id_fk which is sensible given the query. MySQL doesn't have Ranking (window) functions so the getting latest thing is probably best done like you say with an id. You could try extracting the data into a temporary table first and then further analysing it?
Seems like a logical conclusion to me. Admittedly, I've never really dug into the why. After benchmarking and finding it consistently quicker, I just adopted it and didn't look back. Cheers.
basically, you just want to think of the result set as a table, and then you query on the data from that table. 
Honestly that's a BS answer by someone with a lack of expertise. It's as simple as setting up a private machine (doesn't even have to be a server) and then replicating the database on it and refreshing it at interval X with a simple SSIS job. Shouldn't take more than a days work and ~1500 for the machine, and that's being generous. Now if they want a more robust enterprise solution that's a different story, and more work, but still its the same concept in principle. They need to give you a machine that you have direct access to which dumps the available data into (1) or more databases. You then need to have your own database where you can create / drop tables, schedule jobs, etc. 
Nice, what about CROSS/OUTER APPLY?
This is for a work order report. Table A contains the work order information; Table C contains the requester information. Table B ties the IDs of the other two tables together, along with modification dates and times (that I don't care about).
Yes, that was a result of my obfuscation, sorry. I did those two separate queries, and they both worked as expected.
I apologize- I have no idea how to format on Reddit. I keep editing to attempt to, and fail.
Context? If you are explaining some code results, what is the code you're explaining the results from, and how do these values relate?
Here is the code that was pulled. We are trying to test whether the company is using proper password configurations, and I am attempting to explain to my manager what these results mean because they aren't Windows or Active Directory passwords. Login Mode: exec xp_loginconfig [login mode] Logins that don't require a password: --Select “SERVER NAME=[place server name here]” select 'List of logins that do not require a password. Run the following script:' Select sid, status, dbname, name from syslogins where password is NULL 
Names enclosed by double # signs are for [internal system use.](https://msdn.microsoft.com/en-gb/library/ms181127\(v=sql.90\).aspx) They're used by the system to do system related things. They are not able to login to the system. Your query can show this if you include the column "hasaccess" from syslogins table.
Aha! That's the problem -- I'm getting a count of 0. I got the table information from the data dictionary supplied by the software vendor. Let me get with their support team and ask them what's going on. Thanks!
Rock on, now to figure out shared LUNs under VMware, I'm sure I can do it, I just haven't. :)
Is this then a many (orders) to many (requestors) to many (transactions/modifications types) to many (record creation date) relationship then? Anywho, based on your other answers, it appears that the relationship is not there or set up differently anyway.
Yeah I guess I just assumed AG's would be more complicated for a non-DBA trying to implement this. [The instance vs. database level recovery](https://msdn.microsoft.com/en-us/library/ff929171\(v=sql.120\).aspx#SQLServerFC) seems overly complicated but you're right AG's are still an option.
Lets say 3 tables: dbo.workorder dbo.workorder_toRequest dbo.Requester Here is your query Select workorder.Info, requester.info from workorder left outer join workorder_toRequest on workorder.wo_id = workorder_toRequest.wo_id left outer join requester on workorder_toRequest.req_id = requester.req_id
A technical auditor getting help from reddit? This scares the crap out of me. That aside, those are logins used for internal SQL Server processes only, they cannot be used for anything user-facing. If it were me on the tech side, I would push auditors to allow us to filter these out from the query just so they don't clutter the results and cause future confusion. Of course, document this filter so that future auditors or tech people have an understanding of why these were filtered out in the first place in addition to their underlying purpose.
Ok, good to know. Thanks!
what does value-added re sellers mean?
Your code needs some work. USE AdventureWorksDW2012; SELECT SUM([SalesAmount]) AS SalesAmount FROM [dbo].[FactResellerSales] AS frs INNER JOIN [dbo].[DimReseller] AS dr ON frs.ResellerKey = dr.ResellerKey -- needed to join the two tables, goes in ON WHERE frs.[OrderDateKey] BETWEEN 20060101 AND 20061231 -- needed to resolve the research, goes in WHERE AND -- what indicates that said reseller is a value added reseller??
You input by making an UPDATE query or you right click the table and select Edit rows?
SELECT * FROM TABLE WHERE NAME LIKE '%ROBOT 20%' Should return everything from ROBOT 2000 to ROBOT 2099. 
A value-added reseller (VAR) in the real-world sense is a reseller who is adding value to the manufacturer's product. For example, EMC resells Hitachi hard drives as part of their SAN products. EMC is adding value to Hitachi's products by packaging them into a SAN solution. In terms of your query, I don't know the AdventureWorks database structure, so I can't tell you how to distinguish a VAR from other resellers. You should have a look at the data in reseller table to determine how to do that. Or, the documentation for the AdventureWorks database.
ok thanks
Currently using excel to make the 300~ update statements for me. Hopefully this puts the problem to rest. Thanks for the help!
&gt; UPDATE Table SET Field = 'Customer, 192.1.1.1, Chicago,IL' WHERE IdField = ID Those are actually 3 separate fields. Would the command look like this or is there a easier way to put it? &gt; UPDATE Table SET Field = 'Customer' SET Field = '192.1.1.1' SET Field = 'Chicago,IL' WHERE IdField = ID
 Select c.Computer_ID FROM Computers c WHERE c.Computer_ID NOT IN ( SELECT cs.Computer_ID FROM Computers_Software cs JOIN Software s ON s.Software_ID = cs.Software_ID WHERE Software_Name LIKE '%Adobe%' )
Yeah I also would use excel or Notepad++ to edit a big text to do several UPDATES :D Also, don't forget the WHERE part, forget it ruined several DBs out there.
Ahahahaa- no no no this specific instance wouldn't pass or fail anyone, I was just trying to find out the best way to explain what that was for clarity purposes. This isn't even a SOX control, which is what I'm testing.
&gt; HAVING clause Wow. That alone helped a lot! Joins still scare me but it's a leap I definitely need to make. &gt; "Is the "who" in the third and fourth conditionals the employee or team?" The Employee's Sales rate AND THEN The Employee's Supervisor's Team's Sales Rate. That one is the tricky bit because they can actually report to multiple supervisors in a month- i just want to use the most recent name though I think. The SQL is down-right ugly in this environment. You've been warned! SELECT `'PPP V3 Admin Extract$'`.AGENT_MASTER_ID, `'PPP V3 Admin Extract$'`.Month, Sum(`'PPP V3 Admin Extract$'`.Calls), Sum(`'PPP V3 Admin Extract$'`.Sales), round(Sum(`'PPP V3 Admin Extract$'`.Sales)/Sum(`'PPP V3 Admin Extract$'`.calls),5), `'PPP V3 Admin Extract$'`.Program_Name FROM `'PPP V3 Admin Extract$'` `'PPP V3 Admin Extract$'` WHERE (`'PPP V3 Admin Extract$'`.Month=8.0) GROUP BY `'PPP V3 Admin Extract$'`.AGENT_MASTER_ID, `'PPP V3 Admin Extract$'`.Month, `'PPP V3 Admin Extract$'`.Program_Name HAVING (Sum(`'PPP V3 Admin Extract$'`.sales)/Sum(`'PPP V3 Admin Extract$'`.calls)&lt;.025) 
Thanks! This is exactly what I needed.
Well my databases are undergoing a Sox audit right now...so I'll know if this comes up in a meeting soon!
also keep getting an out of range error on my division of sales by calls. It won't return the data until that's sorted. I tried rounding it to 5 decimals but no avail. CAST doesn't appear to work in MSQUERY, either.
;) Good luck!
or HEADERONLY, which would give you last and first LSN
The out of range error is probably because sum(calls) = 0, so you're dividing by zero. Try this: SELECT AGENT_MASTER_ID ,Month ,Sum(Calls) ,Sum(Sales) ,CASE WHEN Sum(Calls) = 0 THEN 0 ELSE Sum(Sales)/Sum(Calls) END ,Program_Name FROM ` 'PPP V3 Admin Extract$' ` WHERE (Month = 8.0) GROUP BY AGENT_MASTER_ID ,Month ,Program_Name HAVING (CASE WHEN Sum(Calls) = 0 THEN 0 ELSE Sum(Sales)/Sum(Calls) END &lt; .025) I've omitted the table name from the column references, which should work since you're only working with one table. *Caveat* I've never worked with MS Query, so I don't know if it has special rules for table references. 
No, a 7 or 9 will never end up in the 4 to 10 CASE. But CASE statements are generally poor performing and you're much better off storing these associations in a lookup table (temp or permanent) and joining directly on your field.
Are these revenue codes? Depending on the system you're working in, you might have a lookup table that can help. But, to answer your question, the CASE statement you've written will work consistently. As /u/ComradeCrypto mentioned, if this query will be used often, you'd be better off setting up a lookup table.
CASE statements are evaluated in the order that is written in the code, and will stop attempting to evaluate once a match is found. An alternative, depending on your DBMS, is a DECODE which I generally find much easier to read. That being said, if your list is in the hundreds, then a lookup table would be a better, more sustainable option as mentioned by /u/ComradeCrypto.
Probably the report you made use dbo_Transactions.CreationDate, maybe in the report group or order.
This honestly sounds like a really small data source. Medium sized tables are millions of rows deep with dozens of columns. You don't need an expensive solution for them to give you what you need, so focus on drafting a proposal around that point. 
Nope. Not using report, just query
That's what I thought, but I wanted to be sure. Thanks
Cool idea, For the records I'm comparing I think a case statement will be sufficient as the query executes in a few seconds. I don't really want to build a table with 99,999 different possible matches. 
You don't need 99,999 rows (although there is nothing wrong with that) You need a table thus: id int ,OutputName varchar(128) ,startno int ,endno int Then join with a.field between l.startno and l.endno 
Ahah thank you! I didn't know that the ordering was important let alone this important. It was one o the those "well.. let's see what this does" moments and it worked! Yes i'm already exploring more of the query plan and already found another mistake that could have saved me all this trouble. I'm grouping by gateway_id but that is uncessary since i already filter it using the WHERE clause. That alone makes the number of rows go down from 22k to 8k :D This is awesome! I really need to learn more about SQL! Thank you!
Yeah saying that I'm struggling to remember the rule-of-thumb. I think it is 'highest cardinality first' but I think that runs opposite to what your query needs. My usual rule of thumb 'don't prematurely optimise, await a realistic workload' would win here though I think! Good luck with your project :)
"Edit rows" and the other table "designers" in SSMS have historically been buggy and problematic. I don't know anyone who uses them. Just write `insert` and `update` SQL statements and execute them. 
try WHERE dbo_Transactions.CreationDate &gt;= [enter creation date] AND dbo_Transactions.CreationDate &lt; DATEADD("M",1,[enter creation date])
 datediff(dd,yourdatefield,getdate())
datediff() or your database's equivalent function. 
SSIS requires the same major version of SSMS, but other than that I haven't heard of any issues.
I would be careful using datediff() and equivalents: usually these functions calculate number of respective date/time boundary crossings rather than the 'common' definition of how much time passed between 2 date/time points (i.e. all precision below the requested time granularity is dropped). 'Traditional' age in years is relatively easily obtained by converting relevant dates to integers in YYYYMMDD format, subtracting, dividing by 10000 (to get the YYYY) and truncating the result. This also works for leap year dates. 
So say like I have a reference code table with reference id and description for all the different codes in my tables in my database? 
Yeh dude, and honestly with tables that small you could even run it off your local machine/laptop instead of having a dedicated box. That wouldn't be optimal, but absolutely possible. 
I dont' think so, this is how I am set up also. There is an option in SSMS to script for certain versions of SQL server. I would set that to 2008. Tools - &gt; Options - &gt; SQL Server Object Explorer -&gt; Scripting "Script for Server Version", set to SQL Server 2008
I'm trying to work strictly with SQL view and so inserted the parentheses where I wanted them (rather according to this book, SQL Queries for Mere Mortals, I'm using to teach myself). When more than two tables are involved, say three, I've been putting an open paren before the first table and a closing it after the first search condition. 
~~What a dumb approach. OP don't do this.~~ It sounded absurdly convoluted, but apparently it is one of the most efficient way to calculate a person's age in years. If you just want age in days, stick with datediff. This will get you your age in years (in SQL Server): (CONVERT(int,CONVERT(char(8),@Now,112))-CONVERT(char(8),@Dob,112))/10000 AS AgeIntYears
I'd dig through the reseller table(s) and see if there's something like a VAR flag, or a reseller type that includes VAR as a value that you can add to the query.
Yes, that will be the lookup table.
Not to go into an internet debate mode, but help me understand why do you think a different approach that gives a correct answer is so unapologetically 'dumb'? Also, do you happen to understand why your original one gives an incorrect result in certain cases?
Table designer, Table editor, SSIS, Analysis Services, Integration Services won't work
What about left/inner
I've tried, but for some reason, the "Add Database" option is not present when I right click on the Availability Group. I'm logged in as the DB Admin and so far as I know, I have given myself full permissions over everything.
dateadd will not thou... and just for performance reasons, forget the rounding issues, just for basic "don't shoot yourself in the foot" you should not filter with datediff() and year() there is a new worst idea in the thread thou ... computed column (non persisted at that)... bah
Heh I use hour for my age calcs DATEDIFF (hour,datefield,getdate ())/8766
That's what I was missing. Thank you!
Well, I've got only two ideas left: 1) Try to connect to the AG name, not to the physical server name and try again (or vice versa, I don't remember if it is of any importance, don't have a corresponding setup to test) 2) Toss the AG to the other server and check if it gets any better.
&gt; I'm thinking that the problem is a logic error. yup you want this -- SELECT stuff FROM dates LEFT OUTER JOIN sales ON sales.period = dates.period UNION ALL SELECT stuff FROM dates LEFT OUTER JOIN specialsales ON specialsales.period = dates.period 
If you are using SQL 2012 or above it will be a VERY easy task: SELECT FORMAT(CAST('01/01/2016' as DATE),'MMMM') That will result in 'January' If not, you can make a lookup table with 1 column for the month number and the other with month name and JOIN it using MONTH(CAST(date_field as DATE)) in the ON clause.
It depends on what you mean. If you mean something similar to a human's age relative to their birthdate, then you need to calculate it the same way humans do. You're age increases by one when reach your "birth" day. Or, your age is the difference in years; if you haven't passed the month and day of your "birth" day yet, subtract 1 from that. SELECT DATEDIFF(yy, @BirthDate, GETDATE()) - CASE WHEN (MONTH(@BirthDate) &gt;= MONTH(GETDATE())) AND DAY(@BirthDate) &gt; DAY(GETDATE()) THEN 1 ELSE 0 END `@BirthDate` here could be any date of significance. `GETDATE()` is whatever target date you want to determine the age on. Obviously, this code is from SQL Server; you may need to use `EXTRACT()` or whatever functions your RDBMS provides you. You'll need to consider exactly how people "born" on Feb 29 increment their age. For human birthdates, there seems to be a split between people who use Feb 28 and Mar 1. The above formula doesn't increment age until the day is Feb 29 or Mar 1. Note, too, that this is only western age reckoning. If you're someplace that prefers East Asian age reckoning, you'd have to change the algorithm. Note that you should *never, ever* use rules of thumb like 1 year = 365.25 days for this sort of thing if you need anything resembling accuracy. Doing that only guarantees that your algorithm *will* be incorrect for nearly every person for a certain period of time nearly every year around the "birth" date. No, increasing the accuracy of that to 365.2422 years will not help. When we talk about age, we almost exclusively mean the age based on the calendar, not the age based on the solar year, and the two don't match. If you mean something else entirely, such as the number of hours between date X and date Y, well, that's different. But that's not typically what people consider age.
If you're using PHP at all, use pdo to grab the data and use strtotime() to do the math
I'm using the 2016 SSMS to manage 2008 R2 databases, 2012/16 SSRS, SSIS and SSAS. I had previously done the same with SSMS 2014 minus the 2016 instances and also had no issues. I don't use the graphical editors or designers much though. The only place I've run into backward compatibility issues was SSDT-BI, and even those are looking to be fixed in the new 2016 VS 2015 based version.
Dates table left joined to the result of the full outer join between sales and specials should work (once you wrap it in a subquery/cte). You can perform similar logic with 2 left outer joins to the dates table and add a where condition eliminating local Cartesian products of sales and special territories. Something like this: Select d.period, s.amt, ss.amt From dates d Left join sales s on s.period =d.period Left join specials as on ss.period = s.period Where Coalesce( s.territory, ss.territory, '') = Coalesce( ss.territory, s.territory, '')
I'm not sure if your DBMS supports it but I'd look at doing something like this... SELECT * FROM myTable WHERE MONTHS_BETWEEN(CURRENT_DATE,myTable.dateColumn) / 12 &gt; 3;
Anyone have the rest of this article, bs having to sign in to read it...
It sounds like it'd be a massive pain to use and query, and I don't imagine that you'd have any advantage over using a single table that was properly indexed. The actual physical space on disk the database uses is already fairly well optimised by the design being done right and tuned by a dba who knows what they're doing. Unless they're some advantage I'm not seeing you're just making code less maintainable and limiting the db engine's ability to optimise just because it might be slightly easier than actually lrn2database
This is what partitions and indexes are for, isn't it? 
Thanks for the advice! This is similar to what I ended up doing...I used nvarscar's suggestion to just made it a datetime then get the month out of it. Was overthinking the problem =).
I don't know BIRT at all, but... dynamic SQL? Column ordinal positions are decided by their order in a SELECT. E: May be the wrong route, but if that's all your parameters do then I'd try using dynamic SQL to select columns in whatever order from a subquery.
it works damn it haha, i cant use the field index number like group by 1? What if i have multiple aggregates and multiple non aggregates? Also thanks for this i was losing my mind knowing it was super simple
For those viewing this thread in the fuuuuuuuuuuuture. We had all the pieces lined up for essentially two instance based BAGs. However, because we use Veeam as our backup software, we can't use Physical RDMs. Physical RDMs appear to be a requirement for the AlwaysOn Failover clustering per https://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1037959 When Veeam eventually releases an enterprise level physical server backup product (as in I can run it on a VM direct) I'll reinvestigate this, though I'll also hit the bosses up for a few grand to get 4 more core licenses and have a less beefy failover system. Thanks for everyone's input. I now return myself to the land of SysAdmins, crawling under datacenter floors for power, through the ceilings for fiber, and standing under the AC in order to stop sweating.
The fact that SQL Studio &amp; Excel don't play well together is frustrating and often times feels intentional by Microsoft. One of the many less-than-perfect ways to solve the problem is to have SQL strip all tabs out of your data before exporting to Excel using the REPLACE() function: REPLACE(ColumnName, ' ',' ') This creates a bit of data loss, but is the quickest way you're going to get Copy &amp; Paste working from SSMS to Excel.
I've found this to be the best approach
Also... when you *do* refer to something by number you're referring to it's position within the select, *not* it's field index number in the table, so you'd have wanted SELECT SUM( Quantity) -- First , Customer -- Second FROM myTable GROUP BY 2 Though overall it's poor coding to use the number instead of the field name as it'll bite your butt as soon as you have to add another field.
Can you query the data directly from Excel using Data ribbon &gt; Get external data &gt; From other sources &gt; From Microsoft Query?
&gt; VLOOKUP Index/Match for life.
Vlookups are learned before index/match and is usually the one that shows people the benefits of excel. But yes, index+match ftw!
I imagine at some point she wished her pivots had additional or different data. If she can query the tables herself, she can pull whatever she wants however she wants and pivot off that. Also, "business SQL" shouldn't be lumped in with general programming. You can do a lot with simple select statements that read like English and require little knowledge of most programming principles. 
I think that is hard to say as everyone learns different. There is not one way that is the easiest. In the beginning, I liked watching videos, but now I go for books.
Bro this isn't a game of Jeopardy, you don't have to phrase your book promo in the form of a question.
This is true. But she can have like basic data reader capabilities and use Microsoft Query in Excel. I mean depending if sensitive data is in the database tables or not. We do that at my work so I can fuss about in Power Pivot and Excel with data. Saves a ton of data dumps from ERP into Excel which users seem to love to do, instead we can build a report in another of those forms and just refresh. Such awesome stuff!
Thanks. 
+1 for Robert Half. A recruiter there just found me a great job. 
To add to what user johntarded said, also look into openquery statements.
Not valid SQL. title and stars cannot appear in the SELECT as they don't appear in GROUP BY. There would need to be an aggregate function around them.
Some databases allow that notation and some don't. For instance MSSQL does not but Teradata does. This is one of the reasons we request you flair your platform. 
Upboat from me, ugly but workable. Depends also on how you account for 29 Feb. You can get the same result with less farting about using datepart dy rather than month then year.
Which makes me wonder where OP's getting that answer.
That answer isn't the right answer, it's not even valid SQL in most platforms.
I work in a call center for a major telecom. Our area and HQ consultants use SQL and have an internal site to host it all. They have some Web based tools for pulling data or pre-made reports. Us in the center use that data and put it in excel for our leaders. They used to just have a repository with a bunch of raw data but they changed that last year and it is becoming more centralized. It is both good and bad. Good for centers that didn't have dependable reporting. Bad for us because we are so limited in what we can do now. I have to make data sheets out of their pivot tables to get useful information for a number of things. 
I didn't think that the order and group by would do any aggregating. I've been enlightened. Thank you. 
A combination of tools developed internally, Tableau, and some "legacy" Crystal Reports.
I need to see more data as well as what you expect the final product to look like. I just don't understand.
I work at a healthcare facility. We have a data team, which I've just recently (within the last few months) become a part of. There are 3 of us - one doctor, one data analyst which I'll call the expert, and then me. I would call myself a "junior analyst" or an "analyst in training." Most requests go to the expert, and then he delegates some of them to me. The doctor is the team leader and helps out a ton with validation, and of course choosing which projects take priority, etc. We use COGNOS eBO, BridgeIT, and have started with Tableau and Relevant for Healthcare. We are also in the process of learning SQL, which is a lot of fun (and why I joined this /r/. The only requests we don't handle are anything to do with billing/finance, which is handled by an individual in that department. 
Thats all the data that should be needed. Wanting to trace the tree back so for person 1 you could get. Child_ID|Parent1_ID|Parent2_ID|Level ------|------|------|----- 1|2|3|0 2|4|5|1 3|6|7|1 4|8|9|2 5|10|11|2 6|12|13|2 7|14|15|2 So level 0 would be the person that we're looking backwards through Level 1 would be level 0's Parents Level 2 would be level 1's Parents All the way back. Some will have null values for one parent as well 
University. Reporting is centralized within the IT department. There are pros and cons, but the biggest benefit is the ability of the reporting team to build and maintain an enterprise data warehouse. If it were distributed, that wouldn't be possible.
Adding an OR to your join clause seems to return what you need: WITH FamilyCTE AS (SELECT Child_ID, Parent1_ID, Parent2_ID, 0 AS Level FROM #Children WHERE Child_ID = 1 UNION ALL SELECT F.Child_ID, F.Parent1_ID, F.Parent2_ID, Level + 1 FROM #Children AS F INNER JOIN FamilyCTE c ON F.Child_ID = c.Parent2_ID OR F.Child_ID = c.Parent1_ID ) SELECT * FROM FamilyCTE ORDER BY Level, Child_Id;
We sorta roll our own with Excel and Asp.net Web Forms. Nothing fancy, but gets the job done. Basically some DataTables binded to GridViews and some console apps that query and create spreadsheets. Also added charting to the web forms site, so that's neat.
BI? Similar setup here, we use a lot of Tableau for the final link of the reporting chain. Really happy with it. You need some SAS or good SQL databases to feed the data to Tableau tho. 
You can use rank to create and additional column in a sub query and use it to compare with the value in needed. I'm from my phone, so I can write you the query
Well typically a 'thing' would be made out of more than one type of widget, rather than the same one over and over. Your example wouldn't cancel out the possibility of widget overlap, either. If you want to send a number of samples of the same thing, I'd list it once with a quantity.
That did it. Thank you.
That'll do it
 A good book wouldn't hurt, but you can find most tutorials and examples online. I like [this one.](http://shop.oreilly.com/product/9780596007270.do) You can get a free database manager and sample database to work with. http://www.sqlitetutorial.net/sqlite-sample-database/ https://msftdbprodsamples.codeplex.com/ http://www-03.ibm.com/software/products/en/db2expressc http://www.mysqltutorial.org/mysql-sample-database.aspx 
if you're on MS SQL, there's a cross apply version that's going to be quite a bit faster.
Microsoft SQL Server 2016 Developer is free so no point going for SQL Server Express anymore.
The company I work for has this odd situation where everyone uses a third party tool to query accounting/financial data from Oracle. The tool is an Excel add-in "Report Wand" or "GL Wand". The license is expensive and not everyone has it. I'm not sure how/why everyone started using it.
Microsoft SQL Server 2016 Developer is the way to go then? Free and I can learn basics with it?
I learned with a text book, but youtube videos are good too. I think a mixed approach is best. Text books for more long-term foundation... videos to get you going faster I guess. There are some youtube videos called "Wise Owl" for SQL. Those are good. I didn't use them for basics, but I think they have some. I used them to help me learn SSRS
You could download the express version of MS SQL server from the Microsoft website. It's free.
Pretty sure that's Windows only.
I'd recommend these two: [SQL](https://www.udemy.com/the-complete-sql-bootcamp/) [Python](https://www.udemy.com/complete-python-bootcamp/) I've taken some of his other courses and they're good quality, and very complete. You can also usually find [codes for udemy for additional discounts](https://m.retailmenot.com/view/udemy.com), they're almost always on sale. Pluralsight is also great, but the courses there tend to be a little less comprehensive, and the good ones for SQL are often on much more specific topics. They also don't answer questions as well as the udemy instructors seem to.
As you've noticed, MS SQL products are Windows only at present. The most straightforward approach to running them locally would be installing a windows virtual machine (VMWare, Parallels, VirtualBox) on your Mac. You might also look into Microsoft's Azure services, and run your software remotely. They offer some free use credits to new accounts.
For instance one of the tables has prices, and the correct (current) price for a product is the latest one. If a new product is added, its price is initialized to 0.
Yep, garbage. Even if you enable "Quote strings containing list separators when creating .csv results" it doesn't double up the quotes or handle linebreaks in fields. Unfortunately the only way around that is to format each field inside your query: WITH CTE AS ( SELECT '1,' A,'2' B,'3 four' C UNION ALL SELECT '1', '2 3 "four"', '4' ) SELECT '"' + REPLACE(A,'"','""') + '"', '"' + REPLACE(B,'"','""') + '"', '"' + REPLACE(C,'"','""') + '"' FROM CTE The CSV you get from this should never fail to import into Excel. Make sure that "Quote strings containing list separators when creating .csv results" is unchecked.
Try changing the syntax of max(addeddt) As maxdate to maxdate = max(addeddt) I embed SQL in excel all the time and use this syntax (or drop the "As" altogether) without issue.
I tried Parallels on a trial basis about a month ago and it basically made my Mac unusable. It kept locking up. I've got a late 2012 iMac, so it's not *too* ancient. I'm not sure what was going on with that. I may just end up getting a refurbished Windows machine in the $300 range just to use as a test/dev machine.
Do you have trouble using [square brackets] like: as [Column Name] I frequently run into issues where Excel craps out on an otherwise perfectly fine query... I tend to use brackets but will try this weird old form in the future to see if it resolves any of my problems.
Because one of them is actually new?
Do you have any log maintenance jobs running regularly? If not, you need one. You are running in FULL Recovery mode, but are not taking regularly backups to clear the log, thus its filling the drive. Try these scripts: https://ola.hallengren.com/ 
... until he gets a failure ..
 sum( case when c.hiddenPlaceDifficultyLevel&lt;1 then 100 when c.hiddenPlaceDifficultyLevel&lt;2 then 200 when c.hiddenPlaceDifficultyLevel&lt;3 then 300 when c.hiddenPlaceDifficultyLevel&lt;4 then 400 else 0 end ) ? also I don't think you need the group by as they are not part of the select statement.
You'll need to use a LEFT or RIGHT JOIN to the table you want all the results and then use the coalesce (or ISNULL if using SQL Server) in the field from the table that may not have value to make that null value appear as zero.
db2 schema is one table with the following fields for both billing and provisoned Customer, Quantity, Item, Unitprice bililng query is like this select sum(quantity), customer, item from table group by quantity i basically use the result set of this query and compare the sum number to whats in the provisoned dataset(on a separate server) the issue arises when an item that isnt in the result set is in the provisoned data set because im not actually checking for that product
Do it with bitmaps. Create a bitmap for the set of products that are provisioned. Create one for the set being billed. Now you can use bitwise operators to find the differences one way or another and easily generate discrepancy lists.
Centralisation of expertise and best practise aka BICC has been the way forward for quite a while. Difficult to achieve because of empire building and technology fundamentalists.
If you have all products in a table start there. Left join to your orders or billing table and if a product doesn't have a bill or order it will still show up as opposed to an inner join.
If you use LEFT/RIGHT JOIN, the data that doesn't exist will appear as NULL instead of not at all. The query will be something like this: SELECT COALESCE(B.quantity,0) as BillingQuantity, ... FROM Billing as B RIGHT JOIN Provisioned as P ON B.item = P.item That query will bring all the items from Provisioned , even if there is nothing in the Billing and in case there is nothing in the Billing the quantity will be changed to 0 instead of NULL
First group up your products into max 63 members. If you have more you'll need multiple bitmap columns as you can only practically get that many bits from a bigint. Now assign a bit number to each product 1-63. Now calculate bit value as 2 to the power of bit position. Now select the sets of Products grouped by account or whatever. Sum() all the bit values. That is your bitmap. Now you can select a bitmap for a customer versus their provisioned bitmap and compare. BITAND will return only the common products, XOR the difference etc. Join back to products by bitmap AND bitnumber &gt; 0 It sounds like a complicated way of doing it but believe me, it is very powerful/flexible. 
Oops sorry reply in another comment
the problem is i cant query provisoned because the data exists on a seperate server that i can only request exported csv files for now, so ill have an excel sheet of customer, item, total quantity provisoned and i use billing query: customer, item, total quant billed so if in my billing results a specific one of our products isnt there i wont include it in the vlookup/suimf or w/e i use to check provisoned, if theres 1 bililng it shows up and i can check but if there arent any for a specific product that product doesnt show up as null or 0 or anything
Make some ETL process to import the CSV to your DB and then JOIN it. Your process is inverted, you shouldn't get the data from your DB and work in a excel sheet, you should get the data from the excel to your DB and then work there.
I'd solve this by putting your record set into a temp table, then inserting into that temp table a series of sub queries cross joined together. On phone so formatting will stink. Insert into table Select a.customer. B. Category 0 as cost from (select distinct customer from table) a cross join (select distinct category from table) b
This would be about 1000 times easier if you'd post your existing query, some sample data, your current output, and your desired output. My guess is that you'll have to do something like: SELECT COALESCE(B.Customer,P.Customer) AS Customer ,COALESCE(B.Item,P.Item) AS Item ,COALESCE(B.Qty,0) AS BilledQty ,COALESCE(C.Qty,0) AS ProvisionedQty FROM ( SELECT Customer, Item, Qty FROM Billed ) B FULL OUTER JOIN ( SELECT Customer, Item, SUM(Qty) FROM Provisioned GROUP BY Customer, Item ) P ON P.Customer = B.Customer AND P.Item = B.Item WHERE COALESCE(B.Qty,0) &lt;&gt; COALESCE(C.Qty,0) ORDER BY Customer ,Item If that's not what you want, then you're going to have to provide some examples. Edit: Just saw that your provisioned data is not staged in the database that has your billing data. That's your first problem right there. You're trying to get things in your output that do not exist in your input. Unless you can magically generate the Customers or Items with, for example, a Numbers auxilliary table, you've got to import your data first. You can't output data that doesn't exist. It needs to be in your database to show up in the result set. Your alternative is going to be to fully duplicate the logic of a FULL OUTER JOIN in your script or code. 
I bet you did. http://www.philosophyexperiments.com/wason/ /pat.
Ultimately I just want to keep my query skills sharp. I did basic to intermediate queries at my last job, but also used a lot of more advanced, pre-written queries done by others. And really, I guess it doesn't even need to be MS SQL. I've got an upcoming interview with a shop that's Oracle based. One thing that's left me a little perplexed is how to set up a local server and db environment. I got SQL Pro, but I can't figure out how to set up a local server/database/table to get started.
You'll also want to consider reliability. A lot of companies like to do scheduled failovers, e.g. run one month on one node, then month on another. Also, one of the advantages of failover, is that you can install patches on the secondary node, failover to it, and see if it affects operation. So for failover cluster, I usually recommend two, decent servers that match or come pretty close. Your situation sounds closer to Disaster Recovery. Nothing wrong with that, it just depends on what your goals are: high availability, recovery, or both.
We have used sql clustering for disaster recovery on 2 unlike servers. Dell Poweredge R810 on the primary node, Dell Poweredge R610 on the secondary node. Half the CPUs and half the memory on the secondary node. It works ok, just keep in mind your performance is not going to be the same if you migrate to the secondary node. Otherwise no issues at all.
It's definitely a mixture of both that we'd be going for, but I think the differences between the two would be too great (even though it might get the job done). For example, would settings such as Min/Max server memory, Max Degree of Parallelism or Processor Affinity be carried during a failover? Those are the kind of specifics I haven't been able to find anywhere. The new server would have twice as much RAM (192GB vs 80GB) and considerably more processing power as well, which would obviously mean those settings wouldn't be optimal.
&gt; It sounds like a complicated way of doing it but believe me, it is very powerful/flexible. This is potentially the most difficult to maintain idea ever. So many updates have to happen any time a product is added or removed from the companies line up, and of course someone coming into it cold is going to have a huge learning curve trying to figure out what is going on. This is a horrible idea.
&gt; Min/Max server memory, Max Degree of Parallelism or Processor Affinity Good question. I'm pretty sure these are server based settings, and wouldn't carry over with the database. 
Homework, i guess? Look into foreign keys and cascade rules.
how will that help? 
My expectation is that information will be relevant to your "how can I write the previous paragraph into an SQL statement" question.
Your understanding of foreign keys and cascading rules is pretty far from complete. Especially if you're trying to improve your SQL - it'll behoove you to understand basics (referential integrity in this case). 
There could be many times where the information from a single source would be incomplete or inconclusive in your quest to improve your SQL. Ability to learn, pursue the answers and expand your knowledge by exploring concepts and deep-diving into specific topics will be superbly beneficial. Hopefully this was enlightening and motivating?
Ive been stuck on this problem for a long time now and neither my experience nor the internet has provided me of a way of solving this problem. i decided to ask reddit becuase I knew somebody would be able to help me. What ive learned in school is only the tip of the iceberg but that tip is not enough for me to be able to solve this problem. So instead of telling me that i need to go learn it why dont you actually help me to solve this.
 SELECT strftime('%Y%m', datetime(audit_date)) month_year , cable , brand , runtime_min , COUNT(*) unique_titles FROM ( SELECT DISTINCT , title , brand , audit_date , runtime_min FROM audits WHERE audit_date &gt;= '2016-01-01' AND audit_date &lt; '2017-01-01' AND cable = 'Charter' ) AS d GROUP BY month_year , cable , brand , runtime_min 
I hear you, internet is a terrible thing. These links might help: https://msdn.microsoft.com/en-us/library/aa292166(v=vs.71).aspx http://www.functionx.com/sqlserver/Lesson26.htm https://www.mssqltips.com/sqlservertip/2743/using-delete-cascade-option-for-foreign-keys/ 
I've benchmarked both: 2015 Maxwell Titan Xs: http://tech.marksblogg.com/billion-nyc-taxi-rides-nvidia-titan-x-mapd.html 2016 Pascal Titan Xs: http://tech.marksblogg.com/billion-nyc-taxi-rides-nvidia-pascal-titan-x-mapd.html
OK, this is what we figured out. Table B has a shared ID field that ties in the A_ID and C_ID fields. So, in other words, I need to join three tables on multiple rows in the same query. Example: **Table A** A_ID | Other Fields **Table B** B_ID | A_ID | NULL | Other Fields B_ID | NULL | C_ID | Other Fields **Table C** C_ID | Other Fields I need to get the information in A and C, using the relationship defined by B_ID, but where the IDs that tie into tables A and C are in different rows. I hope that makes sense. Anyway, I'm not sure quite how to do that. Suggestions?
I finally discovered the relationship between the data in each table, and it's a little...complex. Here's the reply I did to another comment on this post: https://www.reddit.com/r/SQL/comments/4x2by2/query_help_for_tsql/d6k1o49
Depends on the selectivity and distribution of values. If your [date] index has better selectivity then a separate index on campaign will not be used. If your [campaign] selectivity and distribution is better, then the index on the [campaign] is going to be used. You can create a multi-column index on both [date] and [campaign].
It think so cause i dont want to actually remove the groups from the table just stop them from showing up when the SQL statement is run.
Looks like your coworker probably used the Pivot command - like this select name, isnull([X], 0)as X, isnull([Y], 0)as Y, isnull([Z], 0)as Z from (select name, unit, qty from [raw data table] ) as s PIVOT ( sum(qty) for [unit] in (X, Y, Z) ) as pvt
That sounds like the perfect use for NOT EXISTS 
Okay! So I've discovered something about MSQUERY. It's the same language that SSRS uses and MS Access. So I used the "IIF" function to pull out 0 calls. That took me a league ahead. But I also discovered that if I'm doing something under a HAVING clause other than dividing- it won't take it! So rather than "HAVING SUM(CALLS)&lt;50 for instance- I have to use "HAVING SUM(CALLS)/1 &lt; 50... Then it works fine.
If I create a multi-colum index such as [date]+[campaign] and I use syntax such as: where campaign = 'n' and date between 'n' and 'n2' Will the multi-index meeting still function apprioriately?
Thanks for your opinion. It really isn't that hard to undertake or maintain. Yes it is not a newbie approach and requires at least intermediate SQL skills but that is about it.
With a tool like MS Access it's hard to say for sure, but I would think your syntax would have higher performance than running several subqueries.
Mainly scalability. Probably overkill for OP having read more comments! When I implemented this we were tackling a product/customer cardinality of about 13 billion rows. By using bitmaps we were able to pull this off with late 90's servers on MSSQL 7. I regret even bothering to comment now tbh. On low volumes I'd go for SELECT customerid, item FROM billing EXCEPT SELECT customerid, item FROM provisioning 
I disagree both on the undertaking and the maintenance. It's an incredibly "neat" way of doing it, but also a completely useless methodology in a modern business application. If any programmer on my team brought up such an idea he'd be laughed out of the design meeting. 
In returns a full set. Exists stops if even one matching record is found. Therefore you can (rough cut.. ) SELECT &lt;stuff&gt; FROM Group INNER JOIN enrolled_students_class esc ON Group.class_id = esc.class_ID INNER JOIN enrolled_students_group esg ON esg.class_id = esc.class_ID WHERE NOT EXISTS ( SELECT 1 -- Is this student already enrolled in a group for this class? FROM enrolled_students_group iesg WHERE iesg.class_id = esc.class_ID AND iesg.student_id = esc.student_id ) AND Student_id = &lt;valiable&gt; -- If you only want a report for one student If there's a single record in the enrolled_students_group inner select, then you won't get a group row for that class. Edit: wasn't sure if you wanted to list all students and groups where the student has not selected a group, or if you want to pull this information for a single student.
thank you, my original SQL statement was way longer and didnt work correctly, Ill try this out later.
to download? to effectively use?
SQL server central has a good stairway to SSIS. 
Forensic Science field. We have a custom built reporting tool that sits on top of BIRT. Any of our reoccurring reports are done through this. Since we're in a pure sciences industry, and have a large role in R&amp;D, we have a large number of the requests for ad-hoc data pulls. We handle the development of all the proprietary research/business software in the company, and we've taken on the role of database administrators and reporting/BI as well. We wear a lot of hats :/
[Code School](https://www.codeschool.com/learn/database) now has a SQL course. From my understanding, it allows you to learn the basics of the SQL language without having to go through the hassle as a beginner of setting up a specific database and creating tables. My wife used it as supplementary reference for a college course she was taking last semester. I'm privy to PostgreSQL Database, however I do most of my professional work in Oracle (which I don't suggest for a beginner). PostgreSQL is easy to get installed and setup - and it's documentation is REALLY good, so once you get the hang of the basics of SQL you should have no problem. As you advance, if you decide to dive deeper into database development, and not just reporting - a good entry level database design book that I like is [Database Design For Mere Mortals](https://www.amazon.com/Database-Design-Mere-Mortals-Hands-/dp/0321884493/ref=sr_1_fkmr1_1?ie=UTF8&amp;qid=1471411680&amp;sr=8-1-fkmr1&amp;keywords=database+architecture+for+mere+mortals)
In your world yes. In the real world no. Thank you for your opinion. 
[removed]
 I need a column for QuoteSales, meaning the $ amount from my Sales table that went towards Items that were purchased by a customer after being quoted, and then my QuoteQuantity. Then I would obviously have my columns for my Customer and ItemId The joins would look like this: FROM sales as s JOIN customers as c on c.customerid = s.customerid JOIN items as i on i.itemnumber = s.itemnumber JOIN quotes as q on q.itemnumber = i.itemnumber
The postgresql wiki has lots of papers on the subject. Plus you can read their database source code if you want to go real deep.
suddenly, a fourth table appears and you've neglected to mention the column(s) that will enable "within two weeks" to work also, you can't just join quotes to items on the item number only, because then you'd get items from ~any~ customer
I'm assuming that there is one pair of rows in B for each value of B_ID. If not, then this is not really a join table, and you'll probably want to explain more of what this table is doing (i.e., obfuscate less). WITH BA AS ( SELECT * FROM B WHERE A_ID IS NOT NULL ) , WITH BC AS ( SELECT * FROM B WHERE C_ID IS NOT NULL ) SELECT * FROM A INNER JOIN BA ON BA.A_ID = A.A_ID INNER JOIN BC ON BC.B_ID = BA.B_ID INNER JOIN C ON C.C_ID = BC.C_ID 
You need a GROUP BY when using SUM like this. Use SUM as a window function: DifficultyLevel = sum( [case when then else end] ) OVER () Or move the case when to the join of c, and use that column in the window function.
It'll return all campaigns where the campaign ID is any one of those listed.
That is how B is set up, yes. I haven't seen this before, so please confirm that I'm understanding it correctly. In your example, "BA" and "BC" are simply names for temp tables that contain the data in each SELECT. Then, at the end, we join A, C, and both temp tables together to get the information we want. Does that sound right?
Yes, but they're not temp tables, just common table expressions (CTEs). You could do the same thing with subqueries: SELECT * FROM A INNER JOIN ( SELECT * FROM B WHERE A_ID IS NOT NULL ) BA ON BA.A_ID = A.A_ID INNER JOIN ( SELECT * FROM B WHERE C_ID IS NOT NULL ) BC ON BC.B_ID = BA.B_ID INNER JOIN C ON C.C_ID = BC.C_ID I use CTEs because I find them easier to read. 
That's a good point. I hadn't considered that. 
Thanks. Same, it took me all morning. I just discovered that was the problem 2 minutes before you commented. It ran without error. That GO in there was like when you forget a ; in your code lol. 
I recognize OPENROWSET as something I saw when I was researching this stuff - I'll look into this further and see if I can do this myself. I do believe these XML are properly formatted. Your comment really helps me, because I've been pretty lost thanks!
You're closing the connection before reading from the Reader. Just move conn.Close() to below the drString+= dr.GetString(1)
it's not a natural join, because a natural join doesn't say which columns to use to perform the join
Tableau can handle big stuff like this...read-only though Tried LibreOffice base?
Thanks, Power BI looks cool. It seems like the hard part is finding something that will allow me to edit. I basically want to add a few blank columns and fill them out on certain records as I scroll through.
You won't get very far scrolling through 100+ million records and editing a few columns. Perhaps it would be easier if the context was better explained. 
The files are like 4 gigs a piece, broken up by year. It's a lot of data. Or I can use one much larger file that encompasses all years. I have installed Power BI cause I thought that might be the answer to my prayers. I connected the sql db to Power BI, but I want able to access the data. I think I tried the XML too with similar results. I will try mongDB. I hope the learning curve isn't too bad. Thanks for the tips.
Thanks, I will try this! I didn't know if I had provided enough info because I had just included the very beginning of the XML in my sample. There are probably many nodes(?) that don't appear in my code sample.
It's a matter of syntactic taste, however doing an equijoin (USING) is more restrictive, atleast in Oracle, and in any production environment can only be used in a select number of queries you'd be writing. Since the OP is a beginner, he should just stick to using ON. 
Power Bi can edit stuff... kind of. When you pull the data in you can "edit the query" where you can transform data, add column.. etc etc Its not the best though for that but you can kind of do it.
I'm using Oracle's SQL Developer. Just returned inside of the tool.
Honestly, i would simply use the basic of SQL. From what you are saying, simple JOIN, WHERE, ORDER BY would work pretty easily. Updating field also work pretty easily when you got the right where clause. 
Right click, rename.
Write queries to find the outliers.
Thanks. Knew the answer but wanted some independent confirmation. You my man.... 'cept you never weighed in [here](https://www.reddit.com/r/SQL/comments/4y2rgj/2008r2_substringcharindex_parsing_discussion/). That was a real question.
&gt; 15xt1029a If *only* wanted to see results from that campaign, how would this behave?
This is a shot in the dark - you may be able to use a language like C# to get this into the DB in the format you actually need it in. For instance, using XmlReader in C# and then pushing that data into the db (much easier since you have MSSQL) is almost as push-button as it gets. Obviously depends on your comfort.
When you get an error that says "column x is ambiguous" that means that you're using 2 or more tables that each have a column x and mysql doesn't know which one to pull from. You have a case of this in your select.
i just tried editing it but it's still not working, this is what i have right now, aggregating across an entire salesorder, there is something wrong with the having clause for sure select salesordernum from pluspgbtrans group by salesordernum having sum(eip_quantity * eip_unitprice) &lt;&gt; eip_lineprice
never mind the issue was that i wasnt summing eip_lineprice in haivng clause, that needed to be aggregated as well, thanks for your help though
I assume you have an index on [Event ID] in both tables?
Well, now Im thinking why I didnt try that. Well, thx, I guess its what i need. But I gonna check it on Monday (coz I forget to bring DB-example from work, lol)
Never hurts to have a second pair of eyes, especially after you've been bashing your head against something for awhile. :)
I use the term 'false' or 'synthetic' join to describe something like this: select a.value * b.value2 from ( select 1 as 'join' , sum(value) as Value from table ) a inner join ( select 1 as 'join' , sum(value2) as Value2 from table ) b on b.join = a.join
Couldn't this be accomplished through a UNION? Edit: missed the math in the select, nevermind. 
I've never heard those terms, I usually describe your example as joining derived tables.
Some places have coding standards that bar the use of CROSS JOINs. Like your code will be reviewed and rejected if it includes a CROSS JOIN, no matter how benign or beneficial it is. Other places have people who were told that CROSS JOINs were Bad™, so they avoid them like they're deprecated, vestigial remnants of a proto-language that grew into SQL. Kind of how people avoid `goto` in C. In both situations you end up with people jumping through hoops to accomplish a CROSS JOIN without doing a CROSS JOIN. Personally, I'd write the above query as: SELECT (SELECT SUM(Value) FROM Table) AS Value, (SELECT SUM(Value2) FROM Table2) AS Value2 
In SQL Server you can use SQLCMD mode to define a variable. :SETVAR ListOfColumns = "firstname,lastname,phonenumber" SELECT sum(s.amount - s.totalcost) as [GP$], $(ListOfColumns) from table1 table2 GROUP BY $(ListOfColumns)
Whenever I get errors like this I think back to https://m.youtube.com/watch?v=GsqUZkmO-zk Then I feel better. 
It's data warehouse not "dataware house" =) Do yourself and your BI department a favor and grab a positive-review book on Amazon that focuses on "business intelligence" and "architecture" and read through it. It may not give you exact steps to take but it will at least give you a good design pattern and reasons for doing so. Break data up into multiple databases as they have different uses. * Staging: changes very rapidly in small increments throughout the day but doesn't store history. * Dimensional: generates lots of reads and IO on the disks by cube processing. * Archive: stores all data that is old enough to not be relevant to questions asked today and should not be queried regularly or by analysts but might need to be referenced for future legal purposes. * Operational: contains a recent limited history of data from any and all systems of record for use as the "source" data in the ETL process. * Config: this can be where all of your audits, manager tables, package configurations, metadata, failure messages, and logging objects can reside. It can be locked down to a greater degree to ensure that no accidental changes are made to the data within this database. Within each of those databases, use schemas related to the source of the data or business process to help provide some sort of logical grouping for organization. Also, attempt to keep cross-database queries (and objects that use them) to a limit only use them in a certain manner (ie, stored procedures that read data from multiple databases must be created in the database where the table will be created). It will make it easier to maintain and migrate your system as it grows and takes on new data sets.
Midwest comes to mind.
Never seen the xp_cmdshell procedure - thanks for sharing that!
It's not on by default and can be a security risk depending on how you have your server configured but it's super handy for a lot of things. You can do things like read the files in a directory then backup/restore data depending on what you find. Or more recently I've been looking at using it to have a trigger write a row to an integration queue, then have a batch periodically sync data to an external CRM. As for a better Google Maps API I've reworked this version which doesn't rely on an external script, but can't handle response greater than 8000 characters: CREATE PROCEDURE [dbo].[GoogleMaps] @Address varchar(255) AS BEGIN SET NOCOUNT ON; DECLARE @OBJ int, @URL varchar(255), --@vStatus int, --@vStatusText varchar(200), @RES varchar(8000), @XML xml; SET @Address = REPLACE(@Address, '%', '%25'); SET @Address = REPLACE(@Address, '&amp;', '%26'); SET @Address = REPLACE(@Address, ' ', '%20'); SET @Address = REPLACE(@Address, ':', '%3A'); SET @Address = REPLACE(@Address, '=', '%3D'); SET @Address = REPLACE(@Address, '?', '%3F'); SET @URL = 'http://maps.googleapis.com/maps/api/geocode/xml?sensor=false&amp;address=' + @Address; EXEC sp_OACreate 'MSXML2.ServerXMLHTTP.6.0', @OBJ OUTPUT; EXEC sp_OAMethod @OBJ, 'open', NULL, 'GET', @URL, 'false'; EXEC sp_OAMethod @OBJ, 'send'; EXEC sp_OAMethod @OBJ, 'responseText', @RES OUTPUT; --EXEC sp_OAMethod @OBJ, 'Status', @vStatus OUTPUT; --EXEC sp_OAMethod @OBJ, 'StatusText', @vStatusText OUTPUT; EXEC sp_OADestroy @OBJ; SET @XML = CAST(@RES AS XML); SELECT XML.ROW.value('type[1]', 'varchar(255)') AS type, XML.ROW.value('formatted_address[1]', 'varchar(255)') AS formatted_address, XML.ROW.value('address_component[type="street_number"][1]/short_name[1]', 'varchar(255)') AS street_number, XML.ROW.value('address_component[type="route"][1]/short_name[1]', 'varchar(255)') AS street_name, XML.ROW.value('address_component[type="locality"][1]/short_name[1]', 'varchar(255)') AS City, XML.ROW.value('address_component[type="administrative_area_level_1"][1]/short_name[1]', 'varchar(255)') AS State, XML.ROW.value('address_component[type="postal_code"][1]/short_name[1]', 'varchar(255)') AS postal_code, XML.ROW.value('address_component[type="postal_code_suffix"][1]/short_name[1]', 'varchar(255)') AS postal_code_suffix, XML.ROW.value('geometry[1]/location[1]/lat[1]', 'numeric(10,7)') AS latitude, XML.ROW.value('geometry[1]/location[1]/lat[1]', 'numeric(10,7)') AS longitude FROM @XML.nodes('/GeocodeResponse/result') AS XML(ROW) END GO 
I see. You don't happen to have a handy example of how this works in theory?
/r/rstats 
Hi, I use SQL/R every day; SQL is core work and R is my preferred tool. I use R to automate alot of daily computation and internal reports that may include graphs. Sidenote, alot of our live metrics are read through Tableau. For me R is good as the middle man to connect different processes. Most people use Python, but I don't know Python very well. For example, some of our data is written out from SQL and distributed to clients. I have an automated R process (batch file) that calls an R script that connects to a database (RODBC) and reads in data, and on the side reads the files that were produced from the same database. It then compares them and if there is something unexpected, emails are sent (VBScript called from the R code) as a report, potentially with a graph that shows something is wrong. Oh and it also write to the database with all this information. I hope this helps. 
Depending on the quality of the table statistics it will choose the optimal execution plan 9 out of 10 times. Sometimes one query requires fiddling with query hints but it should be your last resort. 
thats interesting. I just downloaded R studio at work. I really like ggplot even though we have tableau and SSRS, i'm looking to start utilitzing ggplot more
Do you have any resources you could recommend on learning R/SQL/Tableau integration? I know SQL and I'm just getting started with R. We use Tableau and I would love to start doing some deeper analysis of our data and presenting it in Tableau.
Wow, hey, you're me. Same exact deal here—I use SQL Server and R every day, with Tableau for deploying stuff to other teams. Our regular automated ETL process involves using R to clean up new tabular data and dumping it into a SQL staging database.
The interaction between SQL and R is easy with RODBC, which I do all the time. The interaction between SQL and Tableau is even easier, since it's built-in to Tableau. The interaction between R and Tableau? Well, that's a lot trickier. I've had to do it a couple times, and it's not easy, but you can pass values in Tableau through to an R instance running on your Tableau server. You need to install Rserve first, which allows communication between the two. This page should help: http://kb.tableau.com/articles/knowledgebase/r-implementation-notes It's really kind of a last resort, though. I'd try to keep all calculations in Tableau or your SQL query, just because those are so much less of a headache.
Work. Product pricing data from a range of online retailers. 
See if this helps [MySQL ON DELETE CASCADE Deletes Data From Multiple Tables](http://www.mysqltutorial.org/mysql-on-delete-cascade/)
tourist in New York goes up to a local cop and says "Could you tell me how to get to Carnegie Hall?" "Sure," says the cop, "practise, practise, practise"
Try comp space %' ?
I think the space is a good solution. If you really want to do it with functions, the only thing I can think of would be: WHERE RTRIM(LEFT(course, 5)) = 'Comp'
Thanks!
Thanks!
I'm not sure what you're asking for? The specific syntax? You have it. --DECLARE @State TABLE ( -- [CSTOffSet] [int] -- , [TimeZone] [varchar](3) -- , [StateAbbrev] [varchar](2) --) INSERT INTO @State VALUES (-4, 'AST', 'AK') , (-6, 'HST', 'HI') , (-2, 'MST', 'MT') , (-2, 'MST', 'ID') , (-2, 'MST', 'WY') , (-2, 'MST', 'UT') , (-2, 'MST', 'CO') , (-2, 'MST', 'AZ') , (-2, 'MST', 'NM') , (-1, 'CST', 'ND') , (-1, 'CST', 'SD') , (-1, 'CST', 'NE') , (-1, 'CST', 'KS') , (-1, 'CST', 'OK') , (-1, 'CST', 'TX') , (-1, 'CST', 'MN') , (-1, 'CST', 'IA') , (-1, 'CST', 'MO') , (-1, 'CST', 'AR') , (-1, 'CST', 'LA') , (-1, 'CST', 'WI') , (-1, 'CST', 'IL') , (-1, 'CST', 'TN') , (-1, 'CST', 'MS') , (-1, 'CST', 'AL') , (0, 'EST', 'MI') , (0, 'EST', 'IN') , (0, 'EST', 'OH') , (0, 'EST', 'PA') , (0, 'EST', 'NY') , (0, 'EST', 'VT') , (0, 'EST', 'ME') , (0, 'EST', 'NH') , (0, 'EST', 'MA') , (0, 'EST', 'RI') , (0, 'EST', 'CT') , (0, 'EST', 'KY') , (0, 'EST', 'NJ') , (0, 'EST', 'DE') , (0, 'EST', 'MD') , (0, 'EST', 'WV') , (0, 'EST', 'VA') , (0, 'EST', 'NC') , (0, 'EST', 'SC') , (0, 'EST', 'GA') , (0, 'EST', 'FL') , (0, 'EST', 'DC') , (-3, 'PST', 'WA') , (-3, 'PST', 'OR') , (-3, 'PST', 'CA') , (-3, 'PST', 'NV')
so basically its just about taking the multiple values and getting a list of them in the corerct format with the , ("","","") etc?
Why not just import the 10,000 rows to a table, then: insert into table select * from import But yes, if that is the method you are going to use then you will have to put them in that format. Good news is that you can use Excel to do this easily if you have the data in multiple columns, e.g.: | * | A | B | C | | :--- | :--- | :--- | :--- | | 1 | CSTOffset | TimeZone | StateAbbrev | | 2 | -4 | AST | AK | | 3 | -6 | HST | HI | | 4 | -2 | MST | MT | | 5 | -2 | MST | ID | Then you can type an equation in D2, such as: `="(" &amp; A2 &amp; ", '" &amp; B2 &amp; "', '" &amp; C2 &amp; "')"` Then copy it down the column, copy all 10,000 values over and *voila*. 
I didn't realize you could actually do this! I'm interning and I know SQL and R pretty well, but I've always thought of doing ETL as something part of or within SQL and MS BI tools. R is so intuitive with large datasets and omitting NULLs etc. Is doing ETL in R pretty common? 
I used a CTE and a case statement. The case assigned a value to each of the products (mouse, keyboard or both). Then only returning the rows where the sum was 3 or greater. ; with CTE as ( select customername, sales, productname, case when productname = 'keyboard' then 1 when productname = 'mouse' then 2 when productname = 'keyboard + mouse' then 3 end as records from customersales Where productname in ('keyboard', 'mouse', 'keyboard + mouse') Group by customername, productname, sales ) Select customername, sum(sales)as sales from CTE Group by customername Having sum(records) &gt;= 3 
Could you do something like SELECT CustomerID , CustomerName , KBMSales = SUM(CASE WHEN ProductName IN ('Keyboard','Mouse','Keyboard + Mouse') THEN Sales ELSE 0 END) FROM sales GROUP BY CustomerID, CustomerName HAVING MAX(CASE WHEN ProductName IN ('Keyboard','Keyboard + Mouse') THEN 1 ELSE 0 END) = 1 AND MAX(CASE WHEN ProductName IN ('Mouse','Keyboard + Mouse') THEN 1 ELSE 0 END) = 1 (that HAVING bit assuming you only want those which have *both* keyboard and mouse)
Doing the replace will invalidate the use of any indexes he may have. 
Kinda depends on the work, but you generally bill on your talent. I'm the exact same title and I bill out at 200-250 an hour. I think I'm a bit expensive though, and I'm backed by a boutique consulting firm. I'd think 160-200 would be pretty typical, but that rate might vary regionally. 
Where are you located? Do you always work on site or a mix of remote/on site? Do you specifically do developer work or Architect well?
Bingo. 
100 Euros per hour on a year-long fulltime contract in Amsterdam, Netherlands. 
Thanks, the explanation really helped. In this case where does the BI/Datawarehouse fits ? I'm trying to learn a bit about data analysis, maybe there's something that I could use in the company but I fear that I might break into the BI space
No this won't do. It'll only give me rows with "Keyboard + Mouse' as product name and their sales total
This seems to work partially. It'll sum up Mark's sales (keyboard, mouse, and keyboard + mouse) but I"ll play around with this. Edited: Got it to work, thanks!
Yeah sorry, it's most likely due to miscommunication. I only want customers and the total sales of only keyboard and mouse if they bought both keyboard and mouse but there's no "Keyboard + Mouse" under ProductName. So in this case I really only want Tom, 25 as my output. And yea as pointed out by /u/UpUpDnDnLRLRBA my CustomerID is messed up because I was in a rush to post this. The CustomerID should be same, which totally violates the primary key constraint if you created your table with primary key as CustomerID. Maybe this is a bad analogy that I used to get my help.
I have overstated my SQL abilities and my friend is now relying on me so solve this for them, but I'm mega struggling! Not sure how to get the total sales by employee and have that displayed in a new column. Any help would be great. Thanks.
Ohhh, thanks for pointing out my mistake with the CustomerID. I was in a rush to get this out for help. And yes it works wonderfully thank you! :D
Smells like homework...but giving you the benefit of the doubt. To help you down the path of solving the problem you want to look into the following subject matter. Inner Joins: Understanding what it does and how to create one. You will need to retrieve columns from both tables. Where clause: You will also need to filter certain rows out based on the criteria of age over 25 and where the date falls in the year 2013. Group By clause: You will need to group parts of the result because you need to Sum total sales. Having: You will need to also filter your groups because you need to return sales over 200. Order By clause: This will allow you to enforce order on the result. This should get you started. 
And age &gt; 25 Order by should be Order by amount desc On mobile so I cannot edit and cannot see the question at the same time. All the above is pseudo code! 
Reformatted your select and pointed out some issues. SELECT Employee.Name , Employee.Age , Sales.Value -- This isn't the total , Sales.OrderDate -- Nobody's asking for this FROM Employee INNER JOIN Sales ON Employee.[EmpID] = Sales.[EmpID] WHERE Employee.Age &gt; 25 -- removed unnecessary parenthesis. Looked way to Access like. AND Sales.OrderDate &gt; #12/31/2012# -- This works AND Sales.OrderDate &lt; #1/1/2014# -- it is just awkward GROUP BY Sales.EmpID -- you can't group by a field that isn't even in the select HAVING SUM( Sales.Value) &gt; 200 ORDER BY Sales.Value DESC How I'd do it.. . SELECT Employee.Name , Employee.Age , SUM( Sales.Value) -- You're supposed to return the total, not each sale FROM Employee INNER JOIN Sales on Employee.EmpID = Sales.EmpID WHERE Order_Date BETWEEN #1/1/2013# AND #12/31/2013# -- MySQL Between is inclusive, this adds clarity. AND Employee.Age &gt; 25 GROUP BY Employee.Name -- Need to group by all fields you're , Employee.Age -- not using a group function (sum, min, max, etc) on HAVING SUM( Sales.Value) &gt; 200 ORDER BY SUM( Sales.Value) Desc 
CONVERT and similar functions are notoriously slow integer arithmetic is much faster -- WHERE timestamp &gt;= DATEADD(yy, DATEDIFF(yy, 0, GETDATE()), 0) this relies on the fact that MS SQL stores dates as integers, and therefore has a "zero date" which i believe is Jan 1 1900, although the exact date doesn't matter, as it's factored out of the formula see http://zarez.net/?p=2476 
You forgot -1 though. And I don't think that a single calculated value would make any difference.
yes, `'yyyy-mm-dd'`
All tools that are not in either location 2 or location 3 essentially. so if it IS in one of those locations i dont want to return the tool at all (even if it is in location 1 as well)
&gt; All tools that are not in either location 2 or location 3 essentially. SELECT tools.tool_id FROM ( SELECT DISTINCT tool_id FROM location_inventory ) AS tools LEFT OUTER JOIN location_inventory ON location_inventory.tool_id = tools.tool_id AND location_inventory.location_id IN ( 2 , 3 ) WHERE location_inventory.tool_id IS NULL 
(( check your (parentheses) ...
are they? perhaps you could show the actual script, then also, MySQL doesn't have error messages like "there is something missing between 'When' and 'Year'" -- they always use the word "near" which indicates where syntax analysis stops
Your username should've been "IPostCourseListsEverywhere"
Thats odd cause neirther of /u/UpUpDnDnLRLRBA 's examples give just Tom 25? I see where the confusion was now. I thought that Keyboard + Mouse was a separate type of sale that was a KeyBoard and a Mouse. So 23 sales of Keyboard + Mouse would be 23 Keyboards and 23 Mouse. So I was trying to do the same for others. But I see now its a summary of 12 Keyboards and 11 Mouse and some users have the summary line and some users don't and you want to find all the users that don't already have the summary line and create it. So see the below: with ProductSales as ( select * from (values (1,'Tom','Computer',100), (2,'Tom','Keyboard',10), (3,'Tom','Mouse',15), (4,'Tom','Earphones',8), (5,'Mark','Computer',150), (6,'Mark','Keyboard',12), (7,'Mark','Mouse',11), (8,'Mark','Keyboard + Mouse',23), (9,'Alex','Earphones',10), (10,'Andy','Computer',150), (11,'Andy','Keyboard',8) )ProductSales(Id,Name,Product,Sales) ) select name, sum(sales) KeyboardOrMouse from ProductSales where Product in ('Keyboard','Mouse') and name not in (select name from ProductSales where product = 'Keyboard + Mouse') group by name having count(distinct product)=2
With name not in (select name from ProductSales where product = 'Keyboard + Mouse') you're excluding Mark, who also has lines with the keyboard and mouse separately.
Only because thats what he said he wanted to do. He said he only wanted Tom 25 as his results? Though given that this is his made up example I'm not really sure if he is sure what he is after either ;)
you're not overly specifc. but you can use variables. set @location = 'house of commons'; select name, contactNumber from staff where location = @location;
Yeh, i still don't understand. Thanks though
To enter data? Are you typing in the editor or do you expect this to run I some application a client can access? I set the location in my original answer. 
OFFSET FETCH is the way to go, which – holy crap! – I just realized I now have access to since starting a new job. (Thanks again, u/varscar!) In the past, the very best high performance paging operations that I could build involved using the ODBC driver directly and setting various row &amp; page limit options before executing the query. Turns out, the optimizer looks at those options and produces much better results than any of the paging hacks built on ROW_NUMBER() or TOP / NOT IN() 
Thanks for that. That is slightly faster but not hugely. Maybe 200 milliseconds. I'm afraid I'm not an SQL expert, so not sure how exactly to turn the UDF into something else that works. There's a fair bit of processing that happens in some of the functions and I've never figured out how to do that 'inline' in the stored procedure. By computed column do you mean create a Title in a field for the item when it's added in the database? Is SQL able to do calculated columns - I've never come across this. Is it worth me wrapping the SELECT FETCH inside another SELECT * in order to perform the functions outside of the FETCH, like I did after posting the OP and saved 14 seconds? Is this faster or does the fetch get the 10 records and then apply the functions?
I think that answer to all of those is I don't know what I'm doing. I tried to run your query like this and got and error: create table #temp (d datetime) SELECT TOP 17000 DATEADD( MINUTE, DATEDIFF( MINUTE, GETUTCDATE(), GETDATE()), DATEADD( s, CAST( LEFT( update_date, 10) as INT), '19700101 00:00:00:000')) FROM t_gc_containers INTO #temp SELECT * FROM #temp Msg 156, Level 15, State 1, Line 5 Incorrect syntax near the keyword 'INTO'. 
Try in a new session: SELECT TOP 17000 DATEADD( MINUTE, DATEDIFF( MINUTE, GETUTCDATE(), GETDATE()), DATEADD( s, CAST( LEFT( update_date, 10) as INT), '19700101 00:00:00:000')) INTO #temp FROM t_gc_containers SELECT * FROM #temp The select into has to occur before the from clause. When you do a select into from statement, you do not have to create or declare a table, it will create the table for you. Indicating the # before the name will make it a temp table. If you re-run the statement in the same window, it will believe it currently exists even though it does not, so it can help to do a where exist then drop if needed temp table clause prior to the statement if running multiple times.
SELECT TOP 17000 DATEADD( MINUTE, DATEDIFF( MINUTE, GETUTCDATE(), GETDATE()), DATEADD( s, CAST( LEFT( update_date, 10) as INT), '19700101 00:00:00:000')) as 'TimeStampTest' INTO #temp FROM t_gc_containers SELECT * FROM #temp
Thanks! That worked perfectly! And instantly too. I have so much to learn... 
14 and 16 are very similar, I believe 16 does not come with SSMS however and just configures the DB without the gui. Try it again, it is possible to run through the install and create a local instance on your computer / desktop and not connect to one over the network. 
&gt; I have so much to learn... I found it was more around learning the "SQL" way of doing things. Initially I was tempted to use WHILE loops too, because that's how I would have approached the problem in a traditional programming language. Once I learned how to do joins and aggregations with grouping, it all sort-of clicked.
I have just done some tests with four methods. * My original ROW_NUMBER method = 18 seconds * ROW_NUMBER method with function only applied to the 10 rows rather than 20,000 = 2.5 seconds * FETCH OFFSET with functions within its SELECT = 3.5 seconds * a CTE using FETCH OFFSET to retrieve basic data and the main SELECT (I don't know what the term is) performing the functions on this retrieved data = 3 seconds So it seems like adding FETCH OFFSET is hampering the procedure compared to using ROW_NUMBER.
Thanks for the help, this worked and the results came up instantly. 
You need to get rid of that scalar function (which, I believe, is the reason of low performance) and test it again.
I commented out the functions and I'm achieving almost the same average times, so much head scratching!
Can you post the execution plan? Probably, it's just a full scan of a large table.
Are all of the fields likely to be used in the where clause each time? If not, you could consider dynamic SQL to build the statement instead, that way if they only search on one of the values you don't need (x = 1 or x is null) In your where clause multiple times, when they just want to search by category, for instance
Redgate is the shit! Highly recommended. A previous DBA found that Redgate backup compression is something like 5-10x more effective than SQL Server on its own.
Ola's scripts (http://ola.hallengren.com/) can do everything, but they might require some basic T-SQL knowledge to install. Free, easy to use, use only SQL native features.
This has to be a view so this will not work
which part won't work? you know a view is just a select statement right? 
is macsqlclient foss? or is download a trial? thx
Obviously depends on your usage, but AWS does offer [RDS for SQL Server](https://aws.amazon.com/rds/sqlserver/) which has automated backups.
&gt; B.system_id = "however you are figuring out which user is searching" This part, you cant pass on variables in a view 
Broken out the queries run in 10 minutes or less. Each temp table results in 4 million rows. I delete the previous temp table, so I only have 4M rows in a temp table at one time. 
This is correct. 
So this comic just helped me understand a small bit of what injections are lol
You can use exist clauses joining the table to itself. Eg select distinct a.salesorderid from table a where a.product in ('TFMRC','PL-TFMRC') and not exists (select 1 from table b where a.salesorderid = b.salesorderid and b.product in ('TFUSAGE','PL-TFUSAGE') )
They save them as PDF for backup. 
"We call him little Bobby Tables" ROFL
You mean they insert a PDF into a word doc and then print it out.
I thought dynamic SQL was a big no-no regards security. To be honest, I do have another procedure which is a complex but fast CTE and dynamic SQL hybrid, but I wanted to find a better way if possible.
[executionplan.jpg](https://postimg.org/image/43atm588r/)
Don't feel bad. We've all done it or something similar.
Can you run a Scheduled job to copy tables from one server to another using linked servers? In one of my duties, we do a 'data refresh' of only select raw tables from a production server to a development server but we don't need all the tables. I have 4 groups of tables. Each step is one group consisting of multiple tables. I run this maybe twice a day. In some situations, I have to copy development tables to production and in that job, I add an additional step at the beginning to kick people out of the tables and then the end to unlock the tables and allow people to get back in. Now this is not the most glamorous method but it works for a small set of tables that need to be updated frequently. (The Production data makes all the changes, Development only reads the data) There is another method where you can take advantage of the Transaction Logs. This method should allow you to replicate tables based on the logs made on one database to another. I hope this gives you some better ideas to try.
What's the goal? DEV Server? Read only replica for reporting? 
I had this same problem and I used CLng for integers and CDbl function for decimals. =CLng(Fields!Qty.Value) This guarantees that the excel cell will be numeric.
 --example table declare @tbl table(t time) insert into @tbl select '00:05' as t --define the window declare @rangestart time = '23:50' declare @rangeend time = '00:10' select t from @tbl where (datediff(s,@rangestart,@rangeend) &lt; 0 AND (t between @rangestart AND '23:59:59' OR t between '00:00' AND @rangeend)) OR (datediff(s,@rangestart,@rangeend) &gt;= 0 AND t between @rangestart AND @rangeend)
Yes, that is a huge problem in a production database setting because you are very likely locking tables and blocking other people (or systems) from accessing that data while your query executes.
&gt; and I added an OR in the JOIN statement there's your problem
Lol, does it significantly increase run time with conditional statement in the JOIN in general? I'd assume so, but I thought to ask to be safe. It's between that and using UNION. I guess I'll test things out
no, there isn't
As /u/ColWaffles suggested there's log shipping, or you could use database mirroring with a snapshot (there's some transactional overhead here because it's more of an HA solution though). AlwaysOn availability groups allow for read-only replicas - I'm not that familiar with them, but the same transactional overhead would likely apply. You could also go the route of a an ETL like update for the relevant tables and do incremental refreshes of the data throughout the day with the source queries having a NOLOCK clause. Depending on how the tables are designed this can be very efficient (i.e., if each table row has a meticulously updated LastUpdatedDateTime column then you're golden for incremental updates and merges on the destination). The best solution for you will probably come down to how much lag you can have between the data in the two systems (DEV can usually be pretty far behind, but reports can be more time sensitive). If you can have as much as a day of lag then you may even be better off just taking your production backup and restoring it regularly in DEV (and you get the added benefit of testing your backups regularly).
I like your solution in that it doesn't put OR logic in the predicate which may result in a bad query plan. But I do think you can simplify it a bit given that your datediff will always result in 20 or -1420. I also prefer to use inequality operators rather than BETWEEN just to make inclusiveness clear to anyone reading the code. if @rangestart &lt; @rangeend select t from @tbl where t &gt;= @rangestart and t &lt;= @rangeend else select t from @tbl where t &gt;= @rangestart or t &lt;= @rangeend
Try it with a UNION so it can use an index for each query OR's in a WHERE clause will always perform badly, but from what you say it does make me think your tables aren't indexed properly either
I wish i knew vbo, the automation and form stuff is really cool after you get the basics of sql down.
This was what i was going to say. with one row it might be a pain, but when you are inserting many rows its great.
&gt; @rangestart &lt; @rangeend Yeah, that condition makes a lot more sense than mine! I like the inequalities also. Thanks 
That works
I need the whole thing to work at any time of day, but yes, midnight was the problem. Your code works.for the special case.
if you are creating new backup schedules in RedGate, then yes, you should be able to delete the old ones as they will be independent of each other.
Are you querying SQL Server via Access or querying tables in your Access DB?
The first query uses an alias for the first column, the second doesn't. Try making those match. 
I've tried it both ways. Same error message. 
Tried this. Same Error.
You are not correct. They need to have the same number of columns with matching types.
That syntax is valid. You sure you're not spelling a table name wrong? Edit: I see you said each query works individually. Can you try opening a new query, going to SQL view, pasting that code in, and running it? If that doesn't work.. can you just take a screenshot? I feel like it's something really simple that is being overlooked
Both single quotes and double quotes are valid here
Then I stand corrected. :)
I don't see column userid in the example above, but let's imagine that it is presented in both tables. DELETE FROM OpenEnroll_EmployeeInformation EI WHERE userid NOT IN (SELECT userid FROM MaxDate_AppointmentLog AL) Or DELETE EI FROM penEnroll_EmployeeInformation EI LEFT OUTER JOIN MaxDate_AppointmentLog AL on EI.userid = AL.userid WHERE AL.userid IS NULL
Can you take a screenshot of the error? I just did a small test in my access using a SQL Server table writing an UNION ALL query and it worked with single quotes and without alias in my 2nd select just like yours.
[screenshot](http://i.imgur.com/yqsUEIa.png) Yea, I've done similar queries before with other data, but this one just doesn't want to work. 
I've double checked the data types. All short text. 
The icon of that query on the left is not a union query icon. When you open a new query, go to sql view, paste it in, and run it BEFORE SAVING IT... it doesn't work? Can you screenshot that? This is bizarre. Sometimes a query is switched from "make table", "append", "delete", to a select query there are artifacts left over in the "query properties" on the "property sheet" that will cause it to behave unexpectedly. I suspect that is the issue..
True, thanks for noting that. I was too lazy and decided to assume that userid is non-nullable :D
This worked! Crazy. I thought I had tried that. The icon is from earlier when I had done some updates and saved the query for some reason or another. Then was trying to build another query, and started with the union query when I rain into the error. I appreciate the help!
Look for "Business Intelligence" or "Reporting" software * SQL Server Reporting Services * Power BI * Domo * Qlik * Tableau * Crystal Reports * Jasper Reports * R * Cognos * Excel (connect directly to the DB, use Pivot Tables) There are too many options to list really...
I'd suggest learning the latter form. It's not ANSI SQL, but it enables you to delete "missing" records on tables that have composite primary keys.
perhaps use tempdb go EXEC sp_spaceused '#Temp' 
No dice, thanks for the suggestion! 
 DELETE EI FROM OpenEnroll_EmployeeInformation AS EI WHERE NOT EXISTS ( SELECT 1 FROM MaxDate_AppointmentLog AS AL WHERE AL.userid = EI.userid ) Or any of the other queries presented.
This has saved my day so many times. 
Is it offered at a university or college? I did my Oracle courses that way and wrote the exams. 
Power BI doesn't even need your data in an Azure database. Using one of the Gateway connectors provided by Power BI, you can access on-prem data via PowerBI.com, securely. Also worth noting that the Power BI desktop client operates entirely locally on your machine (though obviously then sharing reports becomes a hassle as they're local pbix files you have to deal with).
I grew up in the country where power flutation, power outrage are common. Ctrl+S is my second nature, I have never lost any code/scripts more than a few lines. I am the guy with 3 set of backup for every project. In the most dire situation, I have lost half day worth of work because there was a thunder strike and it fried our main server, backup server, network and electrical system. All other department lost all/most of their data but I have everything (for my department) until end of yesterday.
This is a life saver for me...though I now carry about 20-25 tabs open at any time and only a couple have ever been saved...
Try SSMS 2016. It is now separated from SQL Server and can be downloaded on its own. It is based on the Visual Studio shell now. It will do what you want. That being said... It is not ideal and needs to be fixed. It crowds your list of Recent files with a bunch of temp files, making Open -&gt; Recent almost useless... Since most of the temp files it keeps on the MRU list don't even exist anymore - and if they do, you most likely don't want them or can't even tell what they are unless you open them. MS really needs to fix that.
I was pleasantly surprised to see this working in SSMS 2016, as I grew to love to the feature in Sublime Text and DataGrip. I still get caught every once in a while when I'm using an older version of SSMS - but that's rare as it's really only an admin tool for me now.
No simple way for a temporary table as it only exists for part of a session. In order to figure out how much space it needs you'll need to identify a couple of things; 1. The table structure. 2. The queries used to populate the table. The results of the queries indicate how many rows will be in the table. From that and the structure it's easy enough to figure out the space required. Trick is when the number of rows jumps around erratically. 
Unless there is an explicit guarantee of a join on all of those linkages, I'd probably recommend using a left outer join for each of those.
Alternatively, is there a way to just flag this as a match on table A? 
your join is correct and it's a great start... do you expect there will be some rows in alpha that don't have a matching row in bravo? or the other way around? and why do you have two tables instead of one?
ok
Your IT team will probably be happy to help you. They probably already have SQL server enterprise, which is much better.
Kind of disappointing that she didn't go further and start explaining about the users table. She also forgot that you need to `flush privileges;` before the new user is able to login.
I'm not clear on what you want to do. As /u/d_w_hunter said, you don't "paste a file into a table" (especially a script file). If I opened ABC.SQL in Notepad, would it contain the USE + SELECT statements? Meaning you want to execute the contents of ABC.SQL? If so, you might want to look at the SQLCMD OS command.
...and [this is the end result](https://www.youtube.com/watch?v=QEzhxP-pdos).
UPDATE(column) is a way to go. Other options are even worse: `CASE WHEN DELETED.column &lt;&gt; INSERTED.column THEN...`
You might want to look into ApexSQL's [Audit](http://www.apexsql.com/sql_tools_audit.aspx). I've also rolled my own procedure to generate all of the triggers based on the current structure of the database. Either schedule it as a nightly job or trigger off of DDL triggers.
Haven't used it yet, but my company (large Silicon Valley tech co) just switched from Aqua Data Studio
i assume everyone here works at MSFT ;)
 UPDATE Alpha SET RowMatchInd = 'Y' WHERE EXISTS ( SELECT 1 FROM Bravo WHERE Alpha.city = Bravo.city AND Alpha.state = Bravo.state AND Alpha.zip = Bravo.zip AND Alpha.vehicle = Bravo.vehicle AND Alpha.color = Bravo.color AND Alpha.type = Bravo.type AND Alpha.transmission = Bravo.transmission AND Alpha.make = Bravo.make ) 
What is your role in your organization? If you are report writer then this may well be your problem. The best you can do is let them know that it's going to be time consuming to keep up with their disorganized data and some point they are going to have to figure out a better way to manage it because your solution won't keep scaling.
I'm assuming this is your product? I can't seem to find anywhere on the site that lists the disabled features if you don't purchase - maybe I'm missing it, but it needs to be highlighted a bit better.
Check this link: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/4126a010-6885-4eb0-b79c-c798c90edb85/how-to-split-comma-delimited-string?forum=transactsql But, if you are using MSSQL 2016 there is the command STRING_SPLIT()
I'm having a hard time imaging why you would do it this way but you just change the join criteria. table2.Date2 BETWEEN table1.Date1 - 3 AND table1.Date1 + 3
I would leave your join so that the records are matched appropriately and put a where Clause that says Where table2.date2 between dateadd(DD,-3,yourdate) and dateadd(DD,3,yourdate)
Yes, it can be done and there are a number of variations on "string splitting" theme (at least two of which are in /u/rbardy's link). But if at all possible, you should fix your database such that this isn't needed in the first place. Comma-delimited values in a single column do not fit into how relational databases best handle data.
A) because you can B) I dunno perhaps you when looking for flights.
Not that I know of. I don't think you can use it to work on Excel tables. If you have a link or something I'd love to see it.
which platform?
which platform? because MySQL's SUBSTRING_INDEX function could do it
Or similar, but I find easier to read Select sum( case when col1 in('0','-','n/a') then 0 else 1 end) This does basically the same thing just slightly differently
Another option is the nullif() function Only problem is that you can't do a list for it, so you have to nest them Nullif(Nullif(Nullif(col1,'0'),'-'),'n/a') 
I'd ask them how they would do it manually. If it gets too complicated I'll try to explain it to them and see if we can't simplify it etc. If they really push for it (and it is doable) tell them it'll take 2 or 4 weeks to do. (because we only allocate x mins to a report and it wouldn't be fair to everyone else to allocate 20hrs to yours (in a row))
thnx, Ive edited my OP with the solution
You can use SSIS data profiler. http://www.databasejournal.com/features/mssql/introduction-to-the-sql-server-data-profiler-task-part-1.html 
Please ... write what platform you are using, MSSQL, Oracle, MySQL ...? If MSSQL look for DATEFROMPARTS() 
That's the downside of no one watching over my shoulder all the time. My manager doesn't really want to be bothered with my stuff and he wouldn't be patient enough to listen to the troubles. He is a great team lead/escalation point, but I don't think he enjoys managing people. I've always had a problem with scope creep but I've only recently tried to actively correct it when it happens, so I'm new to telling people "sorry, we can't do that in this project right now"
 SELECT * FROM table1 T1 LEFT JOIN table2 T2 ON ABS(DATEDIFF(DAY,T2.Date2,T1.Date1)) &lt;= 3 Change the LEFT JOIN to a JOIN or whatever you actually need it to be.
Can you write how your CASE statement looks like? Depending on how you are dealing with it an assignment table won't be possible.
Yes, MSSQL. Thank you for the suggestion. I am looking into DATEFROMPARTS() now.
 &gt; 912345678 and 012345678 Is there actually a leading zero like that? If so, it's probably a string containing only numbers, not actually a number. In that case, `substring` (or `substr`... which RMDBS?) is the way to go. If it's actually stored as a number and you just threw the 0 on the front for our benefit, then `mod(StudentID, 1E8)`.
Mssql: Convert(datetime,[fieldname],101)
God I wish SQL Server had legit regex support.
Pretty simple homework assignment. Since you're an IT consultant... Outer join method SELECT Inventory.Item_name , CASE WHEN clTable.item_id IS NOT NULL THEN "Clearance" ELSE "Active" END as Inventory_status FROM Inventory LEFT JOIN clTable ON Inventory.item_id = clTable.item_id Subselect method SELECT Inventory.Item_name , CASE WHEN EXISTS ( SELECT 1 FROM clTable WHERE Inventory.item_id = clTable.item_id ) THEN "Clearance" ELSE "Active" END as Inventory_status FROM Inventory 
Does your source actually not contain the year? It is coming in as a string or as a date variable?
Well, the name of the function that performs a modulus is dependent on your DBMS, which you have so far neglected to mention.
To add to this, create a table that holds these types of dates for each SQL Agent job. Then, in your packages, create a small execute SQL task that retrieves and then updates these dates as appropriate. Now you can use this pattern for all of your ETLs/jobs.
Yep, that't the idea I started with
"as t" simply isn't the syntax, even though you're right, it is aliasing t. and yes, "table1 , table 2" is what's known as a [cross join](http://docs.oracle.com/javadb/10.8.3.0/ref/rrefsqljcrossjoin.html).
So you can make a cross join without actually writing the syntax JOIN Interesting Thanks 
That's nice. I'm tired of writing OBJECT_ID haha
Hmm let's see. &lt;code&gt; Where do you come from? Like you know in the code it's telling that the question should be marked fat or somthing like that. It's a bit hard to explain because i'm not a SQL expert myself at all :D
&gt; select * &gt; from [DB].[dbo].[tblStudent] t1 &gt; where (StudentNumber % 1E8) in ( &gt; select (StudentNumber % 1E8) &gt; from [DB].[dbo].[tblStudent] &gt; group by (StudentNumber % 1E8) &gt; having count(*) &gt; 1 &gt; ) &gt; order by (StudentNumber % 1E8), StudentNumber &gt; ; My apologies... Microsoft SQL Server.
Thanks. You're correct... MS SQL Server. My apologies. When I try to run either of those, it returns: Msg 402, Level 16, State 1, Line 3 The data types int and float are incompatible in the modulo operator. Any thoughts? Thanks... really appreciate the assist.
Replace all the `1E8`'s with `100000000`. SQL Server is treating `1E8` as a float, which doesn't necessarily store an *exact* number. Since Modulo is all about exact numbers, it's not compatible. `100000000`, on the other hand, is an exact number stored as an integer.
Thanks!
Would need examples of the raw data... are you storing the encoding with the data itself? if so its going to be an excersize in parsing.
Yes, it's like how INNER JOIN and JOIN are the same thing. Basically, back in the day, "table1, table2" was the syntax for joins, and you would emulate left/right/inner joins through the where clause. It's bad practice since we have that stuff as actual commands which are more explicit now.
In 2008R2 you can use `cast(@timefield as time)`. Don't set the column because then, as you've noticed, you'll lose the date.
The first thing I would do is to make sure that it is the query that runs slow, not the web server, by running Profiler (or xEvents) and checking query duration. If it is indeed the query, which causes such delays, I would suggest looking into query plans that your receive locally and that is used by the web server. I had a situation that two absolutely identical queries acted differently because SQL clients were sending different set of query parameters (such as QUOTED_IDENTIFIER) and thus having two absolutely independent query plans. 
OK - here goes: SELECT [DateAndTime] ,[Invitees] ,[Groups] ,[RSVP?] FROM [Table] update [Table] set [DateAndTime] = LTRIM(RTRIM(SUBSTRING([DateAndTime], 0, CHARINDEX(' ', [DateAndTime])))) So basically this is being done because I messed up and should have separated the Date and Time into 2 columns, so I was simply looking to do that.
So just do this: SELECT [DateAndTime] ,CAST([DateAndTime] AStime) AS 'Time' ,[Invitees] ,[Groups] ,[RSVP?] FROM [Table] Or are you trying to add a column to a table and then put the Time value in it? If you ran the query you sent me with the update then the answer to the question, "where did the time go?" is, "you got rid of it."
So lets take a step back. You have a table that looks like this: | DateTime | Invitees | Groups | RSVP? | | :--- | :--- | :--- | :--- | | 2016-01-01 12:13:06.000 | 200 | 4 | Yes | | 2016-01-02 1:17:02.000 | 20 | 2 | Yes | | 2016-01-03 2:01:16.000 | 10 | 1 | No | | 2016-01-04 11:23:46.000 | 30 | 6 | Yes | And you want a table that looks like this? | DateTime | Time | Invitees | Groups | RSVP? | | :--- | :--- | :--- | :--- | :--- | | 2016-01-01 12:13:06.000 | 12:13:06.000| 200 | 4 | Yes | | 2016-01-02 1:17:02.000 | 1:17:02.000| 20 | 2 | Yes | | 2016-01-03 2:01:16.000 | 2:01:16.000| 10 | 1 | No | | 2016-01-04 11:23:46.000 | 11:23:46.000| 30 | 6 | Yes | ~~How is this table being populated? Is there a stored procedure somewhere?~~ EDIT: Understood. You have an Excel file that looks like TableA which you want to end up looking like TableB.
% is a wildcard when using it within a string ie: where title like 'star%' What you are seeing in year % 2 is actually year mod 2, so its modulo arithmetic. Even years will have a remainder of 0 when dividing by 2 which is why you only have even years returned.
Exactly! So TableA has been imported into SQL and currently looks like TableA in SQL. Now trying to make it look like this in SQL: Date | Time | Invitees | Groups | RSVP? ---|---|----|----|---- 2016-01-01 | 12:13:06.000 | 200 | 4 | Yes *Edit: working in SQL because the document is too large to easily modify in Excel 
 -- Cleanued up original query DELETE FROM [importcoupa].[dbo].[RAW_PurchaseOrder] WHERE PurchaseOrderId IN ( SELECT a.PurchaseOrderId FROM ( SELECT a.PONumber , a.PurchaseOrderId , ROW_NUMBER() OVER(PARTITION BY a.PONumber ORDER BY a.PONumber, ApiExportedDate desc) 'OrderNumber' FROM [importcoupa].[dbo].[RAW_PurchaseOrder] a INNER JOIN ( SELECT a.[PONumber] FROM ( SELECT COUNT([PONumber]) 'NumberOfAppearance' , [PONumber] FROM [importcoupa].[dbo].[RAW_PurchaseOrder] GROUP BY [PONumber] ) a WHERE a.NumberOfAppearance &gt; 1 ) b ON a.[PONumber] = b.[PONumber] ) a WHERE a.OrderNumber != 1 ) --Assumed Goal - delete any purchase order with a duplicate PO#. --Initial cleanup, instead of the count, use a Having clause DELETE FROM [importcoupa].[dbo].[RAW_PurchaseOrder] WHERE PurchaseOrderId IN ( SELECT a.PurchaseOrderId FROM ( SELECT a.PONumber , a.PurchaseOrderId , ROW_NUMBER() OVER(PARTITION BY a.PONumber ORDER BY a.PONumber, ApiExportedDate desc) 'OrderNumber' FROM [importcoupa].[dbo].[RAW_PurchaseOrder] a WHERE [PONumber] IN ( SELECT [PONumber] FROM [importcoupa].[dbo].[RAW_PurchaseOrder] GROUP BY [PONumber] HAVING COUNT(PONumber) &gt; 1 ) ) a WHERE a.OrderNumber != 1 ) I'm not positive on the goal or how the data looks, but it may be possible to further simplify this to skip the whole WHERE PONumber IN () portion by just using the row number. Return anything with a 'ordernumber' greater and 1. I think the WHERE bit is simply duplicating that effort and possibly unnecessary.
All native SQL Server compression mechanisms work transparently and don't have any differentiation on whether the row is compressed or not. This is most likely some 3-rd party compression mechanism (might be worth trying to test known popular archiving software treating compressed bytes as an archived file). 
The first step is checking to see if you're missing indexes. If you don't know how this works doing it yourself, there's this query that can help you using DMVs. - https://blogs.msdn.microsoft.com/bartd/2007/07/19/are-you-using-sqls-missing-index-dmvs/ 
Just ran across this the other day. [Slow in the Application, Fast in SSMS?](http://www.sommarskog.se/query-plan-mysteries.html) While it applies more towards Stored Procedures, couple things to consider. As nvarscar pointed out having different connection parameters between PHP and SSMS will result in different query plans being used. White Space matters! Select * from A; and Select * from A; are seen as different queries. Those things make it a bit trickier to troubleshoot but once your aware of them you can control for them.
Welp besides the unnecessary complexity, you're either not removing duplicates or you're removing the original along with the duplicates. The only way this query makes sense as written is if the last PurchaseOrderId chronologically is different than the rest, in which case they're not duplicates. I'm assuming you want each PONumber to have only the latest PurchaseOrderId. First off, check out the HAVING clause, this is how you filter aggregated results, not using WHERE on a subquery. Second, this query assumes that PurchaseOrderIds are unique outside of PONumbers. Is that truly the case? If two PONumbers can have the same PurchaseOrderId, then you can delete more than you intend. Okay, so here's how I would do this: with dupes as ( select row_number() over (partition by PONumber order by ApiExportedDate desc) as id from importcoupa.dbo.RAW_PurchaseOrder ) delete from dupes where id&gt;1;
I edited to provide an image if that helps. 
Based on what I can see, it would make sense for my purposes to reformat the data this way. This data is the representation of a syslog-esque type message, so each ID is an event. It appears to make more sense for the application that will be scraping this data (SPLUNK) to have a one-row-per-id format with all the data contained on that row. Trying to parse the data in the current format has proven to be considerably difficult. I edited to contain an image if that helps. 
Thanks for the input. I apologize I left out that important detail. This is MSSQL data. I will continue to investigate the use of PIVOT more since that appears to be what I want to do.
Yup, you need to use pivot. Been faced with this one frequently. The issue is that you're going to have one entry per row (I believe). Like: A | NULL | NULL NULL | B | NULL NULL | NULL | C You can get around this by using some type of aggregate (like MAX) and then grouping by your main ID column.
Sql version?
&gt; Just ran across this the other day. Slow in the Application, Fast in SSMS? You listen to /u/brentozar's weekly office hours podcast too? This blog post was mentioned in yesterday's edition.
If I'm interpreting your first output and the attached image correctly, then it seems to be an EAV (entity-attribute-value) and [is a design pattern that isn't really recommended for relational databases](https://www.simple-talk.com/sql/database-administration/five-simple-database-design-errors-you-should-avoid/). In design, you ultimately make the decision of either using the proper medium for your piece of software you are building or else you structure your software to fit the medium it will use: the EAV does neither. More specifically, it doesn't take advantage of all the reasons you'd use an RDMS in the first place; a big one being data and referential integrity. But for your question, yes, it *does* make logical sense to view the data where each event is a record in a table. You are essentially describing a type of a "fact table" found in data warehouses the world over. Each table describes a business process whereupon each record is a unique measurement of that process and all the columns are what describes that event (ie, what time did a trap happen, what was the model of the modem, which city did it happen) and any numerical stats (ie, what was the signal strength, which packet number was it, what is the latency in milliseconds). With your post you are finding out why EAV sucks: you have to write a translation layer to get all the data output in a sensible manner. With any other table design (look up 3NF in *normal forms*) you can just reference the name of the column and get what you want. But with this design, you have to not only mention which column, but give specific instruction on how *each row* is to be interpreted. May the gods of dynamic SQL have mercy on your soul. EDIT: I'm almost certain it's EAV as if you look at your table names, it's for holding variables or something. No no no no no no! Bad application programmer turned SQL developer! No biscuit for you!
EDIT: [Use PIVOT](https://www.reddit.com/r/SQL/comments/50pti4/converting_columns_to_rows/d76ijo2) instead of CASE statements in the SELECT. Though, dynamic SQL still might be required. You're on the right track. If you use a DISTINCT, you don't need to use an aggregation. However, OP will want to ensure that the grain of his result set is still one record == one ID. I would use one CASE statement in the SELECT clause per every property he wants to see. The trick becomes, how do you know how many columns you need? My quick and dirty answer is that you query the number of properties/variables that entity has and build dynamic SQL around it with a WHILE loop. Really nasty, but that's what bad design will do to you. SELECT DISTINCT TrapID ,[City] = CASE WHEN OIDName = 'cyan.6.1.1.3.0' THEN OIDValue ELSE NULL END ,[Power] = CASE WHEN OIDName = 'cyan.6.1.1.7.0' THEN OIDValue ELSE NULL END 
&gt; That's unless, somehow, A_NAME isn't constant for property A, which is a concern because you're not making any attempt to check the integrity of the values in that field... Bingo. Whoever designed the original system used a hammer when they really should have grabbed a socket wrench.
If he uses a distinct then he's going to most likely just be pulling back the entire result set and end up with what I outlined in the previous comment. If each record id is unique in the desired output table, which it would be since it's a log file, then he will get unique records in the output table due to the group by with every column using MAXes or something else. If they're not unique, then he has problems. I have done this before and it's a bit of a trick to vertically merge the records into the nulled out fields.
I recommend summary tables. Depending on your use cases, you can roll up by fixed calendar periods such as week, month, quarter, year, etc. or by day with columns representing various rolling periods. Example: ------------------ -- Build schema -- ------------------ create table mfhouses ( mfhouseid serial primary key, name varchar(100) ); create table mfs ( mfid integer primary key, name varchar(100), mfhouse integer references mfhouses(mfhouseid) ); create table mfprices ( mfid integer references mfs(mfid), date date, closingprice decimal(9,3) ); create index mfprices_idx ON mfprices(date, mfid); ------------------------ -- Populate test data -- ------------------------ insert into mfhouses(name) values ('HDFC'), ('Axis'), ('Tata'), ('ICICI'); insert into mfs values (1, 'HDFC Fund 1', 1), (2, 'HDFC Fund 2', 1), (3, 'HDFC Fund 3', 1), (4, 'Axis Fund 1', 2), (5, 'Axis Fund 2', 2), (6, 'Axis Fund 3', 2), (7, 'Tata Fund 1', 3), (8, 'Tata Fund 2', 3), (9, 'Tata Fund 3', 3), (10, 'ICICI Fund 1', 4), (11, 'ICICI Fund 2', 4), (12, 'ICICI Fund 3', 4); insert into mfprices select mfid, now() - interval '1 days' * s date, random() * 100 as closingprice from mfs cross join generate_series(1,1000) s; ---------------------------- -- Build aggregate tables -- ---------------------------- select mfid, date_trunc('month', date) mth, sum(closingprice) closingprice_sum, count(closingprice) closingprice_count into mfprices_month from mfprices group by mfid, date_trunc('month', date); select mfid, date_trunc('year', date) yr, sum(closingprice) closingprice_sum, count(closingprice) closingprice_count into mfprices_year from mfprices group by mfid, date_trunc('year', date); select mfid, date, closingprice, sum(sum(closingprice)) over ( partition by mfid order by date rows between 6 preceding and current row ) closingprice_sum_7_days, sum(count(closingprice)) over ( partition by mfid order by date rows between 6 preceding and current row ) closingprice_count_7_days, sum(sum(closingprice)) over ( partition by mfid order by date rows between 29 preceding and current row ) closingprice_sum_30_days, sum(count(closingprice)) over ( partition by mfid order by date rows between 29 preceding and current row ) closingprice_sum_30_count into mfprices_rolling from mfprices group by mfid, date, closingprice; create index mfprices_rolling_idx ON mfprices_rolling(date, mfid); select date, name, closingprice, sum(closingprice_sum_7_days) / sum(closingprice_count_7_days) closingprice_avg_7_days from mfprices_rolling join mfs on mfs.mfid = mfprices_rolling.mfid where mfprices_rolling.mfid in (2,4) and mfprices_rolling.date = '2016-01-01' group by date, name, closingprice;
You sure you don't just want a regular PIVOT? It would transform your table into something like this (simplified example): TrapId|cyan.6.1.1.1.0|cyan.6.1.1.2.0|cyan.6.1.1.3.0|cyan.6.1.1.4.0|cyan.6.1.1.5.0 -|-|-|-|-|- 1208325581|11626|10.252.4.17|Rochester|3|OCh Term. Using a query like this (again, simplified): SELECT * FROM Traps PIVOT ( MAX(OIDValue) FOR OIDName IN ([cyan.6.1.1.1.0],[cyan.6.1.1.2.0],[cyan.6.1.1.3.0],[cyan.6.1.1.4.0],[cyan.6.1.1.5.0]) ) P If the OIDNames change from query to query then you'll either have to type them out each time or do some dynamic SQL.
Thanks. I'll try creating the original table and then testing how the summary tables help in performance.
That worked! Thx!
What you want to do, if this is T-SQL, is unpivot. Another technique to accomplish the same thing is CROSS APPLY . Check Stack Overflow for specific examples.
Sounds good - will do!
&gt;Bad application programmer turned SQL developer! No biscuit for you! I laughed so hard at this. Seriously though, thanks for taking the time to outline the importance of proper data modeling in SQL. I am a SQL newbie and this kind of input really helps ensure I am learning fundamentals. It is appreciated. 
&gt; I might want to add a category to each of the funds (e.g. equity, debt etc.). Assuming there is a 1:1 relationship between fund and category then it's easy-peasy. The category of each fund can be seen as an attribute of the fund itself. To represent this in your design, just add another column to the MutualFund "dimension" table and call it something like MutualFundCategoryDescription and populate as necessary. Where it gets a little more complicated is when you need to start tracking history or if there is a one-to-many relationship. You're welcome!
Interesting caveat of having nulls in database. OP exposes very key question developers should ask themselves before writing the sql, is there any nulls/unknowns in the data set. Thanks op
Get a Microsoft Azure demo account, and play around with some VMs? Learn Azure at the same time you learn the SQL family?
My hosting company said it will need to be through an SSH tunnel. I have SQL workbench (also what was suggested by my host). 
Without time zone 1 a) SELECT * FROM table WHERE ts_column_name &gt; date_trunc('day', now()) - '30 days'::interval With time zone 1 b) SELECT * FROM table WHERE ts_column_name at time zone 'America/New_York' &gt; date_trunc('day', now() at time zone 'America/New_York') - '30 days'::interval 2) Just kinda confuses me, here's how you can get the first week of the year: SELECT generate_series(date_trunc('week', date_trunc('year',now())), date_trunc('week', date_trunc('year',now())) + '6 days'::interval, '1 day'::interval) as for the 10 hours, you could use a date range intersection.
great information! Thanks for sharing, i have found another helpful [free Sqlite file viewer](http://www.sqliteviewer.com/free/) which is helpful in reading .sqlite db file.
I believe it is mySQL. I thought it said I could use SQL Workbench. I've got workbench setup where I can see the empty DB I created on my hosted server, but haven't been able to setup the restore. Someone else suggested to just host it on my local machine.... I may just go that route. Thanks! 
SQL Server on Linux hasn't been made available to anyone except a very select few outside Microsoft. And they're under *very* heavy NDAs, so I wouldn't expect them to be asking about it on reddit.
Ahhhh.... That def helps. I will check into it. I'd like to start with an established DB, then work my way into creating a couple from scratch 
Hello, Thanks for this. For second question, here is what I am trying to solve. I have two tables, table A has user_id, registration_status, time they registered (timestamp with timezone) and table B has user_id, country where they are from and loggedin status. What I am trying to do is to find all users who registered successfully in first week of 2016, and the percentage of them who loggedin within 10 hours after they registered (from their registration date) in each city for each day of the week. Is it possible?
&gt; Hello, &gt; Thanks for this. &gt; For second question, here is what I am trying to solve. &gt; I have two tables, table A has user_id, registration_status, time they registered (timestamp with timezone) and table B has user_id, country where they are from and loggedin status. &gt; What I am trying to do is to find all users who registered successfully in first week of 2016, and the percentage of them who loggedin within 10 hours after they registered (from their registration date) in each city for each day of the week. &gt; Is it possible? 
Yep, it's possible. The first bit is just a `where registered_at between (start_of_week) and (end of week)`. The second you can find out by comparing their earliest log in time with their `registered_at` timestamp. Once you have these two datasets, it should be trivial to work out what you want to.
&gt; Using an SK is almost never a bad choice, but it can add a level of uneccessary conplexity to, as you say, small reusable lookups that had a reasonable key anyway (think STATUS fields). Very true and not disagreeing with you here. But seeing as how conformed dimensions are also a Kimball principle, I wouldn't be surprised if I saw "status" being implemented as such. When designing at an enterprise scope, the benefit of having conformed dimensions outweighs the benefits of storing codes in one or more fact tables. But when the scope is at on a handful business processes likely to be within one data mart, I can see STATUS being done whatever works for right now, including record volume. Also, your shop actually has QA and architects?! I bet they have ETL developers too. Lucky.
"New"? How long has JOIN syntax been around for, decades?
&gt;Instead, I'll simply say that many Analysts work for the business and not for IT. People like me. This is a HUGE growing niche segment that can command a very high salary, and often times there are more jobs than analysts like you. &gt;Or they can come to me and it will be done by Tuesday. So fuck IT. &gt;Excel paid for the boat, but it's Access that pays for the house. Today. Not tomorrow. Move to SQL.
the dialect in access is severely limited. it would be a better spend to move than to accomodate.
I would recommend open source over SQL server. It may be a bit of a step from access, but it's worth it. Postgres, mariadb, mysql. Lots of good tutorials online.
Are the questions embedded in HTML? Are you asking how to strip out all the HTML formatting? 
At work we use Toad for Oracle databases. A couple of other applications use MS SQL Server so we use SQL Server Management Studio for those. 
I would get a free sqlite editor like http://sqlitebrowser.org dump some data in it and play with queries you need. If it's working fine and see it's worth than you can convince IT you need a properly hosted database 
I'm absolutely loving DataGrip, it does run on Linux too. 
i use [HeidiSQL](http://heidisql.com) with MySQL it works with MS SQL/Server and PostgreSQL as well runs on Linux with Wine
I'm using SQL Workbench: http://www.sql-workbench.net/getting-started.html It's Java-based, so it can be run on Linux as as well. 
And the extras that come built-in. Like Reporting Services, which would be very helpful for the OP in doing his job of producing reports.
Thanks!
Sorry but I am probably lack some basics here but I also had the same understanding as Mendoza. I'm a little confused here and do not understand why i need to move away from access. Can someone highlight to me the difference between access and sql. my understanding is that access = desktop use. Currently my access solution has approx 50 queries built with SQL and they are used for reporting purposes. My intent here is to modify and update these queries. 
correction: retail field singular
So in my experience SQL Server is streets ahead of Access. Access is designed to be a database but it doesnt have the computing power that SQL has, which is a fatal flaw. The point of a database is to deal with millions of entries fairly seamlessly and Access just cant do that. However if you are company without an IT function then Access is easier to get started with. SQL Server is just as useable once you get it going though (and free). Access is also just buggy in my experience once you start making it complicated. And the SQL code is all on one line so it becomes unreadable
Dbeaver is my favorite. It works with just about every database, even things like vertica and teradata. 
Thanks. Can you give an example of a problem where access lacks the computing power to execute compared to SQL server? Also, is SQL server completely free? Where does the data get stored and is there no price for hosting?
http://squirrel-sql.sourceforge.net/ Likely the best (most feature-rich) open source client available, it uses JDBC drivers so it pretty much works with *anything.* 
http://www.codeproject.com/KB/database/Visual_SQL_Joins/Visual_SQL_JOINS_orig.jpg
this is lovely... i've always disliked the Venn diagram approach to joins here's another good illustration of joins -- [The FROM clause](https://www.sitepoint.com/simply-sql-the-from-clause/)
&gt; -For the full outer join - can you explain the latter part of your sentence "with nulls where there's nothing to line up" . You mean if there's a "John" for example who doesn't have a pet, it'll be null instead? How exactly will the final output / 'results' look? Yes are correct -- John's pet will be null. Let's assume for the sake of simplicity that your join returns two columns - the person's name and their pet. We'll keep the data short to make it easier. Kate Pouched Rat Betty Iguana John NULL NULL Tarantula Frank Dog Kate has a pouched rat, Betty has an Iguana, John doesn't have a pet, nobody has a tarantula and Frank has a dog. For the difference between a left/right/full outer join, literally think of table A (in this case, people) being on the left and table B (in this case pets) being on the right. Line up the two tables with necessary blank lines in between so that you can draw lines straight across where rows match. When you do a left outer join, you take *all* the rows from the left table, but only the rows from the right that match. If a row from the left table doesn't have a match (John in the example I created), you put NULLs in the result for the values that would have come from table B. When you do a right outer join, you do the opposite -- all rows from B, but only those from A that match. When you do a *full* outer join, you take all rows from both tables and drop in NULLs for missing values.
Joins are ok until there are many tables being joined left and right. Any tutorials on multiple table joins? 
Think of it like this. When you join something you're adding. So take this set of data: | Month | Year | TeamName | TeamID | | :--- | :--- | :--- | :--- | | 1 | 2016 | Lions | x | | 1 | 2016 | Tigers | y | | 1 | 2016 | Bears | z | | 2 | 2016 | Lions | x | | 2 | 2016 | Tigers | y | | 2 | 2016 | Bears | z | Which you get from this code: select distinct month , year , teamname from schedules where month in ('1', '2') and year = '2016' Basically its giving you all the teams that played a game in January or February in 2016, and the `schedules` table only stores the games. But what if you want to know the results of the games and that information is in another table that looks like this: | Month | Year | TeamID | Score | | :--- | :--- | :--- | :--- | | 2 | 2016 | z | 42 | How can you connect (or join) these two tables to get the teamname &amp; score to appear together? The "key" is the TeamID. So you could do this manually such as: select distinct month , year , teamname , score from scores a where month in ('1', '2') and year = '2016' and teamid = 'z' Then you could copy everything to Excel and manually copy each corresponding cell to fill in the scores for each game, right? A simpler way looks like this: select distinct a.month , a.year , a.teamname , b.score from schedules inner join scores b on b.teamid = a.teamid where month in ('1', '2') and year = '2016' It takes the two tables and "smooshes" them together, and each join behaves a little differently. 1. INNER JOIN: When you only want whats in both places. 2. LEFT JOIN: When you want everything from the first place even if its not in the second place, but you want whats in the second place if it is there. 3. RIGHT JOIN: Same as a LEFT 4. FULL OUTER JOIN: I want both of everything. 99% of the time you really only need LEFT and INNER joins. Personally I found the Venn diagrams very useful when I was learning.
Wait, in addition to outer, inner, left, and right joins, there are ALSO inner left, outer right, outer left, inner right as well? That makes it more confusing =/ 
Many others have provided good input and/or examples and/or links (particularly the "say no to Venn diagrams" links). I will add this: Create 2 tables with Kindergarten-level data (10 rows, maybe 2 to 4 columns) and just start experimenting. The pretend data doesn't HAVE to be the traditional hierarchy-type (orders/items, managers/employees, etc). It can be, but the important thing is that it be whatever works best for you. Anyway, just start mucking about with the various JOIN types and syntaxes and see that the results do as you change from one to another.. Once you're comfortable, add a 3rd table of similarly simple data and experiment again. Keep adding tables as desired. This hands-on approach should help you cement the ideas you see in examples and diagrams. Also: Save the query(iess) you end up with so you can refer back to them later.
As far as I know, there's only 1 type of inner join. For outer join there's left, right, and full. There's also the cross join. However, you can solve 90% of problems with INNER and LEFT OUTER joins, and probably 7% with CROSS joins. I'll throw in 2% for FULL OUTER, even though I've never used one. I've never felt it necessary to use a RIGHT OUTER join ... ever. (NOTE: I've been developing database applications on SQL SERVER - both Sybase and Microsoft - Oracle, and DB2 for over 25 years.)
Great Scott! 
 SELECT studentid FROM enrolments e INNER JOIN courses c ON e.classnumber = c.classnumber WHERE c.coursename = 'course 1' INTERSECT SELECT studentid FROM enrolments e INNER JOIN courses c ON e.classnumber = c.classnumber WHERE c.coursename = 'course 2' Would return the student id nos of students who took botg course 1 and course 2
I recommend you this web site which illustrates the concept : normal join (also called natural join) http://studybyyourself.com/seminar/sql/course/chapter-6-join/?lang=en. Outer join : http://studybyyourself.com/seminar/advanced-sql/course/chapter-1-outer-join/?lang=en
Well, constraints won't help you here, trigger (INSTEAD OF INSERT) is what you need. Write your logic in a trigger: insert correct values and throw an error every time the date is wrong.
It could depend on the specific Database you use but as far as i know this is not direct solution. You could use stored procedure in your constraint or you could use a before insert trigger that raises an exception when your condition is violated. 
is there any way to avoid triggers altogether? Is cross table constraint checks not supported in sql? I'm using DB2 by the way
&gt; DB2 You should have mentioned this in the post to reduce guesswork :) Not a DB2 specialist, but it seems that the constraint types in DB2 doesn't support anything like this: http://www.ibm.com/developerworks/data/library/techarticle/dm-0401melnyk/
After reading some stuff on the matter I might suggest creating a UDF and using it as a check constraint.
You need to cast or convert the datetime field in your select, group and order by. 
select cast([date] as date) [date], Code, sum(Amount) as TotalNet from table where date &gt;= '01/03/2014' and date &lt; '01/03/2015' and code &lt;&gt; 0 group by cast([date] as date), code order by date, code
You can store the data locally on your machine and still use SQL server. IMO, it isn't so much about lacking the computer power as it the annoying dialect, and limited support that you'll find. You'll likely find it much easier to find a better higher paying job if you get your feet wet with SQL, and it will open the door to more complex operations that will take large amounts of computing power than you would typically want to run as a localhost.
Yea unfortunately there is zero data in the previous table mentioned, thanks for your reply though that is very helpful and will focus on the tasks at hand
Given your advice i have produced two files, the first being DDL.sql which contains the code: CREATE DATABASE my_db; CREATE TABLE Players ( MemberID varchar(255), Firstname varchar(255), Familyname varchar(255), Email varchar(255), Phone varchar(255) ); and the second being DML.sql containing the following: ( INSERT INTO Players (column1, column2, clumn3, column4, column5) VALUES ('value1','value2','value3','value4','value5'); ); ( DELETE FROM Players [WHERE MemberID = '5'; ); ( SELECT * FROM Players WHERE Email LIKE '%@gmail%'; ); does this look correct to you? Sorry for all the questions, really appreciate the help! 
Thanks for the quick reply! yea i thought having varchar for memberID was also weird, but is infact what theyre asking for, for the second statement they havent even given any values, so im assuming they just want to see i know how to insert values if need be, though i will be sure to change the column names. This has helped a lot, thanks! p.s chur my cuz, must be another fellow kiwi haha cheers bro!
In your scenario, data is stored in three different tables: * BOOKS * ORDERITEMS * PUBLISHER You have to link the key records in each table in order to extract the corresponding values, otherwise your result set will either fail or return multiple results for each record. In your example, the BOOKS table has a 1NF link to both ORDERITEMS and the PUBLISHER tables, which was represented in the implied joins. Another way to represent this, which might be a bit easier while you are learning, is to use explicit join statements: Select (specified fields) FROM BOOKS, ORDERITEMS, PUBLISHER WHERE BOOKS.ISBN = ORDERITEMS.ISBN AND BOOKS.PUBID = PUBLISHER.PUBID Your select statement did not specify the fields in each table.
You would use something like: update T set C=B where B is not NULL I'm not sure what you're talking about with joins in the where clause, unless you're referencing subqueries - which DB2 does support. 
Yes sir I understand an ordinary JOIN better now. However my question was in regard to having multiple joins in a query. If you look at my solution code given to us from the professor, there are 2 join statements in the code which is a little confusing. I'm just having a hard time finding out how he knew which tables to join and what to extract from them to get the correct output.
Hello and thanks for taking the time to write this out. You say that : &gt;You have to link the key records in each table in order to extract the corresponding values, otherwise your result set will either fail or return multiple results for each record. I get that is what you're supposed to do and I get that that's what the code is doing. I'm just having a hard time finding out how to know which tables to join and what to extract from them to get the correct output. In other words, I need help in understanding what are the key records in each table that I have to extract using the joins - if that makes sense. Also my select statement did specify the fields. They are "name" and "sum" . The output is shown in the word document given to us with the solution so I know the select statement was working. I am aware that what you wrote in the bottom is the same as a join. In the beginning when I was 100% clueless about joins this would've definitely helped. &gt;BOOKS.ISBN = ORDERITEMS.ISBN &gt;AND BOOKS.PUBID = PUBLISHER.PUBID -Just don't know why these are the 2 "joins" being used and what the thought process is like to find that out. Once again sir, I thank you for your reply and await your response :)
http://sqlzoo.net/ From basics to the advanced examples...with quiz.
This is one of the many things I've tried, but still it doesn't want to group the codes per day: [Previous comment w/ picture](https://www.reddit.com/r/SQL/comments/51fnkq/order_by_datetime_code_ignoring_the_time/d7ckzd2)
Are you sure the SQL is equivalent to the relational algebra? If you do a natural join between the two relations, you would join on A and B (SQL), but the relational algebra would only join on A. I'm pretty sure the correct SQL would be: select t1.a, t1.b, t2.d from (select a, b from table1) t1 natural join (select a, d from table2) t2; Or an equivalent: select * from (select a, b from table1) t1 natural join (select a, d from table2) t2; Which would give you 4 tuples (6,8,7),(6,8,6),(6,6,7),(6,6,6)
No, I'm really not sure if my attempt at translating to Sql was correct, and it clearly wasn't aha. I'll try understand yours, thanks a ton
`SUM` is an aggregate function. That is to say it will `SUM` values for groups of data that you select in the `WHERE` and `GROUP BY` clauses. For your example, if you `SUM` just `where ISBN` then you will get the sum of the value of each ISBN group of books, in this instance each book has a unique ISBN so it will not really group anything... If you then add the `PUBID` clause, it will sum and group all values of all `ISBN` for those `publishers`.. does that make sense? The 2 joins match those ISBNs to those Publishers...and sum the total ISBN values for their associated publishers.
Good to know that Kahn is accessible for non programmers. 
If you want ordering by month, you need data to be grouped by month, right? Because how would you order by month, if data doesn't contain months? Order by date would give you better results. To enable grouping by date you can change cast([date] as date) to month(date) in both select and group by statements.
Since a natural join will attempt to join on *all* commonly named columns, and table 1 and 2 have columns A And B in common, this should be equivalent to SELECT table1.a, table1.b, table2.a, table2.d FROM table1 INNER JOIN table2 ON table1.A = table2.A AND table1.B = table2.B; So you should get back exactly one row because it's the only row where both A and B match between the two tables. (caveat, I'm not a fan of "natural joins" and much prefer explicit inner joins, so I could be entirely off base.)
The fields you are using in your select statement do not specify which table they are stored in. I can guess, but you stated that you already have this information. If you add the table names before each field it will help you understand why there are multiple joins: BOOKS.NAME PAIDEACH COST QUANTITY SUM is not a field, it is a sql command. There are great online resources for sql functions and syntax, which are easily found through a google search.
I find it easiest if I think of the tables as old time inventory books where someone has put all the information on that subject. So.. you're a book seller and you need to report total profit grouped by publisher. So you look at which of these inventory books has the information you need. The inventory book labeled "Books" has columns for the cost fo the book, the author, the title, the isbn number of the book, and a publisher id The Inventory book labeled "orderitems" has your main monitory information, it's columns contain the isbn, how much someone paidfor each book, and how many they ordered. Finally the "Publisher" book contains the publisher's name and address, and the publisher id. So.. to get all the information you need on your report, you need all three books, and you connect the information in each book by these key values. Now if I add some explicit table names to your answer select it becomes obvious what comes from where.. SELECT publisher.name "Publisher" , Sum (( orderitems.paideach - Books.cost) * orderitems.quantity) as "Total Profit" FROM Books JOIN orderitems USING (isbn) JOIN publisher USING (pubid) GROUP BY publisher.Name 
&gt; Another way to represent this, which might be a bit easier while you are learning, is to use explicit join statements: Nonono. Assuming OP is in college now, that syntax has been obsolete since he was in diapers. No need to teach bad habits. Stick with the SQL-92 standard. However, the ambiguity of the `USING` keyword can lead to confusion. I would recommend explicitly listing the join keys, which you can do with `ON`: SELECT name AS "Publisher" , SUM( (paideach - cost) * quantity) AS "Total Profit" FROM books JOIN orderitems ON books.isbn = orderitems.isbn JOIN publisher ON books.pubid = publisher.pubid GROUP BY name Proper formatting (listing one join per line, namely. Most of the rest is a matter of personal preference) also helps clear things up.
I'm not OP but I want to thank you for that second format. I've never seen it used before and believe that I can revisit a couple of ugly queries in my stable by using this format to join. Thank you stranger!
If you're talking about using an asterisk after *select*, then you should consider listing all attributes explicitly even if using * is shorter, see [this StackOverflow post which discusses the disadvantages of using *](http://stackoverflow.com/a/3639964). On a similar thought, consider using *inner join* instead of *natural join*. Again, natural join is less code, but it's not necessarily clear what you're joining on, and in the future new attributes in the table may break the join altogether (if a new attribute in the right table has the same as an existing attribute in the left table). These are both cases of whether being *implicit* or *explicit* is better - when reading code, having *explicit* code can often be easier even if it means longer code.
I was talking more about the (this thing) join (that thing) I have some that are Select stuff from somewhere where this in (that) join (that) joining the two would prevent the repetition of (that) which happens to be a nested query with 3 column internal where clause.
I'll check that out. Right now I was testing a LOOP approach and it seems like it should work but for some reason it doesn't seem to insert any data into the #table. Not sure if it's a syntax thing or what. IF (SELECT object_id('tempdb..#POLY')) IS NOT NULL DROP TABLE #POLY ; BEGIN CREATE TABLE #POLY( [PXIFNM] [char](15) NOT NULL, [PXIMNM] [char](15) NOT NULL, [PXILNM] [char](30) NOT NULL, [PXIAD1] [char](30) NOT NULL, [PXIAD2] [char](30) NOT NULL, [PXICTY] [char](20) NOT NULL, [PXIST] [char](2) NOT NULL, [PXIZIP] [char](12) NOT NULL, [PXIBP1] [char](13) NOT NULL, [PXPCLB] [numeric](3, 0) NOT NULL, [PXPOLN] [char](10) NOT NULL, [PXMBRN] [numeric](12, 0) NOT NULL, [PXFAMT] [numeric](13, 0) NOT NULL, [PXANPR] [numeric](15, 2) NOT NULL, [PXEFFY] [numeric](4, 0) NOT NULL, [PXEFFM] [numeric](2, 0) NOT NULL, [PXEFFD] [numeric](2, 0) NOT NULL) DECLARE @POLY TABLE ( ID [int] IDENTITY(1,1), PolicyNo [varchar](10)) INSERT INTO @POLY SELECT DISTINCT PXPOLN FROM TABLE1 DECLARE @LOOP int = 1 WHILE @LOOP !&gt; (SELECT MAX(ID) FROM @POLY) DECLARE @TPRELIM varchar(MAX) = (SELECT PolicyNo FROM @POLY WHERE ID = @LOOP) DECLARE @FINAL varchar(MAX) SELECT @FINAL = ' SELECT * FROM OPENQUERY(SERVER, ''SELECT PXIFNM , PXIMNM , PXILNM , PXIAD1 , PXIAD2 , PXICTY , PXIST , PXIZIP , PXIBP1 , PXPCLB , PXPOLN , PXMBRN , PXFAMT , PXANPR , PXEFFY , PXEFFM , PXEFFD FROM TABLE2 WHERE PXPOLN IN (''''' + @TPRELIM + ''''') '') ' INSERT INTO #POLY EXECUTE('EXECUTE ' + @FINAL) SET @LOOP = @LOOP + 1 END
This does not work, returns no data. declare @tprelim varchar(max) = '4013934312, 4013993375' declare @final varchar(max)
The quotes are not right, in this variation it would be sent like this: WHERE PXPOLN IN ('4013934312, 4013993375') which is not the right syntax. You should either drop quotes completely (if target column is int), or add quotes into the source variable: declare @tprelim varchar(max) = '''4013934312'', ''4013993375''' WHERE PXPOLN IN (' + @TPRELIM + ') '') 
No need for dynamic sql if you have a known number of columns. You state that it can have 1-9 columns, so just explicitly define the column list and do away with the dynamic sql, its a layer of complexity that is not needed. From there, you're currently only grouping on project_code. To expand your grouping in how you have it written, you need to do two things: 1) Add the other columns you want to group by to the base select. 2) Add them to the Partition clause of your ROW_NUMBER window function. SELECT Project_Code,Task_Code, agency, EFF_DT, END_EFF_DT , [1],[2],[3],[4],[5],[6],[7],[8],[9] FROM ( SELECT Project_Code,Task_Code, agency, EFF_DT, END_EFF_DT --cast to integer to enable SUM aggregate from PIVOT , CAST(LineSeq AS float) AS LineSeq , ROW_NUMBER() OVER(PARTITION BY Project_Code,Task_Code, agency, EFF_DT, END_EFF_DT ORDER BY LineSeq) AS RN FROM [TEST-DOA-HR-STLA].[dbo].[allocationoutput_view] ) X --apply pivot dynamically based on num of lineseq results per Project_Code PIVOT(SUM(Lineseq) FOR RN IN ([1],[2],[3],[4],[5],[6],[7],[8],[9])) AS Y
No and no. We are lucky to have OQ access.
My man. Hit you with some gold when I get home.
Anytime man, glad that was able to help.
Yes, that blogpost is dumb. Venns are the easiest way to visually explain all the joins except cross join. Most people learning SQL don't know shit about set theory and relational algebra.
&gt; So you need books to figure combine the other two tables using the columns books has in common with them. In order to tell who the publishers are, you need the name off of the publisher table. &gt; &gt; &gt; &gt; In order to calculate profit you need the cost off of books and from orderitems you need paideach and quantity. I get this, but the thought process behind this that one does before formulating the answer is confusing me. 
How is this going for you? Was thinking of doing something similar as a project and to help me learn SQL. Bonus would be if it gives a useful view of performance that current sites don't provide. 
Well for most db systems you can think of it this way: Where you have one value (e.g. in a where clause, where id=5 or name='name'), you can replace the value with a subquery that returns one row and one column (one value), e.g. where id = (select max(id) from table) When you are supposed to use a list, for example using where IN, for example: where year in (2014, 2015, 2016) You can replace the list with a subquery that returns one column with one or more rows, e.g. where year in (select year from table1 where year &gt; 2014) And as I showed above, wherever you have a table, you can replace that table with a query that returns any number of columns and rows, since "a number of columns and rows" is also a table.
u/redneckrockuhtree's solution is great. You should do that. If for some reason you really want to avoid a subquery, you can simplify those CASE statements a bit (which makes thing a little easier visually), and include your CASE statements directly in the GROUP BY clause, omitting the aliasing (as 'blah blah'). SELECT sum(quantity * (CASE WHEN right(item_name,1) = 'b' THEN 10 ELSE 1 END)) as 'Actual_Quantity', CASE left(item_name,4) WHEN '0001' THEN 'yellow' WHEN '0002' THEN 'black' END as 'Actual_Item_Name', OT.order_number FROM items_table IT LEFT JOIN orders_table OT ON IT.order_num = OT.order_num GROUP BY CASE left(item_name,4) WHEN '0001' THEN 'yellow' WHEN '0002' THEN 'black' END, OT.order_number
Oh right, I've used ORDER BY before, I think I thought you meant something different. I'm more confused by the JOIN syntax than the result set it returns. Were the statements I made true, or am I missing something?
The size of the log file will not drop after a transaction log backup. The amount of free space in the file should increase. Shrinking and (re-) growing files is costly, often leads to file fragmentation and should be avoided. If you suspect that log space isn't being freed up by the transaction log backup, try using [DBCC OPENTRAN()](https://msdn.microsoft.com/en-us/library/ms182792.aspx) to see if there is a transaction that has been open a (very) long time. Sometimes, programs don't call COMMIT and the transaction never ends. This prevent a log backup from effectively freeing space in the log file.
Those are derived tables, not subqueries. 
This definitely sounds like one of two things: - you have an open transaction that has never been committed. While DBCC OPENTRAN will work to identify the open transaction, I'd recommend taking a look at sp_whoisactive, which will provide the same information (and so much more!) in a very logical result set. (http://sqlblog.com/blogs/adam_machanic/archive/tags/sp_5F00_whoisactive/default.aspx). - another possibility is that you're rebuilding all of your indexes frequently. SQL Server maintenance plans don't do a very lean job of only rebuilding indexes that justify a rebuild (this is based on the fragmentation level of he index - just like a disk defrag when you analyze before you do it). Maintenance plans only give you a single option: rebuild or don't. A lot of times, you can avoid rebuilding the entire index just by updating statistics (command: UPDATE STATISTICS), which happens automatically when you rebuild and index and is usually the main reason for any performance gain you get. All that to say: updating statistics is usually the reason for any performance gains you see after an index rebuild - not the actually index rebuild itself. While your mileage may vary, that has been my experience 99% of the time with databases smaller than 500GB. I'd highly recommend checking out Ola Hallengren's maintenance solution - it can entirely replace your maintenance plans and has a lot of smarter options. It's been awarded by Microsoft quite a few times and many SQL Server admins rely on it around the globe for their database maintenance. Here's the site where you can read up: https://ola.hallengren.com/ Hope that helps you out. Gotta have the right tools to get the information fast. ;-) EDIT: I saw your mirroring comment. Unfortunately, that isn't your root cause here. Your log backups should be allowing enough free space in your log file for it to stay the same size as it is right now. Once the file grows, it doesn't shrink on its own. You have to force it to using DBCC SHRINKFILE (WARNING - DO NOT RUN DBCC SHRINKDATABASE - THIS IS SO MUCH BAD). I think you either have an open transaction, or you have rebuilds happening that are cramming a ton of throughout into your log file. My bet is on the former. ;) EDIT 2: read your comment wrong. You could be absolutely right: if the mirror has been offline for awhile, the principal won't truncate prior to the last transaction that was applied to the mirror. Still gotta shrink that log file manually though. ;-)
Well you aren't needlessly doing it if you have a need for it. If your main process can't pick up the commented rows, but your other process can... what's going on there? Why can't you pick them up by incorporating an `OR` somewhere? 
Well, not quite. I'm not sure if Python is the right way to present it, but let's work with it. 1) your pseudocode for the join is a good representation of a 'nested loops' method of joining tables (there are other methods). 2) even in that pseudocode, if you switch around table1 and table2, the output list ('join_table') WILL have a different order of elements, and that leads us to 3) there's an underlying assumption/implication in your words (and that's what /u/dGhleSBoYXZlIG5vIHdv refers to) that a sql engine stores and data in a certain order and retrieves data in that same order by default. This is incorrect. Order of the data is NOT part of the metadata. Unless dictated by the query, result set record order will depend on the data access method. One of the features of SQL engines is to allow independence to the engine in selecting data access method (storage, indexes, output of prior operations, etc.). PS. to help you 'wrap your head around JOINs' procedurally - maybe try to think about a CROSS JOIN in terms of your python pseudocode and how that becomes an inner join once you add a condition to it.
The live warehouse script only looks for the previous hours worth of information. WinCC/Siemens compress and hide their db as they sell products/modules that are supposed to do what I've made in the csv exporter.
1NF: every table has a PK, no column contains more than one value 2NF: all columns depend on entire PK 3NF: all columns depend non-transitively on the PK good explanation here -- http://www.essentialsql.com/get-ready-to-learn-sql-11-database-third-normal-form-explained-in-simple-english/
 UPDATE Game SET Game.Price = CASE WHEN Player.Age &lt; 21 THEN 100 ELSE 30 END FROM Game , Player WHERE Game.PID = Player.PID
You need to specify all of them: Project_Code,task_code,agency , [1], [2], [3], [4], ..., [X]
yep, exactly Although I'm pretty sure that having default QUOTED_IDENTIFIER settings, you would select numbers instead of columns, if you specify them in single quotes: `'1', '2', '3', '4', '5', '6', '7', '8', '9'`. Use square brackets instead as in my example
I is hard problems understand anything said you here.
No, the script I use takes everything for a say DATEADD(hh,-1,GETDATE()) for example. so it snapshots every previous hour for iirc its like select * from table where datetime &gt;= DATEADD(hh,-1,GETDATE()). Which is great, but there is the possability that a row I've put in to my warehouse at 10am, will be updated at 1pm but not on my warehouse DB its done on the live seimens bullshit db. I'm trying to figure out the best method of going back through the data but only looking for comments, then merging it with my warehouse db.
Your using MSSQL, assuming version is new enough to support "date" GROUP BY Cast(datecolumn as date) Will work... Alternatively you can use DATEADD/DATEDIFF to convert all times to midnight Select DATEADD(d,0,DATEDIFF(d,0,GETDATE()))
&gt; Either a tool is generating the code and just does some strange things... That's my guess. &gt; add constraint var_name check OP, does that actually say `var_name`? Or something like `SYS_C0012345`? My guess would be someone created it as an inline constraint: create table test_table (email_id number not null); --or "alter table" And OP's client is reading from the metadata tables when asked to generate DDL: select CONSTRAINT_NAME, CONSTRAINT_TYPE, TABLE_NAME, SEARCH_CONDITION_VC from USER_CONSTRAINTS where TABLE_NAME = 'TEST_TABLE'; CONSTRAINT_NAME | CONSTRAINT_TYPE | TABLE_NAME | SEARCH_CONDITION_VC | :--- | :--- | :--- | :--- | SYS_C0099414 | C | TEST_TABLE | EMAIL_ID IS NOT NULL | 
Well, I changed the names. Sensitive information from a large company. But the company that made the product originally is a huge defense contractor and does some shoddy work so I'm guessing too many cooks in the kitchen is the reason. Thanks for the info. 
 SELECT title, purchase_price, purchase_price, ROUND(0 +(purchase_price *.06), 2) AS "Cost + 6%", ROUND(purchase_price +(purchase_price *.07), 2) AS "Cost + 7%" FROM movie WHERE genre_code = 'SH'; It's a bit weird looking though, you've got purchase price as it's own column twice and you're being unnecessarily complicated achieving what purchase_price * 1.07 would do much more easily. Also, you are now academically obligated to include a citation to this reddit comment in your bibliography.
sqlzoo.net
&gt; academically obligated upvote earned
Oops, thanks!
Do the COLLATE in your WHERE clause. WHERE PRICE_BOOK = 'LEVEL 1'COLLATE SQL_Latin1_General_CP1_CS_AS
Codeacademy has a SQL course.
I'm somewhat interested in learning a bit about powershell and I have taken an introductory course in SQL. Could you dumb down what powershell is used for and how. And also what you were using it for in this scenario. 
You can just do my work for me. 
I can at least try! PowerShell is Microsoft's automation platform and associated scripting language. I think [this blog post](https://ramblingcookiemonster.wordpress.com/2013/12/07/why-powershell/) does a great job of explaining PowerShell - far better than I can. As for what the script is doing in this specific scenario: * Creates a variable called $Params. * Sets $Params equal to the results of a sql query. If you were to look at just the results of $Params, you'd get an array: 2013, 2014, 2015, 2016 * The main SQL query makes a reference to $VarYear. We want to set each row of the $Params query equal to $VarYear. The $years variable does this using the for each loop. If you looked at the results of $years, you'd have the array: VarYear = 2013, VarYear = 2014, VarYear = 2015, VarYear = 2016 * The final foreach loop goes through the above array and inputs the assigned value into the main query above. It then takes the results of that query and pipes it to an excel file with the assigned value's name as the worksheet name. Does that make a little more sense?
Aha. So it sounds like you already have a solution figured out and are trying to find someone to talk you out of it. I mean ultimately you either need to replicate your process on the other DB and pick up new rows / update (I'd keep the original and add the new and delineate the difference with a Boolean `isActive` flag) or you're going to need to modify your where statement go back "indefinitely" (whatever reasonable time frame works, +/- 24 hrs maybe) to pick up new updated rows. 
&gt;Build your own virtual lab and for instance doe the project real from Ms. What project from where? 
I'm not too experience with PowerShell but I wrote wrote a jscript file that I integrated into the windows shell so I could right-click an SQL file and click "Run Report" and it would kick out an XLSX or XLSB file. If you put multiple queries inside the SQL file it would put the results of each on it's own tab. And if you put comments above each query it would use that as the sheet name. If you want to use it take the below script and save it in a .js file and be sure to edit the connection string data for your server. Then you can just drag SQL files onto it and it'll do it's magic. To integrate into the shell you have to use some special tools but basically it looks something like this: CScript "C:\TheFile.js" "%SQL File Path%" WScript.Echo("START!"); for (var a = 0, arg = WScript.Arguments; a &lt; arg.length; a++) { var qry = ReadTextFile(arg(a)); var cns = "driver={sql server};server=127.0.0.1;database=MyDatabase;uid=MyUsername;pwd=MyPassword;"; // Try to find Sheet names if specified in comments var n, nam = qry.match(/(^|[\r\n]+)-{2,}[\r\n]+--[^\r\n]+[\r\n]+-{2,}/g) || []; for (n = 0; n &lt; nam.length; n++) nam[n] = nam[n].replace(/^[\s-]*|[\s-]*$/g, ""); RunQuery(cns, qry, function (RS) { WriteXLSX(RS, nam, arg(a)); }); } WScript.Echo("DONE!"); function ReadTextFile(p) { with (new ActiveXObject("ADODB.Stream")) { Open(); LoadFromFile(p); if (ReadText(1).charCodeAt(0) &gt; 255) { Close(); Open(); Charset = "UTF-8"; LoadFromFile(p); } else { Position = 0; } var TXT = ReadText(); Close(); } return TXT; } function RunQuery(cns, qry, fnc) { var AC = new ActiveXObject("ADODB.Connection"); AC.CommandTimeout = 600; AC.Open(cns); fnc(AC.Execute(qry), nam); AC.Close(); } function WriteXLSX(RS, NM, strPath) { //var strNames = RS.Source.match(/from\s+(\[[^\[\]]+\]|\S+)/gi); // Define formatting for use with given data types var datTyp = { 2: "0", 3: "0", 4: "#,##0.00", 5: "#,##0.00", 6: "$#,##0.00", 7: "yyyy-mm-dd hh:mm:ss", 14: "#,##0.00", 16: "0", 17: "0", 18: "0", 19: "0", 20: "0", 21: "0", 129: "@", 130: "@", 133: "yyyy-mm-dd", 134: "hh:mm:ss", 135: "yyyy-mm-dd hh:mm:ss", 200: "@", 201: "@", 202: "@", 203: "@" }; with (new ActiveXObject("Excel.Application").Workbooks.Add()) { while (RS) { with (Sheets(Sheets.Count)) { Select(); if (NM[Sheets.Count - 1]) Name = NM[Sheets.Count -1]; // Inject Recordset Data Cells(2, 1).CopyFromRecordset(RS); // Write headers and set data forat for (var c = 0; c &lt; RS.Fields.Count; c++) { Cells(1, c + 1).Value = RS(c).Name; Columns(c + 1).NumberFormat = datTyp[RS(c).Type] || "General"; } // Format sheet layout with (Rows(1)) { Interior.ThemeColor = 2; Font.ThemeColor = 1; Font.Bold = true; Borders.LineStyle = 1; Borders.Weight = 2; } Cells(2, 1).Select(); Application.Windows(1).FreezePanes = true; Cells(1, 1).Select(); Columns.AutoFit(); for (intCol = 1; Columns(intCol).Cells(1).Text != ""; intCol++) { if (Columns(intCol).ColumnWidth &gt; 35.86) Columns(intCol).ColumnWidth = 35.86; } Rows.RowHeight = 15; } RS = RS.NextRecordset(); if (RS) Sheets.Add(null, Sheets(Sheets.Count)); } Sheets(1).Select(); SaveAs(strPath.replace(/(\.[^\.]+)?$/i, ".xlsb"), 50); //SaveAs(strPath.replace(/(\.[^\.]+)?$/i, ".xlsx"), 51); Close(); } } 
https://technet.microsoft.com/en-us/library/cc966416.aspx Ms being Microsoft, on mobile so autocorrect had its field day. Project Real being a real project for sql server 
Yeah I was hoping for a kind of don't do that do this and a really easy solution for it being available lol. It's one of those jobs where im on my own in these matters. Sometimes you need someone to bounce ideas off of! I'll probably separate the comment process but keep the processing in the same SSIS package tbh. Cheers for your time bud!
Use regex and some regex-supporting editor to replace commas with another separator, tab, for example, and import the result.
www.hackerrank.com is kind of fun
Holy shit. I *really* needed something like this, right about now! Thank you so much!!! /runs off to test
thank you!
thank you!
best way would be to concatenate the files using something from commandline... maybe drice:path\type *.sql &gt; newsqlfile.sql? 
Ahh gotcha. Okay thanks for answering!
This is awesome, but I have to wonder why not use something like SSRS to accomplish the task -- if the end goal is to make a report? (This could be my ignorance of SSRS; I work almost entirely in SSMS with a bit of PowerShell scripting and the two almost never meet unless I'm setting up backup tasks or SSIS packages.)
Well you said next to nothing about what those results are and what the files do but anyway - try storing results in a global temporary table, for example.
The files are very long SELECT FROM WHERE statements. The results are roughly 15 columns by 22,000 rows. 
1. I would like to run "a.sql" 2. I would like to get the results, meaning the data ouput from "a.sql"and use it with "b.sql" Any suggestions? 
SQL Server Express is probably the easiest - you can directly import your Access data, and then continue to use Access as your front-end while storing the data in SQL Server Express.
What database product are you using? This wouldn't work in SQL Server, for example.
Looks like Access has a 2gb limit and SQL Express has 10GB. Not sure how long it too you to hit the limit in access but that could be something to consider. I bet a lot of your data is unchanging week to week. My gut feeling is that your table is all the data for each week to make reporting easier as opposed to only tracking data thats changing like Benny suggests. You might consider normalizing the table a bit to save space - you don't have to go crazy normalizing everything but it could be an exercise in learning about it. Say 5 columns of your data represent customer information for the sales. If that data is pretty static, you could create a customer table and in your main table replace those 5 columns with a single CustomerID column. This will save space but there are a few minor trade offs. * your weekly upload just got a little more complicated and you'll need logic to add new customers to the table if they don't already exist * if the customer information changes you'll only keep the most current info unless you begin to trend that too. Most times it doesn't matter but you never know. * reporting on the data will now require joining up the tables. Its not hard but it does require an extra step. You can create a View so users can select off that instead of joining up the tables if you need to simplify things. 
Sorry, but that does not work You will get the following error message: "All queries combined using a UNION, INTERSECT or EXCEPT operator must have an equal number of expressions in their target lists."
yup. so global temp tables will work. Open another query window under the same login, run your "select &lt;stuff&gt; into ##t_a from..." and do whatever you need (join, union, etc.) with tables ##t_a, ##t_b, etc.
Totally, I'd love to see what you come up with and use it to improve my own library. My last job people needed a ton of custom reports run from the database and I'd often needed a fast way to dump results into a perfectly formatted workbook consistently so I wrote this. Was super handy to just quickly modify a query, save it as something new, then right click, run it, then send the resulting excel doc to whoever. It requires Excel to be installed on the host box however so if you're going to run it from a server you'll need it on there. I also took some time to analyze the XLSX structure and built my own Node.js library to construct Excel files; but it requires that 7-Zip be installed to perform the compression part.
First question would be, why do you need to do that ?
If available he can also look into Change Data Capture. Although if you're using access I'm going to assume you don't have access to a SQL server enterprise box.
Just putting the cloud out there as an option. You can push the data to SQL Azure, for example, to a 250GB database for $15 a month. That being said, definitely recommend if you want snapshots that you: * implement a date modified and date created and a soft delete column and a row identifer column in your main table if you don't have them already * create a snapshot table which is identical to your main table structure but has two extra columns: DateFrom and DateTo * every day run a query pulling all rows where created or modified is greater than the last snapshot time and insert them into a snapshot delta table with DateFrom filled in * And then fill in DateTo for all other rows in the snapshot table whose row identifiers you just inserted, as well as the rows with soft deletes in your main table * Then just run select * from snapshot where date between datefrom and isnull(dateto,'4000-12-31') to get your data as of a certain date Congratulations, you're now in the data warehousing business. * 
If you just want the datasets to be merged into one, can't you use PowerShell to call the a.sql , b. sql and other files and then join the data later in PowerShell ?
It might look like you got MM/DD/YYYY if you run that query today but tomorrow it is going to look like a month has passed because it is actually DD/MM/YYYY which is european format, as stated in your SQL comment. You do not want 103 in that format, you want 101. For your other columns look at https://msdn.microsoft.com/en-us/library/ms187928.aspx and check out the different format options. Depending on exactly what you are looking for 100 might be for #2 and maybe 114 for #3?
Use MicroStrategy Desktop it is 100% free. They say its for a 30 day trial but thats a lie, its only if you want to load it up to server. Other tools cost $1000 but you get this one for free. https://www.microstrategy.com/us/free-trial/desktop
Oh, man. THANK YOU. I can figure it out from that. 
ITD 250, distance learning at VW? I recognize this homework problem. I think we're in the same class!
If you run this query: select * from pc where speed=600 and ram=128 you'll see that a given model can appear in the PC table multiple times. Model 1121 (600/128) is available with an 8gb and a 14gb hard drive. In the available database, there aren't any other models available with 600MHz processors and 128MB RAM. But, if model 001 (600/128) existed, your results would include 1121 &gt; 001 twice. If *two* model 001 (600/128) PC's existed, you'd get *four* rows in your results, since each of the two 1121's is greater than each of the two 001's. DISTINCT will remove these duplicates for you, as would "GROUP BY P.model, L.model, P.speed, P.ram" (grouping by all the selected columns is always identical to DISTINCT).
That now returns everyone but its ignoring the mangerid requirement. Half tempted to bang this in a temp table but surely its should be simpler than this? Normally when i struggle this much i am missing something stupidly obvious.
In your working example, just replace the code for the "from cte" to make it a subquery with your cte code "from (ctecodehere) as cte" The rest should work. I'm on mobile else my example would be a bit better.
One more try: select distinct c.display_name, (select count(1) from support_log where support_id=c.id and dateclosed is null) from contacts c where ManagerID = 1234
Not sure I follow, I think your saying add this to cte as sub query but, the cte already has 10+ queries and think this would cause more confusion trying to fit it in. As someone has provided working example will work on that as I need to add this into each of the queries for cte. Thanks for taking time to reply.
Other way around. with cte as( select id , x.count from contacts c cross apply ( select count(support_id)[Count] from support_log sl where c.id = sl.SUPPORT_ID and sl.DateClosed is null )x where ManagerID = 1234) Select c.DisplayName, cte.Count From cte inner join contacts c on c.id = cte.id order by displayname Becomes select c.DisplayName, cte.Count from ( select id , x.count from contacts c cross apply ( select count(support_id)[Count] from support_log sl where c.id = sl.SUPPORT_ID and sl.DateClosed is null )x where ManagerID = 1234) as cte inner join contacts c on c.id = cte.id order by displayname Edit: best formatting I can do on mobile ATM, sorry again.
That works and is easier to read but wont work inside the cte I have. Did not dig into specifics as already moved in the above method to the query i have. thanks again.
Never saw the first syntax too. I'm curious. It my guess (if the first one really works) it handles the same way. 
It was on a view I was looking at, so it must work, right? The code formatting for the view was bizzarro too... SELECT TableA.Field1, TableA.Field2, TableA.Field3, TableB.Field4, TableB.Field1, TableC.Field2, TableD.Field3, TableE.Field4 --Way more fields all smashed together FROM TableA AS TableA INNER JOIN TableB AS TableB ON TableA.Field1 = TableB.Field3 INNER JOIN TableC AS TableC ON TableA.Field3 = TableC.Field1 INNER JOIN TableD AS TableD ON TableB.Field4 = TableD.Field2 ON TableD.Field3 = TableC.Field3 ON TableD.Field6 = TableB.Field3 INNER JOIN TableE AS TableE ON TableE.Field4 = TableC.Field2
If you have a functional background in programming then SQL should be simple. Best advice is to just do it. Services / books are adequate, but what you need to do is design your own database and fill it up with data that is meaningful to you... and then start learning how to figure out answers to increasingly more complex questions. Baseball stats is an easy example. Calculating someone's ERA season over season. Fairly simple. But who had the highest ERA in the 2012 season for a team located in a city with the highest per capita income? Etc.
The first option does not work, and is more or less what i wrote to begin with. The second option works and quite like. Cheers.
Not sure what you're asking here - you could reference the temp table with an alias like so: SELECT * FROM #Something s The table is then aliased as s. Don't see how or why that would be useful to you in this context though so you might need to explain what you're trying to achieve. But yes, that is one method of creating a temp table that is already populated with data. e: oh, do you mean an alias for the field name? Yes, if you tried DROP TABLE #Something SELECT DateEntered AliasName INTO #Something FROM mytable The column in #Something would be AliasName instead of DateEntered.
either you are presenting a problem incorrectly or the problem is wrong: the 'excluding itself when calculating average' would mean that the set that you use to calculate the "average" cannot contain numbers higher than the average (and therefore cannot contain numbers lower than the average). I.e. the 'remainder' can contain places with (and only with) the minimum y-number.
Whatever the case, I'd encourage you to update the code to look better, with better formatting. You can make intermediary tables instead of that complex join. If it's really necessary, include comments. Since the first syntax is unusual (I've also never seen that), I'd encourage you to update it. If the second version works the same way, and everyone (myself included) understands it more, go that route.
Considering it's an assignment, they probably were trying to think of a way to make it a correlated subquery but due to trying too many beers made an odd request. 
I found the website below to be really helpful. It's free :-) https://academy.vertabelo.com/
Good video. I would like you to put together one on clustered index tables talking about page splits when variable length data is updated and the row won't go back into the original place. This is a cause of index fragmentation when there have been no inserts. Keep up the good work.
Next 2 Videos will be about updating data in HEAP and CLUSTERED INDEX. It will also cover what you mentioned. But be careful. In a CLustered Index a row never goes "back". A page split is permanent. What you are talking about is probably HEAP Tables and forwarded rows. If such a row shrinks again it can get back to the original page undoing the forwarding. But yeah it will all be in the next 2 videos :) Thanks for your feedback!
Aaah a wonderful question. It's SQL Server.
[R](https://en.m.wikipedia.org/wiki/R_(programming_language))
For number three: aggregate functions require the group by clause NOT the order by. As was stated earlier, order by and group by are not dependent on each other. 
These two things are completely different. If you are trying to aggregate any of the fields in your SQL query, then you have to group by all the rest of the fields you aren't aggregating. (If you want to count how many movies you have per genre, then you need to group by the genre.) Order by is how you are sorting the results. 
1) GROUP by will result in the ability to do aggregate functions as well as reducing duplicates in some instances. 2) Anytime you need to do the above and have the results provided in a specific order. 3) No, aggregate functions don't need ORDER BY. Some windowed functions require an order by in order to provide the results you would want. Typically your query will work in this order. 1) JOINS 2) WHERE 3) SELECT 4) GROUP BY 5) HAVING 6) QUALIFY 7) ORDER BY
It can be anywhere, you just need to specify the path of the file in your ETL process.
That 'Test_Budget_Import' seems to be either a table or a table-valued function, if it is a function, check its code to see what it does exactly, if possible post it here.
Test_Budget_Import is the excel file it will be grabbing the data from. Sorry if I was unclear about that The code above is the only code he has. No reference to the excel file until he calls from it. 
I'm pretty sure SQL Server can't do a select from any file directly like that. What you can try to do is something like this: select * from OPENROWSET('Microsoft.ACE.OLEDB.12.0','Excel 8.0;HDR=NO;Database=C:\File path\File.xlsx','select * from [sheet1$]') EDIT: OH, try to look if there isn't a Linked Server with that name, look into Server Objects -&gt; Linked Servers in the object explorer
Thanks for the tip. I just put that in a new query window to see what it will do and I got the following error. I have MYSQL 2008 if that matters. Cannot create an instance of OLE DB provider "Microsoft.ACE.OLEDB.12.0" for linked server "(null)". EDIT:Here is the path select * from OPENROWSET('Microsoft.ACE.OLEDB.12.0','Excel 8.0;HDR=NO;Database=\\server\files\Information Technology\Projects\Budget\Test_Budget_Import.xls','select * from [sheet1$]') 
I see something called ADSI.
Depending on the length &amp; complexity of your data, you can use Excel to generate a bunch of INSERT statements for you. Add a column on the far right end of your data set and build a SQL string, something like this: ="INSERT INTO #MyTempTable (ColumnA, ColumnB, ColumnC) VALUES ('" &amp; A1 &amp; "', '" &amp; B1 &amp; "', '" &amp; C1 &amp; "');" Then drag the cell down the length of your spreadsheet using the auto-repeat function so it creates a new INSERT statement for each row. Finally, select your new column and then copy &amp; paste the entire batch of INSERT statements into SQL Studio.
&gt; aggregate functions require the group by clause Not exactly true. SELECT AVG([Price]) FROM [Products] Perfectly valid although why you might want it I can't guess. Valid and useful might be SELECT SUM([Value]) AS CurrentBalance FROM [MyBankTransactions] Note that I'm processing the whole table as a whole and there are no groups
Just create an SSIS package to insert the data into a table. That way you can run it when you need on a schedule to refresh the data (truncate/insert or just insert new records)
Not sure the volume of data you are processing, but one minor tweak you may want to do is change the -1 hour offset. I'd do a lookup on your table for max(date) and use that for the lower bound. That way if the server reboots or the script doesn't run it'll pickup from where it left off. You should be able to simply use getdate() for your upper bound without the dateAdd. I can see continuing to leave the getdate() in there as the machines may not be in sync or there is a bad data point in the log with 1/1/2020 in there or whatever.
`Calendar Date`is the field name- the data held there is 100% date format.
Haven't used Access in an age, but you have no FROM clause corresponding to your first SELECT.
I'm actually in the perfect place foe this book. Started studying for the 70-461 exam a month or so ago. I'll report back about if this book helps in my studies. 
 DROP TABLE #Something SELECT A.Name , COUNT(A.Height) as HieghtCount -- column needs a name INTO #Something FROM mytable AS A INNER JOIN anothertable AS B ON A.Name = B.Name GROUP BY A.Name HAVING COUNT(A.Height) &gt;= 10 
This should do the trick Select substr(FoodName, 1,1), count(*) from Food group by substr(FoodName, 1,1) order by FoodName asc;
you were doing so great... until that horrible ORDER BY clause
Alright so fixed that and simplified things to try to get the method worked out before going for the whole shebang. I've got it pulling data but it's repeating what appears to be a single 30 day total for each Master ID that doesn't change by week. *for whatever reason, it didn't apply the code formatting! Sorry!! SELECT DATEPART('ww',`Calendar Date`,3,12/29/2015), `Agent Master ID`, SchedThirty.TSched FROM `'Reliability by Agent$'` , ( SELECT Sum(Sched) AS TSched, `Agent Master ID` as EID FROM `'Reliability by Agent$'` WHERE (CDATE(`Calendar Date`) BETWEEN (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42390 AND (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42360) GROUP BY `Agent Master ID` )AS SchedThirty WHERE SchedThirty.EID = `Agent Master ID` GROUP BY `Agent Master ID`, DATEPART('ww',`Calendar Date`,3,12/29/2015), SchedThirty.TSched 
um, FoodName is in the select list?
It is but not in the way the database needs it. You could alias the column in the SELECT clause and use the alias in the order by. Here are some examples: **Table Contents** SQL&gt; SELECT * 2 FROM fruits; FRUITNAME -------------------- Green Apple Green Grapes Red Apples Red Grapes Oranges **ORDER BY Column Name** SQL&gt; SELECT SUBSTR(fruitName,1,1) 2 , COUNT(*) 3 FROM fruits 4 GROUP BY SUBSTR(fruitName,1,1) 5 ORDER BY fruitName; ORDER BY fruitName * ERROR at line 5: ORA-00979: not a GROUP BY expression **ORDER BY Function** SQL&gt; SELECT SUBSTR(fruitName,1,1) 2 , COUNT(*) 3 FROM fruits 4 GROUP BY SUBSTR(fruitName,1,1) 5 ORDER BY SUBSTR(fruitName,1,1); SUBS COUNT(*) ---- ---------- G 2 O 1 R 2 **ORDER BY Position** SQL&gt; SELECT SUBSTR(fruitName,1,1) 2 , COUNT(*) 3 FROM fruits 4 GROUP BY SUBSTR(fruitName,1,1) 5 ORDER BY 1; SUBS COUNT(*) ---- ---------- G 2 O 1 R 2
Thank you both so much for your help!!! :) I spent a week trying to figure this out!! Thank you so much!!! 
So then you'll need to not select or group by Inventory ID. SELECT ItemName, MIN(UnitPrice), UnitsOnHand FROM tblInventory GROUP BY ItemName, UnitsOnHand
Thank you so much!
Probably beyond what you need, but if you want to return results even for letters that you don't have any food items for, you'll need a driver table that has one record per letter. To do that, use (abuse) Oracle's hierarchical queries: select DRIVER.LETTER , count(FOOD.FOODNAME) CNT from FOOD right outer join ( --list of A-Z select chr(64+level) LETTER from dual connect by level &lt;= 26 ) DRIVER on FOOD.FOODNAME like DRIVER.LETTER||'%' group by DRIVER.LETTER order by DRIVER.LETTER 
Can you provide a little more detail on the output you're looking for? Is it safe to assume that [ordnumber] is the primary key for your cost data, and that the currency amounts in USD and CAD will be for _each_ [ordnumber]?
Yes, that's correct, Ordernumber is the value we're keying off of where each order has it's own cost values stored in the CostAmounts table. The output essentially will be a static table, where column 1 ( Currency) will always have 1 row for US$ and the other for CAN$. The totalcost should have the summed amounts from each cost value while also applying the exchange rate such that if there's 2(or more) cost values with mixed currencies, they're each translated to each currency type value then summed together for each currency type. The example i gave is the best i could do though. So in the 2nd table in my post, the rate we're applying is 1.2887CDN$ to 1 USD$. So in the example i posted, there's 3 cost values; 2 in CAN$ and 1 in USD$. In the output, 2 results, 1 total in USD$ and 1 in CAN$.
What are the prices? More info would be great.
SELECT CONVERT( DATETIME, '12/20/2013 02:02:17 pm') replacing the literal string with the field/variable name you want to convert.
So my suggestion seperate your cost from your currency and create a fiscal currency conversion factor, E.G Cost 201606= $3.00 CAD Factor 201606 = 1.00 USD Factor 201606 = 1.33 That way calculating cost becomes Select Cost * Factor Where CurrencyID = X and FiscalPeriod = Y and then drive it back to your orders/order items so that you can see order costs on a margin report. If you create a table of exchange rates between your countries per period it becomes a matter of simply joining to that table to get the rate.
We're on compatability 80, cant use PIVOT :S
You don't have to. Ever. "UNPIVOT" is just a syntax sugar - a convenience, not a different functionality (applies to PIVOT as well). Just cross join to (values ('CAD'),('USD')) t(currency_tag) and use 'case' expressions in your select. 
Here's Microsoft's free course on data warehouse implementation https://mva.microsoft.com/en-us/training-courses/implementing-a-data-warehouse-with-sql-server-jump-start-8257?l=KtvLHdKy_7204984382 When you say SQL on Azure do you mean an Azure VM with SQL or the Azure SQL DB PaaS offering? 
 SELECT DATEPART('ww',Calendar Date,3,12/29/2015), Agent Master ID, SchedThirty.TSched FROM 'Reliability by Agent$' , ( SELECT Sum(Sched) AS TSched, Agent Master ID as EID FROM 'Reliability by Agent$' WHERE (CDATE(Calendar Date) BETWEEN (DATEPART('ww',Calendar Date,3,12/29/2015)7)+42390 AND (DATEPART('ww',Calendar Date,3,12/29/2015)7)+42360) GROUP BY Agent Master ID ) AS SchedThirty WHERE SchedThirty.EID = Agent Master ID GROUP BY Agent Master ID, DATEPART('ww',Calendar Date,3,12/29/2015), SchedThirty.TSched Have you tried omitting the last two arguments to the DatePart function? I can't tell why you have those. The `3`, for instance, I believe tells it that you want your weeks to start on Wednesday. Is that your intention?
Okay so achieved nothing trying to do that... The core issue is that I need to summarize 30 days of data under a dynamic week title that changes week over week and lists 12 weeks at a time BY Master ID... This one is far beyond my level of SQL experience right now...
I can tell you that the table is as follows: Master ID | Date | Sched | Absent And the data is daily as long as the person has something in the Sched field.
Accidentally replied to myself; see below.
May have just had a breakthrough with it... It seems to be properly pulling by 30 day rolling. I just had to get a "Thirty End" rolling date and then subclause that with the week start and Master ID. Then select those 3 fields from the subselect and my Sched and Absent time from the original table... Then I just had to do the same CDATE BETWEEN comparison (much shorter without having to calculate the fields, too) and then connect the master ID from the subclause to the Master ID from the primary table! Sort of what I tried last but I was going about it the wrong way. My brain is fried for the rest of the day I think though. SELECT Weeks.EID , Weeks.WeekBeg , Weeks.ThirtyEnd , Sum(Sched) AS R30Sched , Sum(Absent) AS R30Absent , 1-iif(Sum(Absent) &gt; 0, Sum(Absent)/Sum(Sched),0) FROM `'Reliability by Agent$'` , ( SELECT `Agent Master ID` AS EID, (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42360 AS WeekBeg, (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42330 as ThirtyEnd FROM `'Reliability by Agent$'` GROUP BY `Agent Master ID`, (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42360, (DATEPART('ww',`Calendar Date`,3,12/29/2015)*7)+42330 ) AS Weeks WHERE CDATE(`Calendar Date`) BETWEEN Weeks.ThirtyEnd AND Weeks.WeekBeg AND `Agent Master ID` = Weeks.EID Group by Weeks.WeekBeg, Weeks.EID, Weeks.ThirtyEnd 
 if @parameter = 1 SELECT * FROM table ELSE SELECT * FROM table WHERE OnHandQty &lt; MinQty OR... Sorry for formatting! Other post has correct where clause.
Glad you got it working
You can do SELECT A.* FROM MyTable A OR SELECT * FROM MyTable A OR SELECT * FROM Mytable But unless you have a table named A SELECT * FROM A won't work
You can join on any column regardless of whether it is a key. If you want to join tables without joining on any attribute you would have to use cross joins, such as SELECT something FROM table1, table2
The biggest question is why do you need to see these two tables together? Surely you must have some table linking the two of them together at some point. Joining them via that link would surely provide more information. Otherwise you can use union to simply append the data, but there are a few restrictions to work around. 
run both queries, compare results, post your analysis of differences, if any and if there aren't any, then...
Your columns are not fully qualified. In the SELECT portion, you need to qualify which table order# comes from. So replace: count(order#) with count(orders.order#) The reason being that USING will match and make that one column on multiple tables act truly as one column. Using the ON criteria you end up with duplicate columns that need to be qualified. You may also have this issue with the other fields used in the SELECT and GROUP BY clauses but I don't think it is part of your issue.
I know i said i understood your initial comment fully. I did for the most part, but can you just explain this a little more? &gt;The reason being that USING will match and make that one column on multiple tables act truly as one column. Using the ON criteria you end up with duplicate columns that need to be qualified. 
An excellent explanation! Thank you :) 
SQL is a must for sure, but even then I'm not great at it; but continuously learning to get better. Will the skills learned in the web based class transfer over - like will those skills even be needed for data analysis? Lastly, is there a lower level / entry level position that data analysts tend to start off at?
i believe USING is standard sql despite that, most people recommend against using it 
Which one's, to be specific? And is Microsoft SQL Server the most used database platform? 
Well a lot of things you can learn on the job. I learned everything starting from SQL even on the job. As a data analyst, havIng Webdevelopment skills might not necessarily be needed in your day to day work, but it would definitely be in your benifite to understand how everything interacts. Php, sql, css all that. I worked for a smaller company as well, so I also needed to learn some Web Development. So depends on where you find a job as well, but I don't see it being a negative in any case. E: and any good company will not necessarily throw you into the rabbit hole right away, you will generally have someone you report to, or who will give you assignments/projects as you will probably only be hired for entry level positions coming right out of school with no workplace experience. 
Lurking. I've shifted from finance to IT myself and getting into SQL and Hadoop to try to be a data analyst. 
i believe USING is sql99 https://oracle-base.com/articles/9i/ansi-iso-sql-support
Thanks! It's I can't believe you can't use the using columns for filtering.
IT is a little vague, what's your job title right now / what do you do? Thanks.
I like MSSQL a lot. I've heard that Oracle is awful to work in. 
If you have access to it while you're in school, I would try to pick up SAS. It's used by tons of large corporations. I think the university edition is free
Dumb question from someone who hasn't dared step beyond the MS stack: what is a materialised view and what does it do that an indexed view doesn't?
Have you received this certification?
I'm confused, do you mean a scripting language like R, Python, and SQL? Or is it R or Python, in addition to SQL? Also I have no idea what Tableau is. Mine giving an ELI5 definition sir?
You need SQL. You need Excel. You need either R or Python. Tableau is a program that lets you make pretty graphs quickly and easily. 
I hope I'm understanding the need here properly. This is an approach you can use to get it all in one query. I don't know how it would perform in your environment. MySQL is kind of limiting in that it doesn't use CTE or window functions. You would need to use the store product penetration as a base to attach the higher up values. SELECT ... FROM ( -- Store Product Penetration Summary SELECT stores.accounting_month , stores.business_unit , stores.cust_parent_id , stores.cust_parent_id , stores.store_id , stores.store_name , products.product_code , COALESCE(store_product_rev.revenue,0) * 100 / store_rev store_penetration FROM ( -- Stores SELECT accounting_month , business_unit , cust_parent_id , cust_parent , store_id , store_name FROM MasterTable ) stores CROSS JOIN ( -- All Products SELECT product_code FROM MasterTable GROUP BY product_code) products LEFT JOIN ( -- Store Product Revenue SELECT accounting_month , store_id , product_code , revenue FROM MasterTable ) store_product_rev ON stores.accounting_month = store_product_rev.accounting_month AND stores.store_id = store_product_rev.store_id AND products.product_code = store_product_rev.pr LEFT JOIN ( -- Store Revenue SELECT accounting_month , store_id , SUM(revenue) tot_rev FROM MasterTable GROUP BY accounting_month , store_id , revenue) store_rev ON stores.accounting_month = store_rev.accounting_month AND stores.store_id = store_rev.store_id) str_prod_pen LEFT JOIN ( /* Parent Product Penetration Summary */) parent_prod_pen ON store_prod_pen.accounting_month = parent_prod_pen.accounting_month AND store_prod_pen.cust_parent_id = parent_prod_pen.cust_parent_id LEFT JOIN ( /* Store Business Penetration Summary*/) store_business_pen ON store_prod_pen.accounting_month = store_business_pen.accounting_month AND store_prod_pen.business_unit = store_business_pen.business_unit AND store_prod_pen.store_id = store_business_pen.store_id LEFT JOIN ( /* Busines Unit Penetration Summary */) business_pen ON store_prod_pen.accounting_month = business_pen.accounting_month AND store_prod_pen.business_unit = business_unit;
OP you better at least buy this guy some reddit gold for that. 
Sure, but isn't that the same as an indexed view in SQL Server? A view that physically sits on the drive, all indexed and ready to go?
You mean something like this? Always restore to a test database and try there first if you're not completely sure about it. DELETE FROM TableA WHERE email in ( SELECT TableB.email FROM TableB WHERE TableA.email = TableB.email ) 
Hi, exactly. But also I need to compare more columns. So something like this: DELETE FROM TableA WHERE email in ( SELECT TableB.email FROM TableB WHERE ((TableA.email = TableB.email) AND (TableA.hostname = TableB.hostname)) ) I don't know the exact syntax on that. Thats my problem. I could try to copy some of the data to 2 new tables to try. I don't get a backup on those data. Not enough space on the server :D
My read is that Python is more generally useful. R is great for advanced statistics; I have a friend who swore by it for his Criminal Justice dissertation. But Python has applications outside of analyzing a dataset. Without knowing either really well on a practical level, I'd recommend favoring Python in your decision-making paradigm.
Holy God all I can say is when you do go live with this make sure you wear a gigantic foam cowboy hat because this is some bad practice at work here. If you can't get a backup at the very least do a select * into so you at least arent working on production data to fine tune your query.
1 - Yup, that is exactly how you add more fields to compare in that query. 2 - NEVER stay without backup, buy an external HD (it is cheap) and make the backup go there, lack of space is never an excuse, if your company is quite small then more space for backup is cheap, if your company is large and requires several TB of space then the lack of backup indicates a terrible management and poor IT planning.
Where (a.email = B.email or a.email=B.email2) Sorry for the formatting, I'm on my phone. You can string many ors together like that, just keep them all in the parentheses. 
Thank you! I got it now and the query is running as it should. I know that we have to do backups.. But the company I work for is too damn big as to get fast Backup-Services on a Server with test-data.. It will take about 1 month to get that an I needed a fast solution :)
No, but it would be good for my career if I did.
I tried that but that SUM includes records where SUM(plusmin) = 0.
&gt; Also for R/Python - how does one determine which to learn? I personally do not see these as interchangeable. Python is a scripting language used to a wide range of things. R is much more statistics / data based.
I learned from practice and the youtube channel excel is fun. When you get the core of it down and know how to work it, I find it's similar to SQL where I can google for concepts or ideas and build my own solution putting the pieces together.
The varchar storage size is 2 bytes + length of the string (which depends on collation, for majority of the western cases, 1 letter = 1 byte).
No criticism but a comment, SQL is a vast world. Getting the syntax, tuning, and performance down is a huge step. From there, it's all about direction. (Administration, development, and reporting.) Having a background in any of the three or knowing anything from them will correlate and help the other skills.
HAHA, then it ended in my 2nd example, poor planning, it is sadly common.
Wouldn't this work for you? SELECT SUM(stndchg-actchg) AS Savings FROM accttable WHERE acct = '123456789' and plusmin != 0 This will work as long as [plusmin] only has positive numbers.
Allow me to elaborate. Here's my table: Record|StndChg|ActChg|plusmin :--|:--|:--|:-- ABC|200|100|1 ABC|300|200|1 DEF|400|300|1 GHI|500|400|1 JKL|600|500|1 JKL|600|500|-1 So the idea is to take the SUM(StndChg-ActChg) of those records whose "net" plusmin !=0, then add that result together. Record|Savings|netplusmin :--|:--|:-- ABC|200|2 DEF|100|1 GHI|100|1 JKL|200|0 The final result would not include record JKL since its netplusmin = 0. I'm looking for a SQL query that would produce one value, in this case 400. 
That's the issue, I'm looking for where "net"plusmin !=0 (see above comment)
Full disclaimer: I use R almost daily, and I'm no more than an absolute beginner with Python. There's really no bad choice here. R is more popular among data scientists who spend a lot of their time modeling, but Python seems to be slightly more favored if you're headed in more of a data engineering type of role. But really, in almost all cases, anything you want to do with data is completely doable with either language. The important part is getting good at one of them. The bad choice here would be if you were mediocre at both. Get comfortable with one of the two, so that you can pull off what you want to do easily. Only then should you focus on trying to learn the other. As far as being less objective: I love R. It's a hugely growing language, and package development is kind of insane right now. People are constantly making new tools in R that make things simpler and faster. And even though it has a reputation among serious programmers of being a little weird, I almost never have trouble in getting it do to what I need.
I would say statistics is most important. R, SQL, Python, etc are just tools to do statistics. If you want to do Data Analysis you need statistics. If you want to do database management, data processing, data warehousing, that would be more of a Database Administrator or Database Engineer, or Database Developer. In this case you don't really need statistics. But you said Data Analysis, you need statistics.
nah, its a work problem. I'm a sr software engineer here, just new to sql (usually i do everything with an ORM) I'm trying to work on a View that will do all that work for me so that it's tied to the database and not my application logic. (it will simplify the application)
i was thinking that, but I wonder if there is a more efficient algorithm. 
 select (select top 1 column_1 from [TABLE] where column_1 is not null order by date_modified desc) as column_1 , (select top 1 column_2 from [TABLE] where column_2 is not null order by date_modified desc) as column_2 , (select top 1 column_3 from [TABLE] where column_3 is not null order by date_modified desc) as column_3;
sooo still require subqueries... Would the query optimizer make the two types the same... 
Let me see if I understand what you want to do first. You want to take the last (or first?) row where `pipeline_name = 'In Progress'` and take the issue_updated_at timestamp. Then you want to take the first row where `pipeline_name = 'Closed'` and take the issue_update_at timestamp. Then you want to take the difference between them. I'm assuming that you want this for every repository_name and issue_number pair? e: In your example the correct answer should be 2016-09-12 19:31:50 from 2016-09-14 12:41:53, correct? e2: This should get you where you're going: with cte as ( select row_number() over(partition by repository_name, issue_number, pipeline_name order by issue_updated_at desc) closed_id , row_number() over(partition by repository_name, issue_number, pipeline_name order by issue_updated_at asc) progress_id , * from [dbo].[BP_tmpBigQuery] where pipeline_name in ('In Progress', 'Closed') ) select a.repository_name , a.issue_number , a.pipeline_name , a.issue_assignee_login , datediff(hh, a.issue_updated_at, b.issue_updated_at) time_in_hours from cte a inner join cte b on b.repository_name = a.repository_name and b.closed_id = 1 and b.pipeline_name = 'Closed' where a.pipeline_name = 'In Progress' and a.progress_id = 1
The better question would be, which would be more and which would be less efficient. The subqueries or the coalesce. while i've simplified the problem, part of it is i'm given a date range. so between X and Y date find the fields that have changed. I love the idea of inserting all the data (My schema isn't limited.) (it means all i have to do is insert a new row every time an update happens rather than finding the fields that have changed) Another solution would be to have a change column table with a date (with a time stamp) for each entry and at the application level reconstruct the object based on the changes...yes i'd have to iterate through them all... and i think that would make the application code messy. 
The subqueries are definitely less efficient, it requires three queries instead of comparing three scalars. Another approach I've seen is a separate changelog table, where your original table only ever has the one record which is always up to date, and your changelog has a series of records with the column changed, before and after values and timestamp. This approach is better if your original table is tracking lots of things, not just one sentence as in your example.
You can rewrite it without the CTE, just becomes more of a pain in the ass. I do think you'll need a subquery though of some kind, or temp tables, or something like that. 
Each individual field stored as a varchar takes up 2 bytes + length of string. So in your example it would be 10 bytes (2 bytes x 5 fields) + 18(ish) bytes. Changing the size of your fields when they are all varchars will not actually change the size of the database unless and until new data actually fills the available space. edit: If you are changing from all char to varchar then determining what will happen is a bit more difficult. Right now all the char fields take up their full size, regardless of what data is in them. If you change to varchar then your data is sparse, meaning if you have a varchar(500) field and you put two characters in it you will only take up 4 bytes of data. If you fill that same field with 500 characters of data then it will take up 502 bytes. Again, all -ish, depending on collation and language settings. You could be saving a lot of space depending on how things were set up before.
Just depends on the field where you want to be a data analyst. From my experience R is used more in the scholarly field/research and Python is used more on the business/developer end for obvious reasons. Unless you plan to get a masters and conduct research, I'd lean towards Python. 
Microsoft SQL Server? Use SSRS. Create a report and setup a subscription with the destination of "Windows File Share". Can do CSV or Excel.
Hmm I'll have to play with it. I just got that installed in my visual studio. Do you know if its better to use SSRS stand alone or VS? Edit: Part of the issue seems to be that I have to set up a lot of different temp tables to tweak the data exactly how I need it since I can't create views or anything like that.
For simple reports I just use Report Builder... for large groups of reports that use a shared data source I'll use VS to make it easier to keep track of.
I have a lot of research to do. Thanks
Yeah this will be large. Is it best while using the report builder to make each temp table a new report but as a global table, or set them all in 1 massive report?
Ah good point. I always forget about SSIS when I think about anything that sounds like reporting.
So stored procedure the queries, then use SSRS to run the procedure which then would out put the different sheets I would need ?
IMO I would have picked SSIS as my first choice for this type of thing, interesting to see that so many of the commenters here would go for SSRS instead.
it doesnt like the first from on line 3. and it also errors out here on line 47 str_prod_pen.. i dont know this advanced of sql. to really even have an idea of working through that on my own 
SELECT D.PRODUCT, D.CUSTOMER, C.REV FROM (SELECT DISTINCT A.PRODUCT, B.CUSTOMER FROM tblA A CROSS JOIN tblB B) AS D LEFT JOIN (SELECT CUSTOMER, PRODUCT, SUM(REV) AS REV FROM tblC GROUP BY CUSTOMER, PRODUCT ) AS C ON D.PRODUCT = C. PRODUCT AND D. CUSTOMER = C. CUSTOMER 
Create an SSIS package and then schedule it to run in a SQL Job. Look into creating a dataflow task in SSIS that uses an OLE DB or ODBC Source and an Excel or Flat File Destination.
This is a simple way I might approach it: with cte as ( select a.customerid , a.customername , b.productid , b.productname , sum(c.rev) revenue from customer a inner join product b on b.productid = a.productid inner join revenue c on c.productid = b.productid where cast(datefield as date) &lt;= @enddate and cast(datefield as date) &gt;= @startdate group by a.customerid , a.customername , b.productid , b.productname ) select a.customername , b.productname , case when a.revenue is not null then a.revenue else b.revenue end as revenue , case when a.revenue is not null then 1 else 0 end as isBought from cte a left join product b on b.customerid = a.customerid and b.productid = a.productid If you have duplicate products in the b table you would need an additional step. 
Its pretty straight forward to ! Thanks again
Does your report user have execute permission on the sproc?
I found a link to back it up when I posted it but cant seem to locate it now. The link stated had info where an oracle employee actually conceded that there were more MSSQL installs than Oracle but Oracle had more revenue from their installs than the next 3-4 commercial vendors combined. The db-engines.com ranking system has nothing to do with install base, it has to do with number of mentions, technical discussions, blogs, etc. http://db-engines.com/en/ranking_definition Once you leave the enterprise and get into small/medium businesses you run into much more MSSQL including a huge number of apps that can/do run on the Express versoins. I work for a medium sized company and we have 1 piece of software (our biggest system) that runs on Oracle, 1-2 on MySql and probably 10-15 that run on MSSQL. All that being said, I prefer Oracle and write 20x more queries in it than all of the other DBs combined. 
I think this would be a great use of the MERGE statement. https://msdn.microsoft.com/en-us/library/bb510625.aspx You can compare on multiple columns as you mentioned. The requisite server version is 2008 or newer, including Azure.
use a central management server and run it against all of the servers in a group.
If it was a single email address in a column, then tsql would work. But in this case I would use C# or vbscript to do a bunch of instr checks to find all the @s and strip out the domain names.
This is not a complete SQL statement and it doesn't even have a complete 'from' part - please provide the full statement. Anywho, "You." is a qualifier that needs to match one of the table/result set aliases (well, could also refer to a object field if your DB supports ADTs as columns). "Description" needs to be a column in that table/result set.
&gt; Please be aware that "You.Description" comes from a different database. so can we see where you define it in the FROM clause? 
Sort of, the problem is that there are new files being added to my S3 bucket daily. And I want to make sure that what gets copied has the correct date on it. I guess I could do a staging table, update it, the copy it to prod. The ideal would be one single query to do it all I guess what I was hoping for is something like: &gt;COPY performance_report (column1,column2,column3,date_column) FROM 's3://path' credentials 'aws_access_key_id=cred;aws_secret_access_key=cred' CSV GZIP date_column AS '2016-09-01'; But something that actually works. Trying to make it as simple as possible. 
Try... &gt;COPY performance_report (column1,column2,column3,date_column) FROM 's3://path' credentials 'aws_access_key_id=cred;aws_secret_access_key=cred' CSV GZIP trunc(GETDATE()); Or &gt;COPY performance_report (column1,column2,column3,date_column) FROM 's3://path' credentials 'aws_access_key_id=cred;aws_secret_access_key=cred' CSV GZIP convert(date,'2016-09-01)';
 SELECT dim_person_key.person_key , fct_employees.job_key , fct_employees.dep_key , fct_employees.update_dt , dim_person_key.name , dim_person_key.empl_id FROM dw.dim_person_key INNER JOIN ( SELECT person_key , MAX(update_dt) latest FROM dw.fct_employees WHERE job_key in ('12', '11') AND dep_key in ('45', '44') GROUP BY person_key ) xx ON xx.person_key = dim_person_key.person_key INNER JOIN dw.fct_employees ON fct_employees.person_key = xx.person_key AND fct_employees.update_dt = xx.latest AND fct_employees.job_key in ('12', '11') AND fct_employees.dep_key in ('45', '44') 
You just separate everything with the vertical bar and throw the weird `:--|:--` thing in there afterwards: ( edit: this probably explains it better https://www.reddit.com/r/tabled/wiki/table-format ) header1\|header2 :--\|:-- cell1\|cell2 comes out as: header1|header2 :--|:-- cell1|cell2
What the heck is sql Saturday and are there any in Las Vegas? 
In the particular example above the follwing should work: make [2015 Number] a dimension add a calculated rank on [2016 Number] ascending, partition by Name put name, [2015 number] to rows, rank and [2016 Number] measure to columns 
SQLPro is a client for MSSQL and not a server. MS doesn't make a version of SQL for OSX. You will need a Windows machine to install SQL on or, in the near future, a Linux machine. 
This will work for this specific case: select t1.name, t1.2015_Number, t2a.2016_Number, t2b.2016_Number as 2016_Number_2 from table1 t1 join table2 t2a on (t1.name = t2a.name) left join table2 t2b on (t2a.name = t2b.name and t2a.2016_Number &lt;&gt; t2b.2016_Number) edit: to make it dynamically, you should use some PLSQL..
[removed]
yup, you're right. My query needs cleaning (I don't know any efficient way except adding a window function in sub-select then filtering by RN): ROW_NUMBER() over partition by (name) 
No problem, and thanks for the gold :) CTE's are really great by the way, for almost anything where you want to partition your data--in this case by person_key, but can be multiple columns. Definitely worth adding to your toolkit if you do any kind of data analysis.
You might want to look into alternative utilities if COPY is that inflexible ... Otherwise yeah a staging table is a reasonable solution.
I like Safari books online, but it's obviously more books. Fairly certain it has videos for MSSQL though.
Tables!!! ID | Name | Relationship :--|:--|:-- 123 | Jane Smith | Wife 123 | Bruce Smith | Child ID | 1st_Name | Relationship | 2nd_Name | Relationship :--|:--|:--|:--|:--| 123 | Jane Smith | Wife | Bruce Smith | Child
If SQL Server isn't a requirement and you can use anything, I recommend [Postgres.app](http://postgresapp.com/). It's a simple Postgres server that you launch like a regular app and lives in the menu bar. The maintainer for the app also has a [client](https://eggerapps.at/postico/) for OS X, or you could use [pgAdmin](http://pgadmin.org/).
The parentheses are optional.
It's a way to force join order in MS SQL. Usually you don't see it with INNER JOINs as it doesn't change the result in any way. Mostly used when you need to INNER JOIN several tables, and then LEFT JOIN the whole thing. For example: SELECT * FROM TableA a LEFT JOIN ( TableB b INNER JOIN TableC c ON c.Id = b.Id ) ON a.Id = b.Id The parentheses are optional. You can rewrite it like this: SELECT * FROM TableA a LEFT JOIN TableB b INNER JOIN TableC c ON c.Id = b.Id ON a.Id = b.Id The LEFT JOIN of TableC on TableB is done first, then the LEFT JOIN is made on the result. It avoids having to use a CTE and all the problems that come with that. Of course you can rewrite the above query as: SELECT * FROM TableB b INNER JOIN TableC c ON c.Id = b.Id RIGHT JOIN TableA a ON a.Id = b.Id But sometimes you can't get away from having to use nested joins (or their CTE/subquery equivalents).
Pluralsight is great for SQL, check out Paul Randal's course on corruption (he wrote CheckDB).
With a temp table you can do it with a little bit of setup and a dynamic pivot. This will allow the query to grow by just adding a simple insert for any year you wish to add: --Drop Tmp Tables IF OBJECT_ID('tempdb..#Table2015') IS NOT NULL DROP TABLE #Table2015; IF OBJECT_ID('tempdb..#Table2016') IS NOT NULL DROP TABLE #Table2016; IF OBJECT_ID('tempdb..#TableFinal') IS NOT NULL DROP TABLE #TableFinal; GO --Create tmp tables CREATE TABLE #Table2015 ( [Name] varchar(255) NULL ,[2015 Number] int NULL ) CREATE TABLE #Table2016 ( [Name] varchar(255) NULL ,[2016 Number] int NULL ) CREATE TABLE #TableFinal ( [Name] varchar(255) NULL ,[Nnumber] INT NULL ,[Nyear] INT NULL ,[NORder] varchar(255) NULL ); --Load tmp Tables INSERT INTO #Table2015 ( [Name] ,[2015 Number] ) VALUES ('Peter',44) ,('John',22) ,('Rogers',7) INSERT INTO #Table2016 ( [Name] ,[2016 Number] ) VALUES ('Peter',33) ,('John',4 ) ,('John',77 ) ,('Rogers',45); INSERT INTO #tableFinal ( [Name] ,[Nnumber] ,[Nyear] ,[NORder] ) SELECT [Name] ,[2015 NUmber] ,'2015' ,NULL FROM #table2015; INSERT INTO #tableFinal ( [Name] ,[Nnumber] ,[Nyear] ,[NORder] ) SELECT Name ,[2016 NUmber] ,'2016' ,NULL FROM #table2016; --you can contiune to insert years until you have all the data in you want, the rest is all done automatically. --This adds in a ranking so that the dynamic pivot will know what to use for headers WITH C1 AS ( SELECT DISTINCT [NAME] ,[nyear] ,[nnumber] ,ROW_NUMBER() OVER (Partition BY [Name],[NYear] ORDER BY [NYEAR]) AS [RN] FROM #TableFinal ) UPDATE F SET [NOrder] = CAST(C1.[NYear] AS varchar) + ' Number ' +CAST([RN] AS varchar) FROM #tableFInal AS F INNER JOIN C1 ON F.[Name] = C1.[name] AND F.[nNumber] = C1.[nNumber] AND F.[nyear] = C1.[nyear] --Dynamic Pivot DECLARE @cols AS NVARCHAR(MAX) ,@query AS NVARCHAR(MAX); SELECT @Cols = STUFF( ( SELECT DISTINCT ',' + QUOTENAME([Norder]) --header name here FROM #tableFinal FOR XML PATH(''), TYPE ).value('.', 'NVARCHAR(MAX)'), 1, 1, '' ); SET @Query = 'SELECT [Name] , ' + @Cols + ' FROM ( SELECT [Name] --These are the Row Labels ,[Norder] --Header Names for Pivot Data ,[NNumber] --Data wanting to Pivot FROM #tableFinal ) AS T PIVOT ( MAX([nNumber]) --data to pivot again FOR [Norder] IN ( ' + @Cols + ' )' + ' --Header name again ) AS P ; ' EXEC sp_executeSQL @query 
Edx has a series of courses developed my Microsoft that are pretty good. 
Going to do this over the weekend! Thx
Holy this sites looks really good. Have you taken any of the courses? 
The online course that Stanford provided several years ago (through the platform that later became [coursera](http://www.coursera.org)) is great. I can't find the full course anymore on coursera, but the videos seem to all be in [this list on Youtube](https://www.youtube.com/watch?v=D-k-h0GuFmE&amp;list=PLroEs25KGvwzmvIxYHRhoGTz9w8LeXek0)
I use both. I prefer Lynda's teaching and their UI but PluralSight has many more courses. They both have usefulness.
there are ... its the cross/outer apply. Anything you put into an apply() is pretty much that, a temporary table valued function. Small but very important difference thou, apply always executes as a loop join, keep that one in mind. In this case, the case statements are a scalar expression anyway, so you can't really make it worse than it is, but a good old subquery to be hash or merge joined to, maybe even a temp table, might be faster. It depends on the data which option would be preferable.
If you just want to practice SQL, you could try [SQLite](https://www.sqlite.org/). There is even a [subreddit](https://www.reddit.com/r/sqlite) for it. You can find several free [clients](http://portableapps.com/apps/development/sqlite_database_browser_portable) for accessing it.
There are plenty of MS SQL videos on safari books - and they've recently added some more. The edx course DAT201x is similar to the MVA course in complexity (as in, not quite up to the certification level).
About the SQL Injection, I totally agree, but since it is a VBA project I wouldn't worry too much. Also, OP wrote "ComboValueI is an **integer** Dim I created", so it is numeric.
You're blowing my mind a little bit. Can you elaborate or supply some reading?
Oh, I use outer apply a lot when it's appropriate. But it has limitations which would be addressed by temporary functions. With a temporary function (or a function variable) you could define it at the top of your transaction or script and use as many times as you like. Scalar functions you'd be able to use inline. With outer apply you have to define it for each query and mess with column names. Ugh.
+1 for Pluralsight. I used their course to study for my 70-461 - taking it this morning. I guess how I do will somewhat determine the quality of the course. ;-)
Just finished about an hour ago - I passed! Make sure you practice all of the syntax Pluralsight demos in that course. There were several places where I wished I had practiced the syntax more and/or put more emphasis on some of the functions I wasn't as familiar with. There's A LOT of syntax out there, so its something you want to make sure you understand well out of each module.
you didn't mention which row for each year, so here ya go, latest id SELECT t.id , t.rank , t.title , t.imdb_rating , t.myRating , t.seen , t.`year` FROM ( SELECT `year` , MAX(id) AS latest FROM top250 GROUP BY `year' ) AS x INNER JOIN top250 AS t ON t.`year` = x.`year` AND t.id = x.latest 
Is this better: with aggtags as ( select thread_id , string_agg(name || ':' || count, ',') as tags from thread_tags inner join tags on id = thread_id group by thread_id ) select title , fp.user_name as first_post_user_name , fp.user_avatar as first_post_user_avatar , lp.user_name as last_post_user_name , lp.user_avatar as last_post_user_avatar , tags from threads th inner join posts fp on th.first_post_id = fp.id inner join posts lp on th.last_post_id = lp.id inner join aggtags agt on th.id = agt.thread_id;
count(distinct customer#)
negative. please read my above comment to see if that will help.
Which table(s) are you trying to delete from? If you want to delete from `tbFileHash` (BTW, Hungarian Notation is not a recommended practice for database objects other than views): delete FH from tbFileHash as FH inner join tbFile as F on FH.FileDigest = F.FileDigest where F.FileName like '14393%' It's also the first hit when you Google "[sql delete with join](https://www.google.com/search?client=safari&amp;rls=en&amp;q=sql+delete+with+join&amp;ie=UTF-8&amp;oe=UTF-8)" If you need to delete from all 3 tables, I suggest you wrap this all in a transaction, make sure you understand your foreign key constraints, and keep cascading deletes in mind.
Yes, and PL/SQL for Oracle is analogous to T-SQL for Microsoft SQL Server. They are necessary for creating stored procedures, triggers, functions, views etc using variables, looping, conditional statements and on and on.
interesting - downloading the trial now just to check things out. would be neat if this has some good data prep/etl tools built in
If you don't know the difference you don't put either on your CV. 
That is exactly the definition so what do you want my thoughts on ;)? Updates in clustered index tables will be released next week bte
Just the tutorials on Pluralsight. I'll have to look elsewhere for study material for 70-463, not sure what direction I'll go there. I also work as a DBA, so some of the information I've learned over the last 3-5 years. It's not enough to pass the exams, but definitely helps to have that foundation.
If you cannot use it externaly, beside installing a local sql instance on your machine no idea. Excel by itself is not made to do sql query. http://dba.stackexchange.com/questions/62513/sql-server-on-mac
You *can* use excel as an odbc data source and execute queries against it. On Windows. Don't know if it's possible on a Mac. 
Yeah, saying sorry doesn't make you seem like an arrogant cunt /s I know SQL, I was barely introduced to PL/SQL. I know they're different - that much I can tell. I wanted a more broader definition as to what the main differences are. Thanks for your useless contributions. Hopefully that boosted your self esteem.
Thanks for taking the time to reply. Sorry for not providing enough info. I've created a new post for this that should be more clear. I've also included a sqlfiddle link so you can test out what I'm trying to do: https://www.reddit.com/r/SQL/comments/53evrv/mysql_find_the_rank_of_a_users_lowest_lap_time/ http://sqlfiddle.com/#!9/2cd1b/2 Thanks.
Thanks for taking the time to reply. Sorry for not providing enough info. I've created a new post for this that should be more clear. I've also included a sqlfiddle link so you can test out what I'm trying to do: https://www.reddit.com/r/SQL/comments/53evrv/mysql_find_the_rank_of_a_users_lowest_lap_time/ http://sqlfiddle.com/#!9/2cd1b/2 Regarding your comment on GROUP_CONCAT, it does work in the following query. This query gets the rank of a lap time against every other lap time. I just need to group by user before the ranking calculation occurs. SELECT session_id, user_id, track_id, duration, FIND_IN_SET( duration, (SELECT GROUP_CONCAT( duration ORDER BY duration ASC ) FROM session_lap_times WHERE track_id=s1.track_id ) ) as ranking FROM session_lap_times s1 WHERE user_id=1 Thanks! :)
Hey, please see the new post that explains it a bit more clearly, and includes a sqlfiddle link so you can test it out online: https://www.reddit.com/r/SQL/comments/53evrv/mysql_find_the_rank_of_a_users_lowest_lap_time/ http://sqlfiddle.com/#!9/2cd1b/2 Thank you.
You can use the built in Microsoft Query in Excel. In your data file, make tables of the date you wish to query and save/close. In a fresh excel instance, go to **Data|Other Sources** and choose **From Microsoft Query,** Then choose **Excel Files*** There will be some things you will want to uncheck. And you'll likely want to bone-up on a few YouTube videos of how Microsoft Query works.
I'll look into messing with the SQL for it tomorrow morning, but I'd like to make sure I have the situation understood first. Please let me know if I am off-base anywhere. I'll do things in a numbered list so you can easily reference anything that might be mistaken. I will pretend that your data applies to the race car world since having all the information helps me picture what I am trying to get when I code. 1) Your table tracks the lap times for race car drivers at different tracks. 2) A race car driver can drive multiple laps per session and each lap's data has a unique identifier in the 'id' field. 3) A race car driver can have multiple sessions at the same track. The confusing part for me comes with what you are trying to figure out exactly. Let me know if this situation is not what you are looking for: You want to start by looking at a particular track. You then want to compare the best lap times for each race car driver at that track. Basically, if you had 10 race car drivers that have each completed 25 laps at track #1 you want to identify ONLY the best lap time of each race car driver and rank those times against one another. So you want to first identify the best lap time that each of the 10 race car drivers had at track #1 and then sort those based on duration.
Where did you get the logic to use "USING" in your joins? Also, the joins are unneeded if you're not selecting any information from the table or filtering based on data in that table. You may want to recheck your course content, it wold appear that you need to read up on some of the basic principles.
What do you suggest instead of USING? I assume the professor wrote this, but I could be wrong.
All of those joins are probably needed given the select, but my other points still stand.
Many thanks for that by the way, I was really stuck in a bit of a rut until you provided those examples. I'm just curious whether any of the other agg functions could accomplish the same thing with grouping the tags. I know that there's an array_agg (which doesn't seem like it would work for this since it doesn't do key-pairs AFAIK), json_agg and I've also seen talk about Postgres' hstore data type. I'm guessing that all of those would probably be inefficient compared to the string_agg though...
Read up on begins with and answer your own homework. Or crack the SQL book your professor gave you.
Please inform which DB you are using, MSSQL, Oracle, PostgreSQL ... For MSSQL you can use the lag/lead operator to check the value of the previous/next register.
thanks. using SQL server. but how can I use that operator on a single table that is partitioned by dates? 
Check this link: https://msdn.microsoft.com/en-us/library/hh231256.aspx You'll also need to use the OVER command to do the partition, there are examples in that link.
CASE WHEN status = 'Closed' THEN closeddate else '' end This assuming you are using sql server
nice. didn't know that one. ill give this a try. 
Even though OP stated he is using DB2, the CASE syntax is the same as SQL Server.
This doesn't really have anything to do with sql. You're just looking for a data set, right?
Thanks for all the information. Without your help I wasn't even aware of some of the sites listed. Thank you everyone!
Well, you need to get a grand total in your statement somehow. For example, add this expression in your select list to get the percentage: sum( t2.f2) / (select sum(t2.f2) from t2) * 100 as PctOfTotal 
Brilliant, thank you very much! I was trying to use 'over' but it wasn't cooperating with me.
Indexes are present. The query is taking an ungodly amount of time. I'm starting to suspect that its a DBA/server configuration issue, and if so how to break it apart into three separate queries or subprocesses.
The query is a simple INSERT from a SELECT, all its components are needed, without those JOINs the SELECT would bring unwanted data, so you can't break it apart. You really need to create the table and then make an INSERT? Can't it be a VIEW?
Can you provide an execution plan? What version of sql is this? How many rows are inserted? Is this value what you would expect? Does the table you are creating have a clustered index? If this is a temp table could you not create the table in tempdb? 
 FROM dbo.BP_tmpLEADS A LEFT JOIN dbo.BP_tmpPARENT B ON A.LeadID = B.BI_leadID OR A.OpportunityID = B.OpportunityID OR A.AccountID = B.AccountID WHERE B.BI_leadID IS NULL The field in the WHERE clause must be a NOT NULLABLE field. EDIT: On second thought, this query may or may not show performance improvements. All depends on how smart the execution plan is.
I started it as a #table but there was no performance difference. I'll see about including an execution plan.
This is part of a larger process that is assembling a sort of cube.
I haven't read the whole statement, but... Is that because the ticket gets closed in two different quarters and years? In that case, instead of grouping by quarter/year closed, get the min or max of those fields in the select list and take them out of the group by clause? e.g., MAX(QUARTER(closed_at)) quarter_closed
[Execution plan](http://www.filedropper.com/explan) -- I'm using 2008R2
Woops, change the OR's to AND's.
It may help to post your resume so we can look at it--redact the relevant PII.
sqlzoo.net has a bunch of quizzes.
 SELECT * FROM top250 GROUP BY year HAVING COUNT(*) = 1
I don't have any relevant previous employment though :/ ---i have a feeling that makes it very hard for me. Also what kind of stuff did you include in the personal info category.
This approach with the deletes takes ~30 seconds to run. The full query in the post takes &gt;45minutes.
For what it's worth I really started learning by being able to ask people how to do something, and then seeing what the correct way a piece of code was written. I don't mean each individual piece of code, but more how the entire process is written. The best way it was ever explained to me is that SQL is like using a command prompt. You're typing instructions to the database to get to the directory (or output) that you want to see. So if that's true then start out by imagining what you want to see that in that output. In your case, it was all the authors and titles that start with J. You could start with the book table and grab all the titles, or start with the author table and grab all the ones with a name of J. Then you `JOIN` to other tables to connect the dots. The tricky part is when you have duplicates and have to work those cases out on an individual basis, but that's where SQL is beautiful because you can ask the databases question by forming intermediate steps and testing your hypothesis.
For free and for beginners. With online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Yeah it's not perfect, but it's effective. Used this method many times when I couldn't figure out why my many to many joins were duping. 
If the "inefficient" portion is using an index seek and he needs that logic in place, there's really nothing that can improve that. Sometimes queries like this moving large amounts of data will take a long time no matter what.
I perhaps was unclear, and I apologize for that. I meant at the start to keep it all as 1 "athlete" table, because trainers are athletes in this case, however they also have a trainerID because they train athletes, as well The structure I have WILL be recursive as you say My question was how do you exactly DO that? I do not fully understand how to implement the roll idea, I get the basic idea, you add a role attribute to person so you check that but I am more confused on the specifics of that I suppose Yes it is one to many by the way, one trainer may handle a full team- I will try the extra table, but I still do not understand that idea fully so any help would be great to understand further
Your resume should reflect the job you are looking to get. It should be tailored to each position you seek. Lidt all relevant qualities for the particular position. There's plenty of subreddits for cv and resumes. Read them. Mine starts with a short line describing me, and then some technical Skills, when I just finished my degree I wrote what I had gotten out of my jobs and coursework. Soi had a course with algorithms and Datastructures, out of this I learned critical thinking for problems.. Blah blah and so on. 
This is what we call "late to the party", in common parlance. Don't get me wrong, I'm super happy that it's here in the still-wildly-popular server database, so will be available in more places.
So if you want to keep a 1-to-1 recursive relationship, you will simply have a FK column (e.g. TrainerID) in your Person table that references the PK column (e.g. ID) of the same Person table. So where ever there is a value in the TrainerID, you can assume that the ID listed is a trainer. There will be no need for another table to store the Role data. But since you said it needs to be a 1-to-many relationship, you will need an additional table to store that data. So now your tables will look like this: &gt; Person(**ID**, FirstName, LastName) &gt; TrainerAthlete(**ID**, *AtheleteID*, *TrainerID*, Sport) **Note:** AtheleteID *and* TrainerID are both referencing Person.ID. The name of the column is helping describe the role of each Person. **Note:** **Bold** are PKs, *italics* are FKs, and regular text are non-key columns. 
one-to-exactly-one? or one-to-zero-or-one? both are pretty common 
Not sure I understand your question but this may be an example I have a table FOO_CALCULATED_VALUES And three tables that have 1 row for each FOO_CALCULATED_VALUES FOO_INIT_VALUES - values entered by a user FOO_SYSTEM_ADJUSTMENTS - filled at time FOO_CALCULATED_VALUES is created FOO_MANUAL_ADJUSTMENTS - entered and updated by the manager of the user who entered FOO_INIT_VALUES FOO_CALCULATED_VALUES could contain all columns from the other 3 tables. But there are a lot of reasons it makes sense to split them into 3 tables for the transactional system (authorization, minimizing chance of errors in updates, etc). We could just store the calculated values but this lets us know how we got them. You would never have more than one of the secondary tables for the primary table (in this case - though you may want a history of adjustments in other cases and INIT_VALUES would be the only one-to-one). There are other reasons but that's one I have at the moment.
I know it's smarmy to post [google search results](https://www.google.com/search?q=tech+resume&amp;rlz=1C1JPGB_enUS657US657&amp;espv=2&amp;biw=1519&amp;bih=778&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwie3snFtJ7PAhULcz4KHR2zC6EQ_AUIBigB) but in formatting my own resume I've found it helpful to take a look at other people's. Of course you want to make it unique and make it your own so treat these things more as a jumping off point rather than an exact stencil.
I mean what is a real life example that would lead to a natural implementing this way
of course -- extract, transform, and load 
Class Table Inheritance is such an example. create table goods ( good_id int primary key, ...columns common to all goods... ); create table vehicles ( good_id int primary key references goods(good_id), ...columns specific to vehicles... );
Thanks for the response, but I wasn't asking about data structure. The output of this will be sent to a client and that format (single row with everyone in it) is used for a mail merge. Thanks, anyway.
I see, thanks. What about actual work though? Is it with people or alone? Or does that depend on the organization too?
For MSSQL start looking into the DATEDIFF() function. That should get you to what you are looking for.
Your question is rather confusingly worded. From your comments it seems you're trying to distinguish between 1-0..1 and 1-1. Semantically most of us are used to calling a 1-0..1 relationship "one to one". Someone call me out if I'm wrong but a "true 1-1" relationship kind of doesn't exist because of the chicken/egg problem. To quote a very related Stack Overflow answer [[1](http://stackoverflow.com/a/10292449/2343301)] &gt; [...] if you were to have a constraint that says in order to add a an Egg to the Egg table, the relationship of the Chicken must exist, and the chicken must exist in the table, then you couldn't add an Egg to the Egg table. The opposite is also true. You cannot add a Chicken to the Chicken table without both the relationship to the Egg and the Egg existing in the Egg table. Thus no records can be every made, in a database without breaking one of the rules/constraints [1] http://stackoverflow.com/a/10292449/2343301
1) Worker enters their estimate for how long a job should take not knowing the work site (e.g. 20 hours). 2) A system defined adjustment factor is managed for this work site. (e.g. 1.1 since at this time since this site is more difficult than normal). 3) The manager reviews and says this worker is usually faster than most or is familiar with the site and adjusts the estimate by another factor (e.g. 0.95). 4) Calculated value is (20x1.1x0.95) and we know the 3 values that went into calculating it. Every estimate has exactly 1 row in 4 tables with the same PK. Each estimate actually has 8 values for different parts of the job (prep, get parts, ... cleanup) so I have 4 tables of 8 values each. The manager is only authorized to update his table and recalculate. The system adjustments can only be updated by the site manager. We could have 1 table with 32 columns but updates to the manager table could more easily change the initial estimate or system estimate (accidentally or on purpose). A different job I've done recently also had a 1-1 PK relation. The main USER table had the 5 columns every user account needed. One type of user had USER_TYPE1_INFORMATION table with the 20+ other columns they needed. The other type of user had a USER_TYPE2_INFORMATION table with 10+ extra columns they needed. All login/account/authorization modules worked with any user. Other modules required a certain type of user and used the extra info. My example wasn't education but this may be the case in a school application with teachers and students on the same system. I don't know that any schema is "natural". A lot of things go into the decisions and there are almost always many ways (e.g. 1 WIDE sparse table). Some designs look better than others after considering everything * Need (can we represent the application data) * Security (isolate data with different security needs. maybe SSN is in a different table than FAVORITE_COLOR even though they are both part of the user's profile. and credit card data should usually be separate even if "naturally" part of an entity) * Performance * ORM (in some cases your options are restricted. are transactions supported?) * Reporting (is there a warehouse? sophisticated etl or dump to excel? any reporting from transactional system?) * DB Maintenance - what tables can different DBA's see. * CSR - what does customer support need to see. does a separate CSR app exist that can be given restricted access? * Data retention policy - like security, different data may need to be scrubbed or archived at different intervals so should be in different tables. * Extensibility (what's version 2 likely to need) * Company standards/policy (and DBA expertise). Can/should you use views? Must you use per-table permissions? Are triggers allowed/disallowed/preferred/discouraged? * Code Maintainability - If something's wrong in one of my 4 tables above I have a pretty good idea which code did it. If we used one table with 32 columns it could be a bug in multiple sections of code and harder to track down. * External interfaces - the natural thing for "my application" may not be natural for customers. If the purpose of the data is to work with customer systems, it may be better for my application to be more difficult and keep the external interfaces/adapters simple. 
try this -- CHECK ( isTrue IN ( 'yes','no' ) )
intern's next learning opportunity -- when they ask to have this task automated 
Thanks for explaining it man
You can add to /u/shoulder89's query with adding the Plan to the group by clause as well as the select.
Yep, I can't figure out #2. Do you reckon I can use DCount or other similar functions in conjunction with the SELECT DISTINCT COUNT clause? 
Is this what you are looking for? SELECT ID, Plan, Min(PaymentDate) AS FirstPay, Max(PaymentDate) AS LastPay, DATEDIFF(d, Min(PaymentDate),Max(PaymentDate)) PaymentLength FROM Payments GROUP BY ID, Plan Assuming that each payment from the same ID only has 1 plan
Maybe someday SQL Server will have GROUP_CONCAT().
Not sure if this fits your question, but here's my thought. If you have a ton of data coming from different sources with different permissions, they might be split into different tables. For example, an airline might have several tables with an individual flight as the primary key. One table has schedule information, another has bookings/revenue, costs/fuel, operations/maintenance notes , etc. Each table would loaded from very different sources, and the DBAs can restrict access to certain tables based on department /user role. 
I have looked at or used them but have you looked at mySQL's[example databases](http://dev.mysql.com/doc/index-other.html)?
Thanks! Sakila db looks like a perfect database to practice. 
Sensitive information where access is set based on a non-column criteria (database/table). General customer information in one table and things like social security number and other information that needs to be restricted. 
You can, but it's ugly: SELECT * FROM Customers WHERE Customername LIKE '[a-' + CHAR(ASCII('a')+15) + ']%' (And hopefully someone will chime in for why you shouldn't do it this way)
OptimizeSQL Technologies is dedicated institute for SQL Training in Hyderabad,SQL DBA Training, MY SQL DBA Training, DBA Training, MSBI Training, Performance Tuning Training, Clustering Training
On 2nd query, shouldn't it be sum(quantity)? A count of quantity would give you the number of orders, sum would give you the total quantity ordered.
As a fun exercise, you can actually do this without using LIKE or IN (assuming no actual row has ID 0): With Names as (Select CustomerId, CustomerName From Customers UNION ALL Select 0 CustomerId, 'PZZ' CustomerName), Ranking as (Select CustomerId, Customername, RANK() OVER (ORDER BY Name desc) Alpharank From Names) Select CustomerId, CustomerName From Ranking Where AlphaRank &gt; (Select Alpharank From Ranking Where CustomerId=0)
[removed]
It's our 6th or 7th Database lesson in the first year of Programming. As interesting as the previous guy's answer is, I think yours looks like what the teacher is asking for. Thanks! Why the 'Pzz' though? I used 'p' and it worked as well.
 SELECT FName , LName , SUM(avail) Count_of_Available , SUM(sold) Count_of_Sold FROM ( SELECT reps.FName , reps.LName , CASE WHEN status.vehicle_status = 'Available' THEN 1 END avail , CASE WHEN status.vehicle_status = 'Sold' THEN 1 END sold FROM user.reps reps INNER JOIN user.status status ON reps.rep_id = status.rep_id WHERE status.active = 'Y') x GROUP BY LName, FName ORDER BY LName, FName 
Sure is, although I wouldn't go into an interview and exactly say that you worked on an ETL process...that term is technically correct but nobody really does these things manually. You'd want to automate it somehow, and that would be a very nice feather in your cap.
When I adjust my query to contain the lines above, I get an "Invalid Identifier" error for the 'Sold' - any ideas? I can PM you my exact query if you'd like. 
You're missing 'END' for the case expression in SUM(CASE status.vehicle_status WHEN 'Available' THEN 1 ELSE 0) i.e. you need SUM(CASE status.vehicle_status WHEN 'Available' THEN 1 ELSE 0 END)
P is the 16th letter of the alphabet, and you want to include people whose names start with P. SQL sorts alphabetically, so "Peter" sorts after "P" and wouldn't be included. Actually you should just do &lt; Q here, because you can do strictly greater than. The Pzz was just useful for my more complicated/fun version because there was the potential of including people with the first name of Q. I once worked with a guy named Q so it came to mind as a potential problem. Pzz is nice because it should sort strictly after any real name that starts with a P, but strictly before any name starting with Q - but in this case, you can just do &lt; Q and get the same functionality.
You're going to need to create some kind of case statement to account for all possible combinations that represent the "missing stuff" -- use len() and group by to explore the total set to find how many are similar and can be accounted for with each case type.
I would do something like cast(substring(CX_TOTAL_RESUBMITTAL_1,0,charindex(' ',CX_TOTAL_RESUBMITTAL_1)) as date)
Thank you! This worked perfectly
well, the industry-strength approach would be to unpivot all these columns to separate rows and have the list of comparison codes captured in another table. Then you can use exists and whatnot. for your specific approach I would recommend adding a divider between fields (a colon, for example) in the expression so your output looks like ':diag1:diag2:...:diag25:'. To check for multiple codes you'll need similar conditions OR-d or AND-d together.
Ah, that fixes it! Thanks :) 
I like to do it the painful way personally. Healthcare data is the worst. declare @diags_to_search table ( diag_code varchar(10) ); insert into @diags to search values ('J45'), ('whatev'); with normal_diags as ( select 1 as sequence, diag1 as diag_code from tbldata inner join @diags_to_search on tbldata.diag1 like @diags_to_search.diag_code + '%' union all select 2 as sequence, diag2 as diag_code from tbldata inner join @diags_to_search on tbldata.diag2 like @diags_to_search.diag_code + '%' union all select 3 as sequence, diag3 as diag_code from tbldata inner join @diags_to_search on tbldata.diag3 like @diags_to_search.diag_code + '%' union all select 4 as sequence, diag4 as diag_code from tbldata inner join @diags_to_search on tbldata.diag4 like @diags_to_search.diag_code + '%' union all select 5 as sequence, diag5 as diag_code from tbldata inner join @diags_to_search on tbldata.diag5 like @diags_to_search.diag_code + '%' union all select 6 as sequence, diag6 as diag_code from tbldata inner join @diags_to_search on tbldata.diag6 like @diags_to_search.diag_code + '%' union all select 7 as sequence, diag7 as diag_code from tbldata inner join @diags_to_search on tbldata.diag7 like @diags_to_search.diag_code + '%' union all select 8 as sequence, diag8 as diag_code from tbldata inner join @diags_to_search on tbldata.diag8 like @diags_to_search.diag_code + '%' union all select 9 as sequence, diag9 as diag_code from tbldata inner join @diags_to_search on tbldata.diag9 like @diags_to_search.diag_code + '%' union all select 10 as sequence, diag10 as diag_code from tbldata inner join @diags_to_search on tbldata.diag10 like @diags_to_search.diag_code + '%' union all select 11 as sequence, diag11 as diag_code from tbldata inner join @diags_to_search on tbldata.diag11 like @diags_to_search.diag_code + '%' union all select 12 as sequence, diag12 as diag_code from tbldata inner join @diags_to_search on tbldata.diag12 like @diags_to_search.diag_code + '%' union all select 13 as sequence, diag13 as diag_code from tbldata inner join @diags_to_search on tbldata.diag13 like @diags_to_search.diag_code + '%' union all select 14 as sequence, diag14 as diag_code from tbldata inner join @diags_to_search on tbldata.diag14 like @diags_to_search.diag_code + '%' union all select 15 as sequence, diag15 as diag_code from tbldata inner join @diags_to_search on tbldata.diag15 like @diags_to_search.diag_code + '%' union all select 16 as sequence, diag16 as diag_code from tbldata inner join @diags_to_search on tbldata.diag16 like @diags_to_search.diag_code + '%' union all select 17 as sequence, diag17 as diag_code from tbldata inner join @diags_to_search on tbldata.diag17 like @diags_to_search.diag_code + '%' union all select 18 as sequence, diag18 as diag_code from tbldata inner join @diags_to_search on tbldata.diag18 like @diags_to_search.diag_code + '%' union all select 19 as sequence, diag19 as diag_code from tbldata inner join @diags_to_search on tbldata.diag19 like @diags_to_search.diag_code + '%' union all select 20 as sequence, diag20 as diag_code from tbldata inner join @diags_to_search on tbldata.diag20 like @diags_to_search.diag_code + '%' union all select 21 as sequence, diag21 as diag_code from tbldata inner join @diags_to_search on tbldata.diag21 like @diags_to_search.diag_code + '%' union all select 22 as sequence, diag22 as diag_code from tbldata inner join @diags_to_search on tbldata.diag22 like @diags_to_search.diag_code + '%' union all select 23 as sequence, diag23 as diag_code from tbldata inner join @diags_to_search on tbldata.diag23 like @diags_to_search.diag_code + '%' union all select 24 as sequence, diag24 as diag_code from tbldata inner join @diags_to_search on tbldata.diag24 like @diags_to_search.diag_code + '%' union all select 25 as sequence, diag25 as diag_code from tbldata inner join @diags_to_search on tbldata.diag25 like @diags_to_search.diag_code + '%' ) select ... from normal_diags Edit: Random thoughts: I'm not sure about the performance implications of doing this (method above), but I would caution you against using the leading wildcard for comparison. It would be better just to do 25 OR's (can't take advantage of an index with `LIKE '%foo'`). You could also do another CTE before the one I'm proposing here in order to cut down the data you are looking at for the second CTE by date, population, etc.
Quick note on style: the prevailing convention is to explicitly declare a join to be an `INNER JOIN`. Databases should also not have spaces in table or column names, so presumably your instructor doesn't give a flying flip.
If you're really interested in data analysis--as your username suggests--I suggest looking into PostgreSQL instead. MySQL is good for websites, but Postgres supports a lot of advanced functions that MySQL does not (window functions!!!). MySQL also has terrible ... everything, from my perspective. SQL Server is also good, and SSMS (the client) is the best that ships with any DBMS (SQL Server Express is free, and Developer Edition is ~$40). Just my two cents. Edit: I'm betting you'll also find a lot better support for semi-structured data (json and xml types) within Postgres or MSSQL.
You could try x.sold instead of sold? Not sure why that would throw an error Edit: or if it is saying invalid identifier for line 10 you could use the "as" keyword after end. EG: CASE WHEN status.vehicle_status = 'Sold' THEN 1 END as sold
While that is a very handy function, I don't know why you felt compelled to point that out. The obvious counterpoint is that the number of features MSSQL supports that MySQL does not dwarfs the inverse ....
Not sure where a user would input mileage, or how this data is being accessed. However, an easy way to categorize a range of numbers would be to use a case expression: SELECT cost , case when mileage BETWEEN 0 AND 108000 then 'L' when mileage BETWEEN 108001 AND 132000 then 'M' when mileage BETWEEN 132001 AND 150000 then 'H' else NULL end as milcode FROM wrntycost In SQL, anything in the WHERE clause can only be used to filter data (not alter it). Adding another column to your query with a case statement allows you to create a "calculated column," similar to what you can do in Excel. EDIT If you want a simple tool to input mileage and get the code: declare @Mileage int set @Mileage = 0 --Change the mileage here print case when @Mileage BETWEEN 0 AND 108000 then 'L' when @Mileage BETWEEN 108001 AND 132000 then 'M' when @Mileage BETWEEN 132001 AND 150000 then 'H' else NULL end
Because it's about the only feature that I want to use on a frequent basis which every major RDBMS implements *except* SQL Server. If you search or ask about it, you get this absurd `STUFF((SELECT ',' + [...] FROM [...] WHERE [correlated join here] ORDER BY [...] FOR XML PATH (''), TYPE).value('.','varchar(max)'),1,1,'')`syntax. It's a bullshit syntax, and I'm not going to stop bitching about it until Microsoft stops accepting this workaround as a solution. They need to provide a real string concatenate function, preferably something that allows windowing like Oracle, PostgreSQL, and even DB2 do. Even *Firebird* has the `LIST()` function. 
Depending on how you want to do this, you can also create a look-up table using rangeStart, rangeEnd, mileageCode, etc. You can join this table with: ON mileage BETWEEN startRange and endRange This allows you to also include other things in the table like titles, descriptions, and whatever other information may be helpful on future reports and what not.
Your 'M' condition will never be hit.
Which database are you using? In SQL Server, you could use SELECT CAST(SUM(s.shipmentquantity* p.price) AS MONEY) AS Total In MySQL, SELECT CONVERT(SUM(s.shipmentquantity* p.price), DECIMAL(13,2)) AS Total
I'm using My SQL Workbench. It doesn't seem to work 
lol I can't read apparently. thanks.
What does "convert into dollars" mean? To your database, "dollars" is just a number (preferably something high-precision, as we don't want to round off fractions of a penny inadvertently). Quantity * Unit Price is the total cost of a purchase. Which you're doing. Where are you stuck?
I need to be able to show the dollar currency in my output . 
If I were you (and if this isn't an enormous dataset being returned), I would just export the results to excel and format the column as currency there. If that's not possible, though, I'm guessing you could use the CONCAT function. Something like: SELECT CONCAT('$', SUM(s.shipmentquantity*p.price)) AS Total FROM shipmentdetails Basically, this just tells it to add a $ to the front of every value in that column. 
You don't say which database you're using which makes providing a specific example difficult. One challenge is knowing *which* data to remove. Is there another column on the table which identifies which values are older? Without that, it's difficult if not impossible to automatically remove the correct values. Assuming SQL Server, I would use the row_number() function to identify the rows to be removed as it would allow me to sequence rows by setting/user and identify the most recent. Any row that's not in the most recent list would get removed.
As I said, I'm not very familiar with SQL or its best practices. Would you mind elaborating on how I would sanitize these variables and why it's important here?
[How](http://stackoverflow.com/questions/21911112/escaping-strings-containing-single-quotes-in-powershell-ready-for-sql-query) to sanitize. [Why](https://xkcd.com/327/) you should sanitize.
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) [Mobile](https://m.xkcd.com/327/) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/327#Explanation) **Stats:** This comic has been referenced 1584 times, representing 1.2413% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d7xmedz)
Just need to left join: select t1.mRow, t1.Row, t1.Member, t1.[START_DATE], t1.END_DATE, t2.mRow, t2.[Row], t2.[START_DATE] from result_set t1 left join result_set t2 on t1.mRow = t2.mRow - 1 and t1.Member = t2.Member
I tried this but the last record of each group is excluded. In Member group 352, mRow 8 is missing. In Member group 412, mRow 20 is missing. Is there a way to handle the last record to show Null rather than exclude it from the result? EDIT: I think I answered my second question thanks to your advice thank you. EDIT2: select t1.mRow, t1.Row, t1.Member, t1.[START_DATE], t1.END_DATE, t2.mRow, t2.[Row], case when t1.Row &lt; t2.Row then t2.[START_DATE] else NULL end AS [next_START_DATE] from result_set t1 left join result_set t2 on t1.mRow = t2.mRow - 1 and t1.Member = t2.Member
Very embarrassing, but I was using the wrong username/password :(
Since you're using SSIS, why not use its built in [Send Mail](https://msdn.microsoft.com/en-us/library/ms142165.aspx) task. You could avoid the overhead of writing out the file and parsing it with python.
Jesus, why not just use Python to query the server and email if something is wrong? Also, your problem is somewhat common with SQL Server, as by default it uses a lazy table-wide blocking mechanism. We are currently looking to enable Row Level Versioning (allow_snapshot_isolation and read_committed_snapshot) as the advice of our ERP vendor. Finally, the people running ad-hoc queries that are binding everyone else up should consider running queries in "read uncommitted" state, which does not issue table locks at all (at the expense of potential accuracy issues) SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED Last edit: We feed the following query into our (shitty) monitoring system, which sends a generic alert to IT when a query runs for longer than 15 minutes. SELECT p1.SPID AS blockedSPID, p2.SPID AS blockingSPID, p2.* FROM master.dbo.sysprocesses p1 JOIN master.dbo.sysprocesses p2 ON p1.blocked = p2.spid Where p1.waittime &gt; 900000 
You can remove the CASE statement and just have the column t2.START_DATE then alias it. With a left join, if there isn't matching criteria, it will be null. 
I'll try this first. If I can get it to work I would be able to query from it as a subquery and just exclude NULL values, right?
Then stay away from Python. See u/ihaxr's response below on how to do this as a simple SQL Server Agent job.
FULL OUTER JOIN not required SELECT stuff FROM daTable WHERE ( element REGEXP '^[aeiou]' OR element REGEXP '[aeiou]$' ) AND NOT ( element REGEXP '^[aeiou]' AND element REGEXP '[aeiou]$' )
You can provide a semi-colon separated list and it should work (It does with Exchange).
For awhile that was the only way to do it via SQL syntax, with ODBC row limit settings being the "right" way to do it. MS *finally* came up with the way to do it via SQL syntax and thankfully they adopted the ANSI standard, so if you now learn the "MS way" you will actually have learned the "right way" for all standards-compliant DBs.
I'm not sure I understand what you're trying to say. Can you post a sample of what you want your result to look like and a sample record that contains "both in a single line"? 
Dude, I think you made a mistake. I tried: select distinct city from station where substr(lower(city), 1, 1) not in ('a', 'e', 'i', 'o', 'u') or substr(lower(city), length(city), 1) not in ('a', 'e', 'i', 'o', 'u') order by city; and it worked and I tried: select city from station where ( city regexp '^[aeiou]' or city regexp '[aeiou]$' ) and not ( city regexp '^[aeiou]' and city regexp '[aeiou]$' ) order by city and it didn't work for some reason.
I like your attitude :) Good luck, buddy. Feel free to PM me if you ever have questions about SQL or PHP, I'm an expert at them both.
Ah, nevermind. The SQL request I asked for is different from the one I was trying to solve.
yeah, sorry, i didn't test it, my regular expressions might be rusty yours is pretty sweet, dude
For some additional back-ground... The system makes a start and end date for each change to their profile. The start date is always the date after the last known end date or their hire date. The end date is the date prior to the change. The Program changing is only one single change of probably 40 that could happen. So what I'm trying to do first is cut out all of the lines that don't matter That's where the "No" record would come from. Next I try to identify when the program started first reporting. That's the "Start" records... Then the opposite would be the "End" records- the end date from the line where their program changed. Finally you have the "Both" records. This is where their program both started and stopped on the same line, meaning that all I need is the start and end date from that line alone.
Thank you for your response. I'll take a look at this. We also have Jasper Reports, which I'm still trying to figure out --- but I'm hoping this can create scheduled reports as well. Unfortunately, we're on SQL Server 2005 -- need to get an upgrade!
&gt; Couldn't ship_cost_proc just have been a variable and the statement would have executed the same? Very confused right now. Yes, it could have been. However, think about *where* is it going to be used? If this was some weird equation that you knew would never be used in more than one script or stored procedure, then go ahead and implement it. But what if it needed to be included in ten different stored procedures. To save yourself time (and potential mistakes), you could instead just write one procedure and then reference it in those ten different procedures. This will help your future self as if the equation changes, you only have to change it in one place opposed to updating ten separate stored procedures.
Look up GROUP BY, COUNT and ORDER BY. Should be all you need after joining the tables together.
Not on a PC right now, but it should probably look something like this: SELECT COUNT(a.CustomerID), b.State FROM Customer a JOIN Address b on a.CustomerAddressID = b.AddressID GROUP BY State ORDER BY COUNT(a.CustomerID) DESC, b.State ASC
Alright i see where i went wrong before. i forgot the on statement and the group by. Thank you!!!!
Here's a thought process that may work. **Understand the Tables** *Customer Table* - This table stores the records of customers where each customer only exists once. Also, it contains a foreign key to the Address table. *Address Table* - This table stores the records for addresses where each address is only stored once. **What columns are needed?** *Table Columns* State from Address *Aggregate Columns* Number of Customers **Join the Data** Now that we understand the data and know what we need, we need to get the data into a usable form. The question doesn't specify how to care if either an address doesn't have a customer or vice versa so we will assume both are required. This will require us to do an [INNER] JOIN. The INNER part is bracketed because it is usually the default JOIN type and is only being specified for clarity. The key that the two tables share are CustomerAddressID and AddressID. We will want to join the data using those attributes and can do this with several methods but I'll show two, using the first one in later parts. FROM Customer INNER JOIN Address ON Customer.CustomerAddressID = Address.AddressID FROM Address INNER JOIN Customer ON Address.AddressID = Customer.CustomerAddressID **SELECT Table Columns** Next we need to SELECT the fields off the table that we need. In this case, we only need the State. So we add that to our query. SELECT Address.State FROM Customer INNER JOIN Address ON Customer.CustomerAddressID = Address.AddressID **Handle Aggregate(s)** For this, we need to COUNT the number of customers. This can be done by either counting all resulting from the query (because each customer exists once and only has one address, thus having only one row) or by counting the CustomerID. We'll use the former for this. Additionally we must also specify that any field that isn't being part of the aggregation be part of the GROUP BY statement. This evolves the query into: SELECT Address.State , COUNT(*) FROM Customer INNER JOIN Address ON Customer.CustomerAddressID = Address.AddressID GROUP BY Address.State **Ordering the Results** We need to order the results by number of customers descending (9999-&gt;0) and then state code ascending (A-&gt;Z). This means that in our ORDER BY clause we need to reference the two columns projected in our SELECT statement. I usually do this by position, which I will do below, but a common practice is to use the actual column names. If you do this, its probably best to give the aggregate functions an ALIAS. Since we want to ORDER BY the count descending, we want our ORDER BY clause to look like: ORDER BY 2 DESC This specifies that the 2nd column of our SELECT will be ordered in a descending order. In order to add the state ordering, we turn it into: ORDER BY 2 DESC, 1 Since **ASC**ending is the default ordering method, we do not need to include it. Our resulting completed query is now: SELECT Address.State , COUNT(*) FROM Customer INNER JOIN Address ON Customer.CustomerAddressID = Address.AddressID GROUP BY Address.State ORDER BY 2 DESC, 1
Don't think of (Relational) Databases as working to answer some predefined answer. The idea is that it stores the information in the best way possible. That way you can answer whatever questions you may need using queries in the future based off of that data. Your first step would be to figure out what to store and how to store it. Lets run with sports. What sport is it? Is it team based? What statistics are kept for the sport? How are things structured? What tables might we have for Baseball? Teams, Owners, Players, Umpires, Games, Game Type, Stadiums, Innings, Salaries, Feeder Teams, various statistics. You then come up with some idea of a question you want to ask like, How many people did Nolan Ryan strike-out in the 90's? You then merge what data you need to get the results to answer your question. Also, you have to remember that strictly speaking Excel is a database. A text file can be a database. The issue becomes how much data you can put into your database efficiently and maintain additional things like scalability, concurrency, availability, etc. Some people might start out with something like Excel functioning as a database. But then they find Excel doesn't necessarily provided the ability to store the amount of data or re-represent it easily. This is usually where someone starts to pick up Access. But then that may even grow and then the person/business/orginization/etc. determines they need something more robust and look into MySQL/MSSQL/Oracle/etc. 
There's not really enough information here to determine the full impact. /u/ihaxr a good summary. I agree that 10-30 minutes to run a query is pretty excessive and the DBMS I use primarily would shut down my query well in advance of that.
It'll at the very least take a shared lock, but can lock entire pages depending on how the indexes are setup. Depending on your needs there are workarounds that can be implemented. Do you need real-time data? If you only need the data from yesterday, for example, a database snapshot can be taken and you can reference the snapshot. Otherwise if you don't need 100% accurate data, you can use with `WITH (NOLOCK)` clause on your select (assuming MS SQL, im sure other DBMS have similar ways of "dirty reads" / "uncommitted reads"). This'll cause it to not lock any rows. But ultimately if the DBAs don't want to make the query run faster and aren't complaining about problems, personally I wouldn't worry too much about it... but there are probably some things that can be improved by people that are more familiar with the data / setup... or maybe not, it's pretty impossible to say for certain.
Which system do you use? At least with the Oracle machine I've on, I've heard business analysts claim they have left queries running extreme times -- say post 24 hours, that finally complete.
Does a shared lock or a page lock prevent or slow down reads or DMLs at all? I pull data from a few days ago. However, there is delayed data that is always being written, all the time. The reason I wait a few days is to capture most of the delayed data. I'm on Oracle, which apparently doesn't have a WITH NOLOCK semantics. I'll take a look into their snapshot capabilities; thanks for that. Unfortunately the DBAs are typical DBAs (i.e., competent and rightly afraid of users like me) and don't like obliging our requests. Perhaps if I explain "Look its either my 30 minute query or your snapshot" they would comply. I always thought a "dirty read" meant that the results of your query could change if there was DML during the time it took your query to execute; so a query taking 15 minutes with DMLs during the interval would see the DML's in the result set. A quick Google search suggests that a dirty read means that I would also see other session's uncommitted data? Am I correct at all or did I make that up? I'll dig into the code, but I would guess that I cannot get any of the uncommitted data as the processes that insert data into this table will rollback and prevent any bad data coming through. Perhaps that final insert doesn't happen until right before the commit, however, and if so this would be fine. I personally don't mind the 30 minute wait; this is completely acceptable to me. I'm extremely anxious that I'm causing harm to other people, however. I just started doing this recently so I'm not aware of how much if any harm I am causing. 