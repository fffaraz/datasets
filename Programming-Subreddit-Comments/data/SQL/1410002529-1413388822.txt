If all you're doing is practicing basic SQL, SQL Server Express Edition (any release) would be enough for you. 2014 was only released 5 months ago, and while it's considered stable it hasn't had any major patches/updates yet. For you, it doesn't have any really compelling features over 2012. I'd skip 2008/2008R2 for now.
Two completely separate data sets? Use two completely separate queries. Unless the schema for 'item' is identical to 'customer', which it probably isn't. But if it was, you could do: select * from item union all select * from customer;
Wow thank you for this explanation, very informative.
Thank you, that sounds like a plan.. just wondering, can I have more than one version of sql server installed on top of a windows OS without problems (for example 2012 enterprise full version and sql server express)
Yes you can. But I'd recommend only running the services for one at a time unless you need to use both simultaneously. They coexist just fine, but you'll run short on memory quickly. 
Let's assume that you have a bridge table relationship between Persons &amp; Phone Numbers &amp; you have the following three tables: PersonTable has PersID, FirstName &amp; LastName fields, PhoneTable has PhoneID, AreaCode &amp; PhoneNum fields, Pers_Phone_Bridge table has two fields PersID &amp; PhoneID. Queries look like: Select P.FirstName, P.LastName, Ph.AreaCode, Ph.PhoneNum from PersonTable as P inner join Pers_Phone_Bridge as B on P.PersID = B.PersID inner join PhoneTable as Ph on Ph.PhoneID = B.PhoneID ------------------------------------------------ Your where clause can then be things like: where P.LastName = 'Smith' and Ph.AreaCode = 213 etc. Hope that helps
That would work for the specific example, but I would like to be able to pull data from any random table with any random number of columns in a database and have it returned like my example. While I could use VB to programatically construct a query by first pulling column names from sys.columns (Microsoft SQL, neccessarily) and then sending the constructed query back into SQL to retrieve the record data, I was hoping I could build an all-in-one query to do the job.
For Oracle, you'd make a functional index. For MS SQL, (I think) you want to index a computed column. Hopefully this will get you started: http://stackoverflow.com/questions/13014108/function-based-index-in-sql-server-2005 and http://msdn.microsoft.com/en-us/library/ms189292%28v=sql.90%29.aspx
For T-SQL, you can do something like this to get the column names: SELECT COLUMN_NAME,* FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'YourTableName' AND TABLE_SCHEMA='YourSchemaName' the * shows all the info available including the column name. Courtesy stackoverflow.
See also this for pivotting a single row: http://stackoverflow.com/questions/16612878/pivot-a-single-row
they are two separate queries, just wanting the engine to return two separate results.... union won't work.
&gt; In databases a null value is used to represent a value is unknown. Me thinks there is a word missing right there. Or perhaps the database serving the harticle came up with a null value?
The video on this page walks through the setup - including paths... http://www.essentialsql.com/get-ready-to-learn-sql-how-to-install-sqlite-and-the-sample-database/
http://stackoverflow.com/questions/13026675/calculating-distance-between-two-points-latitude-longitude
 Try this explanation http://www.movable-type.co.uk/scripts/latlong.html 
Yes it blocks due to the locks held. You can do dirty reads with no lock or changing your isolation level but understand the effect it causes by doing so
You can use snapshot isolation to get similar behavior as Oracle. But that's not just a client side setting. http://msdn.microsoft.com/en-us/library/tcbchxcb(v=vs.110).aspx If it's just about bulk loading, have a look at the loading guide. http://technet.microsoft.com/en-us/library/dd425070(v=sql.100).aspx
Here's an example without using any GPS specific functions of the Haversine method. declare @dlon [decimal](18,11) declare @dlat [decimal](18,11) declare @clon [decimal](18,11) declare @clat [decimal](18,11) declare @deltalon [decimal](18,11) declare @deltalat [decimal](18,11) declare @a [decimal](18,11) declare @c [decimal](18,11) set @clon = -6.2365000/57.29577951 set @clat = 53.3880800/57.29577951 set @dlon = -6.1365000/57.29577951 set @dlat = 53.2880800/57.29577951 set @deltalon = @dlon - @clon set @deltalat = @dlat - @clat set @a = POWER(SIN(@deltalat/2),2) + COS(@clat) * COS(@dlat) * POWER(SIN(@deltalon/2),2) set @c = 2 * ATN2(SQRT(@a), SQRT(1-@a)); SELECT 6378140 * @c;
Thanks - I made the wording less awkward.
Thanks ! Which version(s) of MSSSQL behaves more like Oracle on Inserts ? 
Here's a function I made a while back for this purpose. But be warned if you're scanning a large table, while this function is very accurate it's also very math intensive for your server to process. A performance optimization I'd recommend is to first index the longitude and latitude. Then in your query calculate the outer edges of your search area as longitudes and latitudes. This will narrow results down to kind of a sloppy square area that deteriorates in accuracy as latitude increases but is super fast to search. Then run the the remaining results through the below function to trim off the edges of that sloppy square to get a neat circles search area. Doing this will prevent your server from having to run unnecessary complex algebra on millions of records unnecessarily and result in lighting fast search results that are super accurate. This will work on a lot of databases but depending on what server your using you may have built-in options that can handle much of this for you. CREATE FUNCTION [dbo].[GeoDist] ( @Lat1 numeric(10,7), @Lon1 numeric(10,7), @Lat2 numeric(10,7), @Lon2 numeric(10,7) ) RETURNS money AS BEGIN RETURN Degrees(Acos(Sin(Radians(@Lat1)) * Sin(Radians(@Lat2)) + Cos(Radians(@Lat1)) * Cos(Radians(@Lat2)) * Cos(Radians(@Lon1 - @Lon2)))) * 69.09 END GO
Probably whatever your job is using
None
If you aren't specifically looking to get a job as a SQL admin or developer then it doesn't matter nearly as much which flavor of SQL you learn IMO. For you, the goal is to get familiar with the workings of an RDBMS. The basic concepts are going to be the same for MS SQL, MySQL, Oracle, etc. There can be some significant differences in syntax sometimes, but that can be searched online ;) As /u/bistec said, whatever is being used at your current job is probably a good start and assuming you can get access to some of your company data it would be easier to make sense of it all as the data will be more meaningful to you than some sample database you find online.
I wouldn't worry so much about it if I were you, I think this is as complex as it gets in the real world. This is my second job that uses SQL and I started off a little over a year ago knowing nothing. I went to SQLzoo.net to practice and I remember it not getting too complex. The more exposure you get to writing the queries the more you get used to. If you do enjoy it a bit I recommend going into IT or somewhere in the insurance industry. I work as an actuary and it helps a lot to know just a bit about databases.
If those are the only two options, MySQL since it's free and will probably be able to get more community support for things. Personally, I prefer Postgres, but for just learning MySQL is fine and easy to set up.
It seems like in most cases, people either have a full Linux stack or a full MS stack. You rarely see them mix. If you want to work in a mostly MS environment then learn MS SQL. If your going to be mostly in a Linux environment learn MySQL. The only real exception here is web applications, which seem to be MySQL 95% of the time.
Also MSSQL has certs available you can get. If you are learning something anyway might as well work towards the Microsoft certs and have that on future resumes.
Both wouldn't hurt.
An idea: use a parameter to let them select a date. Have the report display the selected date in a text box that links to a subreport. The text box can say "Update to xx.xx.xxxx" or whatever. In the subreport, have the dataset use a stored proc that updates your table with the date passed to it as a parameter. The proc can return a row that just says something like "Date Updated" and the subreport can display that. Stored procs for datasets normally return data but there is no reason they can't update tables too.
 * Firstly do a DB by DB migration to the new server, just to make sure everything works. * Your SQL agent is probably not setup correct to run the service on the server (probably the system default) * Flipping the server name over will be the most difficult; There will have to be a period of 'read-only' where you can takes the backups and migrate them over. For the name itself I'd personally just change the DNS entry and make sure Active Directory isn't going to override it later. This is quickly revertible to the old server (a few seconds to propagate on a LAN) 
&gt; Firstly do a DB by DB migration to the new server Check - DBs are happy and connect-able. I'm not at this point yet for the migration (have to schedule a night), but to address: &gt; You SQL agent is probably not setup correct to run the service on the server I set the SQL Agent to use the same domain account as the source server, ie: domain\sqlagentuser. It starts and I can browse around in SQL Agent in SSMS, open jobs, view settings, etc. That should work right? It's when I try to run the job that the job gives that authentication error. &gt; Flipping the server name I had originally attempted this by just changing the IPs (disconnecting the source server completely, then adding the IPs to the NIC on the destination) - so DNS should still point to the same place. Could the fact that there is an AD object called "SQLServerCluster1" (for the one-server cluster server) be causing the authentication issues? Thanks for your help!
&gt; I set the SQL Agent to use the same domain account as the source server, ie: domain\sqlagentuser. It starts and I can browse around in SQL Agent in SSMS, open jobs, view settings, etc. That should work right? It's when I try to run the job that the job gives that authentication error. * Check to verify that the job isn't using a full [four part server name](http://www.mssqltips.com/sqlservertip/1095/sql-server-four-part-naming/) or isn't running under the job agent credentials. This could be causing the problem with the DNS entries. * Also verify that any non-domain users [aren't orphaned](http://www.fileformat.info/tip/microsoft/sql_orphan_user.htm). &gt; I had originally attempted this by just changing the IPs (disconnecting the source server completely, then adding the IPs to the NIC on the destination) - so DNS should still point to the same place. Could the fact that there is an AD object called "SQLServerCluster1" (for the one-server cluster server) be causing the authentication issues? * Active directory has a bad habit of overwriting/superseding DNS entries, it would be better to attempt changing any entries for the old servers over to the new servers. If you need a quick revert process just have two identical [PowerShell scripts](http://technet.microsoft.com/en-us/library/jj649850.aspx) written that can change the location or change it back without issue. 
Just to clarify - there isn't an issue with the DNS entries as far as I can tell, since I migrated the IPs from source to destination server (all of them) - so it should be exactly the same. But I get what you're going for, so next time I do the migration I'll look out for that. Here are the error messages I was getting: * Logon Error: 17806, Severity: 20, State: 14. * Logon SSPI handshake failed with error code 0x8009030c, state 14 while establishing a connection with integrated security; the connection has been closed. Reason: AcceptSecurityContext failed. The Windows error code indicates the cause of failure. [CLIENT: sanitized]. * Logon Error: 18452, Severity: 14, State: 1. * Logon Login failed. The login is from an untrusted domain and cannot be used with Windows authentication. [CLIENT: sanitized] 
&gt; If those are the only two options, MySQL since it's free and will probably be able to get more community support for things. MSSQL before MySQL. Every day of the week
My guess that error is related to a duplicate [Service Principal Name](http://msdn.microsoft.com/en-us/library/ms677949%28v=vs.85%29.aspx) being found.
Unless, you know, you don't have any equipment running MS and don't want to pay for licensing.
&gt; You can always get MS SQL server express, Unless, you know, you don't have any equipment running MS and don't want to pay for licensing for windows.
Or if i wanted to start out learning SQL and wanted to have a half way decent DBMS to start out on so I don't have to deal with all the BS that MySQL brings with it MSSQL Express is cost free as much as MySQL is
What BS does MySQL bring? I prefer Postgres, personally, but MySQL is good enough. Also, MSSQL still costs a windows license, and if you don't have one, then... It also means you can't just work on something and grab cheap web hosting to throw it up because you're now tied to an MS-only product.
&gt;What BS does MySQL bring? Ill grant that much is due to very very bad defaults, but we are talking about learning SQL so i'd say its fair game. So lets see.... One of my favourites, implicit type conversion from string to integer defaulting to 0 instead of an exception or NULL. Or even more fun, silently thrown conversion errors on expressions. Try a DELETE FROM table Where someIntegerField = 'i am a shit DBMS' Dont do it on a procution system thou... Anything that can do THAT, is a piece of crap in my book. In addition, the cost based optimizer sucks golf balls trough a garden hose, which i think is a bit of an issue. MySQL is good enough like Access is good enough. I would not push either on anyone that does not have the background yet to make an informed choice. *Edit: And even if &gt;your&lt; instance of MySQL does not do shit like that, the fact that it MIGHT is bad enough. TLDR: MySQL sucks big time. Postgree is ok thou. 
Or you could go get an AWS server for the horrendous rate of $0.018 an hour and not pay licensing directly... And they have pre-built images with Windows Server and SQL server express already installed that take less than 5 min to spin up.
MSSQL FTW
Tell me more about these MS SQL certs. For code or are they for sys admin stuff?
Gosh, if only they had trial editions.
If you're working on internal corporate systems, It's Oracle or MSSQL. With Oracle leading by miles. If it's a client-facing and external web-based environment, it is MySQL and their other web-friendly variants.
One test is code one admin and one data warehouse. Passing 1 makes you a MCP passing all 3 makes you a MCSA. Its not super valuable in the market experience matters most but its a leg up on those that dont have it to get through HR.
Also a lot of government work requies the contractors to be certified now and theres a lot of gov work. Thats why I have them.
MSSQL if you work in a windows enterprise and use a lot of internal business apps. MySQL/PostGRESQL if you work in a linux enterprise and most of your business apps are client facing. 95% of it is learning the stack, .NET with MSSQL is cake, same thing with PHP and mysql, or Ruby and PostGRE for example.
OK, so use PostgreSQL. It's just as free as MySQL and, you know, not crappy.
Also, any company that's a "Microsoft Gold Partner" has to have a certain percentage of their people holding certifications.
OK, that was a stupid question. I know that mysql sucks. Insert into table (Varchar-2-field) values( "hello") doesn't error. What am I talking about MySQL not having BS.
I've done the same for our analysts who need to "lock" financial reports after month end close 
Oh you poor soul! I still have to work with a 15 year old version of Informix, and I thought I had it bad. At least it isn't FoxPro, right?
The large CA project I was out didn't buy license for data studio or anything so we were forced to use command center...we all chose to do our large SQL work in notepad instead :( Quitting was definitely 50% having to use the outdated IBM products
open source didn't take off they way Linux promised it would in 2001. It's fine and everything, and there are lots of open source tools in enterprises. But IT Directors want support and want to easily fill positions when staff leave or when the organization grows. Open source tools just don't scale easily. (here comes the downvotes..) so because of that, I would recommend learning enterprise applications. You'll find work easier, you'll find solutions to work problems easier, you'll integrate with other products more easily. learn MS SQL. and also learn SSIS, SSRS, SSAS, PowerBI, Excel (including VBA)
Not necessarily true. It's quite common to see a Linux stack with a MSSQL Server DB. This is mostly done in Java shops focused on SOAs. It's cheap and easy to run MSSQL on an MS VM, like AWS. I've seen it a few times personally. When it comes to web stuff, you're more likely to see either the full MS stack or the full open source stack.
The w3schools.com sql into isn't bad. It gets you the concepts, but you'll have to figure out how to apply them on your own. 
Coldchaos seems to have you helped out great here, but I have to know: what was the theory behind a single-server cluster? 
The single-server cluster was the result of an unsuccessful cluster to standalone migration (to eliminate the use of an old SAN) from a while ago. The cluster is the last thing using a SAN that needs to be decommissioned (costing a LOT to support as it is old, and coming up for yearly renewal soon so we don't want to pay for it). It's all legacy apps using this older instance, but since they will be gone in less than a year (being replaced) I'm not really able to request time or to do a real migration. I'm all ears for better ways to do this (kind of a noob when it comes to DBA stuff). With the caveat that they only require me, less than 2 hours of downtime, and don't require reconfiguration of any apps or services connecting to the instance. :) 
what was the question?
I have no idea what the question was but that query would not execute because `id` is not in the `GROUP BY` clause. I can't imagine that's a useful answer to any question, even if it worked.
Yeah, so that query fails to do that. You need a correlated select or CTE (not sure what dbms you were using for this to know if that's an option) to answer this questions. Something like this: SELECT P.user_id , ( SELECT TOP 1 id FROM dbo.posts WHERE user_id = P.user_id ORDER BY date_modified DESC ) AS id FROM (SELECT DISTINCT user_id FROM dbo.posts) P 
Not a SQL guru by any means but you should be able to do: SELECT id, user_id, date_modified FROM posts WHERE user_id IN ( SELECT MAX(user_id) FROM posts GROUP BY id ) But I'm not positive, I'm kind of a try/fail type of person ;) Edit: This is also assuming user_id is 100% unique.
Thanks for the help, I originally wanted this approach (since it's how I would do it were I writing code) but wasn't really sure how to make SQL do it (not a big DB guy). Thanks for the feedback
pretty sure this is just going to return every record. For starters you could try looking at the date_modified in the subquery
That's just going to give you the `id`, `user_id` and `date_modified` for the one user with the highest `user_id` who has ever made any post. Not the correct answer.
Good catch, I missed that.
I tried placing the as decimal as the last portion but it did not work. It only seems to work in the current position.
Thanks for the extra info.
Thanks for the answer with specific terms I can go research. I appreciate that. 
You want a reporting platform. It's pretty standard that a developer designs the SQL queries, aggregations, and any visual niceness, then the user just enters some parameters and gets a nice report. The company I work for http://www.actuate.com/ recently made a free version of our Enterprise product (check out the iHub F-Type). We have also been developing the Open Source Eclipse BIRT project for years now. Within our product there are about four main ways you could end up with Excel output. It all depends on what your user wants between him/herself and the XLSX, how much data you will return, and how quickly they expect to get it. 
Cool, thanks for that. Yeah, I'm getting basically familiar with all of the things you mentioned. The product I work on came with a bundled warehouse but I'm not quite sure if it's built to handle the type of reports that are asked of it. I have a sneaky feeling there will be a new one in the future. I'm just now exploring what I can get out of SSAS and figuring out MDX. Powerpivot has been fun to explore. I haven't hooked it up to SSAS yet but exploring DAX and greatly reducing my VLOOKUPs has been a boon. Moving my brain from our transactional DB into the DW schemas, then MDX and DAX is quite a challenge. It feels a lot like I'm back to my first job and absolutely clueless of what the big picture is. 
Thanks everybody for your answers. I know only 5 people responded but that gave me a direction to research, which is what I needed to kick start this. 
Here, I made this moderately more legible, although it's still quite a clusterfuck. If this were me, I would use a CTE, a table variable or a nested select (whatever makes the most sense and is available on your dbms (since you didn't tell us)) to make this a bit more sane. This is as much time as I want to spend on this mess. SELECT DISTINCT Company_Name AS Company , CASE WHEN isnull( ( CAST( IsNull( SUM(Margin) , 0) / CASE isnull(SUM(invoiceamt), 0) WHEN 0 THEN 1 ELSE SUM(invoiceamt) END * 100 AS decimal(18, 2) ) / 70 ) , 0 ) &lt; 0 THEN 0 ELSE ( CAST( IsNull(SUM(Margin), 0) / CASE isnull(SUM(invoiceamt), 0) WHEN 0 THEN 1 ELSE SUM(invoiceamt) END * 100 AS decimal(18, 2) ) / 70 ) END * 100 AS Service_Goal FROM v_cbi_Agr_Mo_Billing_History3 WHERE (1 = 1) AND (dbo.udf_cbi_InternalCompanyName() &lt;&gt; Company_Name) AND (DATEDIFF(mm, GETDATE(), MonthStart) BETWEEN 0 AND 0) AND (AGR_Type_Desc IN ('Agreement1', 'Agreement2', 'Agreement3')) GROUP BY Company_Name HAVING (SUM(Margin) &lt;&gt; 0) OR (SUM(InvoiceAmt) &lt;&gt; 0) ORDER BY Company
My DBMS is SQL Server 2012
meh, like I said I'm newb too, figured I'd give it a shot and someone here would post the correct answer.
&gt; , CASE &gt; WHEN &gt; isnull( &gt; ( &gt; CAST( &gt; IsNull( &gt; SUM(Margin) &gt; , 0) &gt; / &gt; CASE isnull(SUM(invoiceamt), 0) &gt; WHEN 0 THEN 1 &gt; ELSE SUM(invoiceamt) &gt; END &gt; * 100 &gt; AS decimal(18, 2) &gt; ) &gt; / 70 &gt; ) &gt; , 0 &gt; ) &lt; 0 &gt; THEN 0 &gt; ELSE &gt; ( &gt; CAST( &gt; IsNull(SUM(Margin), 0) &gt; / &gt; CASE isnull(SUM(invoiceamt), 0) &gt; WHEN 0 THEN 1 &gt; ELSE SUM(invoiceamt) &gt; END &gt; * 100 &gt; AS decimal(18, 2) &gt; ) &gt; / 70 &gt; ) &gt; END * 100 AS Service_Goal Wrap that whole statement in FLOOR( Crazy stuff here ) that will make it return an INT. If you want your Service_Goal output like 33.33 then do FLOOR( All that stuff... * 10000)/100 AS Service_Goal EDIT: Forgot to add the problem is your casting as decimal too soon just like /u/CalvinLawson said. Do that at the very end. If you cast at the end as decimal it may round up, using FLOOR will always round down. Not sure which you desire.
Where do I enter my log/lat variables? 
You could select them as part of a [Common Table Expression](http://msdn.microsoft.com/en-us/library/ms175972.aspx), aka CTE. You could then modify/query/play with them as a table? with CTE_Example as ( SELECT 100 AS Price, .07 AS TaxRate ) select Price, Taxrate, Price*Taxrate from CTE_Example You could also select multiple "rows" by using UNION. with CTE_Example as ( SELECT 100 AS Price, .07 AS TaxRate UNION SELECT 200 AS Price, .14 AS TaxRate ) select Price, Taxrate, Price*Taxrate from CTE_Example Edit: I forgot, my example is using TSQL/MSSQL. The syntax is probably different for the other types.
Thank you, this is VERY helpful! I appreciate that you took a few minutes out of your time to help me out.
you can also make them variables, and reference the variables.
 You're doing decimal math - get rid of all integers. And simplify your math - if invoice amounts cannot be negative (well, I should order something from your business otherwise) so if your total margin less or equal 0, the Service Goal amount is 0; you don't care for invoices that aren't priced (wonder where the margin comes from, but I digress) u get 10.0/7.0 of Margin to Invoiced ratio (~142%). So I get something like this: cast( case when sum(margin) &lt;= 0.00 then 0.00 else 10.00/7.00*sum(margin)/isnull( nullif( sum(invoiceamt),0.00),1.00) end as decimal(18,2) ) 
 Select t.Price , t.TaxRate From ( Select Price = 100 , TaxRate = .07 ) t Price|TaxRate --:|---: 100|.07 
 @clon @clat Is the longitude and latitude of the first point. The number before the /57.29577951 is the GPS point. @dlon @dlat Is the longitude and latitude of the second point. It's written out in a lengthy manner so you can easily see each step of the process but it's easy enough to convert to a function/stored procedure.
&gt;The presenter demonstrated that, behind the scenes, "the ORM is writing SELECT * FROM Table". What a stupid, stupid thing to do. Anyone who has tuned SQL and databases knows that you seldom if ever use a SELECT * for anything. This impacts performance. It's just the wrong thing to do! Okay. So whats the right way?
Cool, thanks.
&gt; Atleast with most other procedural languages, there are tutorials and interactive help by the bucket load. There are some resources for SQL out there but it's still quite elusive. Er, no. The net is loaded with SQL guides even whole lists of common queries and SQL queries using the most common optimal method for getting the desired result. Anyone who actually wants to learn and understand SQL has a glut of resources available to them, both general and specific to whatever database they want to use.
I don't think developers are afraid of SQL. This whole article is a massive generalisation.
Thank you for the help I was able to figure it out.
True the resources are there but I wouldn't say they're in as a presentable format as resources for some popular languages. 
For me, I didn't really 'get' SQL until I started working with real data. 'Contoso' doesn't mean jack to me. So yeah, there are a ton of tutorials out there that show you how to do basic operations. So unlike with javascript, where you are given a task like "make this div fade in when you click this thing" and you have a clear goal which you can relate to, SQL didn't mean anything til I had to draw up a report of our users with their average time in a course and how their time in course related to their test score. 
I don't agree with you here. I do think there's some disparity between how good the resources are for certain databases though. E.g. Oracles community seems to be less easy to navigate for answer whereas MSSQL has great resources and it's easy to find stuff.
Which you can do easily in any [half-decent ORM](http://sequel.jeremyevans.net/). artists.select(:name, :age) &gt;select name, age from artists; 
I always have to "select \*" at first because I'm never sure what columns I want. Then I rewrite it neatly and properly and pretend I knew along. 
update YourTable set YourColumn = LEFT(YourColumn, CHARINDEX('#', YourColumn, 0)) + LEFT(YourColumn, CHARINDEX('#', YourColumn, 0) - 1) where YourColumn like 'F%' 
&gt; seldom if ever use a SELECT * As a point of interest (to me, anyway), there is one place where it doesn't hurt. That is IF EXISTS (SELECT * FROM Blah WHERE FlippedyDoo) SQL is smart enough to NOT return any columns in that case. I got into an argument with a coworker once and finally did it with SELECT 1 and SELECT * with STATS on and proved my point. However, just to appease everyone who has a "never use SELECT *" mindset (which is almost everyone I know), I still use SELECT 1 in those cases. I don't know why I told you that. My mind wanders. 
I don't think it's really about the lack of tutorials but a more simple chicken and egg problem. People have little reason to learn how to use a database until they're faced with a fully populated database at work that they have to figure out how to use. When confronted with a decision on how to store a given set of data, people that don't already know how to use a modern database will generally just resort to rolling their own simple datastore by serializing data structures to JSON or XML files.
Thank you. 
&gt; update YourTable set YourColumn = LEFT(YourColumn, CHARINDEX('#', YourColumn, 0)) + LEFT(YourColumn, CHARINDEX('#', YourColumn, 0) - 1) where YourColumn like 'F%' So the field is ntext. I believe I cannot run this one :(
Thanks for the response. I like this idea. I was trying to accomplish everything in too few tables I think.
That's a much stronger argument than mine he he. I agree 100%, I never learnt SQL until I had to for the reasons you mentioned
There is a very nice Stanford course on Databases with good SQL instruction.
http://blog.8thlight.com/uncle-bob/2013/10/01/Dance-You-Imps.html
You are correct. (With MSSQL anyway) SELECT * and SELECT 1 within a IF EXISTS clause has little/no impact.
Yeah, I made this mistake a lot when I first started working with DB's. I thought I was being smarter and more efficient getting things into as few tables possible, when in reality it seems as if the more tables you can break everything into (within reason, obviously) the better the DB is constructed. 
i don't know that such a function (split.a.value) exists in sql server. if it does i'd be interested to see the syntax. do you want all the roles to exist on one person or do you want to have a one to many type relationship pairing each role with a key field that would join to a person record? i'd recommend using a cursor that uses charindex to find the comma(s) in the field and then substring to set a value to a variable to insert into your new table or a temp table. 
IF EXISTS (SELECT 'Help Im trapped in a subquery' FROM Blah WHERE FlippedyDoo) 
This is already solved, and the solution is pretty damn neat. I'm not actually sure who came up with this method, but it's based on the numbers table from Itzik Ben-Gan. [Link](https://www.simple-talk.com/sql/database-administration/creative-solutions-by-using-a-number-table/) Best of luck with it! o7 (edit: You want the "numbers table" part as well as "Splitting a String based on Delimiter" of that article)
The creation of the "Numbers" table requires windowing functions, which SQL Server 2000 doesn't have.
&gt;I have a field that contains multiple, comma separated roles for a person. I want to split it into a tall table. You don't "want" to - you **need** to. This (comma-separated values in one field) is a terrible design for most purposes. &gt;I'm using SQL server 2000...which may be my issue. `CROSS APPLY` doesn't exist in SQL Server 2000. SQL Server 2000 hasn't been supported for a long time. There have been **five** releases since it came out. You need to get current.
This is the solution I found....from my spot checks t seems perfect. SELECT a.id, SUBSTRING(',' + a.roles + ',', n.Number + 1, CHARINDEX(',', ',' + a.roles + ',', n.Number + 1) - n.Number - 1) AS [Roles] FROM Name_Demo AS a INNER JOIN master..spt_values AS n ON SUBSTRING(',' + a.roles + ',', n.Number, 1) = ',' WHERE n.Type = 'p' AND n.Number &gt; 0 AND n.Number &lt; LEN(',' + a.roles + ',')
My brain seems to have glossed over that uncomfortable truth in order for me to cope better with reality. At this point all the help I could offer is my condolences.
Oh, I do have current....on my laptop, SQL server 2012 at least. The nature of the small organization I'm working in~ Our antiquated member management system runs on the server, on SQL SERVER 2000. For my purposes, our SAN is not encrypted, so therefore I cannot put most of my sensitive data on it....hence SQL being on my laptop~ Brilliant huh? Trust me....I'm fighting those battles
Even if the SAN itself isn't encrypted, you can still store encrypted data on it. It's even built into SQL Server (newer versions, anyway) at various levels - TDE, encrypted fields/tables, etc.
That meet requirements for HIPAA? Out of curiousity
I'm learning SQL right now so reading this was useful. Thank you for not only writing it, but posting it here so we become aware of it. However, there was a part I found bafflingly written as it somewhat voids out the point of the article. A small clarification is all I need I suspect. "You could argue that for text values you could use blanks, such as one space ‘ ‘, or even an empty value, which is two single quotes ‘’ to represent a missing value. Yet this strategy doesn’t work well for numbers or dates." -Without explaining 'why' two single quotes or a space ‘’, ' ', do not work well for numbers or dates I cannot know from this why NULL is a better option. The article does explain why '0' should not be used (bad averaging,) but unless ‘’ or ' ' swaps out null values for 0s randomnly, it is not clear. So, having read that, why is NULL compared to two single quotes or a space better? What function does it provided those other options do not? I cannot tell. I have read the article three times to try and understand it, but each time I get to that quoted part my brain's gears start spinning and I keep concluding there is no useful difference between two spaces, single quotes, or NULL. The examples of how NULL might kill an expression I do understand. 
I've never had to deal with HIPAA data, fortunately, so I don't know. Does HIPAA allow for that sensitive data to be on your laptop (it's unclear from your post whether you're doing that or not)?
If you wrote that on a goddamn cell phone you sir, are the king of the Internet and I would buy you a beer if you find yourself on the Canadian prairies. A hero for the ages.
Haha I did! Thanks very much 
I would first use a Select statement with your above variables and see if it returns the data you are wanting to purge. if your above doesnt return what you want try. select * from TABLE where rec_date &gt;= dateadd(d, -2555, getdate()) 
Yeah, I'm not claiming ORMs are perfect, just stating that when used properly they don't always require 'select *'.
"Which countries have a GDP greater than every country in Europe?" You don't need to compare against every country in Europe, just the country in europe with the highest GDP. Hint: Max function
I've been doing the tutorials in order and I don't think I've been introduced to the max function yet. Since this question immediately follows the ALL intro, I feel like I'm supposed to use that. I know there's multiple ways of getting the desired results, a couple questions prior to this one I got the desired results in a way that was different from what it was trying to teach me, but I'm trying to learn everything it's trying to teach me. It says ALL acts over a list, so shouldn't it work here anyway?
I will now be using that from now on. Thanks!
Always run your `DELETE`s as `SELECT`s first (with the same `WHERE` clause) to verify that you're going to be deleting what you think you should be deleting.
You've got your comparison operator backwards. OP needs anything where `rec_date` is 7 years or more in the past, so less than today minus 7 years.
&gt;Should I be optimizing database columns to "just fit" the data? Your column types should be the most appropriate type possible for the data being stored in them. If you need a `bigint`, you use a `bigint`. If you're recording hours per day worked on a task, you don't need more than a `tinyint` However, when you get to smaller types, it starts to become micro-optimization with your row sizes **unless you're using field sizes to help enforce constraints** (which isn't necessarily a bad thing - see the example of "hours worked in a day" - that can't ever be more than 24 hours). If you have room on your table for `int` everywhere instead of having to use `tinyint`, it likely won't make a difference performance-wise except in situations where you have limited bandwidth between the application code and the database. That said, .NET will [implicitly cast smaller (fewer bytes) variables to larger (more bytes) ones](http://stackoverflow.com/a/9721786/1324345) but not the other way around, so I suspect that you can use the various int types in the database without explicitly casting in your application code if you structure that application code with that in mind.
You're right. i left the minus out. 
Ever delete the contents of a system-critical table at 5 PM on a Friday because you didn't do the above?
Ah I see now. The problem is that when evaluating the GDP vs all europe GDP, it's running into null values. X &gt; null will always result in false. You need to filter out null GDPs in your subquery.
Ohh ok that's why it gives you the heads up beforehand. Thanks. Only thing is I have no idea how to do that haha. Everywhere I looked into has said SQLzoo is the best free learning site, but from what I can tell it doesn't do too much explaining. 
Yeah, that was the only example I could come up with quick and it wasn't the best. 
Amusingly, I'm also having trouble coming up with a better one.
In that case, use a TINYINT and you won't need a CHECK. The data type naturally enforces the same constraint. (Minor nit: ASCII only represents 128 characters, you may be thinking of CP1252) I will add that using the smallest data types necessary is a bit more important than just disk space, it will also affect indexes and query performance. Still, I would just stick to INTs and FLOATs unless you have a really good reason to get into the extra types - usually they just lead to trouble like this. NUMERICs for example are pickier about parsing and require more bytes to store and compare.
To be honest I think [GalaXQL](http://sol.gfxile.net/galaxql.html) is a much better beginner tutorial. This is the code you're looking for: SELECT name FROM world WHERE gdp &gt; ALL( SELECT gdp FROM world WHERE continent='Europe' and gdp is not null ) You could also do it this way, the max function finds the single biggest value: SELECT name FROM world WHERE gdp &gt; ( SELECT max(gdp) FROM world WHERE continent='Europe' )
Doesn't matter too much. I learned sqlite and immediately went to work with T-SQL, the core stuff doesn't vary widely enough to wory about where you start.
Ok thanks a lot! Especially for the alternative to zoo, I'll be sure to check that out.
If there aren't physically separate I/O channels, there isn't much benefit to separate drives.
If they're all on the same San, you can split the drives but it won't do much other than a more organized layout and I personally would do it so that I know if drive L fills up, I'd know it's full of log files, etc. Makes it a little easier to manage that way IMO. As far as sizing goes, it depends on how much space you have to start out with. I'd say if your primary data files are 300 GB, go with at least 600 GB for the data drive. That should give enough room to grow. If you were to split the drives, another perk would be to designate a separate drive for tempdb. While you wouldn't get the benefits of it being on a separate physical drive, you'd still be able to create enough data files (according to Microsoft best practices, roughly one file per core) and presize them all to take up a good portion of your drive. 
found the answer. the ADO.Net driver for Vertica does indeed return a dataset with multiple datatables (if the sql is written that way). All Good.
Isn't that what rollback is for ;-)?
rollwhat?
I've done this in t SQL before using substring and recursive CTEs. Give that a Google :)
Why are you using cursors at all? This looks easily query-able with a GROUP BY and a few aggregate functions. 
How so without having a separate column for each grade for each year?
You're trying to get "percentage of each grade for each letter for each term for each subject". Write one query that counts each grade for each letter for each term for each subject, e.g. SELECT grade,letter,term,subject,count(grade) as grade_count FROM table GROUP BY grade,letter,term,subject then write another query that counts the number of grades for each letter for each term for each subject, e.g. SELECT letter,term,subject, count(grade) as grades_total FROM table GROUP BY letter,term,subject now join these two queries on letter,term,subject, something like this: SELECT A.letter,A.term,A.subject, A.grade_count/B.grades_total as percentage FROM ( SELECT grade,letter,term,subject,count(grade) as grade_count FROM table GROUP BY grade,letter,term,subject ) A JOIN ( SELECT letter,term,subject, count(grade) as grades_total FROM table GROUP BY letter,term,subject ) B ON A.letter = B.letter AND A.term = B.term AND A.subject = B.subject EDIT: fixed duplicate query as pointed out by Thriven
a. First, look up how to use CTE to push your subqueries into a single statement (Get_Terms, Get_Subj, etc.) and have decently labeled columns. b. if the # of output columns is pre-determined, you can use one of the many techniques for pivoting. c. if the # of columns cannot be pre-determined, but the desired output is still a SQL result set, you can build dynamic sql to pivot results. d. if the desired result is a csv-like file output with unknown number of columns, look into one of ways of producing a concatenate aggregate for SQL Server. 
My first thought is a `PIVOT` but I haven't looked at the query closely yet.
I don't want to try and reverse engineer your schema from that abortion of a process. Can you tell us what the important tables and columns look like? A sample of the output you're looking for would be helpful too.
Experience is a cruel yet fair teacher
I tried this out and it looks great. That only issue I have going this route is that there is a class with no Bs for example, that count would never exist, I would like it to return a count of 0. My attempt the procedure give a delimited set that I can just place into Excel and it's all done. Any ideas?
Check with your local [PASS chapter](http://chicago.sqlpass.org/). You'll almost certainly find someone for hire or good local recommendations there.
Do the GROUP BY in a sub-query and then format it in a wrapper SELECT. 
So I don't know if this will be faster, necessarily. But you can rephrase with a `having` statement. select Phone from PhnPhone group by Phone having count (*) &gt; 1
If I'm reading the situation correctly, I'd just attach Profiler to whichever SQL Server instance(s) the packages hit, and snarf up the queries that way for later review. The filter options in Profiler are sufficient enough that you can set it to capture only what you're really after. I don't mean to downplay the DMVs in the query you indicated. I typically use them to get a larger overview of an instance. The most frequently issued queries or batches (can I optimize something?), the indexes that are and aren't being utilized (am I wasting storage?), etc. I might have misunderstood your goals, I'm just shooting for the least headache-inducing answer.
The above dmv would give me a similar result...albeit in a massive string. Ill check the profiler out. What id like to get out of my question is perhaps a way to parse the results out. For instance Table.field used or not.
Probably will be faster. Sybase optimizer is probably not smart enough to hash the correlated subquery and is causing a per row execution of the subquery.
You duplicated the queries in your comment rather than combine them as subselects.
I agree this sound like what a PIVOT is exactly designed for.
Your first point is well taken. But your exist statement.. wouldn't that find find literally everything? 
Ah, great point, I fluffed that query That too would need a PK of some sort, so criteria would be something like p.Phone = a.Phone and p.id &lt;&gt; a.id or I tend to use &gt; in these cases, so its not having to just re-search the same records as its already dealt with in the parent query
I don't typically handle things like this, but I know a lot of our clients submit pipe delimited files which are easily parsable, this would likely be true the other way around. Not a lot of data sets out there (at least in my field) use pipes within the actual data, the same might be true for what you're working with. [Here](http://www.sqlservercentral.com/Forums/Topic621534-338-1.aspx) is a link to a sqlservercentral forum post that has some details on how you might go about this. Hope it helps!
Just go into Excel and preformat the columns that are giving you worries as text before pasting? 
If all you care about is how many times something occurs and are happy with the above QueryText, take the complete text block and do a replace with the schema.table or table.field (whatever keyword you're searching for really) and replace it with nothing. Compare the length before and after the replace to determine how many times something occurred. Example: * QueryText = "select * from some.table where table.randomfield = 20" declare @searchphrase nvarchar(100) = "some.table" select (len(QueryText) - len(replace(QueryText,@searchphrase,''))) / len(@searchphrase) Should return 1 as the difference will be 10 and the length is 10 thus it occurred 1 time. There are likely many ways to implement checking multiple values but that's the basics of it anyways... Edit: some words and formatting
re you cutting and pasting or exporting to csv first? Alternatively, have you considered an intermediary program like VS to build out a .rpt file and prepare you formatting there?
Awesome idea! Willy try and post back. 
How do you mean? 
I am really surprised; my experiences are very different. I love SQL!
I mean, I do too... but compared to something like COBOL or RPG it's a fucking asshole.
I'd agree Pipes are pretty rare in most data sets. You should be able to export it at pipe delimited text with the export wizard and import it into excel pipe delimited.
Thank you all for your comments. Your input has been very insightful. Now if I could just dial this database back to at least 10 (it went to 11) we'd be in much better shape.
Hm. Interesting perspective. I think the problem really lies in poor design/use of nulls and/or not being aware of that implementation when accessing the database. Clearly, documenting your database design is key here, and understandably some don't have the luxury of having a well documented design to work with, and the person who designed it is long gone. Generally, I don't see this (personally) as being a real big issue. I prefer to avoid allowing nullable fields in my designs, because it removes any confusion. Then in the software side of things, if I have no value i set it to some other static/preset value. For date fields, this becomes 1/1/1900 and/or 12/31/2999 depending on the context. For numerics where I only store positives, -1 becomes the "no value" equivilent. For other numerics, 0 is typically fine for replacing an otherwise "null". For text fields, no problem there, just zero length strings, or if applicable, some note or message relevant to the field. I like this method of design because there is no confusion for anyone coming behind really. every field has a value even if that value is "empty". On the other hand, I can see why multiple "null" types might be valuable. I don't necessarily think this is deficiency in design though. I think this is just something that is more helpful in certain use cases. Ideally, an outer join would return something different than an identical NULL to what would be in a nullable field. eg a nullable field instead returns "no value" or "unkown" data type or something, and an outer join only returns null if no matching row. or vice versa or something. I can definitely see the utility in that, for the cases where we have nullable fields. At the end of the day though, I still avoid nullable fields because you cannot reliably/predictably perform operations on any field that is nullable. NULLS as is are here to stay, which to me means we should adjust and where ever possible, choose not to allow nullable fields in our designs.
you do realise that you're citing SQL Server behaviour in an Oracle based question? Anyway, for Oracle, a CTE, when referenced 2+ times, is usually run once and stored in a temp segment, so that it doesn't have to be re-executed more than once. i.e. in other words it CAN improve performance (and reduce it of course - test always). 
Identical hardware? Are the compared times for first runs or cached runs? Also what version are you using of each DB?
Same hardware. Also tested the query on different kind of hardware for Oracle but no significant difference in runtime. The mentioned compared time was first run. Versions: MSSQL 2012 SP1 and Oracle 12.1.0.1
Nice article, I really enjoyed it. Thinking wider about null problem recently I came across the opinion, that NULL is one of the biggest problem for Java developers while writing SQL for few major reasons , because - NULL is also called UNKNOWN. - JDBC maps SQL NULL to Java null when fetching data or binding variables that may lead to thinking that NULL = NULL (SQL ) what would behave like null == null(java) . - another example pointed was misunderstanding NULL when NULL predicates are used with row value expressions. - or misunderstanding the meaning of NULL in NOT IN anti-joins. Accoridng to history of null refernece, it's appropriate to point that Tony Hoare , the inventor of NULL reference call it a mistake. In his speaking at a conference in 2009 :"The invention of the null reference in 1965 is a billion-dollar mistake ... which led to innumerable errors, vulnerabillities and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years"
I don't know those versions of either product, but I'm going to throw out a few guesses based on what I've seen similar products do: 1. Could it be that MSSQL is automatically adding a primary key index to the tables? I don't know the structure of your database, but if, for example, b.client is the primary key, then the join is much more efficient. 2. Have you allocated the same amount of memory for both? I can't recall if either allows you to restrict how much memory they use, but if MSSQL can load more of the data in memory than Oracle, that would make a difference. 3. Are the disks configured the same? This is purely a guess, but if you have two different RAID setups, that can have an affect on performance. I doubt that much, but you never know. 4. On the same topic, are your tablespaces configured properly for Oracle? You'll need a good DBA to help you with that, but if your data set is large and you don't set them up properly, I believe that can be an issue. (Not a DBA, so I can't be 100% sure) 5. Are you sure there's nothing else going on with those machines? Perhaps another query is being run? You may want to use Toad or similar to see what queries are being run. 6. Are the databases set up similarly? I'm thinking specifically partitioning as that can improve performance on large data sets. Hopefully this helps. It's been a while since I've used an RDBMS, so perhaps I'm missing something, but your query looks fine to me. Hopefully I at least helped get you started looking for a solution.
According to further historical concepts: " In The Relational Model for Database Management: Version 2, Codd suggested that the SQL implementation of Null was flawed and should be replaced by two distinct Null-type markers. The markers he proposed were to stand for "Missing but Applicable" and "Missing but Inapplicable", known as A-values and I-values, respectively. " "Chris Date and Hugh Darwen have suggested that the SQL Null implementation is inherently flawed and should be eliminated altogether, pointing to inconsistencies and flaws in the implementation of SQL Null-handling (particularly in aggregate functions) as proof that the entire concept of Null is flawed and should be removed from the relational model."
Further reading: [the Nulls And Relational Model page on the C2 wiki](http://c2.com/cgi/wiki?NullsAndRelationalModel).
I agree, the experience with your own real life data is essential. However some theoretical guide is sufficient. I recommend the set of the most known places, where you can learn the basics: http://www.vertabelo.com/blog/notes-from-the-lab/18-best-online-resources-for-learning-sql-and-database
You can use the sqlcmd command in DOS .. yes, DOS .. to write and schedule a .bat file. However you need permissions both to read from SQL and write to the destination folder using the same account. Here's more information. http://www.excel-sql-server.com/sql-server-export-to-excel-using-bcp-sqlcmd-csv.htm
According to your article: &gt; NoSQL database is awesome for its preferd domain. That doesn't mean it can replace SQL. NoSQL databases are hot topic right now. Many times I came across the opinion that NoSQL will fully replace the relational databases. I agree with you, that it won't happen. Any attempts will fail. We could say that SQL is almost ubiquitous. Indeed, there are many examples that sql is better than many of its replacements. I don't mean that NoSQL are bad or something, but it's just can't do everything as well as sql can do.
We had a similar problem with an Oracle Group By query, turns out it was related to an incorrectly set [PGA_AGGREGATE_TARGET](http://docs.oracle.com/database/121/REFRN/refrn10165.htm#REFRN10165)
&gt; the punctuation in the nvarchar columns is mangling the spreadsheet. This would indicate that the OP has commas within the actual data set as well as quotes. &gt;The man's name, while wrong, was "McQueen, James" There is no easy way to put that sentence, which is returned as a single column, into a CSV without altering your delimiter and/or qualifier. Edit: Changed my example a bit
Can you capture and provide execution plans for both cases?
&gt;The man's name, while wrong, was "McQueen, James" CSV can handle this just fine without changing the delimiter. Escape quote character with another quote character, and enclose everything in quotes. In a CSV this field would look like this: """McQueen, James""" which is what SSMS does when you save as .csv. (If it's not doing that, then there's a setting in options, query results to make it do that) You can even do this yourself in your query if you really wanted to by replacing all double quotes with double double quotes, and encasing every field in double quotes.
Apologies for format, doing this from my phone. Ase version should work on mssql also once function names are changed. Declare @value = "my, random, string, 1.23456,20140808" Select COL_ID = char_lenght(substring(',' + @value + ',', 1, number)) - isnull(char_lenght(str_replace(substring(',' + @value + ',', 1, SV.number), ',', null)), 0), left(substring(',' + @value + ',', SV.number + 1, 999), case when charindex(',', substring(',' + @value + ',', SV.number + 1, 999)) -1 = 0 then 0 else charindex(',' + @value + ',', SV.number + 1, 999)) - 1 end) from master..spt_values SV where SV.type = 'P' and SV.number between 1 and char_lenght(@value) + 1 and substring(',' + @value + ',', SV.number, 1) = ',' -- replace @value with column name to split data from a table. Use select max(case...) to pivot back to rows, should you wish to. 
So I tried what you suggested. I copied my example above select 'The mans name, while wrong, was "McQueen, James"' upon running it, I right clicked my results and saved my results as a .csv and upon loading the results I had 4 individual columns broken up as follows: * The mans name * while wrong * was "McQueen * James" So that did not appear to work. Looking through the options you mentioned, I think you're referring to is "Output Format" which allows you to change between column aligned, Comma delimited, tab delimited, space delimited or custom. This would be the same as changing the delimiter which is what I suggested is the easier approach. I was unable to get ssms to automatically escape quotes within a result set, but I think this additional work would fall under "no easy way without changing the delimiter". Let me know if I followed your directions incorrectly as I'm curious about this. 
I work for a very large company that runs both. SQL Server is always faster in our environments. 
This is the option I was talking about: http://i.imgur.com/oafCy3h.png 
Hmm, this topic is somewhat hard to walk away from (despite my best efforts) So, a long post &amp; hopefully some of it will help or resonate. 1. A set with no members is an empty set. My best guess (based on the outer join reference) that in your article, 'NULL set' is a nickname for a set with exactly one tuple of a given length with all elements being NULL. 2. "when I do a left join and the row to the right is not found, does that mean we don't know the answer yet or that there is no value associated?" The actual answer is that we've started with set union operation being restricted to tuples of the same length. This wasn't practical/complete enough, so the union of tuples of varying lengths was introduced and it was ugly to code/math around, so it didn't hang around much other than in the 'outer join' semantics, where it was shoehorned to the same-length tuples result set via defaulting unmatched tuple elements to NULL values (or the 'NULL set'). Since relational theory generally puts little stock in data lineage vs other kinds of metadata, the 'original' tuple size is lost/not implicitly carried. Again - this follows the general principle that if data lineage matters, it should be carried by the data explicitly. In other words, NULLs in outer joins result sets is a convenience/implementation feature, not the original sin of the 'NULL' itself. 3) X + NULL, etc. I.e. if a known quantity is associated with an unknown, why do most scalar operations lose the known part in the result? It is a somewhat valid concern for people who don't agree with 1NF as the basic storage format for relational databases. The '1+NULL' expression represents a tuple of 3 elements (pardon for the obviousness of this statement) and fitting it to 1 element (a result of scalar operation) of a particular data domain results in loss of any additional data that was carried by the original 3-tuple. In my example, the choices for the result are 'a known integer value' or the NULL. Again, it seems obvious, but the only valid answer within our constraints is the NULL. 4) The use of NULL as 'not applicable' is a result of shortcut/convenience development and design (an antipattern, quite possibly true). If an element is not applicable to the relationship/tuple, theoretically, it shouldn't be there. In practice though, it is more convenient sometimes to mix and match tuple sizes. It's a trade-off decision that is often made by the party that will not bear any negative consequences. The 'intended/proper' tuple elements list is not a part of metadata that's covered by the conventions. 5) Three-value logic. It's a wonderful thing where you care about the 3rd value until you don't, really :) Wiki has a more [math-y explanation](http://en.wikipedia.org/wiki/Null_(SQL)) (look for 'weak representation'), but it boils down to that in a system that doesn't allow NULL as a state (i.e. the presence of a record in a result set cannot be NULL/unknown), the final state of a predicate A is derived as (A IS NOT NULL) AND A. If you consider this, the very popular (and a pretty good example of a false equivalence, IMO) example of a "union missing data from A and NOT A predicate" example: SELECT * FROM Emp WHERE Age = 22 UNION SELECT * FROM Emp WHERE Age &lt;&gt; 22; After explicitly stating and negating the original implicit condition, it becomes the following statement, nicely covering all of the original data set: SELECT * FROM Emp WHERE AGE IS NOT NULL AND Age = 22 UNION SELECT * FROM Emp WHERE AGE IS NULL OR Age &lt;&gt; 22 P.S. tl/dr. The NULL is/behaves as much more of a "specific metadata state" than an "unknown data value".
One quick thing I noticed is that you can remove the &gt; A.IP is not null bit from your WHERE statement and you can change your LEFT JOIN to an INNER JOIN. By left joining and requiring that A.IP is not null and B.VisitorID is not null, you are effectively only looking at the records that matched in the join. An INNER JOIN will do this for you. You may also be able to remove the rest of your WHERE statement depending on why you are requiring that B.VisitorID is not null.
This join could potentially be replicating a lot of data as well if Keyword_List.IP is not distinct. For example if you have 5 entries for 127.0.0.1 in Keyword_List and 10,000 entries in Web_Logs for 127.0.0.1 you've essentially created 50,000 rows worth of data for one IP. This is usually what adds significant time to queries like this one where you have a many to many relationship and one column on the join.
&gt; Keyword_List.IP This is not unique. There can be multiple IP values, but a final outcome where IP is unique and VID is multiple. There are no 127.0.0.1 IP's, but there could potentially be bot traffic in there which is something I can't determine unless I validate them on a individual level. Maybe I should select the distinct IP#'s first and then join?
I just indexed date on the larger table and then added in the where clause &gt; 1/1/14 and brought the execution time down to about 7 minutes.
Check the sidebar - there are plenty of links.
Find some study material for the 70-461 exam. Forces you to touch and get experience with a very wide spectrum of SQL concepts.
SQLZoo!
From a readability standpoint, this is recommended, but it most likely will not affect the performance of the query. In MS SQL at least, the query plan will be the same even if it's changed to an INNER JOIN, because the optimizer is smart enough to realize the WHERE clause contains a non-null limiting attribute on the outer joined table.
#1: Ensure that you have an index on the WebLogs table that covers VisitorID and [c-ip], in that order. With as large a table as you have described WebLogs to be, you're going to want 100% of the data you're trying to select to be included in the index. In MSSQL, You can use INCLUDE keyword when creating the index to add this data to the leaf nodes, but be warned that this will decrease performance of inserts, updates, and deletes to it. So use discretion. By including all desired data in the index itself, the INDEX SCAN that will result from your query will not need to be followed up with nested KEY LOOKUPS or CLUSTERED INDEX SCAN followed by a MERGE or HASH MATCH to get the output columns you want added. #2: Consider caching the distinct set of IP addresses from Keyword_List in a temporary table before you join. Since you only care about the unique set of results, by getting the unique set of IPs from your base table as a separate operation, you will cut down enormously on the amount of records that the query engine is going to have to HASH MATCH after the join to WebLogs. Give this a try: SELECT DISTINCT IP INTO #TMP FROM [OPSDB].[dbo].[Keyword_List] SELECT DISTINCT a.IP, b.VisitorID INTO [OPSDB].[dbo].[IP_List] FROM #TMP A INNER JOIN dbo.WebLogs B ON A.IP = B.[c-ip] WHERE B.VisitorID IS NOT NULL DROP TABLE #TMP 
From what I can see, what you are trying to do is return all the c-IP and VisitorId values from Weblogs where the c-IP appears in the IP field in the Keyword_List table. Is that correct? If so since you don't actually need any of the data from the Keyword_List table you are probably better off using an exists rather than a join. SELECT DISTINCT W.[c-ip] IP ,W.VisitorID INTO [OPSDB].[dbo].[IP_List] FROM [OPSDB].[dbo].WebLogs W WHERE EXISTS( SELECT 1 FROM [OPSDB].[dbo].[Keyword_List] WHERE IP = W.[c-ip] ) AND W.VisitorID IS NOT NULL; Not sure if VisitorId is actually nullable so you may not need the last line. PS: As others have said make sure you have appropriate indexes ie one that includes VisitorID. Also ensure that c-IP and IP have identical datatypes otherwise it won't be able to utilise these indexes.
I'm simplifying the bulk of the query because there are other values that I'm more interested in as the query scales up. There are other joins and outputs which are needed before this piece can execute and be joined. The other pieces are all functioning in short periods but I knew that this piece was going to be the longest in terms of performance. So I ran with that and wanted to specifically focus on getting this down in terms of execution time/indexing. It seems that indexing on date is really important. The table is 25 million records but since 1/1 there is probably only... 8 million records? Ball park guess, but nothing being produced by the other processes can *possibly* go back earlier than 1/1. Your approach is interesting, too, especially when considering the larger query that will be need to written to unify all the different parts.
Can you talk a little about the order of importance for indexing?
Thank you for this response. I was really curious about optimization techniques not really rewriting the query from a syntax perspective... unless it had meaningful implications on optimization. This is a small segment of a larger query that will need to run.
Awesome, thank you!
No problem :)
Quite honestly, until you find yourself with the need to write complex queries, you won't learn much beyond the theory and basics. That means to really learn it you need a) a database of data, and b) questions you want to answer. IMX, you don't really learn SQL until someone else can look at your results and say, "No, you're missing some results here." About the best you can do is try to emulate that. I would get the MS SQL Server AdventureWorks databases and look for exercises that use that. I know SQLZoo used to have some. There are many others around, too. I would learn on Postgres, Oracle, or MS SQL Server. I would *not* recommend learning SQL on MySQL in spite of how popular it is. That RDBMS breaks a lot of SQL conventions and doesn't complain enough when you do something bad. Not wrong, *bad*. [MySQL's GROUP BY extensions](http://dev.mysql.com/doc/refman/5.6/en/group-by-extensions.html) in particular really confuse people when they go to other RDBMSs that require aggregate queries to be deterministic. It also doesn't force you to learn security, or force you to learn what schemas are, or force you to present data to the DB engine correctly. This is why developers love it. It doesn't actually make the developer think about the DB like other RDBMSs do. That's why it's "easy". 
I'm using SQL Power Architect. I'd give it 3/5 stars as it hasn't really blown me away yet.
If by SQL case you mean data modeling, I've been jumping back &amp; forth between Power Architect &amp; MySQL Workbench's tool. They're both OK and both somewhat limited. I don't offhand remember which is which, but one at least does not support category discriminants (sometimes called 'subclassing'), and if memory serves they both have some difficulty with relations that use differently named columns (on parent &amp; child table) - really, there's no comparison to the more robust tools like Embarcadero &amp; Erwin - but then again, there is a bit of a price difference. That said, MySQL Workbench has proven to be more stable, and I like the way it syncs, supports views, procedures, and good separation betwene logical &amp; physical data models.
I'll second Power Architect and give you Oracle Data Modeler as another free alternative. http://www.oracle.com/technetwork/developer-tools/datamodeler/overview/index.html. Its good to have options.
Place the values you want to obtain into temp tables Create table #weblogs (IP varchar(15), visitorid int) Insert into #weblogs Select IP, visitorid From web_logs Where visitorid is not null Then join on your temp table. SELECT distinct a.IP, b.VisitorID INTO [OPSDB].[dbo].[IP_List] FROM [OPSDB].[dbo].[Keyword_List] A LEFT JOIN #WebLogs B on A.IP = B.[c-ip] where A.IP is not null and B.VisitorID is not null Your temp table is probably going to have a few less rows than web_logs and definitely less columns. So its a much smaller data set to read from. Also .. tell me about IP_list. How many columns and rows does it have? What kinds of indexes does it have? If its long and wide with lots of indexes, reads are going to be fast but inserts will be painfully slow. So if IP_list is long and wide with indexes, consider dropping those indexes before the insert, then rebuilding them.
Thanks for the suggestions all. I've actually used MySQL in the past and I really liked it but it was super buggy. I'll have to give it another try as well as take a look at Power Architect. As to the Oracle Data Modeler, I have a VM of it from a couple of Oracle classes I took though I never actually used it. I'll have to check on the licencing just to be safe. We are trying to make sure all our i's are doted and t's crossed. Thanks again. 
I would actually say its probably inaccurate to mark those columns as primary keys on the Orders table. The way you have it diagrammed now, it suggests that you are using a composite primary key on the Orders table that consists of those three columns.
Have you tried gathering statistics on the Oracle tables? 
I agree. The order number and line item would be the PK and Customer and Product would be foreign keys. 
Where do you see line item? I see a "lineTotal" field, but isn't a good field for a PK. Based on what I see, the PK for the Orders table should be the OrderID and the DonutID. You really shouldn't have two listings for the same donut in one order. However, if one is concerned about that case, then you could add a "LineItem" field to be part of the PK, which is good since it allows for modifications to the order to be made and tracked, much like a grocery store receipt.
Look into using the Lag function to find the difference between each row and the previous row. Then group / filter that data the way you need it. 
Here is one way you can do this you can replace the where clause with an "AND" on the join if you want to improve performance. SELECT DISTINCT A.invoice_id FROM TABLE_A A INNER JOIN (SELECT invoice_id FROM TABLE_B) AS TableBQuery ON TableBQuery.invoice_id &lt;&gt; A.invoice_id WHERE A.invoice_id = 'xyz' 
 select id ,event_name ,event_start ,event_end from event where '2014-09-15 09:15:00' between event_start and event_end ;
Vertabelo has an in-house license that allows you to install Vertabelo on your own server while providing the same features as a company account in the SaaS model: http://www.vertabelo.com/pricing/enterprise
This query do the things that I want.
For this subReddit I can give you how the query will be constructed: Select FirstName, LastName, Email from ContactTable Where BirthDay = Date(). As for the automation, you'll need to look elsewhere. Depending on which version of Access you're using you'll need to build it using VBA or .Net. Also, Access is not an application that runs on its own, so you'll need something to kick off the process. On the cheap you can set up a Windows Task Schedule task to run this every day. Of course you're computer will have to be on, and you logged in.
I think what you want is NOT EXISTS. SELECT * FROM table_a a WHERE a.client_id = 'xyz' AND NOT EXISTS ( SELECT 1 FROM table_b b WHERE a. Invoice_id = b. Invoice_id ) You could also do this via a LEFT JOIN and IS NULL. Or via a NOT IN ( subquery ). The NOT EXISTS should give better performance though.
 SELECT DISTINCT invoice_id FROM tblA WHERE client_id = 'xyz' AND invoice_id NOT IN (SELECT DISTINCT invoice_id FROM tblB)
According to decision about database engine: On Reddit there are many similar threads about how to start learning, which database choose etc. It's really worth to explore. Many agree that MySQL isn't a good choice as a basis for learning. The reson is that MySQL supports a very old dialect SQL:92. Even that is not really standard compliant in the default configuration: MySQL supports only a subset of the specification. What's more there is an entire section of the MySQL documentation devoted to MySQL differences from the SQL standard. The most important differences aren't really about syntax, but rather about that fact that "MySQL performs operations differently in some cases." According to tools helping beginners to get started with SQL I recommend: http://www.vertabelo.com/blog/notes-from-the-lab/18-best-online-resources-for-learning-sql-and-database . You can find the list of the most popular resources to learn sql and database concepts with short review of each of them. Many of them gives you possibility to run queries online against various databases. This is great to compare and contrast SQL statements in different database back-ends, or when you don't have a particular database platform readily available but would like to see what a given query would look like in that environment. 
Looks like SQLite doesn't support MINUS.
Thanks! This is the one that's easiest to read/understand and it works, so it's the one I'm going with.
I love this question's formatting 'Create an sql'. "Brb, on phone with IBM/Oracle's lawyers, they mad."
There may be some performance differences between the two. Best way to test this and perhaps understand the difference is to view the execution plan of both.
To me, installing the SQLite ODBC driver and querying from desktop software (Access, Excel Power Query, MSQuery) would be the easiest way. This will allow you to choose only the columns you want in the query, as well as filter by criteria. http://www.sqlite.org/cvstrac/wiki?p=SqliteOdbc Installing this will allow you to query your SQLite via ODBC using desktop software.
I second this--use excel's data connections with the SQLite ODBC driver installed. Added bonus of refreshing data right into excel, so you can straight to slicing and dicing the data.
This is really more of a DBA question which could be why you haven't gotten much traction here. Also, you may want to post in the sub for whichever specific DB you're using such as SQL Server, SQLite, etc. On first glance I don't see any reason your query doest finish. DBCC can take a long time depending on how large of a DB, server resources/hardware, etc. 
Do you have read permissions to any databases already? If so there is your answer. As you already have knowledge of [Access](/r/MSAccess) I suggest you look into passthrough queries which will allow you to write SQL to interface directly with the server.
Ah cheers, Thanks for the reply.
I can't imagine the left join preforming better, but depending on the engine and the factors above, the difference could be considerable, negligible or the left join might optimize to be an inner if the engine is smart enough. 
You could nest these two into one formula as well, but I like using temp tables to show step by step. SELECT * ,CASE WHEN Lighting.TYPE_FIXTURE LIKE '%cobra%' AND Lighting.SIZ &lt; 15 THEN 0 ELSE 1 END AS 'IncludeFlag' INTO #TEMP_A FROM Lighting SELECT * FROM #TEMP_A WHERE IncludeFlag = 1
Thanks, I see the logic and it makes sense to me, but when querying in Access I'm getting an error for missing operator. I'll see if I can figure out how to make it run. 
Just to add to this, because unfortunately I work in MS Access more than I would like... ,CASE WHEN Lighting.TYPE_FIXTURE LIKE '%cobra%' AND Lighting.SIZ &lt; 15 THEN 0 ELSE 1 END AS 'IncludeFlag' Should be ,IIf(Lighting.TYPE_FIXTURE LIKE "*cobra*" AND Lighting.SIZ &lt; 15, 0, 1) AS IncludeFlag Untested, but I believe that is correct
He/she means Microsoft SQL Server. SQL Server Express is free, plus all the tools you would need and you can get the sample database Microsoft calls "Northwind" also for free that you can use to learn the basics.
There's also SQL Server Management Studio Express which is free along with the various AdventureWorks sample databases.
Installing postgres means installing a postgres server. Install postgres, following instructions, and you will have a "server". You don't need to know what is going on to start inserting / querying.
Actually, simpler (and possibly faster) version: SELECT DISTINCT g.groupName FROM groups AS g WHERE memberId IN (&lt;w1&gt;, &lt;w2&gt;, ..., &lt;wN&gt;) AND groupId NOT IN ( SELECT groupId FROM groups WHERE memberId NOT IN (&lt;w1&gt;, &lt;w2&gt;, ..., &lt;wN&gt;) )
T-Sql, as in SQL Server. I'd say the earliest software you would want to get into would be version 2008r2. It's not listed on that link for whatever reason, but http://sqlfiddle.com/ is a good area for testing out queries too. T-SQL stands for transact-sql. It's a great base for learning sql, and you can take most of the principles you'll learn to most of the other systems.
postgres is the new standard database used in the open source world. it is free and implements more of the standard sql interactions than mysql. mysql used to be the standard used by the open source community until mysql was bought. in general though, learn about tables, third normal form, selecting, inserting, updating, deleting, and joins for your first topics.
If the columns in question have nulls, the not exists will likely perform better than the not in. edit: Not sure how MySQL optimizes not in vs SQL Server (which is what I was referring to).
Those were the two factors in my "possibly." Compared to SQL Server, MySQL optimizes poorly if at all.
Wow, I had no idea the syntax was so different in Access. What a pain. I'm still getting errors, but they're different at least. I need to spend a bunch of time going through the SQL training resources in all the other threads. I'll start doing that tonight, but I need this query ASAP for work... This still seems like it should be easier than this, considering I've spent all day trying to figure out this one query. How can it be so simple to produce the list of lights that need to be excluded, but so friggin' difficult to actually exclude them? SELECT * FROM Lighting WHERE Lighting.TYPE_FIXTURE LIKE '*cobra*' AND Lighting.SIZ &lt; '16' If only I didn't need to filter them out but still keep them in the database, I'd just replace SELECT with DELETE and be done with it!
How about just using a NOT SELECT * FROM Lighting WHERE NOT (Lighting.TYPE_FIXTURE IN ('Cobra','Bronze Cobra') AND Lighting.SIZ IN ('10','15'))
Thank you very much. That worked perfectly. I knew I was making this too complicated, but I guess I never tried putting the NOT between the WHERE and the table.
Hey man, I tried to run this but it gives me a syntax error. Msg 102, Level 15, State 1, Line 38 Incorrect syntax near 'order'. when i double click the error it goes to the first order by line, but I am unsure whats causing the syntax error
change ROW_NUMBER() OVER (ORDER BY .item) + 1 as Depth to ROW_NUMBER() OVER (ORDER BY a.item) + 1 as Depth Looks like I missed a table alias
same problem. The order it is having trouble with is in the last select statement "SELECT MIN(l.partno) OVER (ORDER BY l.DEPTH ASC) as Level1,"
Microsoft SQL Server Management Studio 10.0.1600.22 ((SQL_PreRelease).080709-1414 )
They should be ints. Row_Number returns an INT and 1 as a value in the anchor should be an int. Whats the exact error message?
Msg 240, Level 16, State 1, Line 6 Types don't match between the anchor and the recursive part in column "depth" of recursive query "list". and if i change that value just to get past the error I get one more error at the end.... ,ROUND(SUM(COUNT(*)*AVG(l.QTY)),8,1) as QTY --This should be more accurate than rounding first then multiplying Msg 130, Level 15, State 1, Line 40 Cannot perform an aggregate function on an expression containing an aggregate or a subquery. Im still trying to digest this recursive function. Sorry I seem incompetent.
Thanks, I'll give that a try.
cool, this actually works. It doesnt do exactly what my query did, but it gives me a great starting point to understand how recursive functions work.
Creating new indexes increases write times, that impact can be negated a little bit by using index padding but it's always going to be a tug of war between read/write speed depending on what your app does and how your indexes are designed. I'm assuming that you mean your system center install database was missing Microsoft recommended indexes and your call to them resulted in them providing code to recreate them? Where'd you come up with the design for the other indexes?
As he mentioned, /r/SQLServer would be a better place for this. 
Sometimes the GUI just gets in the way, it likes to do this with check constraints and calculated columns, too. Instead of using the view editor, try creating the view through straight DDL in a query window: CREATE VIEW vw_blah AS SELECT [...] 
Oh gotcha. Well without really getting into evaluating your database and its unique usage stats I would say a couple things. Those are probably good but they're going to be "Recommendations" that focus on read performance and do not have the ability to account for overall performance impact in regards to the write overhead that they'll add to the system. So enact those with care and evaluate before and after, or preferably setup a test environment and implement them and then test performance of your top used queries against the given tables. If I'm talking over your head and this is just some ancillary task I would say the best course of action is to leave things be until there's a need to investigate slowness on reads. :) Another thing is that the query you posted relies on data that's gathered since the last reboot of the database engine. So if the database is restarted weekly for instance, maybe rerun that query and reevaluate its recommendations at the end of a normal work week before the reboot to make sure you're evaluating the largest data set possible. Who knows maybe people run all their big reports on Friday and you've yet to see the real offenders. Any way you slice it adding indexes will increase the time it takes to add data to the database. So depending on your implementation and usage, that may cause trouble in other areas instead of simply being a good proactive thing to do. Proceed with caution.
cool, i'll give that a try.
I'm pretty sure I was getting a different row count when I run the first vs the second
So say, another table that has three herbs in it? Then when I insert a record into the prescription table I'd just include that table in the inset statement? Does that mean for each prescription I'd need to create a table for the list of herbs? I also re read some lecture notes to try and understand what I'm doing better. I believe what I'm aiming to do is a many to one relationship between herbs and the prescription.
Are you looking to declare a variable for a query? I'm not quite sure what you're asking for here.
So, one prescription can have many herbs, and the same herb can be in many prescriptions? This is a classic many-to-many relationship. The preferred method is called a [junction table](http://en.wikipedia.org/wiki/Junction_table). Try this: CREATE TABLE Prescription ( id INT PRIMARY KEY NOT NULL, patient_id INT NOT NULL, --Foreign key to Patient table doctor_id INT NOT NULL, --Foreign key to Doctor table issue_date DATE NOT NULL ); CREATE TABLE Herb ( id INT PRIMARY KEY NOT NULL, name varchar(300) NOT NULL ); CREATE TABLE PrescriptionHerb ( id INT PRIMARY KEY NOT NULL, prescription_id INT NOT NULL, --Foreign key to Prescription table herb_id INT NOT NULL, --Foreign key to Herb table dosage_mg NUMERIC(10,5) NOT NULL --Sample field you might want in the junction table ); Note that's not any vendor's version of SQL, even MySQL. It's probably closest to T-SQL because that's what I usually use. It's not guaranteed to run anywhere. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Junction table**](https://en.wikipedia.org/wiki/Junction%20table): [](#sfw) --- &gt;In [database management systems](https://en.wikipedia.org/wiki/Database_management_systems) following the [relational model](https://en.wikipedia.org/wiki/Relational_model), a __junction table__ is a database table that contains common fields from two or more other database tables within the same database. It is on the many side of a one-to-many relationship with each of the other tables. Junction tables are known under many names, among them __cross-reference table__, __bridge table__, __join table__, __map table__, __intersection table__, __linking table__, __many-to-many resolver__, __link table__, __pairing table__, __pivot table__, __transition table__, __crosswalk__, or __association table__. &gt; --- ^Interesting: [^Many-to-many ^\(data ^model)](https://en.wikipedia.org/wiki/Many-to-many_\(data_model\)) ^| [^Foreign ^key](https://en.wikipedia.org/wiki/Foreign_key) ^| [^Fact ^table](https://en.wikipedia.org/wiki/Fact_table) ^| [^Associative ^entity](https://en.wikipedia.org/wiki/Associative_entity) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ckklxrn) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ckklxrn)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Ok thanks for this. It's given me a good idea on how I should approach it, and the wiki link for junction tables was greatly appreciated :D
So, you would have Prescription which has one medicine and this medicine can contain many herbs. However, a herb can be found in more than one prescription, correct? This appears to be a many to many relationship. I would add a third table called HerbContents that links Prescriptions with Herbs. A prescription can have many herbcontents and a herb can be included in many herbcontents. HerbContents would have a FK from Herb and Prescriptions. Say for example if you did it this way and queried a Prescription record for a patient (that only had one prescription) but that prescription used three herbs it would look something like this. SELECT Prescription.presc_id, Herb.herb_id FROM Prescripton INNER JOIN HerbContents ON HerbContents.presc_id = Prescription_presc_id INNER JOIN Herb ON Herb.herb_id = HerbContents.herb_id WHERE Prescription.presc_id = 128283 That query is poorly optimized but you get the idea. The output shuld be something like this: presc_id | herb_id --------|------- 128283 | 18 128283 | 48 128283 | 6 I *think* this is what you are looking for but let me know and I'll be more than happy to see if I can provide any assistance. 
Oh, one thing I forgot. there should [probably] be a unique constraint on `(prescription_id, herb_id)` in the `PrescriptionHerb` table.
SQL?
That's way too vague for me to begin an answer.
This works perfectly, thank you!
Glad I could help
Ah yes, the hardest part of being a DBA... People asking you to do shitty tier 1 help-desk work.
ok, i'm pretty sure they were different yesterday...
That first point is actually pretty good after the correction. Its a unavoidable fullscan on the table, followed by a simple search. My problem is just that, for some reason, Phone is not a index on the table. Really, whomever designed this should have its face dragged through asphalt.
It is an alias for the subquery that allows you to reference the fields that it returns. You can think of it as giving it a nickname.
Every time you reference a table in a FROM or JOIN statement you can give it a nickname.
As /u/di_lam and /u/jaygee999 said its an Alias that acts like a nickname to the table. This is useful for writing queries with long table names or tables in other databases, and is requirement much of time when referencing tables on linked servers in join statement. Select dbo.mylongasstablename.first_name, dbo.mylongasstablename.Last_name, dbo.mylongassreferencetablename.description, dbo.mylongassreferencetablename.value from dbo.mylongasstablename inner join dbo.mylongassreferencetablename on dbo.mylongasstablename.referencevalue = dbo.mylongassreferencetablename.id where dbo.mylongasstablename.gender = 'M' This is shortened to Select t.first_name, t.Last_name, r.description, r.value from dbo.mylongasstablename t inner join dbo.mylongassreferencetablename r on t.referencevalue = r.id where t.gender = 'M'
god, that's awful in fact, this submission is pretty much just spam
if it's a subquery in the FROM clause -- also called a **derived table** -- then you not only can give it an alias, you *must*
Ok to spell it out further. We have backup device names that corresponds to a database backup name from an application. It's location and what it will be called. I have over 60 of these backup device names on my production database server that I need in place on my hot site database server. Is there a way to replicate that information to the HS database server instead of manually creating the backup device names? They share the same location on each server. 
I'd do it like this: With A AS ( SELECT ROW_NUMBER() OVER (PARTITION BY ID ORDER BY VALUE DESC) AS RowNo ,ID ,Value ,Note FROM @TBL ) SELECT * FROM A WHERE RowNo = 1 EDIT: forgot to order by DESC
make sure you study up on XML queries, transaction isolations, different types of joins(like Except/Intersect etc...). I'm probably missing topics, but you might want to look into a preparation guide. * source * I'm a MSCA(SQL 2012), MCITP(SQL DBA 2008)
True dat
 &gt;from dbo.mylongasstablename t I too like to live dangerously.
The Big Picture: The key task of an optimizer is to find the (hopefully) best execution plan for a given SQL statement under certain conditions (like the existences or absence of some indexes). For that, it *basically* creates all possible execution plan variants (e.g. one using a specific index, another one not using it) end does a so-called cost estimate for each of them. The cost estimation takes into account how much effort it will be to run an execution plan in terms of IO but often also considering CPU and other factors. In the end, the optimizer just picks the execution plan that has the least cost. One very important input for the optimizer to do the cost estimation are the so-called statistics: these are numbers that tell the optimizer something about the data (number of rows in table, number of distinct values in a column, and the like) so that the optimizer can do a so-called cardinality estimate without actually running the query. E.g. if you have `WHERE a=7` in your query and the statistics say that the table has 1.000.000 rows, with 10 distinct values in column `a`, it might estimate that the query will return 100.000 rows. Because data is not always evenly distributed, the statistics often also contain histograms. This approach is called cost-base optimization which is the state of the art. Previously, there was rule-based optimization which was just using some hardcoded rules like if there is an index, use it. As you thought, evaluating all possible execution plan variants might be time consuming too. The more complex the cost model becomes, the more costly the optimization becomes. Also, the more complex the query is (most importantly: # of tables joined) the more complex the optimization. Therefore it often makes sense to cache the "best" execution plan for a statement (e.g. Oracle, SQL Server do it, MySQL not). This caching has problems related to bind parameters. Learn more about this [here](http://use-the-index-luke.com/blog/2011-07-16/planning-for-reuse) and [here](http://use-the-index-luke.com/sql/where-clause/bind-parameters). 
This is spread throughout the world of computer science, but you can basically think of an optimizer as a dedicated search engine. Different types of access and activity are given default scores. Metadata is maintained by the database engine and that metadata is fed into a variety of algorithms in order to figure out what will be the shortest path to query results. This is covered in a few papers listed in the [CS286 syllabus](http://www.cs286.net/home/reading-list) and [Readings in Database Systems](http://www.amazon.com/gp/product/0262693143/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1535523722&amp;pf_rd_s=lpo-top-stripe-1&amp;pf_rd_t=201&amp;pf_rd_i=1558605231&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_r=1CZETJ7N15JCY842KK2K). The future execution time is just a guess and isn't represented as a time but as an arbitrary cost based on the bogus scores that are assigned to each operation.
I haven't taken the course but have been using the popular study guides along with the practice tests -- from what I can tell so far, you don't need a very thorough understanding of the functions off the bat. I've worked mostly in SQL Server 2008 so a lot of the 2012 stuff is new for me, but with the practice exams a lot of it is basic knowledge/process of elimination/etc.
Beat me to it! This is exactly the way I'd suggest, although if VALUE could be null, you should put in a "nulls last" clause as they will appear first in descending sorts. ROW_NUMBER() OVER (PARTITION BY ID ORDER BY VALUE DESC NULLS LAST) AS RowNo
&gt; [..] find the (hopefully) best execution plan for a given SQL statement under certain conditions [..] I realize you were simplifying for OP here; just adding my own two cents: Sometimes it's not just about what is going to be potentially the fastest route, it can also be about which has the lowest risk (the big O). An example might be: SELECT * FROM users WHERE username LIKE '%tocker'; An index scan could technically be used here instead of a table scan. If one row matches in the index scan, it might be more efficient than a tablescan (since the index is probably more compact to scan). However, since there are no statistics available, the optimizer has to be prepared that all rows could match. In that case, there could be considerably more work than the table scan which has a relatively fixed cost. So from that perspective, not all cases where optimizer hints make a query faster can be considered a bug.
Worked with T-SQL wrote lots of queries singles, unions CTE's with multiple joins, recursion. Thought I was pretty knowledgable. Bought the MS Training Guide took the practice tests. Scored 90-98% on multiple passes. Paid my money, took the actual test - scored 67%. Questions regarding obscure parts of the language that I'd never dealt with. Will retake, if I can ever squeeze in the time between working with C#, Javascript &amp; jQuery, etc. YMMV
Wow are the later portions that hard? I have no illusions of it not being challenging I'm just starting on my road though and I'm being required by work to move towards a DBA role which I'm taking my first steps on. It's all scary, exciting, daunting at the moment. I have the interesting position of having access to a production SQL server with no real true DBA running the show. It's been fine for years like this but the most anyone has had is remedial querying abilities at best. I also have this remedial knowledge but clearly my new role is dictated a much more hands on knowledge. 
This has nothing to do with this subreddit and has been marked as spam.
If you want a really in depth of how they work, go to Amazon and find yourself an internals book. [I think that a good used SQL Server 2008 Internals book is still relevant.](http://www.amazon.com/gp/offer-listing/0735626243/ref=tmm_other_meta_binding_used_olp_sr?ie=UTF8&amp;condition=used&amp;sr=&amp;qid=)
Zero content answer, but as someone who works with databases I like to think about the Optimizer when I'm really high... Mind blowing.
Source code for [PostgreSQL's query optimizer](http://doxygen.postgresql.org/dir_7175d082973170882d93e297d0d6db83.html) is online. 
Thank you very much for your post. Especially the statistics part, that led me to [this pdf](http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-stats-concepts-110711-1354477.pdf), which seems to be worth familiarizing next. I've also read the references and would like to say, you've got really didactic approach. Excellent blog. TIL quite a lot and my name is Luke, so thanks once again :)
This Berkeley link looks perfect but it's a fair number of papers there as well :) I've already been reading for couple of hours, so I'm done at the moment. Will continue tomorrow. Thanks a lot for the source.
I'd say it's mid-level detailed. You do have to know or recognize syntax and structure and understand concepts, but you aren't tested on every little exception. Definitely difficult but not impossible. I had next to zero experience in SQL Server and only basic SQL skills. Studied about 3 months and passed. The MS press training kit will get you most of the way to passing, but maybe the MeasureUp or Transcender practice tests will take you home. The practice tests included in the training kit are way too easy compared to the exam. 
At the very least, this makes boo sense to me.
The way I did it was something like this. select hName, prescAmount, pricePerUnit from Prescription p, UsedHerb u, Herb h where p.pId = 1 and u.pId = 1 and u.hName=h.hName I think this will get me the result I want, but unfortunately I can't actually connect to the database to run the query at the moment.
Haha alright looks like I will be sending emails to my teacher then
&gt; you've got really didactic approach Thanks. It's actually my main business to explain these things. Just in case someone is interested in more details: http://winand.at/services/sql-training
Those scripts also fail to notice when it produces a duplicate index, and you've got duplicates in there. You could reduce that tbDeadDeployment to a single index, including all the needed columns into a single include option. The third tbRevision recommended index is also a duplicate that can simply be eliminated, as it would be handled by the first.
I rarely use ROW_NUMBER() so using PARTITION BY and limiting the results never occurred to me but this is pretty much exactly what I'm looking for and will work for a few things for me. So thanks for taking the time to share this, it will make a few things faster and easier for me.
Do you mean an [input mask](http://office.microsoft.com/en-au/access-help/input-mask-syntax-and-examples-HP005187550.aspx)
I have to place the constraint on the data that can be entered in that particular field. Each entry has to follow the upper, lower, lower, upper, lower sequence. Im really sorry if that doesnt make a ton of sense. I not totally sure how to frame my issue. 
It would be better if you gave us a simple hard coded example of what your issue is. 
I think you're going to have to write a custom function that performs a regular expression match, and then reference that function in the Validation Rule for the field. Unfortunately I can't get much farther than that as I'm a SQL Server person who avoids Access like the plague.
For Reference: 001 DECLARE @BUSUNIT NCHAR(2) = 'FW' 002 DECLARE @ACCTMGR NCHAR(3) = '' 003 004 SELECT 005 DCC.CurrYear, 006 DCC.CurrWeek, 007 DCC.CurrSeason, 008 CASE DCC.BusUnit WHEN 'FW' THEN 'FOOTWEAR' WHEN 'CI' THEN 'COMFORT INSOLES' WHEN 'HB' THEN 'HANDBAGS' ELSE DCC.BusUnit END AS BusUnitName, 009 DCC.BusUnit, 010 DCC.Division, 011 DCC.DistroChannel, 012 DCC.DistroChannelName, 013 DCC.ReportAcctExec, 014 DCC.ReportAcctExecName, 015 DCC.Customer, 016 DCC.CustomerName, 017 DCC.CustomerReportFlag, 018 RepOrderQty, 019 RepOrderPriceExt, 020 CASE WHEN RepOrderPriceExt &gt; 0 THEN 0 ELSE RepOrderQtyLY END as RepOrderQtyLY, 021 CASE WHEN RepOrderPriceExt &gt; 0 THEN 0 ELSE RepOrderPriceExtLY END AS RepOrderPriceExtLY, 022 SeaOrderQty, 023 SeaOrderPriceExt, 024 CASE WHEN SeaOrderPriceExt &gt; 0 THEN 0 ELSE SeaOrderQtyLY END as SeaOrderQtyLY, 025 CASE WHEN SeaOrderPriceExt &gt; 0 THEN 0 ELSE SeaOrderPriceExtLY END AS SeaOrderPriceExtLY, 026 ClsOrderQty, 027 ClsOrderPriceExt, 028 CASE WHEN ClsOrderPriceExt &gt; 0 THEN 0 ELSE ClsOrderQtyLY END as ClsOrderQtyLY, 029 CASE WHEN ClsOrderPriceExt &gt; 0 THEN 0 ELSE ClsOrderPriceExtLY END AS ClsOrderPriceExtLY 030 031 FROM ( 032 SELECT 033 CAL.ARYear as CurrYear, 034 CAL.SeasonCode as CurrSeason, 035 CAL.WeekNbr as CurrWeek, 036 BusUnit, 037 Division, 038 RTRIM(DistroChannel) as DistroChannel, 039 RTRIM(DistroChannelName) as DistroChannelName, 040 RTRIM(ReportAcctExec) as ReportAcctExec, 041 RTRIM(ReportAcctExecName) as ReportAcctExecName, 042 Customer, 043 RTRIM(CustomerName) as CustomerName, 044 CustomerReportFlag, 045 SUM(isnull(RepOrderQty, 0)) as RepOrderQty, 046 SUM(isnull(RepOrderPriceExt, 0)) AS RepOrderPriceExt, 047 SUM(isnull(RepOrderQtyLY, 0)) AS RepOrderQtyLY, 048 SUM(isnull(RepOrderPriceExtLY, 0)) AS RepOrderPriceExtLY, 049 SUM(isnull(SeaOrderQty, 0)) as SeaOrderQty, 050 SUM(isnull(SeaOrderPriceExt, 0)) AS SeaOrderPriceExt, 051 SUM(isnull(SeaOrderQtyLY, 0)) AS SeaOrderQtyLY, 052 SUM(isnull(SeaOrderPriceExtLY, 0)) AS SeaOrderPriceExtLY, 053 SUM(isnull(ClsOrderQty, 0)) as ClsOrderQty, 054 SUM(isnull(ClsOrderPriceExt, 0)) AS ClsOrderPriceExt, 055 SUM(isnull(ClsOrderQtyLY, 0)) AS ClsOrderQtyLY, 056 SUM(isnull(ClsOrderPriceExtLY, 0)) AS ClsOrderPriceExtLY 057 058 FROM SIMPDataCustCalc AS DCC 059 INNER JOIN FiscalCalendar AS CAL ON DCC.FiscalYear = CAL.ARYear 060 WHERE 061 (CAL.CalDate = CONVERT(date, DATEADD(day, - 7, GETDATE()))) 062 and DCC.Week &lt;= CAL.WeekNbr 063 AND DCC.Season = CAL.SeasonCode 064 AND (@BusUnit = '' OR BusUnit = @BusUnit) 065 066 067 GROUP BY 068 CAL.ARYear, 069 CAL.SeasonCode, 070 CAL.WeekNbr, 071 BusUnit, 072 Division, 073 DistroChannel, 074 DistroChannelName, 075 ReportAcctExec, 076 ReportAcctExecName, 077 Customer, 078 CustomerName, 079 CustomerReportFlag 080 ) AS DCC 081 WHERE 082 ((DCC.RepOrderQty + DCC.SeaOrderQty + DCC.ClsOrderQty) = 0 and (DCC.RepOrderQtyLY + DCC.SeaOrderQtyLY + DCC.ClsOrderQtyLY) &gt; 0) 083 084 FULL OUTER JOIN 085 086 SELECT 087 (CAL.ARYear AS CurrYear, 088 CAL.WeekNbr AS CurrWeekNbr, 089 CAL.SeasonCode as CurrSeason, 090 CASE DCC.BusUnit WHEN 'FW' THEN 'FOOTWEAR' WHEN 'CI' THEN 'COMFORT INSOLES' WHEN 'HB' THEN 'HANDBAGS' ELSE DCC.BusUnit END AS BusUnitName, 091 DCC.BusUnit, 092 DCC.Division, 093 RTRIM(DCC.DistroChannel) as DistroChannel, 094 RTRIM(DCC.DistroChannelName) as DistroChannelName, 095 RTRIM(DCC.ReportAcctExec) as ReportAcctExec, 096 RTRIM(DCC.ReportAcctExecName) as ReportAcctExecName, 097 DCC.Customer, 098 RTRIM(DCC.CustomerName) as CustomerName, 099 DCC.CustomerReportFlag, 100 SUM(isnull(DCC.RepOrderQty, 0)) as RepOrderQty, 101 SUM(isnull(DCC.RepOrderPriceExt, 0)) as RepOrderPriceExt, 102 SUM(isnull(DCC.SeaOrderQty, 0)) as SeaOrderQty, 103 SUM(isnull(DCC.SeaOrderPriceExt, 0)) as SeaOrderPriceExt, 104 SUM(isnull(DCC.ClsOrderQty, 0)) as ClsOrderQty, 105 SUM(isnull(DCC.ClsOrderPriceExt, 0)) as ClsOrderPriceExt, 106 SUM(isnull(DCC.RepOrderQty, 0) + isnull(DCC.SeaOrderQty, 0) + isnull(DCC.ClsOrderQty, 0)) as TotalOrderQty, 107 SUM(isnull(DCC.RepOrderPriceExt, 0) + isnull(DCC.SeaOrderPriceExt, 0) + isnull(DCC.ClsOrderPriceExt, 0)) as TotalOrderPriceExt 108 FROM 109 SIMPDataCustCalc AS DCC 110 INNER JOIN FiscalCalendar AS CAL ON DCC.FiscalYear = CAL.ARYear 111 WHERE 112 (CAL.CalDate = CONVERT(date, DATEADD(day, - 7, GETDATE()))) 113 AND (DCC.CustomerDateCreated &gt;= CAL.BeginTripDate 114 AND DCC.CustomerDateCreated &lt;= CAL.EndTripDate) 115 AND ((DCC.SeaOrderQty &gt; 0 and DCC.SeaOrderPriceExt &lt;&gt; 0) 116 or (DCC.RepOrderQty &gt; 0 and DCC.RepOrderPriceExt &lt;&gt; 0) 117 or (DCC.ClsOrderQty &gt; 0 and DCC.ClsOrderPriceExt &lt;&gt; 0)) 118 AND DCC.Season = CAL.SeasonCode 119 AND DCC.Week &lt;= Cal.WeekNbr 120 AND (@BusUnit = '' OR BusUnit = @BusUnit) 121 GROUP BY 122 CAL.ARYear, 123 CAL.WeekNbr, 124 CAL.SeasonCode, 125 DCC.BusUnit, 126 DCC.Division, 127 DCC.DistroChannel, 128 DCC.DistroChannelName, 129 DCC.ReportAcctExec, 130 DCC.ReportAcctExecName, 131 DCC.Customer, 132 DCC.CustomerName, 133 DCC.CustomerReportFlag, 134 DCC.Season) t 135 136 ON DCC.CurrYear = CAL.ARYear I'm not sure what you're trying to do here, but it looks to me like you're fine (relatively) until the FULL OUTER JOIN statement on line 084. You can't have a JOIN *after* a where clause in a SELECT statement, and both sides of a JOIN must be a table, a view, a table expression, etc. Line 086, This is a SELECT clause, with no columns. Line 087, the "(", closed on 134 creating table expression [t], itself has no SELECT CLAUSE, could be a typo here? Line 136, ON references nothing. I *think* you're attempting to JOIN/UNION the entire set from ABOVE the FULL OUTER JOIN with the entire set below it, but I can't be sure. You'll have to clarify what you're trying to do, JOIN, or UNION, keep in mind they are *not* the same. Easiest way for a beginner to look at it, (at least it helped me): - JOIN puts two tables together *horizontally*, matching row values Table1.Col1 = Table2.Col1 - UNION puts two tables together *vertically*, Table1: Col1,Col2,Col3, stacked on top of Table2: Col1,Col2,Col3. You are correct in your assumption that UNION requires a matching set of columns for top/bottom.
how about fun uses of said functions to do useful work? get rid of the time portion of a datetime: cast(round(cast(@date as float),0,1) as datetime) calc the first day of the month (works for finding the first of pretty much anything with some tweaks): dateadd(d,1-day(@date),@date) 
You've got some peculiar stuff going on and I think you're way over thinking this. * You can shorten your set for @StartDate to "dateadd(month, -6,GETDATE())' since you are just adding 0 in the nested dateadd function. * You should just add a week dimension to your results rather than trying to create a week temp calendar. Just do something like SELECT DATEPART(week,Point_Date) [WeekNumber], DATEPART(year,Point_Date) [Year], ...., SUM(P.Points) [Points]. You don't want to have DATEADD in your SELECT because that is just going to take your Point_Date and add 6 week for your ExpireDate and subtract one week for your Point_Week. I would rewrite this if I could if I had access to the DB but since I don't I'm not going to try. You need to do some research on grouping results. You're trying to sum up values without using a GROUP BY. 
Excluded the group by and sum because I didn't know which to group on. The biggest issue I am running into is that these points need to be on more than one sum - every week sum between point add date and expire date - basically a weekly running balance. Tried the week temp table join but that didn't work. The reason I originally did this is that there were no points for some weeks but I still need to show a balance for those weeks DateAdd(day, -1 * datepart(dw, Point_Date), Point_Date) as Point_Week - this portion removes the number of days to return the date of the first day of the week. Grouping by just the week only gives me the balance for the week in which the point occured.
I think the only way to do this is with a "BeforeUpdate" event on the textbox. From there you can check the constraints in straight VBA code using UCase and LCase. If the input fails the constraints you can cancel the update as well as explain why it was cancelled. Either way, I don't believe any of the built in functionality is necessarily going to do this (though I have never tried), I believe it will require code however you do it. EDIT: This will also mean you can't edit the table from straight datasheet view... though you could create a form that displays as only datasheet view and from there gain control of all the input as if it were a straight form. EDIT2: alinroc's solution seems very viable, but I have never gone that route before
Two view points This likely seems like data validation. You may have to get creative because it would appear things in Access are case insensitive. [Here's a Link](http://en.tekstenuitleg.net/articles/software/access-validation-rule-tutorial/text-validation-rule-examples) An alternative view point is that this is a formatting issue. If everything is going to be "AbcDeAbcDeAbcDe" then "abcdeabcdeabcde" should be perfectly valid in your database and your application should likely do the formatting. This depends on how things are developed though. Much like database data types should fit the data and the application (or associated queries) left to format the data as required. 
Its a odd problem. The specs my professor gave are very odd but im doing my best to find a creative solution and learn the most universally accepted method Access makes that a bit hard. Thanks everyone for the help
I refer to tables like this all the time, and I think it would be more useful to me if Expected Result were the first column, then Function, Query Example, and finally Description. Also, it might be easier to parse if there were color coding and mixed case used for function parameters.
Functional dependency is a dependency that can be expressed via a function, i.e. f(x) = y; In your case, for example, if a specific part with partNum can be stored in different warehouses, the user of the DB should be able to determine onHand quantity given partNum and warehouse, so F(partNum, warehouse) = onHand. 
You can do this too: CAST(CAST('2014-09-18 20:19:00' AS DATE) AS DATETIME) You may not even need the outer cast as a date evaluates at 00:00 when compared to a datetime
Thanks so much -- I ended up coming up with a similar solution, except using a LEFT OUTER JOIN - almost identical query to yours though. Thanks again! I appreciate the time it took for you to write this out.
not really sure about sql-server but in sybase ASE I do this: declare @date datetime select @date = "2014-09-19" You don't need any cast. 
You can do the same in sql server.
While learning SQL, I have created a small simple database of some of my books. One table "Authors" contains id, fname &amp; lname. The other table "Books" contain id, title &amp; year. I get the books with a simple SELECT with INNER JOIN: SELECT a.fname + ' ' + a.lname AS 'Author' , b.title , b.[year] FROM Authors AS a INNER JOIN Books AS b ON a.id = b.id ORDER BY a.fname This works fine! I decided I wanted to only list the newest book by each author, and I can do this with the following code: SELECT a.fname + ' ' + a.lname AS 'Author' , MAX(b.[year]) AS 'Newest Year' FROM Authors AS a INNER JOIN Books AS b ON a.id = b.id GROUP BY a.fname + ' ' + a.lname This works, but it doesn't show me the title of the latest book. If I add the title in my SELECT statement, I'll have to include that in the GROUP BY, and that messes up the result. The only way I've gotten this to work is by this long and somewhat overkill queue: WITH NewBooks AS ( SELECT a.id , MAX(b.[year]) AS 'Newest' FROM Books AS b INNER JOIN Authors AS a ON a.id = b.id GROUP BY a.id ) SELECT a.fname + ' ' + a.lname AS 'Author' , b.title AS 'Newest book in database' , b.[year] AS 'Released' FROM Authors AS a INNER JOIN Books AS b ON a.id = b.id INNER JOIN NewBooks AS nb ON b.id = nb.id and b.[year] = nb.Newest ORDER BY a.fname This works, but isn't there a simpler way to do this? Thanks, Ole 
Seems like you have the right idea, assuming I am following your directions correctly. The only thing I can suggest is using a HAVING statement. SELECT a.YEARWEEK, b.VALUE FROM Table1 a JOIN TABLE 2 b ON(a.YEARWEEK=b.YEARWEEK) HAVING b.VALUE=MIN(b.VALUE) GROUP BY a.YEARWEEK
One analogy I've used is that Excel is kinda like driving through McDonald's, Access forms are like heating up a TV dinner, and SQL is cooking from scratch. It sounds like you may have just been bitten by the chef bug!
We will definitely play around with the layout in the future and see what is optimal for users.
With the amount of data you have, store the row ID numbers off in a temp table for the records that changed (which is probably a small subset of the whole list). With this subset, you'll want to compare each of the columns to spot the differences. If you don't need to put it back in a database, you could get away with putting the data set into text files and running a diff between them (such as WinMerge).
your GROUP BY does not match your SELECT, and this produces an error in most database systems (it will run in mysql but the values for b.value are indeterminate) also, it returns only matched rows, not all rows of table1 as required also, your HAVING condition makes no sense
What other optimizations have you attempted before going the in-memory route? Have you monitored your system while loading that order screen to see what exactly the bottleneck is? What else is running on your VM hosts (and SAN, I'm guessing you're using a SAN)?
You can use Change data capture in Oracle http://docs.oracle.com/cd/B19306_01/server.102/b14223/cdc.htm 
Is it a specific query or are you asking to run the entire database 
The entire database - the goal would be to use that db just like any other SQL table. Thanks
If you can't mess with the query, then you'll need to tune the host system (if you're getting a lot of waits/resource contention) and indexes on the tables. Given that you're not really a SQL person and you're dealing with software you've purchased from a vendor, using In-Memory OLTP is most likely *not* a viable solution. Sure, it might work on your test instance right now (after hacking the schema to pieces), but that doesn't mean a lot for this situation. You are *probably* in a situation where the disks and I/O subsystem are not configured optimally for SQL workloads, which will not help matters. Go get [Brent Ozar's diagnostic scripts](http://www.brentozar.com/first-aid/sql-server-downloads/) and run them; they'll help you identify what's wrong. Are you maintaining your indexes &amp; statistics regularly? Have you asked the vendor of your CRM software? If this is a problem that is common for their software, they should have recommendations they can give you.
I think you can, but it gets kind of ugly. You'd be better off moving the tables from Access into SQL, then linking to those tables from Access if you still need to use them from there.
&gt;I just used a join to a self-subquery to find duplicates across 3-field combinations in table. If you're interested in more... learn CTEs. http://msdn.microsoft.com/en-us/library/ms186243%28v=sql.105%29.aspx
I'm a tester focusing mainly on ETL processes, so I run table compares pretty frequently. There's a summary query I use to compare expected vs actual (in your case, before or after) tables. I do a full outer on the primary key and then with some help from my good friend Excel, write a SUM(DECODE(...)) expression for each field to sum up the counts where the values are different. It looks like this: SELECT -- These counts summarize inserts/deletes SUM(CASE WHEN a.pk_field IS NOT NULL AND b.pk_field IS NOT NULL THEN 1 ELSE 0 END) MATCH_CNT ,SUM(CASE WHEN a.pk_field IS NOT NULL AND b.pk_field IS NULL THEN 1 ELSE 0 END) MISMATCH_BEFORE_CNT ,SUM(CASE WHEN a.pk_field IS NULL AND b.pk_field IS NOT NULL THEN 1 ELSE 0 END) MISMATCH_AFTER_CNT -- These counts summarize updates per field ,SUM(DECODE(a.field2, b.field2, 0, 1)) field2 ,SUM(DECODE(a.field3, b.field3, 0, 1)) field3 ,SUM(DECODE(a.field4, b.field4, 0, 1)) field4 ---...and so on for field5-field200+ FROM before_table a FULL OUTER JOIN after_table b ON a.pk_field = b.pk_field ; PL/SQL Developer has a nifty single row view button that transposes it into a vertical list, and I can easily scroll through 100+ columns and see which ones had changes. If one has a particularly troubling amount of records with different values, I SELECT those fields with the same join and WHERE DECODE(...)=1.
Thankyou. I will check the link and see what I can do. I appreciate your advice. 
You can use the UNPIVOT function on each table to convert columns to rows and then execute a MINUS between the two to get the difference (changed values) for each column. SELECT * FROM BIO_CURRENT UNPIVOT (value FOR column_name IN (firstname, lastname, email)) MINUS SELECT * FROM BIO_OLD UNPIVOT (value FOR column_name IN (firstname, lastname, email)) List all your columns except for your PK in the array. This example assumes your table is structured like (id, firstname, lastname, email). This query will return a row that identifies the PK, column name, and the new value.
You can set up a 'linked server' in SQL Server, so that you can access it from SQL Having tried this in the past, let me just say its a very bad idea. Access is a piece of shit and not designed to be a heavily concurrent environment, you will hit a tonne of performance problems even just with simple queries or joins. What do you want to do this for? There are usually better alternatives such as installing MSDE on desktops and using merge replication, or for one way replication, just import the Access database into SQL Server periodically.
So you need the least date &gt; today for each site w/o an exercised status And the exercised date for each site that has one? You will need two queries that you then union. First will group by location smallest date &gt; today You'll need a subquery to id locations that have an exercised status, use it to exclude locations from the first query. On a phone right now. Holler back later if this doesn't help. 
Thank you so much. I wouldn't necessarily have to have the Exercised date, only if it is the next date. If another Lease had a date coming up sooner I would want that one instead. But if an Active Lease is Exercised I need the Exercised date. That is somewhat helpful. If anything a point in direction. Thank you, again.
"Nice job making an omlet in your skillet. Now go make this braised lamb with cranberry chuntny" Subqueries to CTEs is a bit of a leap. And AFAIK Access doesn't do CTEs
The specifics escape me, but you add an OLE DB source and point it to SQL server. Same way you add an excel file. Then you can query it and just like it were a database right in Access. Just google how to add a data source, you'll figure it out.
you could set up a linked server using the piece of crap jet4.0 and query trough that. Honestly, and I really don't want to rain on your parade, what you are trying to do is a really bad idea. Access sucks big time as a database, if it even deserves the name. Frontend for non programmers ok, but DB, please dear god no. The Jet4.0... I wouldnt even use that piece of shit to import an excel file i have to say. You would be soo much better off, migrating your data from access into your MSSQL server, I can't even find a metaphore.
Also, I'll add that I know nothing about SQL.
&gt; What do you want to do this for? There are usually better alternatives such as installing MSDE on desktops and using merge replication, or for one way replication, just import the Access database into SQL Server periodically. nah, migrate the data to the SQL server, and have linked tables in access. That way you don't have 2 different data stores you have to keep in check. Performance will still suck, since access sucks, but you can maybe mitigate with views and stored procs on the SQL Server. At least you dont run into data integrity problems thou
Looks like school work. Can you please show what you've tried so far?
Hahaha, how do you REALLY feel?
It does indeed look like school work, but I can point you in the right direction: 1.) [Check out the count function, particularly with distinct if there's a possibility of multiple authors to a title.](http://www.w3schools.com/sql/sql_func_count.asp) 2.) [Look at the max function](http://www.w3schools.com/sql/sql_func_max.asp) 3.) [Look at joins](http://www.w3schools.com/sql/sql_join.asp) The last one will be conceptually much harder than the first two -- google is your friend. Good luck!
Haha, no, not school work. It was for a job interview (that I already went to). They emailed me a packet with some questions about various things and said to answer what I could. This question was one that I didn't answer because, like I said, I know nothing about SQL.
You should study this kind of questions, must of job interviewers/tests ask the same basic stuff. SELECT COUNT(*) FROM Titles SELECT MAX(Price) FROM Titles SELECT t.* FROM Titles AS t INNER JOIN Authors AS a ON t.au_ID = a.au_ID AND a.au_lname = 'Jones' Edit: format
You're skipping a join in there. Titles doesn't have au_ID, so you need to join it to TitleAuthors first, then join the Authors table.
pubs.dbo.titles, not dbo.emdb.titles.
uh. you could insert the table into a temp table, with an identity key, ordered by manager name. you could then do a cursor, and search for every record where every manager name = manager name, and the identity/cursor = identity/cursor-1. store those results into another temp table. that might work. I can't believe I just recommended a cursor though.
The next version would ideally be browser based so we could get at it from a tablet, and would have a fair amount of hierarchy, so Access probably has to go. It's encouraging to start to realize how much I'll be able to just do in SQL but I'm going to be sad to lose all the UI stuff I get for free. I don't know the first thing about setting up a web server, much less whatever coding it'll require to work in a browser. I mean, I could live off omelets if I had to. They're still pretty good.
select distinct e1.manager from emp e1 join emp e2 on e1.manager = e2.manager and e1.employee &lt;&gt; e2.employee
Yeah, then just move the whole thing into SQL &amp; use Access as a front-end with linked tables if you need it.
Have you told the company that you don't know SQL? These are really, really basic questions and any SQL tutorial should give you enough to answer them. Let's say you provide perfect answers because someone here gave them to you and the company says "hey, this guy needs to be working on queries full-time for us!". Now you've lied your way into the job and you don't even know where to start when you get your first assignment. What then?
Oh yeah. I thought about cheating TBH but figured they'd ask me more so I told them flat out that I don't have experience with it. 
annoying and stupid "cannot use such-and-such" homework assigment use a LEFT OUTER self-join with an IS NULL check
Yep. Classic interview question. Wait a minute... you aren't being interviewed right now are you?
If you do this work for a living you have nothing to worry about. I just passed this one a month ago. I think some of the other replies are right on, XML, new functions, altering views, altering table are the main points. In the "Skills Measured" section of MS website is pretty accurate description of the subject matter. I think the test is a good balance of the main concepts and fine level details. I have been a professional for about 4 years, and I learned many things working through the study guide that I read. [https://www.microsoft.com/learning/en-us/exam-70-461.aspx](https://www.microsoft.com/learning/en-us/exam-70-461.aspx)
A combination of SUBSTRING &amp; CHARINDEX will get what you want. Something like :- SUBSTRING(ColumnName,{START POINT},{NUMBER OF CHARACTERS}) You'll need to insert dynamically locate the {START POINT} like this :- CHARINDEX('/tips/',ColumnName)+6 You'll need to dynamically calculate the {NUMBER OF CHARACTERS} like this :- CHARINDEX('.jpg',ColumnName) - CHARINDEX('/tips/',ColumnName)-6 Then - string it all together like this :- SELECT SUBSTRING(ColumnName,CHARINDEX('/tips/',ColumnName)+6,CHARINDEX('.jpg',ColumnName) - CHARINDEX('/tips/',ColumnName)-6) FROM TableName Once you're selecting the right value - you can then modify your statement to update the other column :- UPDATE TableName SET Imagename = SELECT SUBSTRING(ColumnName,CHARINDEX('/tips/',ColumnName)+6,CHARINDEX('.jpg',ColumnName) - CHARINDEX('/tips/',ColumnName)-6) FROM TableName **N.B. - I don't have access to an MS SQL Server at the moment - so all of the above is untested &amp; from memory.** Also - you'll need to play around with the numbers (+/-6) &amp; the strings you're searching for ('/tips/' &amp; '.jpg'), but this should give you enough to work out the principle.
Thanks, this seems very close, it briefly shows the data then flashes to an error. Msg 537, Level 16, State 2, Line 1 Invalid length parameter passed to the LEFT or SUBSTRING function.
Am I missing something important... are there people who interview for jobs who *don't* know the difference?
Because everyone on this subreddit has interviewed for a database admin job before?
With the HAVING clause, you can specify a predicate to filter groups as opposed to filtering individual rows, which happens in the WHERE phase. Only groups for which the logical expression in the HAVING clause evaluates to TRUE are returned by the HAVING phase to the next logical query processing phase. Groups for which the logical expression evaluates to FALSE or UNKNOWN are filtered out. Because the HAVING clause is processed after the rows have been grouped, you can refer to aggregate functions in the logical expression. For example, in the querybelow, the HAVING clause has the logical expression COUNT(*) &gt; 1, meaning that the HAVING phase filters only groups (employee and order year) with more than one row. The following fragment shows the steps that have been processed so far. FROM Sales.Orders WHERE custid = 71 GROUP BY empid, YEAR(orderdate) HAVING COUNT(*) &gt; 1 
Do the % calculation when you are displaying the data as it make more sense for the UI side to figure out SELECT t.category_id, c.name, sum(t.value) FROM TRANSACTIONS t INNER JOIN Category c on c._Id = t.category_id WHERE t.DATE &gt;= 1411169790910 AND t.DATE &lt;= 1411170064940 AND c.TYPE = 0 GROUP BY t.category_id, c.name 
That probably means that it's finding a row that doesn't have '/tips/' or '.jpg'
You can order a resultset randomly with `order by newid()`. Combine with `TOP 1` and you're set. select top 1 Part from dbo.phoneme order by newid();
That is really good and I will probably end up using it. Now for bonus points, I need it to come out the same every time if I provide a seed.
GROUP BY!!! Of course! Thanks r3pr0b8, you're awesome.
Then it's not really random, is it? 
You are right. Ideally I was looking to make the solution optionally random. Some executions would need to have the same results, others would not want any of it planned. 
You can use a CASE statement in the ORDER BY clause, and use a parameter @rand ORDER BY CASE @rand WHEN 0 THEN [predictable ordering] ELSE NEWID() END
Well, restore from backup is it. If you try to restore from a corrupted file, you will have all kinds of data issues later. What is someone says that they paid but you have no record of it? The answer falls on your shoulders, and it would take a while for you to search through the remains of the database until you find it (if it even existed in the first place) or not find it. So really, the best option is to restore from a backup and deal with the loss. Your business users should have been prepared for this type of scenario in your discussions with them "how much of a data loss is acceptable?" If no data loss was acceptable, then backups should have been performed, and a mirror or log shipping setup. If it was half a day, you should have log backups every four hours or so, and if it was a week, then you should have weekly full backups somewhere. The answer to this is always 'restore from backup' because you don't want to be stuck trying to piece together broken data. That said, a google search will turn up many results, some commercial, some not. It would be helpful to know what you tried. Here's one: "http://semnaitik.wordpress.com/2014/02/22/how-to-repair-corrupted-ms-sql-database-file/" that is using the built-in tools with sql server. Here you go with a LMGTFY link... http://lmgtfy.com/?q=corrupt+mdf+file 
Thank you for your help. Sadly, I am not able to get this to work. I had to make sure I did everything I could before pushing back on the crazy request. The database needs to be fixed. Way too many workarounds on the front end. Thank you, again. Have a wonderful week! :)
When I started with my current job, I had basic sql knowledge which is exactly what my boss was looking for (obviously if I came with more, then the better). I was not asked this question as my interview was more of a funtional test to see how I could reasonably solve issues. My boss' philosophy is that you can teach sql, but you can't significantly adjust someone's problem solving methods. I would likely have fumbled out an answer to this question similar to what /u/selectpanic described in that having works against aggregates whileas where is a condition on a specific field... TLDR: Yes, some people don't know. It all depends on what your potential employer is ultimately looking for and willing to accept.
I think my question was more the result of the "classic interview question" or "pre-screening question" comments. I'm curious what the average state of SQL knowledge is for interviewees if employers have to wonder whether or not they know what I think of as very basic question about the language. I'm not being sarcastically rhetorical, I genuinely don't know. I've thought about applying for other jobs but I've always waved away the thought under the guise of "ehh I only what I learned for my current job."
GROUP BY is your friend
I'd be interested to see the code you were writing for hours to try and solve this. Finding out where you were going wrong might be better than just knowing how to do it. edit: /u/r3pr0b8 and /u/ichp are both right btw.
Would it be ok if we spelled occurrences correctly? But really, as already mentioned, this is a simple GROUP BY task. You're going to select all the columns you need, SUM(occurences) and then group by any columns that aren't aggregates. 
Pretty cool. I used to do something similar, but I had the Pushover code wrapped in a .PHP script and the SQL job merely made a cURL call to the PHP code. 
HA! Yeah, PHP is a love/hate animal. I started using PHP in college back in the PHP 3.0 days, so my mental deficiency started a looooooong time ago. As with anything else, there are many tools you could leverage to tickle the Pushover API....there are published examples for Perl, PHP, JavaScript, for starters. Just to be clear, SQL is not interacting with PHP directly. The SQL job makes an xp_cmdshell call that runs this: curl.exe "http://my_admin_webserver/reports.php?report_id=1" The report.php script has a handful of canned messages that can be initiated by simply calling the URL. I even sexed it up a bit with an ad-hoc capability so one-off messages could be sent directly from URL without a need to modify the script. Low-tech FTW.
You're advocating denormalizing the data. You generally don't want to do that unless you've got a good reason (typically performance concerns under specific conditions). Keeping it normalized is more flexible and will perform better over a broader set of queries. If you denormalize your source data to fit a specific scenario, you might find yourself having to normalize it in the query to do something else, and that will often be slow and cumbersome.
haha! nice catch on the spelling, that would have driven me crazy later.
Thanks for the Feedback! I decided to go with the following: **1st Table** SELECT TOP 100 percent BigCircle ,COUNT(A.dbkey) AS Occurrences ,SUM(A.Severity) AS TotalSeverity FROM TableA A GROUP BY BigCircle ORDER BY BigCircle **2nd Table** SELECT TOP 100 percent BigCircle ,MiddleCircle ,SmallCircle ,COUNT(A.dbkey) AS Occurrences ,SUM(A.Severity) AS TotalSeverity FROM TableA A GROUP BY BigCircle, MiddleCircle, SmallCircle ORDER BY BigCircle, MiddleCircle, SmallCircle [u/OrrinPorterRockwell](http://www.reddit.com/user/OrrinPorterRockwell) said &gt; Any reason you can't combine the circles in some way, instead of having separate entities for each? This seems like a good case for a mod operator or something similar. Perhaps it's just me. I agree completely. I wanted to just do a: CONCAT(BigCircle, MiddleCircle, SmallCircle) AS [BullsID] but MS SQL 2008 doesnt seem to support it. (P.S. - With my dataset, concatenating those values results in a unique Identifier which can scale effectively.) Any idea how to pull something like that off in MS SQL 2008?
You need to fix whatever is wrong with it. That's the best answer you're going to get.
Try `tail -100 logfile.err`, make sure nothing sensitive is in there, and stick that up on pastebin.com. Without being able to see any error messages, there's no way to know what's going on.
 declare @d1 datetime, @d2 datetime -- where @d1 is the start date and @d2 is the end date select @d1 = '2014-07-01 09:07:09.540', @d2 = '2014-07-10 10:27:06.343' select @d1, @d2, datediff(dd, @d1, @d2) - (datediff(wk, @d1, @d2) * 2) - case when datepart(dw, @d1) = 1 then 1 else 0 end + case when datepart(dw, @d2) = 1 then 1 else 0 end as Business_Days_Difference --to give credit i found this originally on stack overflow, although i don't recall the exact thread. --made a quick mod, removed an item from the select statement that existed in a larger query. --second mod placing variables in right spot (again, part of an existing larger query
awesome thanks so much I will give this a try...however the date diff(dd,strt,end) and datediff(wk,strt,end) what do these functions do. i am assuming it gives you the difference in days. so what do the dd and wk mean? day difference and week difference? 
The first parameter of the Datediff function specifies the 'increment' that the date difference is given in: in this case, 'dd' specifies days, and 'wk' specifies weeks, so it looks as though the code gets the number of days between the 2 dates, and then subtracts (2* the number of weeks) between them ie it subtracts 2 days for every complete week, since every week has 2 weekend days. It then does some stuff in case either the start or end date is a sunday I think. 
datediff calculates the difference between two inputs (in the example you gave strt and end) based on the criteria entered into the query (see the 'arguments' section of this link: [http://msdn.microsoft.com/en-us/library/ms189794.aspx](http://msdn.microsoft.com/en-us/library/ms189794.aspx) In the example above dd is day and wk is week. is will calculate the difference in days and then subtract the number of weeks *2 to pull out weekends. The two case statements take into account if the week starts or ends on a weekend. 
Mhm there might be a "migrate to postgree" incoming too
Did you ever figure this out? Curious with what you came up with.
Does this solution account for holidays as well? 
Based on what is in that log I suggest you run some disk checking, as it's possible you have disk errors which are causing the corruption of the database. If the disk check out you may want to restore from your backup if you are unable to repair the database. Make sure the file /var/run/mysqld/mysqld.sock exists in your server. if not exist you can create it. this tuts will help you: http://www.filerepairforum.com/forum/microsoft/microsoft-aa/sql-server Or if you do not go, you can resort to extreme measures - install special software (I advise you to install only paid content, it will give a better chance of a successful recovery and a safer) to restore a damaged database: http://www.mysql.recoverytoolbox.com/ 
Erm, how about the performance critical aspect ?? Where filterest rows on the execution of the query itself (those that don't follow, read up on execution plans). The Having is a filter applied to the recordset AFTER all the joining and filtering is done. With any half way decent optimizer, the Where clause filters are a LOT better for performance than the having clause.
Are you sure that es.id = ca.studentnumber?
 or i get my information with EVERY current room assignment That sounds to me like a cartesian join, which happens when you retrieve data from two tables and don't join them on the appropriate primary keys. You need to inspect each table individually and understand the relationship between the data, when multiple rows can be returned for a primary key etc, and then you can use aggregate functions if needed to reduce them down to a single row.
This. Keep in mind if one is a CHAR column and one is a VARCHAR2 there could be padding on one of the tables.
I'm guessing he's just removing the WHERE clause when that's happening. But that's just a random guess. OP - could you provide the query that is returning everything if the one you posted is returning nothing?
solved problem ....realised oracle SQL doesn't use LEFT/RIGHT and uses Substr. played around with that and got it working :)
Great, because I don't think anyone would try to understand your query with such bad formatting.
If you run the stored procedure `sp_who`, it will return a list of all connections to the instance. The `hostname` field will tell you where those connections originate.
Thank you! That did the trick. I really wish I had more time to study SQL...
What's your database and version? Is the table heavily indexed? Are you doing the inserts individually OR as part of a single transaction?
I'd try the following on the staging environment, take with a grain of salt/caution as always though: * Drop all non-clustered indexes during the INSERT and reapply them afterwards * Turn off auto statistics during the INSERT and enable/update them after the insert has completed. * Verify that your SAN / Network IO isn't bottle-necking your SQL Server during the INSERT * Check to see how large your rows are, 90 columns is much to large for any useful purpose; I don't think even SharePoint's poorly designed database has that many. I'd hamper a guess that you are writing many pages per row insert which can be very hard on the IO. * Check to see that the staging table doesn't have a primary key, it should however have a clustered index(preferably the PK from your production database). * Would it be faster just to backup and restore the database to your staging environment, I've encountered this multiple times of people attempting to do the INSERTs across the systems when an automated backup and restore ran orders of magnitude faster. 
This works fine for me, though you don't need your where condition as it is contained within the join criteria. See below for the test. If I had to guess, I would guess that the Account field isn't an actually an int field and probably contains some white spaces. try either * casting/converting the field to an int in your join criteria cast(TableOld.Account as int) = cast(TableNew.Account as int) * changing the datatype within your existing tables * LTRIM() and RTRIM() the fields The above depends on if the data in that column is purely numerical or not. Here's my test case that works fine... create table #TableOld ( Account int, Name varchar(50), DOB date ) create table #TableNew ( Account int, name varchar(50) ) insert into #TableOld (Account, Name, DOB) values (123,'Smith','4/5/1977') , (456,'Herp','9/18/1943') , (879,'Derp','11/21/1999') insert into #TableNew (Account, Name) values (123,'Smith') , (456,'Herp') , (879,'Derp') select dob from #TableOld a inner join #TableNew b on a.Account = b.Account
Are you using Microsoft SQL Server? You might need "administer bulk operations" permissions. 
Correction: I did change something else, which probably made a difference too: I had originally imported (from a CSV file) TableOld assigning variables with type "*string*", and the last time I used *varchar*(35). That n00bness.
This is a complete shot in the dark, but that likely means that the Account field in one of the tables is a varchar field and in the other table it is an int? Implicit casts could force your join criteria to match, while the implicit cast on the where condition was still resulting in a mismatch. Glad it is up and working for you though! 
Where is the xampp test environment running?
This just hit me like a ton of bricks, but since I am going to the 2 drives for data and logs, do I even need mount points anymore? I should just be able to use the K: and L: drives as described above, correct? Feeling really dumb now...lol
It is running on my workstation.
A lot depends on who the intended users are and what type of database server you are using. If you are thinking of something for general users and its MS SQL Server, I'd look into ASP.NET MVC and build it as an intranet app or use C# to build a desktop app. If the users are more technical (DBA, IT, Sys Admin), then I'd consider either using parameterized sprocs within SSMS or write a Powershell script to run the queries. You could also use something like SSIS and timer jobs. I'd encourage thinking through the purpose and audience before committing to a console app as the solution. It's possible that's the best solution, but it's not the most likely. 
Users dont get a session ID? 
Long story, bro. Short answer: No. Long answer: The reason my blood pressure is so high. There is a value for the time that the visit ended.
Is the service running locally on the DB sever? Are you sure the database/catalog name is correct? Is there AV software on either sever that has a firewall component? Have you procmonned the service? Have you tcpviewed the service?
Is it an interactive console application you want? If so, you could use a curses library and make it pretty fancy. C/C++ would be good for that. That actually sounds kind of fun to make. 
UAC?
Try listening to a lecture on how a derived object is actually implemented in memory in Microsoft C++. I went to that session at DevNet one time and I have never been able to look a compiler in the eye since.
I am a developer who has similar type of service developed and deployed several time. Most of the time is firewall or anti-virus (two different things. If you are working with Norton .. simply turning off won't work. I had to properly remove(uninstall) that software from the server on ONE occasion.) 1. Are you connecting the server from the server ? I mean both your SQL Server and custom service are located on the same server ? Then check your UAC or custom AV/firewall for permission relating to this service. 2. Do you have enough space for your temp database ? Have you check the free space of the drive where your database is located ? If you have enough (extra) space, check your database whether they can grow themselves or not. 3. And this SSMS &gt; goto your database &gt; Database Properties &gt; Options &gt; Compatibility Level &gt; SQL Server 2008 (100)
I got around the double quotes by encapsulating my script work as either DOS batch files, .CMD batch or .VBS scripts. The effect is still the same, but the interface between SQL and shell is much more elegant. This might also be an excellent time to plug the wonders of Python. *nudge* *nudge* 
On the computer with the error message, start/run/cliconfg (not a spelling mistake). Enable named pipes and TCP IP on the local machine as well. I'm mobile ATM, pretty sure you can also setup aliases that'll help point it in the right direction.
It's a network issue. The error message is pretty clear about that. The client can't find the server. It sounds like you've done [all this](http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx). Make sure the server is bound to the correct IP addresses. It's possible that your install script decided to bind to the wrong network. The next thing I would do is make sure the service is reachable on the port. Verify that Windows sees the port as open with `netstat -an | findstr 1433` or `netstat -abon`. Now stop the SQL Server and verify that the port is no longer open. Start the server again and try to telnet to port 1433 (note that the telnet client is no longer installed by default). "Connection refused" is the response you get if you're firewalled. If all that works, fire up PowerShell and use that to connect. You can try using the SQL Server module if it's installed or use the System.Data.SqlClient.SqlConnection objects and methods. Remember if you use a named instance (you can verify that with the name of the SQL Server service in Service Manager) that you need to specify the instance name when connecting. Make sure there is no existing instance that you might be connecting to. If none of that works, remove the instance and try again. My next guess would be a reconfiguration of the server's service account security, but reinstalling the server software is pretty easy.
I'm guessing you're using Windows/SQL Server from your reference to an .exe. I'd recommend C# using Visual Studio. If you don't have it already, there is a free Express edition. I use EPPlus (https://epplus.codeplex.com/) for making Excel spreadsheets from within C# apps. Not sure about using a console app that takes user input though; I'd use a console app more for something you want to schedule and which doesn't take input. If you want to prompt the user I'd go with a WinForms app.
Think an entry in the hosts file of the other computer should be enough to reach it. You'll have to write a redirection to the workstation like "192.168.0.X localhost" (with the first IP being your workstation) Think that's all you'll need (the hosts-file is in windows/system32/drivers/etc/ or on Unix in the /etc/ folder) Edit: considering the IP string in the c# db connector is localhost
Why would you create while default console application is available ? Why you don't want to use SQLCMD ?
First of all I'd suggest you rewrite the ()&gt;0 AND ()&lt;73 to BETWEEN 0 AND 72 just for better readability. This goes for both CASEs. Did you know you can also write this: CASE tableA.[IndividualValue]/TableB.[RegionalBaseline] WHEN BETWEEN 0 AND 72 THEN 1 WHEN BETWEEN 73 AND 82 THEN 2 .... ELSE 5 END For even better readability. I don't think there's a big difference in performance however in both CASES, especially after you rewrite them.
Is this SSMS on a local computer or on the server? Did you upgrade SSMS or reïnstall? I'd start with a fresh install of Management Tools. I run Microsoft SQL Server 2012 (SP1) with SSMS 11.0.3128.0 (X64) and have no issues whatsoever. Both are fresh installs though.
And why is Grade not a numerical column?
Service is running on the server, yes. No AV software yet installed, it's a newly spun up VM. Haven't procmonned or tcpviewed the service, good suggestion.
Off
Very good suggestions, I will do these today... I did mean to telnet to it as a test but got a phone call and forgot to go grab a telnet client. Was too frustrated to go back to it late last night and get it.
Thanks, will give that a look
1. Yes, the service is running on the SQL server. No UAC nor AV running. Firewall services have been deactivated. 2. 1 TB available. DB's are set to autogrow without restriction at the moment. 3. Haven't checked compatibility level, but will do. Thanks
i'm sort of guessing, but i believe the database engine decides that the result of the CASE should be a DATETIME datatype, based on what you want produced in the THEN, so then when you get to the ELSE, that's not a DATETIME datatype use CAST or CONVERT in the THEN and see if that works
the diagram for FULL OUTER JOIN is erroneous -- there will never be a row with NULL NULL produced, and none of the tables have a NULL NULL row that takes part in the join in fact, now that i look more closely, **none** of the examples actually show the rows that are produced i'd have to grade this article a big fat D
I really don't like the descriptions of the "OUTER EXCLUDING JOIN". SELECT * FROM tableA LEFT JOIN tableB ON tableA.name = tableB.name WHERE tableB.name IS NULL The statement "LEFT JOIN" make it seem like it is some kind of special join. But it's the same as the "LEFT OUTER JOIN". The "excluding" part only comes from the WHERE clause "IS NULL". However, this is mentioned nowhere.
The reason is that your else needs to be cast as a character to use the string operator '+'. r3pr0b8's idea of using case should be done inside the case for both results, not just the asterisk one if you are going to take that route. Personally I'd create another bit field so that the result could be easily filtered or pulled back as a DATETIME type without a bunch of dickery to remove the asterisk again. SELECT Field1, Field2, IFF(Updated_Last1 &gt; Updated_Last2, Updated_Last1, Update_Last2) AS Updated_Last IFF(Updated_Last1 &gt; Updated_Last2, 0, 1) AS Source_Used *Edit Note - IFF is an SQL Server 2012 onward feature, otherwise use case.*
It's not the engine determining it for the CASE, it's that he's using a string '+' operator on a datetime type in the else. 
Thanks for the reply - I tried using both cast and convert and it didn't seem to work. The query runs fine when I remove the + '*', but it won't seem to work when I add it in. So here was the code I tried using with the ELSE Line: ELSE CONVERT(VARCHAR(MAX), Updated_Last2, 120) + '*' And then received this error again: Conversion failed when converting date and/or time from character string.
I'd say using the concatentation operator '||' instead of '+' should fix your issue.
Yup, you're correct, I'm not using good logic to want to include an asterisks to note the source in a datetime field, I should absolutely just have used an additional field to note source. lol dickery... Thanks!
Glad to be of help.
I see, thanks for the info. :) In Oracle, for example, || would have been the right choice.
Does that work for Transact SQL? It's pulling up an error when I use it.
As far as I can see you want to do the following: SELECT ... FROM ( SELECT ... FROM ... WHERE ... UNION ALL SELECT ... FROM ( SELECT ... FROM ... WHERE ... ) ) GROUP BY ...; If I'm right, you are missing the last ")". So directly before the GROUP BY put "))" instead of ")".
Can you explain this a bit further? I thought [Grade] was a numerical column, since the set of possible values it can take is {1,2,3,4,5} Do I need something inside a CAST AS functon to make it a numerical column? (I'm really new to SQL). Oh, and [Grade] can be can have several digits after a decimal - I used the &lt; because want to make sure that 72.99999 is still grouped with 72, 71, 70, 69, etc. Thanks!
When I do that I get this: ORA-00904: "SM"."CT_PREMIUM_HOURS": invalid identifier 00904. 00000 - "%s: invalid identifier" *Cause: *Action: Error at Line: 85 Column: 11 but as far as I can tell it's not invalid.
I'd be interested in finding something also.
Lovely, thank you!!!!! 
Me too. A class on coursera or edx (or whatever) would be nice.
Have a look at the execution plan and the wait stats. 2000 rows in 40 minutes is bad beyond an easy fix and suggests to me that you are doing something very wrong there. I don't even see IO subsystem as a possible bottleneck, 40 minutes is so bad I wouldnt be able to create such a shitty io subsystem if I tried. I'd have a look at the execution plan and then fix the query.
It looks like it's only listening on the loopback address then. The only valid address it's bound to is 127.0.0.1. 
This doesn't have to be a mystery novel. Just search "beginner|intermediate sql courses|exams|tests|certification". [Here's 873,000 results](https://www.google.co.uk/search?q=sql+courses&amp;oq=sql+courses&amp;aqs=chrome..69i57j0l5.1355j0j7&amp;sourceid=chrome&amp;es_sm=0&amp;ie=UTF-8#safe=off&amp;q=beginner%7Cintermediate+sql+courses%7Cexams%7Ctests%7Ccertification). First page: https://www.udemy.com/practical-sql-skills-from-beginner-to-intermediate/
I don't know a single thing about hierarchyid but the +1 looks misplaced. Try SELECT max(ArchVal)+1 FROM table
I doubt there is going to be one that will garner any credibility for free. A personal project that you could show/explain might be a good idea
Yep, typical. Glad you got it sorted, though.
Yup, me too.
I haven't worked with MS SQL in a while, so you'll need to verify that the OR clause matches the first operand in the order supplied. SELECT TOP 1 * FROM courses c JOIN course_details cd ON (c.id = cd.course_id) WHERE course_id = &lt;variable&gt; AND (cd.locale = &lt;variable&gt; OR cd.locale = 'EN')
For example, you can join twice to the details table Select course.id, isnull( localedetails.Id, defaultdetails.id) From course Join coursedetails defaultdetails on Defaultdetails.courseid = course.id and Default details.locale= 'end' Left join coursedetails localedetails on Locale details. Courseid = course.id and Localedetails.locale = @mylocale 
I don't care much about the actual certification. Regardless, I actually began that class. But there was too little SQL for my taste. And I'm also after something a bit more intermediate. Thanks for sharing though.
Bulk inserts aren't what nosql db like Mongo are really for. The real comparison is inserting thousands or millions of records individually in parallel 
Intermediate as in...the SQL in the class isn't advanced enough? I remember thinking some of those homework/quiz SQL assignments were fairly challenging
Perhaps I need to reconsider. I didn't get very far. I've worked with a wide array of XML languages and felt that I wanted a class about SQL only.
use CAST or CONVERT in the THEN and see if that works you just did it in the ELSE, not the THEN
Because you use apostrophes around it, sql will cast both sides as an nvarchar (not sure which type), thus increasing load. But in newer versions of SQL the engine recognizes this and uses ints but still, its a good practice not to. Just use Grade=1 
The art of sql is a good book for this. It's not perfect but it has a lot of good design patterns. 
First check that last_name is unique. Its very unlikely to be, in which case all you're going to get are headaches. SELECT LAST_NAME, count(*) FROM whd.CLIENT GROUP BY LAST_NAME HAVING count(*) &gt; 1; If you want to avoid using last_name and just use old_email, then paste your table schema, its much eaiser than trying to explain everything in english... SHOW CREATE TABLE wmd.CLIENT; SHOW CREATE TABLE wmd.JOB_TICKET;
You know what, this may actually work. The new accounts are bringing in identical user names with the new e-mail address (which are unique to the user), so I may be able to switch LAST_NAME to USER_NAME and get away with this.
 USE [Database] GO CREATE TABLE [dbo].[Grade]( [key] [int] IDENTITY(1,1) NOT NULL, [Letter] [varchar](2) NULL, [Number] [numeric](18, 0) NULL, [CentLow] [numeric](18, 0) NULL, [CentHigh] [numeric](18, 0) NULL, CONSTRAINT [PK_Grade] PRIMARY KEY CLUSTERED ( [key] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] INSERT INTO dbo.Grade (Letter, Number, CentLow, CentHigh) VALUES ('A','4',93,100) INSERT INTO dbo.Grade (Letter, Number, CentLow, CentHigh) VALUES ('B','3',83,92) INSERT INTO dbo.Grade (Letter, Number, CentLow, CentHigh) VALUES ('C','2',73,82) INSERT INTO dbo.Grade (Letter, Number, CentLow, CentHigh) VALUES ('D','1',0,72) Select tablea.assisgnment ,g.Number ,ISNULL(g.Letter,'Unknown/F') as Letter FROM tablea INNER JOIN tableb on tablea.somevalue = tableb.somevalue LEFT JOIN Grade g on TableA.[IndividualValue]/TableB[RegionalBaseline] between g.centlow and g.centhigh 
You are going to want to reformat it into a standard you and your coworkers are familiar with. I'm still a new BI developer but haven't run into a situation where a right join is necessary. Most likely (as you stated) the problem is a logic error. Keep in mind that your question could be flipped to say "are left joins necessary" and I would provide the same answer. 
&gt; Keep in mind that your question could be flipped to say "are left joins necessary" and I would provide the same answer. Ha, right, of course. It's actually pretty interesting; he's mixing right and left joins all over the place and I'm not sure why. It's almost like he took the statement I wrote and rather than reorder it in a way that made sense, started with the minutiae and worked back toward the starting point. 
The worst part of SQL is the database-specific aspect of it. But in general, I think it is fairly decent as a mostly cross-platform language for controlling data in a database. 
Are you able to post the SQL statement. Maybe yours and his. I'm interested in seeing what your talking about and learning from it.
SQL Dev here. 10 years experience in the BI stack. I don't ever use Right Joins. Inner, Left, and FULL OUTER sure, but not Right.
I mean database-specific syntax, as opposed to standardized syntax. 
Thanks -- I was wondering if I was ~~crazy~~. not crazy, just missing something since I'm not that experienced.
As long as you were to always do it the same way right or left is no real distinction. I have seen some platforms not enjoy right joins so I always stick left, if you do right it's fine though.
The only time I've seen a use for them is in complicated queries with DB engines that don't support CTEs. And even then, it's usually not hard to rework it into a LEFT.
&gt; What does that even mean? The "database-specific aspect"? Where to even begin?! * handling nulls with coalesce() vs nvl() vs ... * creating/using sequences * returning a finite number of rows Those are the first three that come to mind presently. There are many more. Pretty much as long as all you want to do is select, insert, or update, the syntax of the different RDBMSes are fairly similar, but many of the finer points like those listed above are accomplished quite differently...&amp; that has much to do with SQL getting a bad rap.
I query in MS ACCESS DB through... MS Access... I'm not sure what is preventing that from happening.
I wouldn't say that's the worst part of it. It's kinda the ONLY part of it. :) 
Well at least I understand he was saying now. I hardly think that's a huge problem for a competent developer though.
[There are only two kinds of languages: the ones people complain about and the ones nobody uses.](https://www.goodreads.com/quotes/226225-there-are-only-two-kinds-of-languages-the-ones-people)
Because they don't understand it, and they've only dealt with really poorly-designed databases.
You may have more luck in /r/SQLServer or /r/BusinessIntelligence. However, try looking at the grouping. There should be a keep group together property. I'm not sure if I understand your problem, but you may want to mess around with a group footer or a row inside the group to find a solution to the problem.
My theory is that it's because database interactions are subtly different than nearly all other forms of programing. Most programing is *algebraic-based* - that is, you do one thing at a time and iterate through problems until you get to a solution, databases are *set-based* - that is, you are more concerned with groups of things. Programmers who think in terms of algebraic solutions often try to make SQL behave the same way, but it is and should be very clumsy at it. SQL is designed around sets, and it has only a few obscure features that allow you to do things in an algebraic way - e.g. cursors. 80% of the time, cursors fail every time. 
There is a standard sql ... ansi sql. Oracle and MS add their own twist on top of ansi sql. If you know ansi sql you should be good to go, with the basics, on most dbs.
&gt; he's mixing right and left joins all over the place and I'm not sure why Did he keep your table references in roughly the order that you started with? That would be the only important reason to mix that stuff together - it would let you diff your queries and see exactly what had changed most effectively.
Ha, at least you're honest!
&gt;What does that even mean? The "database-specific aspect"? SQL has been around for so long that many 'standards' or 'flavors' of it have been developed each with slightly modified syntax. Microsoft and Sybase have [T-SQL](http://en.wikipedia.org/wiki/Transact-SQL), Oracle has [Oracle Database](http://en.wikipedia.org/wiki/Oracle_Database), and then you have [ANSI and ISO](http://en.wikipedia.org/wiki/SQL#Standardization) standards of SQL. With T-SQL, MS added various functions to add additional features but that makes it essentially incompatible with ANSI/ISO standard SQL or Oracle DBs. I've only ever worked with T-SQL so I don't have any complaints but if I had to rewrite thousands of lines of T-SQL code to make it work with an Oracle DB I can imagine being a little agro.
I think this is very true. If you can't think in sets, if you only know how to iterate through data, you're going to have a bad time in SQL. I was once asked to figure out why a certain sp was bringing the system to it's knees for 3 hours every night. The query used 3 cursors (one nested), to basically join 3 tables. 
The man speaks truth. 10+ years in multiple RDBMS, once in a LONG while I'll encounter it in someone else's code, but if I am doing any real work to the other developers code, I will refactor it as Left, Inner, or Full. 
Can you expand more upon the 'set-based' theory and what you mean by that?
That's just not true. The first things you have to do (create a database, create a table, possibly import data from an external source) are all extremely vendor specific (IMX, the `CREATE TABLE` syntax always has some vendor specific extension). And that's completely discounting the installation process.
People who *hate* SQL are the people who cannot warp their mind around SQL language or RDBMS in general. It is normal for people to despite the thing they don't fully understand and had to use it anyway.
This has been my experience as well - at least for analyzing or troubleshooting data. If I were to find that I really did need a right, it would only be temporary and I'd convert the query to a LEFT to keep it consistent with the rest of my scripting.
A set is just that. A set. You don't buy one plate, you buy a set. So you don't manufacture one plate at a time from start to finish, mold it, bake it, and paint it. You make dozens at a time, each molded the same way, each painted the same. So set based statements or equations says "find all records with this value" and "match against all records with this value" and "update all of the results with this value". You don't look for one at a time, and you don't apply changes to one at a time; your code should quickly scan everything, and make big universal changes, quickly and efficiently. 
I got so much shit and so many downvotes when I first started using reddit, because I said RIGHT JOINs had a decent purpose to them and made an example using one vs LEFT. I don't use them very much but they're useful to know how to use.
Theres nothing wrong with them. They are in the standard for a reason. I imagine they are easier to follow logically for people fluent in a right to left language. However, I do understand from a code comprehension point of view - left joins are just easier to follow with a simple reading of the code, at least as a left-to-right language individual.
It's pretty essential. If you're writing SQL and you're thinking in terms of loops and iteration and imperative/procedural programming, 99% of the time you have a problem and should find a different approach. You need to think in terms of sets, and operations that filter or combine sets. It's not too far from the kind of thinking associated with functional programming to some degree really. (which is why it's not completely terrible that the C# designers choose to rename map, filter and reduce to Select, Where and Aggregate.)
Not sure why the 'expert' did that, except maybe to get the derived (subquery) table to be the first table in the list. However the way you did it (with all lefts) is perfectly acceptable and is probably how I'd do it. 
ho-kay. is anybody reading this thread? maybe I should have called it "how to change a hierarchyid value". Anyway, I figured out how to do this. Instructions are below. You need to know two things. The current hierarchyid value, and the one you want to change it to. But in order to know what you want to change it to, you need to know the *parent* that you want to change it to, and also the next hierarchyid value underneath that new parent. So you need to know four things. - current hierarchyid - new parent hierarchyid - max hierarchyid underneath that new parent - next available hierarchyid underneath that new parent. So you begin by declaring your four variables. DECLARE @ParentNode hierarchyid, @MaxNode hierarchyid, @NewNode hierarchyid, @OldNode hierarchyid Next you get the current hierarchyid. This is easy. Set @OldNode = ( SELECT HierarchyIDValue FROM ColorTable WHERE Color = 'Orange' ) Now you need to know the parent where you're going to move the hierarchyid. In this example, I'm moving it to the root, so I'm calling it "root". SELECT @ParentNode = HierarchyIDValue FROM ColorTable WHERE Color = 'root' If your table doesn't have a meaningful value for your hierarchyid, you can also hard-code your parent by using the PARSE command. SELECT @ParentNode = hierarchyid::Parse('/') OK. So what's the "biggest" or "most recent" or "last in line" hierarchyid value underneath this destination parent node? The function is "GetAncestor". You can put any value after the function's parentheses. For example, you could put (*) in there and get every hierarchyid value. But in this example, we want the most recent one, the last one in line - so we can identify the next one in line. SELECT @MaxNode = MAX(HierarchyIDValue) FROM ColorTable WHERE DomainHierarchy.GetAncestor(1) = @ParentNode Now. The part that prompted this thread. This was so simple, once I found the function. How do I get the next one in line? The function is called "GetDescendant". All you have to do is pass the last child node assigned to the parent, and it provides the next. SELECT @NewNode = @ParentNode.GetDescendant(@MaxNode,NULL) OK. So now we have the two values we need. The current hierarchyid value, and the new one we want to assign to it. So all that's left, is to UPDATE the table and assign the new hierarchyid value to the record you want to edit. The function is called "GetReparentedValue" and all you need to do is pass the old node, then the new node. UPDATE ColorTable SET HierarchyIDValue= HierarchyIDValue.GetReparentedValue(@OldNode,@NewNode) WHERE Color = 'Orange' That's it. That's all there is to it. The funny thing is the Sr SQL Developer at my company didn't know how to do this. I think he inherited the hierarchyid model from someone else. He was recommending that I just drop the record and re-create it. But I work in the merchandising unit for a retailer, and doing this would have meant re-pricing a lot of data, possibly even re-building customer permissions/relationships with the "new" category (which isn't actually new, but was just categorized in the wrong place). So figuring this out saved us having to make tons of update statements and making embarrassing calls to our clients and partners. And it runs in just a few milliseconds. So I'm pretty stoked.
Laziness. Every dev I have worked with who avoids SQL invariably access relational databases in loops or iterators, and eventually have serious performance problems. Others attempt the same patterns in stored procedures, then blame SQL for the performance. As mentioned in other responses, SQL requires a different approach to working with data - sets vs one...element...at...a...time (I have seen loops used to iterate over columns too), and transitioning is a learning effort which some won't invest in. I have seen competent developers go to extremes to avoid working with data in the most efficient way for the platform, reinventing database functionality to cram it into an OO paradigm. A one table per class approach is one sign, denormalizing data not for performance but convenience is another. 
The best way is visually: http://www.khankennels.com/blog/index.php/archives/2007/04/20/getting-joins/ http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/ Basically, in database world we do not, in our heads, say 'I need to write an algorithm to pick up one record, do something to it according to some algorithm, then move on to the next record and apply the same algorithm.' What database tools and code are all about is 'I want to manipulate *all* records that have these given characteristics. I don't care how many they are, or if they're in any particular order (well, you do, but not when you're writing code)'. Beginners generally do one thing at a time in a series of statements. More intermediate database people can write the code fancy enough that they can be covering several different cases in their algorithm depending on the data. Master database people go back to doing one thing at a time because they've thought through their future needs and designed the schema to facilitate simple queries doing complex things. Regardless of what's actually occuring at the bare metal of the machine, the way to *think* is to assume that when you do an operation that it basically 'all happens at once'. TL;DR, the ideal way of approaching database programming is to think in terms of applying *one* algorithm, *once*, to *all* of the records. 
You should probably retitle this "What are Ten Things a Junior DBA should know *about*?" since you didn't really explain anything.
You mean vendor/implementation-specific.
Ughh fucking COBOL
Ehmm what. Limit the memory, as you always should.
If you have both configured such that they can run simultaneously, they'll probably be short on memory (small caches). If you have them both configured optimally for individual usage, you will likely not have enough system memory to accommodate them running simultaneously.
First off, that's an entirely arbetary statement, seeing as we know neither of the setup specifications, nor the expected workload. But based off the fact it is somewhat of a newcomer (OP) I'd assume we aren't talking GB tables. SQL will per default save as much to memory as possible; meaning if your data size is larger than your memory pool; you'd see swaps. Based on an express edition; with adventureworks; you can store the entire thing + sql in memory with just around 1 GB RAM.
What I meant is if you know the basics like... * select * from &lt;table&gt; * update &lt;table&gt; set &lt;something&gt;=&lt;something else&gt; where &lt;only these records&gt; * insert into &lt;table&gt; (&lt;columns&gt;) values (&lt;values&gt;) which all work on mssql, oracle and mysql... you should be able to do work.
This is perfect, thanks!
I've never really thought about how the syntax/functions/etc. *should* be, I was just worried about learning how to query and manipulated data. Syntax is just syntax, same learning curve would be there for any changes. I disagree about NULLs though, I think the major issue with NULL values (at least what I've seen personally and what every book or article on the subject) is that people don't understand how they are evaluated, and assume that they are a stand in for an empty string or 0 or something to that effect.
Going off your "poorly-designed database" comment, can you recommend a resource that explains what a well designed database is supposed to be. I'm only just now starting to work with databases so any pointers would be appreciated.
 select top 1 place_service_associated from whatever_the_table_is order by encounter_date desc Better formatting would bring you better answers.
&gt; Am I correct in the above, that your RDBMS is what you actually open as an application and put all these SQL queries into? When you see tutorials giving you syntax etc, is this where that's entered? When it comes to Access, I'd say yes. However, in the general case the RDMBS is just the part that is installed on a server somewhere. This part will actually run your queries. To access that server, you'll need a client software that allows you to connect to the server, send the query over and fetch the result. Access can be used in both ways (AFAIK): as a RDBMS running on your local machine storing data in local files -or- as a client software connecting to some remote RDBMS such as SQL Server. &gt; How do things like MySQL, Oracle, MS SQL etc fit into this? I've a feeling what I've described is more like Access, which seems to be just a more basic version, but honestly I just don't know. These are all RDBM systems that do the server side part (obviously they also bring some client software because otherwise you could not access them). Basically they are just different flavors each of them having different strength and weaknesses. Describing Access as a "basic" RDBMS might be appropriate, but read on. &gt; Is there a platform or whatever I can download and run on my laptop to mess around with and put this stuff into practice? One of the tutorials linked to some exercises, which asked me to download a file with extension .sql, but honestly I have no idea how to open or use it. Indeed, I'd strongly suggest against using Access to learn SQL. Take PostgreSQL instead. It's free and very powerful. You won't hit it's limits after three days (which is pretty likely to happen on Access ;) You can download it from there: http://www.postgresql.org/download/ The Windows and OS X packages also contain a nice client which you can use to connect to your new PostgreSQL database and run the SQL from. You might need a few hours to get started with this path, but I think it's worth the initial effort. 
Fantastic post, thank you I really appreciate it! Really, you've cleared up all my questions and confusion and it's much clearer now. Downloading postgresql now.
Than we should also mention DB2 Express-C which is also free and powerful :)
The title of the post threw me off. I'd never met anyone who hates SQL in my work experience. But I have met people that aren't very good at using it and prefer to have other people handle queries for them. I can't even think of a better, more efficient way to store and retrieve large volumes of data than in an SQL database. Implementations may vary and I've seen some mess databases but it's ignorant to hate a platform for the way some people use it. My best guess as to why someone would hate it would be a software developer, not a database administrator that either 1. has to use it in a limited fashion but isn't fully education on it so finds it frustrating not knowing the most efficient syntax or not grasping the proper use of joins and look-up tables. 2. Someone who doesn't understand it at all and simply butts heads with their database admin over design decisions. I'd be curios what these people would say if asked directly "why do you think SQL is crap, and what would you use in it's place?". I'd imagine the people that "hate" it are much akin to a child that *hates* going to school, not fully understanding the available options and long term ramifications of their misinformed preferences.
You can download any of the following for free: **MS SQL:** http://www.microsoft.com/en-us/download/details.aspx?id=29062 **My SQL:** http://dev.mysql.com/downloads/mysql/ **Postgre:** http://www.postgresql.org/download/ But, keep in mind while the overall structure of queries are similar every version of SQL can vary slightly. For example in MSSQL you use GETDATE() to get the current date and time while in MySQL it's simply NOW(). Or in MSSQL you'll use "SELECT TOP 100" while in MySQL you'll end your query with "LIMIT 100". If you want to be strong with the platform as a whole you'd do well do be familiar with these differences so you can adapt. But, if you're just starting to learn SQL you'll probably want to pick just one for now and get really good at it. As to which one to pick, this depends on what platform you're developing on. I personally deal more with Microsoft world so I use a whole lot of MS SQL and it's got a solid easy to install, use and learn express server you can grab (linked above). However MySQL has HUGE traction in the community and is the overwhelming preference of most web hosting options. I'm not as familiar with Postgre so I can't speak to it too much. But if you're using Windows, my personal recommendation would be to grab the SQL Server 2012 Express. It's a single download, installs easy, walks you through setting up a database and has a straight forward query editor. You'll need to get some of the fundamentals down early on. Understanding that a server has databases, databases have tables, tables have fields fields has specific data types, etc. Once you get over the initial hump of understanding how everything hooks together then learning basic select syntax should go pretty fast. There's a lot to know but the basics are quick to learn.
I was in the same boat until I, by happenstance, ended up working on a new project at my org that was on the Ruby/Rails stack. It ended up totally blowing my mind and being totally fun. I would recommend making that jump to anyone. There's so much to learn out there (object oriented, functional programming, etc.) that you don't get to think about in just SQL, and I always found that having a really solid understanding of the DB is really helpful. As far as how to make that jump, it happened randomly for me, but maybe you could look at a bootcamp or something?
How good are you with stored procs? What about optimizations (can you use trace/profiling, stuff like why/when would I want columnstore or a bitmap index)? Can you model/navigate a complex structure a-la BoM (bill of materials) or healthcare providers network? Can you model/navigate a time-dependent model (Inmon DWH) and set up data delivery to a data mart? Can you pivot/rank data in your sql without a long session of trial and error? If I throw an industry codebook at you, can you set up a Type 2 dimension for transactions coded with this codeset? Send me a PM if you answered "yes" to all (or most) of these :)
First, context for my answers: I am a "Data Systems Analyst" at a large regional health insurance company in the northwest. My general duties involve both ad-hoc and scheduled reporting, SSRS reports, and problem and data analysis (figuring out why things went wrong, where they went wrong, and what needs to be fixed). I deal quite a bit with things from all facets of the business, from customer service reporting, to claims billing (ASO business) to eligibility and enrollment information. &gt;How good are you with stored procs? This is probably one area already on my radar for learning about - it requires getting a lab set up at home though, since I don't have access to perform that kind of operations on our databases at work &gt; What about optimizations (can you use trace/profiling, stuff like why/when would I want columnstore or a bitmap index)? I know some of those words :P - When I think optimization, I think tweaking the SQL to lower the cost of the query plan. Database design is more of a "beginning" thing for me, there again, I really need to set up a home lab, the problem being finding a "project" that makes sense to play with. &gt;Can you model/navigate a complex structure a-la BoM (bill of materials) or healthcare providers network? Can you model/navigate a time-dependent model (Inmon DWH) and set up data delivery to a data mart? My primary job actually involves working directly with the databases that support our healthcare plans systems for reporting purposes. Since the documentation available is absolutely atrocious, I have spent the last several years basically reverse engineering table structures to determine how to get at the necessary data. Part of that has involved some basic ETL work, however since my job doesn't call for it specifically, I haven't had hands on with a majority of our enterprise ETL work. I can say with confidence however, that our datamart is a complete mess, and even I with limited experience could do better with a few weekends and a Talend server. &gt;Can you pivot/rank data in your sql without a long session of trial and error? Not 100% sure if I am interpreting your meaning right - but when you say pivot/rank, I think window/analytical functions. One of the things I am often tasked with doing is taking a dataset that someone is current manipulating in excel/pivoting/etc, and boiling that down to a concise data output that has been pivoted, aggregated, or whatever the need calls for. &gt;If I throw an industry codebook at you, can you set up a Type 2 dimension for transactions coded with this codeset? If I understand your question correctly, you are discussing the idea of essentially setting up a data dimension for a data warehouse that never deletes or overwrites existing data - in essence you keep an audit history or historical record of all entries for all time, and any changes to existing entries would incur a new row with its own unique key. I have never had opportunity to do this, however I understand the concept quite readily, having worked with such data sources in the past, and being frustrated by their poor design (such as lack of linking record id's that tie the new row to the original row to indicate a state change or indicate that the new row has any relation to the old row (example - in my mind, any such dimension should always include a column or data point pointing to the row key of the row it is replacing). &gt;Send me a PM if you answered "yes" to all (or most) of these :) Not sure I qualify here :)
Its quite interesting really, in that your situation quite describes how I originally picked up SQL type work. I got tired of trying to get information from others, and being told that certain reports weren't possible, and so figured out how to do it myself. that was... 7 years ago now? cant recall exactly.
Step up a level from just writing queries to being DBA?
Have you looked at Data warehouse? Trying to actually understand the needs of queries and what they can tell you if you have good data? Using it in day to day life for the better planning and operation of the business. In short Business Intelligence! 
This is good, but what should I be doing to make sure I am ready for that jump? Specific things I should learn about?
Database administration. Query &amp; index tuning, how to read an execution plan, how to locate, query &amp; interpret the diagnostic data that your RDBMS makes available to you, backup/restore, capacity planning, host system tuning/optimization for SQL workloads. There are a number of [Accidental DBA](https://www.google.com/webhp?q=accidental%20dba#q=accidental+dba) blog series and even books that will get you rolling.
Why not implement your search in a stored procedure and perform your "handling" there? Attempting to re-implement something SQL already does is asking for trouble; will you be able to handle everything, including edge cases, *exactly* the same way?
Thanks
Thanks
Can you give some examples of what sort of stuff you do with this, and how you got proficient on Ruby? Or worded differently, how did your SQL knowledge play into your Ruby learning?
So Rails uses an ORM and abstracts away as much as possible (you almost never see SQL peppered throughout an app like you might with PHP), but at the end of the day all everything that's persisted (barring things like key-value stores) goes into a SQL database. So when you look at your logs or try to understand why a certain operation is slow, it helps if you are aware of how the DB works. I think the last problem where I thought, "damn I am glad I spent time understanding how databases work", was when we had a background job that kept a transaction open for a long time, and happened to lock rows in certain tables in such a way that if someone happened to try to modify one of those locked rows, every request behind it would wait until the entire transaction finished and the lock was released. I guess the long and the short of it is that, like it or not, relational databases are central to virtually all web-apps, so it's not a bad foundation to have as you expand your skill-set.
My main issue right now is that formsof (inflectonal 'search-string') doesn't catch everything as I would think it should. For a specific example, if I have a numeric column who's value I want to key off of, formsof (inflectional 001444) returns the result I want, but formsof (inflectional 1444) does not. I want to implement a method in .NET that isn't limited like that, but I need to account for ranking as well which would have been automatically taken care of by containstable ().
If you're working with SQL Server right now, talk to your DBA(s) to understand what they think can be improved - most likely they will mention stored procedures in context of reporting. Definitely learn about different ways the data is stored in the database (on the physical level) and how optimizers work (or don't work). If you think that the DB is a mess, start trying to build your 'standard' queries into views that can be implemented in the DB, so it's easier to navigate and report against. Read about DWH technologies, this would certainly help (or at least present options) to decide how to arrange/navigate your data for querying/reporting. Of course, you can try to see if a more general app dev will attract you (it's actually valuable in itself). 
Thanks, I am actually working primarily with GreenPlum - that is where our main Data Warehousing is done. I have been doing views as a means to simplify access for some of the more arduous data joins (ugh, healthcare) - but I can only get those created after review, etc. Working in enterprise healthcare, you don't get the sort of freedom to "play" to learn as you might elsewhere.
The need to review/get a buy-in is always there, and it becomes even more important with the growth of project, your role and the enterprise. I would consider this a part of the professional growth. Do you guys have QA/Dev (or some other lower-level) environment where you can test/play around with different concepts? If not, see if you can convince higher-ups of the need to get that configured and give you more access to that. PS. Learn the business and the industry. Knowing a lot of healthcare can give you an incredible jumpstart. Payer is a less "sexy" vertical but try to get deep(er) exposure/experience in area (for example, know accounts &amp; plans setup from A to Z, learn how plans &amp; benefits are configured, what happens and captured in enrollment - step by step, how Medicare is different, etc.)
You're getting a min(Cost) for a book not written by sam smith. I'd try - SELECT top 1 title FROM books JOIN bookauthor USING (isbn) JOIN author USING (authorid) WHERE fname ='SAM' AND lname = 'SMITH' order by Cost;
That didn't work. I tried it, but it returned the incorrect book. I need the cheapest book returned. This is what leads me to the MIN function. EDIT: I am in Oracle 11g and it also didn't let me put the top 1 in the SELECT statement. EDIT2: I am still stuck and really need help... [6:33pm EST] EDIT3: Ok, I am getting somewhere now. I have this code: SELECT title, MIN(cost) FROM books JOIN bookauthor USING (isbn) JOIN author USING (authorid) WHERE fname = 'SAM' AND lname = 'SMITH' GROUP BY title FETCH FIRST 1 ROWS ONLY; Does anyone know how to take off the cost column in the results? [6:46 pm EST]
Going to dare to disagree now. SQL Server pros still stand to earn significant salaries and in the UK at least are in high demand, especially if you have BI skills too. Even in the US, take a look at the consulting rates of some of the top individuals and companies, the rockstars of SQL land such as SQLSkills or Brent Ozar. DBA permanent salaries orbit US 100k and contract rates around US700/day. Whereas transitioning to become a developer, with the greatest respect to skilled devs out there is going to be difficult to reverse out from. You can throw a brick out of the window and hit a developer these days and their salaries and rates are half that of skilled database professionals. If money isn't a primary concern then go for it but be careful, as there's thousands of web devs out there and few DB gurus. Have you thought about specialising? Ie some aspect of BI, or SharePoint / CRM, or performance tuning, or working with Qlikview/Crystal Reports etc? Companies are REALLY interested in niche skills at the moment. Source: I'm a DBA.
Thanks! I can't believe I didn't see this logically. It makes sense that we have to re-specify how to format the results we want. Thanks so much for your help! I tried to do this without subqueries because we haven't gotten to that yet, but I didn't see any other way to only return the title.
Stored procs are essential to all aspects of database work. I feel all reports should use a stored proc instead of a query. You should learn how to write them. I would suggest downloading SQL Express on your work PC and use it to learn. 
&gt;I would suggest downloading SQL Express on your work PC and use it to learn. Yea, not going to happen unfortunately. Work PC = Install nothing. Like I said, enterprise and healthcare security protocols. Still, I've seen a lot about stored procs, so I will be looking into them once I get a home lab set up.
What you are attempting to do is bad idea. The SQL Database Engine is designed for this type of task, you'll just have to built a little custom logic to handle your example (1444 vs 001444 etc.). If you are dealing with data, you will have to pull entire data sets off the server and process locally if you attempt to use the .NET framework (assuming this is a client application and not a server-side service with direct access to the Data Store). 
I have used [SQLite Manager](https://addons.mozilla.org/en-US/firefox/addon/sqlite-manager/) for Firefox for a couple years.
It is a web application with direct access to the data store. I'm aware that there are some pretty hefty trade offs to consider at this point. Pulling all the data from a specific table is certainly going to have it's drawbacks on memory, but that's for another time to decide. What I'm mostly interested in right now is how the rank() function works. 
I've ended up accidentally specialising in Qlikview. Plenty of work about but I'm worried for the future. What if it falls out of flavour? Any advice
You only want to group by product. Put the case within the sums. So SUM(CASE WHEN _ THEN _) 
the fuck is that join syntax?
The RANK() function basically: * Groups on a partition, then * Ranks on a subset You could mimic this in other languages but it's not quite so straight forward. As an example, say you had a list of people with credit scores (using this example because I'm currently arguing with someone on another sub :P): John, 700 John, 700 John, 620 Kim, 719 Kim, 650 Kim, 700 Kim, 650 And you wanted to rank each record based on the Person, and their Credit History: John, 700, 1 John, 700, 1 John, 620, 2 Kim, 719, 1 Kim, 700, 2 Kim, 650, 3 Kim, 650, 3 Without using SQL RANK(), you'd have to iterate through the list and compare the current Score to the prior score for each Name. Basically, at this point it becomes a bubble sort. But instead of sorting each record into a new sequence (1, 2, 3) you group when current Score = Prior score (1, 1, 2, 3). 
Yup, sounds like he needs a group header for username and repeat it on new pages.
I *think* it's MySQL, not SQL. Certainly not any syntax I'm familiar with.
You just gave me the idea for my other issue. Awesome :)
 SELECT Client_ID ,Client_First_Name ,Client_Last_Name ,Address_Line_1 ,Address_Line_2 ,City ,Zip ,Home_Phone ,Work_Phone ,Encounter_Date ,'Last_ICS_Visit' as Last_ICS_Visit ,Place_Service_Name FROM ( SELECT ev.Client_ID , ev.Client_First_Name , ev.Client_Last_Name , mc.Address_Line_1 , mc.Address_Line_2 , mc.City , mc.Zip , mc.Home_Phone , mc.Work_Phone , ef.Encounter_date , ed.Place_Service_Name , ROW_NUMBER() OVER (PARTITION BY ev.Client_ID, ef.Encounter_date ORDER BY ev.Client_ID, ef.Encounter_date DESC) as ROW_NUM FROM ed INNER JOIN ef ON ed.Encounter_Key = ef.Encounter_Key INNER JOIN cd ON ef.Client_Key = cd.Client_Key INNER JOIN ev ON ev.Client_ID = cd.Client_ID INNER JOIN mc ON ev.Client_ID = mc.Client_ID ) a WHERE a.ROW_NUM = 1
ugh no one is responding and I actually deleted my 1st response since it was breaking 3NF as well. Let me throw this in and I'll let others judge if I'm actually breaking 4NF with this. In short, 5NF is about higher-order relations (ternary and above) being expressed via lower-order relations + some magic formula (metadata knowledge). So, if I have A,B,C-&gt;D, and external knowledge of D=f(x,y). If you can find transitive relations (A,B)-&gt;x and B,C-&gt;y, you can drop the original relation and get the original data via a join of the other two. So, say I have Items of 3 different Colors stored or sold at Stores, with Quantity on Hand. (A = colors, B = Items, C = Stores, D = QoH). My magic formula is "If a Store sells an Item, it holds 1/3rd of the stock in all Colors of the item" (D=1/3(TotalQuantity*ItemStored_Ind)) So instead of having this data in the Inventory_Table( Store, Item, Color, QoH) I can have two tables Item_Inventory( Item, Color, TotalQoH) and Store_Items( Store, Item) and figure out Store's QoH of a specific Color for an item as 1/3 of the TotalQoH for the Item,Color. 
hey you leave my mess alone haha
Now I am stuck with this question. * Show top 5 employees, excluding the top employee (show employees ranked 2-5) in terms of total sales done by those employees. In the query display employee's first name, last name and TotalSales sorted in descending order. How do I only show rank 2-5? 
Oracle 11g SQL syntax. For School. Great to know I am learning odd syntaxes lol... that's gonna help in the real world.
Give each of those a shot and post your code as far as you can get. We can help you from there.
&gt;Q1: Show top 5 employees, excluding the top employee (show employees ranked 2-5) in terms of total sales done by those employees. There's more than one way to achieve this but the query tool thing that W3Schools uses will allow LIMIT and OFFSET. LIMIT affects how many rows you return, and OFFSET allows you to start at a certain point. The syntax is : SELECT blah FROM foobar ORDER BY meh LIMIT x OFFSET y (Where x and y are positive integers)
Are you writing an app for this? If so you input to variables and make sure the item does not already exist prior to writing to db. 
I'd recommend the same. You could also add a unique index on the field, but you want to check for this proactively.
A unique index or a trigger. But if you are creating the database why wouldn't you want a primary key? 
Can't you use unique? Or don't most SQL implementations have it?
There's a ton of different ways to do this depending on what database and what interface you're using. But in general most cases would work best if you simply select what you're looking for and if you get no results insert it. Some drivers will allow you to hold open the connection and insert using the interface object. And in some SQL languages you can use IF syntax in the query it's self or MERGE syntax would do the job. If you intend to get any actually useful feedback you'll need to be a lot more specific in what technology using and what specifically you're trying to accomplish.
I do a lot of this too when I need to total multiple values based on multiple conditions. Something like: SUM(CASE WHEN HERP = 'DERP' THEN LuLz ELSE 0 END) or simply using a 1 to get a count. This allows your query to make only one pass through the table and is more efficient than mashing together multiple queries. 
That works too but doesn't produce one row per product. 
[TOP keyword SQL Server](http://msdn.microsoft.com/en-us/library/ms189463.aspx) http://msdn.microsoft.com/en-us/library/ms189463.aspx
It depends on how specific you want to get, but assuming you're writing this in C# or some similar language, you could try something like this: var movieExists = "Movie already exists."; var prefix = ""/*Whatever the user entered*/; var sql = @"SELECT * FROM Movies WHERE MovieName LIKE '@MovieName'"; using(var command = new sqlCommand(sql, connectionString) { command.Parameters.AddWithValue("@prefix", prefix); var reader = new sqlCommand(sql, connectionString); while(reader.read()) //reader.read() returns true if the command returns anything { return moveExists; //Or display an error message, or some other third thing } } This may not work quite perfectly for you, but it's some pseudocode I put together from a few projects I worked on over the summer. Hope it helps!
To get you started, here's what I think you'd need for Q1. The rest of your questions seem to be pretty basic derivations of the same query. The key is to use "OFFSET." SELECT e.LastName, e.FirstName, SUM(od.Quantity*p.Price) AS OrderTotal FROM [Employees] AS e JOIN [Orders] AS o ON o.EmployeeID=e.EmployeeID JOIN [OrderDetails] AS od ON od.OrderID=o.OrderID JOIN [Products] AS p ON od.ProductID=p.ProductID GROUP BY e.EmployeeID ORDER BY OrderTotal DESC LIMIT 4 OFFSET 1 
You have to multiple the weeks by 2
Use several fields as your primary key. Movie name and release year, for example.
Ahh OFFSET! Thanks!
Q3. SELECT p.ProductID, p.ProductName, count(od.Quantity) as Quantity FROM Products p INNER JOIN OrderDetails od ON od.ProductID = p.ProductID GROUP BY od.Quantity ORDER BY count(od.Quantity) DESC LIMIT 5; This is what I had for Q3, but the quantities kept repeating.. 
It wasn't particularly clear whether the data is already there. I'll answer it like there is, since someone already answered the empty table scenario. You will probably need to compensate for misspellings, so depending on your language, you could use the [Levenshtein distance](http://en.wikipedia.org/wiki/Levenshtein_distance) function and some logic to compare the typed word to the movie names in the table. After that it's all up to you, but that's how I would start. Edit: This would probably be computationally expensive method, I am aware.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Levenshtein distance**](https://en.wikipedia.org/wiki/Levenshtein%20distance): [](#sfw) --- &gt; &gt;In [information theory](https://en.wikipedia.org/wiki/Information_theory) and [computer science](https://en.wikipedia.org/wiki/Computer_science), the __Levenshtein distance__ is a [string metric](https://en.wikipedia.org/wiki/String_metric) for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after [Vladimir Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein), who considered this distance in 1965. &gt;Levenshtein distance may also be referred to as __edit distance__, although that may also denote a larger [family of distance metrics](https://en.wikipedia.org/wiki/Edit_distance). :32 It is closely related to [pairwise string alignments](https://en.wikipedia.org/wiki/Sequence_alignment#Pairwise_alignment). &gt; --- ^Interesting: [^Edit ^distance](https://en.wikipedia.org/wiki/Edit_distance) ^| [^Damerau–Levenshtein ^distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) ^| [^Vladimir ^Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein) ^| [^String ^metric](https://en.wikipedia.org/wiki/String_metric) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ckzgemq) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ckzgemq)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It could be lock contention. If another process is reading from the table while you're writing to it, the database will wait for the read lock(s) to release before granting the write lock. If the table is large and the update is highly selective, you might want to put an index on the combination of char_id and item_id. This will allow for a bookmark lookup before the update, which could be a lot faster than a table scan. That's less likely, though, based on your symptoms. It's probably a locking issue, especially since this table constantly has SELECTs running against it. 
UPDATE user SET sla_signed = 'N' WHERE sla_signed ='Y'
You're welcome.
&gt; Can anyone see what I am doing wrong? Is this a riddle? Just tell us which bit of it isn't working, and then maybe someone can help you fix it
It's only selecting from the current database that I am using and not all of them
I think I understand what you want. Assuming T-SQL: SELECT @my_date = DATEADD(month,6,yearend_Date) SELECT DATEADD(DAY, -1, DATEADD(MONTH, 1, CONVERT(VARCHAR, Month(@my_date )) + '/1/' + CONVERT(VARCHAR, YEAR(@my_date )))) What this will do is convert the date you have to the first day of the month, then add one month, then subtract a day, leaving you with the last day of the month you started with.
I know SQL 2012 has EOMONTH function http://msdn.microsoft.com/en-us/library/hh213020.aspx.
Im not sure you can call USE @dbname with EXEC, but I havent worked with SQL Server for some time But you don't need to anyway. Try querying INFORMATION_SCHEMA.VIEWS (which is a view in itself), which is the ANSI compliant way of getting metadata Theres other INFORMATION_SCHEMA views you can join onto to get the view columns or anything else you need 
What DBMS are you using? In MySQL, you can do this: select table_name from information_schema.tables where table_type='VIEW'; You can also select `table_schema` if you want the database too.
MS SQL 2012
It looks like I have the same issue with information_schema :(
Try grouping by ProductID and ProductName instead of Quantity. Usually you will want to keep any columns not referenced in an aggregate (SUM, MIN, MAX, ect...) In the group by clause. SELECT p.ProductID, p.ProductName, COUNT(od.Quantity) as Quantity FROM Products p INNER JOIN OrderDetails od ON od.ProductID = p.ProductID GROUP BY p.ProductID, p.ProductName ORDER BY COUNT(od.Quantity) DESC LIMIT 5;
I wish I could update, we are on 2008.
The problem is that SQL deals with descriptions, wheras programming deals with actions. Writing actions is more fun and rewarding than writing descriptions
(NO LOCKS) (NO LOCKS) everywhere
If you only have a 'Y' or 'N' option then you could even take out the Where clause. 
Totally, that's a good solution if your application doesn't mind dirty reads. That might be a problem for an inventory table, though! Read committed might be a better isolation level.
Dirty reads on a production inventory database, FTW!
Assuming Number is unique key for CourtCases and assuming cte returns ONE record per Number. With Judgment_SOLCalc as ( Select Number, S.StatuteInDays From dbo.Master as M Inner Join dbo.CourtCases as C On C.AccountID = M.number Inner Join dbo.Judgment_SOL as S On S.State_Abbrv = M.State Where c.motioncutoff is not Null ) UPDATE cc Set cc.StatuteDeadline = DATEADD(Day, cte.StatuteinDays, cc.motioncutoff) from dbo.CourtCases cc join Judgment_SOLCalc cte on cte.number = cc.number; Go 
You kid, but this is no laughing matter! &lt;stern face&gt;
Actually AccountID is in Court Cases and that equals Number in Master which is joined up above. That did it though with a minor tweak in the join. Awesome thank you so much. I guess I was thinking the DateAdd would produce the unique number but you have to join it again it seems. Interesting. 
Without knowing how your database is exactly set in regards to uniqueness up I'm only guessing here... but would something like this work? UPDATE C SET C.StatuteDeadline = DATEADD(dd, S.StatuteinDays, C.motioncutoff) FROM CourtCases AS C LEFT JOIN Master AS M On C.AccountID = M.number LEFT JOIN Judgment_SOL as S On S.State_Abbrv = M.State WHERE C.motioncutoff IS NOT NULL
SQL Server 2014 Dev version is only 60 bucks and has all the functionality of Enterprise. Check it out for your learning purposes.
You're trying to do an ugly thing that's going to have an appropriately ugly answer. What you need to do is iterate through the rows of `sys.databases` then use dynamic sql to iterate through the rows of `sys.views` one each of those databases, similar to how you're doing now but using a 3 part name (`[DatabaseName].sys.views`). I started typing up the solution for you then decided I didn't want to waste a ton of time implementing a bad idea. It's a huge pain in the ass to manage security at the object level, especially when you're talking about so many objects that it's more economical to make a query rather than just identifying the ones you want to manage manually.
This is almost certainly a bad idea. But if you really, really, actually need to do it, this will work. Keep in mind that this is a super hacky solution that relies on feeding an undocumented system procedure a script in order to dynamically generate SQL. EXECUTE master.sys.sp_MSforeachdb 'USE [?]; DECLARE @useText VARCHAR(500) DECLARE @viewNM VARCHAR(500) DECLARE @schemaNM VARCHAR(500) DECLARE @dbNM VARCHAR(500) DECLARE @sqlCmd varchar(4000) DECLARE ViewCursor CURSOR FOR SELECT ''exec ''+DB_Name()+''.dbo.sp_ExecuteSQL N''''GRANT SELECT ON '' + ''['' + SCHEMA_NAME(schema_id) + ''].['' + name + '']'' + '' TO [account]'''''' FROM SYS.VIEWS V OPEN ViewCursor FETCH NEXT FROM ViewCursor INTO @sqlCmd WHILE @@FETCH_STATUS = 0 BEGIN PRINT(@sqlCmd) FETCH NEXT FROM ViewCursor INTO @sqlCmd END CLOSE ViewCursor DEALLOCATE ViewCursor' You can find more information on that proc here - http://weblogs.sqlteam.com/joew/archive/2008/08/27/60700.aspx
[Learn SQL The Hard Way](http://sql.learncodethehardway.org/) uses SQLite for the lessons as well. It's a good primer not just for SQLite, but for SQL in general.
Maybe it's cuz it's Friday and my brain has left for the day but I don't quite get it. How are you joining the tables? How was your nonclustered index designed? Also clustered index doesn't have to be on a unique field unless it's a high insert table. I think seeing a sample query you run would be more useful in helping you design an index.
Before we go any further... you need to clarify this: &gt; I am having trouble building a query that selects records from a master table of 800m records. Are you selecting all fields (i.e. `select * from ...`), or just a few? Can you show what you're doing when you say you do a 10m subset? There are ways you could butcher that such that you get really misleading timings. Honestly, without seeing the fields/schema/queries, it makes things a lot harder. Is this a data warehouse? Is the 800m table a dimension table? I forget if SQL Server does partitioning as I've been using Oracle for the last project... but... I'd definitely partition an 800m dimension table if I were in Oracle. 
I am using the separate table simply as a list of the Type &amp; Code combos that I want to pull from the main data. So the join would be something along the lines of INSERT INTO SubsetTable SELECT * FROM MainTable INNER JOIN Separate ON MainTable.Type = Separate.Type AND MainTable.Code = Separate.Code (I'm not at work so don't have the real code on me) The non-clustered index was created through the SSMS GUI and specifying the two fields Type &amp; Code, since they are the ones I'm using to specify which data to pull from the main table.
Pinal Dave wrote a blog post on [this.](http://blog.sqlauthority.com/2007/08/18/sql-server-find-last-day-of-any-month-current-previous-next/) SELECT DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE())+6,0))
Hey, thanks for you help. I don't have access to the data right now (as I'm not at work). I created this thread to see if there were any general principles I was overlooking - really I was just surprised/disappointed that having created my index I actually saw a detriment to performance. But if you need more specific details I can come back in a few days with the real code. In answer to your Q's though: * Yes I'm selecting all fields * This is not a data warehouse, and it's not a dimension table. * SQL Server does apparently to support partitioning. I'll read up on that and see if it helps (this is not my area of expertise, so I hadn't heard of this).
&gt; Also can you explain why I would need to include the primary key Well, I'm not saying you would need to. Just typically if you are updating rows based on two columns, and aren't using the PK, then it tends to suggest you are updating more than one row at a time. Lets look at it another way... Are there any other tables which store a reference to the inv_id field? If not then you have added a surrogate key for no real reason, and you may aswell drop that, and just use (char_id, item_id) as a compound primary key. As for slowness, I see nothing there to indicate it is slow. 2ms seems acceptable to me, and I'm kinda anal about performance. What gives you the impression this query could be faster? Bear in mind an UPDATE involves actual disk writes, so will never be as fast as an equivalent SELECT 
I'd use a clustered index on type and code, especially since the 800m row table is read only. Out of curiously, what is this business process that requires such a large associative table?
Is there no clustered index on either table already? Clustered index on Code then type will work fine and have lower disk footprint since you say there's no inserts to this table if that's the case. Non-clustered indexes with includes on all the other columns your selecting would work too. If it did a table scan with the old indexes it wasn't using them.
SQL is one of the most sophisticated programming language which deal a lot of actions in a few line(s) of code. To do similar thing in C++ or Java would be more fun but I don't think it's rewarding unless you are making totally a whole new level of data manipulation algorithm which beyond the power of current RDBMS.
It is an extract of several years worth of [Hospital Episode Statistics](http://www.hscic.gov.uk/hes). Code here is actually [ICD Code](http://apps.who.int/classifications/icd10/browse/2010/en) and we want to use it to define various Conditions as different combos of Code (e.g. all malignant cancers have Code like C* but exclude skin cancer c44, while just breast cancer is C50). So after defining all the ICD codes associated with a particular condition, we want to pull out all records with those codes. These records then let us examine the trend in incidence of that condition over time.
In short try for a Business Intelligence job. Using Cognos would be a good next step into BI. With your level of SQL experience you would fit into a Cognos BI developer role pretty well. Have a Google for Cognos Report studio. Cognos experience is very sort after (especially here in UK) and you can pretty much stipulate what wage you want. 
&gt; major issue with NULL values ... is that people don't understand how they are evaluated, and assume that they are a stand in for an empty string or 0 What's kind of funny is that Oracle treats varchar2/nvarchar2 empty strings AS null... http://sqlfiddle.com/#!4/a6558/2 
Interesting! I was totally kidding about not joking, and there are scenarios where no lock is totally valid. Not sure for the one you're describing, though. I know in the Business Intelligence world, NO LOCK can be fine. Basically, if you don't give a crap about dirty reads, then go to town! 
What would be the threshold for dirty reads? I can imagine it'd be a problem in stock prices or other environments where data is written/updated every few microseconds to seconds. In the environment where I work there are maybe a couple of updates every couple of minutes. Most all of our stored procedures only take less than 30 seconds to complete and so it seems like the possible window that could allow a dirty read is rather low. Example: A manifest for incoming product is inserted in the morning. Throughout the day salesmen will book orders and refresh the sales screens. There will be maybe 5 to 10 orders booked per hour. Correct me if I'm wrong but dirty reads are based on non-committed transactions, yes?
Make a list of what you need for your job that the tool doesn't have. Explain it in short, clear, non-emotional language and give it to your manager. Phrase it as, "these are things that help me build reports fast, and I can't seem to find them in Composite. Can the vendor explain the workarounds or tell me how to accomplish these?" That puts the ball in your vendor's court and alerts management that the tool will take more time for you to build things they're used to getting quickly. Management may already know about it - they may have purposely chosen a cheaper tool and known it would result in higher staffing costs. Or, the vendor may have sold something they can't do. Or maybe the vendor genuinely wants feedback to make their tool better. Starting from a calm, factual report will help you understand what's going on and what your options are.
For the first query, I hope you have reached to this point, which uses GROUP BY over two columns: &gt; SELECT date, result, count(*) FROM matches GROUP BY date, result; +------------+--------+----------+ | date | result | count(*) | +------------+--------+----------+ | 2013-10-31 | Win | 2 | | 2013-10-31 | Loss | 2 | | 2013-12-25 | Win | 2 | | 2013-12-25 | Loss | 2 | +------------+--------+----------+ 4 rows in set (0.00 sec) The next step requires [pivoting](http://stackoverflow.com/questions/11624676/database-pivoting-what-is-the-purpose), which is not a standard SQL operation. I do not know if you are using Oracle or SQL server, in that case you can use the [PIVOT keyword](http://www.oracle-base.com/articles/11g/pivot-and-unpivot-operators-11gr1.php), or you are supposed to use a subquery on a derived table. If pivoting is not intended or supported, you can emulate it. For example, in MySQL, where PIVOT is not a available, I would do: mysql&gt; SELECT date, sum(result = 'Win') as Wins, sum(result = 'Loss') as Losses FROM matches GROUP BY date; +------------+------+--------+ | date | Wins | Losses | +------------+------+--------+ | 2013-10-31 | 2 | 2 | | 2013-12-25 | 2 | 2 | +------------+------+--------+ 2 rows in set (0.00 sec)
For the second query, there is a standard and easy way to do it: [the HAVING keyword](http://en.wikipedia.org/wiki/Having_%28SQL%29). This is how I would solve it in MySQL (which should be relatively standard syntax): mysql&gt; SELECT name FROM cars GROUP BY name HAVING max(time) &lt;= 55; +-------+ | name | +-------+ | Civic | +-------+ 1 row in set (0.00 sec) I am aggregating by car and selecting those where its maximum time is lower or equal to 55. We need HAVING and not WHERE because the filtering is done after the grouping, not before. I can thing of a different way of calculating it using subqueries, but it is very unnatural for me: mysql&gt; SELECT name FROM cars c1 WHERE 55 &gt;= ALL( SELECT time FROM cars c2 WHERE c2.name = c1.name ) GROUP BY name; +-------+ | name | +-------+ | Civic | +-------+ 1 row in set (0.00 sec) 
Yeah, non-committed transactions. I'll be honest, I don't know how to calculate a "threshold" for that. Usually it's a black and white issue; either dirt reads produce a report that's slightly off, but is correct the next time you refresh. Which means nobody cares. Or dirty reads lead to another transaction inserting incorrect data due to getting an inconsistent picture of the database. In that case it's quite serious. But the grey in between? IDK, but I would recommend caution. 
What benefit do you expect to realize by indexing the source database immediately before extracting the data? Are you using SSIS for your ETL process?
It's not entirely clear what you're doing. Is this a migration from MS SQL to MySQL in preparation for decommissioning the MS SQL instance so that MySQL can replace it? Or is this like a longitudinal reporting database meant for analysis of the data from the MS SQL server that will be updated periodically, but both servers will remain in use? If you're literally just dumping entire tables whole hog for a one time transfer prior to a decommission, then indexes won't really do much at all. Index definitions can be recreated, but the old index won't do anything to create the new one. If your transforms involve complex joins, then it *might* be beneficial but it's probably only worth it if you're doing fairly frequent data pulls of tables whose key fields change frequently. 
Hmm. Assuming I've been hired with no particular mission (i.e. improve performance) and an implied from your post mission of 'just don't let anything fall over': Since I can't answer your actual question about tools within SSMS that track where everything goes (I'd google that) I guess I'd start with developing lists, first of servers, then of databases. Then I'd start trying to assign business priorities to them, and focusing on those. Start with the basics - backups are solid? Get some automated tools for Security - is security something you're reasonably comfortable with? Next I'd hit what seems to be bothering you the most - figuring out some way to automate the process of fixing these 'dynamic sprocs'. Really that sounds fairly sinister. Next, based on my list of priorities, and with backups and security out of the way, I'd next start looking at performance metrics of sprocs using SQL Trace of the most critical databases. Edit: For sprocs, generally the top 20 sprocs are what are doing 95% of the activity. So, best thing to do is go look up a tutorial on SQL Trace and run a trace into a table on the local machine. Filtering takes some tweaking, but you can basically ask to see only sprocs executed, and export them into a table. Next use things like select count(sprocname), sprocname from tracetable group by sprocname to get a list of the numbers of sprocs called within a given time (also you can track things like CPU and duration). Pick up the top 20 of sprocs called, by number of sprocs called or # * AVG (Duration) or AVG(cpu) and go crack those open. That will give an idea of most of what's going on in a strange database, assuming everything is in sprocs and not raw sql statements being sent in. 
SSMS has some nice built in reports (right click on a db, reports). They are a good start. Table sizes, index size and use, sproc performance, etc. 
Something like this? select b.slug from blogs b join (SELECT userId, MAX(dateModified) maxdatemod FROM blogs GROUP BY userId) m on m.userid = b.userid and m.maxdatemod = c.dateModified 
[Here's an example](https://gist.githubusercontent.com/peteryates/6cfb40a199483d6026bf/raw/a170947d3eeb22df28f7859497f9640a4dfefd07/Public%20Holidays) of how a public holidays table might look.
thanks much!
I'm not from the US. So this might be expensive for you, but I have 2 bank accounts: 1 for fixed expenses and 1 for everything else. Every month I give myself an x amount on the variable costs account. I *should* export this account to an SQL table for quering. So the tip I would like to share is to create 2 views: 1 for fixed expenses and another for variable expenses. This allows you to study your variable expenses better. My bank ING had a housekeeping book software online, but they stopped providing. From what I've seen there I could recommend to save the "opposing bankaccounts" in another table with a description for future reference.
thank you again. I am working on Q5 and seem to be stuck again, this is my code: SELECT SupplierID, SupplierName, p.ProductName, count(s.SupplierID) AS ItemsSupplied FROM [Suppliers] AS s INNER JOIN [Products] AS p ON p.SupplierID = s.SupplierID GROUP BY p.ProductID, p.ProductName ORDER BY COUNT (p.productID, p.ProductName) DESC, s.SupplierName
Except, ya know... Mint uses your data for whatever the fuck they want so if you value your privacy yeah, it matters.
Money Manager EX
Thanks! I was actually pretty close to this before but was missing the AND expression. If it's not too difficult, would you mind explaining the need for that part?
To 'uniquefy' the join condition you need both fields. This works under the condition that one user has never modified two blog posts at *exactly* the same moment.
It's saying : if TotalPrivatePayment is null, treat it as 0, if TotalPublicPayment is null, treat it as 0 So if the sum of the two (when null treatment above is considered) = 0 then return 'TRUE' otherwise 'FALSE' 
&gt; I thought datetime and smalldatetime are automatically convertible You're correct in that DATEADD returns either datetime or smalldatetime - depending on JudgementDate's data type. Apparently, LastSummaryJudgmentDate is where the conversion goes wrong and depending on data types you will have to cast either to datetime: Set LastSummaryJudgementDate = cast(DATEADD(Day,cte2.StatuteinDays, CC.JudgementDate) as datetime) or to smalldatetime: Set LastSummaryJudgementDate = cast(DATEADD(Day,cte2.StatuteinDays, CC.JudgementDate) as smalldatetime) *edit PS I think the CTE is probably not necessary (depends a bit on your datamodel) and something like Blitzsturm's query below should also work. 
 SELECT s.SupplierID , s.SupplierName , SUM(od.Quantity*p.Price) as OrderTotal FROM Orders o JOIN OrderDetails od on od.OrderID = o.OrderID JOIN Products p on od.ProductID = p.ProductID JOIN Suppliers s on s.SupplierID = p.SupplierID WHERE o.OrderDate between '1997-01-01' and '1997-12-31' GROUP BY s.SupplierID , SupplierName ORDER BY OrderTotal DESC LIMIT 10 
What's the datatatype of the field dbo.CourtCases.LastSummaryJudgementDate ?
This statement is meant to to show true if there is a payment. Wouldn't the way you are explaining it mean that total payments equaling 0 would give you 'TRUE'. My understanding is that 0 is being used as a logical operator, rather than a value. Please correct me if I'm wrong. However it is behaving as 'TRUE', meaning that there is a payment in either category. So therefore can not be null.
You might try boolean INTERSECT, EXCEPT, MINUS and appropriate combinations of these to identify that body of content present in one table but missing from the other. Make sure you do it both ways. This should help you find the rows in backup that are missing from current and vice versa. From here you should be able to build a new table from the common rows from both tables, plus those from backup that are missing from current, plus those from current that are missing from backup. You are lucky if you have only one table to consider. HTH.
I have recently imported/exported data using quickbooks for a client and not once did it touch the web or was it stored in any of their DB AFAIK.
Use Excel. It can connect to practically any database.
&gt; You are lucky if you have only one table to consider. &gt; &gt; &gt; &gt; HTH. I don't have only one table, but I have a staff of users of the application whom I can lean on to repopulate the data once I find it for them. And thank you, this (as well as the post below from ziptime) did help.
EXCEPT is the most efficient, (one full table scan per instance). In "big O notation" its complexity is an O(2n) problem. You could use "not exists" to do the anti-join, although this is at worst O( n^2 ) - a potentially far more complex operation. select * from [backupcopy].[schema].table b where not exists (select null from [currentcopy].[schema].table c where c.PKcolumn = b.PKcolumn)
Care to post your schema? 
It didn't make sense to me either. I have never used boolean operands in SQL. I am going to test this statement more tomorrow. Thanks 
any chance this would work without the case statement and have it display the actual rating instead? 
I don't like that you have 2 event ids in the same table. You should rename that column. Also, will coaches salary change? Will you need to track that?
Whoops. that was a typo. It's Event Date. And no, it's for a uni assignment, so we're given a set of values to populate the table. Does everything else look up to scratch? Thanks for replying!
This is precisely what I am working on as a side project currently.
Tableau without a doubt. Http://www.tableausoftware.com They have a free trial you can check out, I love it. 
Does [this](http://imgur.com/a/ylYvE) look about right? I keep getting an error on sqlfiddle about a missing right parenthesis on the first table?
I second tableau. You will never want to use excel again once you get going with tableau. 
Or, perhaps **this** is the issue: http://stackoverflow.com/questions/129077/not-in-clause-and-null-values I'm going to go check to see if that table has a null in it. I can always just look for all non-nulls pretty easily.
I wouldn't use CHAR()'s as your primary key. It's going to be purely internal anyway, (nobody's going to search for "C001", they're going to search for Tom Hanks) so you're better off using a NUMBER instead, for a couple of reasons: 1. It's dead-simple to sort. You're going to be doing queries like "FIND ME A PARTICULAR ATHELETE, AND HOW HE DID IN ALL THE EVENTS", or "WHAT WERE THE RESULTS AND PLACES OF ALL THE ATHELETES OF A PARTICULAR EVENT". Those queries require JOINS, which go faster when you can sort easily, and nothing sorts easier than garden-variety integers. In Oracle, the data types you'll want to use for integers are: * TT_TINYINT - 1 byte, unsigned * TT_SMALLINT - 2 bytes, signed * TT_INT - 4 bytes, signed You'll want to choose your INT type based upon how large your range could conceivably get. It seems likely that you could have more than 255 athletes, coaches, and/or events. Maybe not 32,000. 2. Integers can be auto-incremented. You probably don't know how to do that right now, but it'll become useful to you later on. It'll be easier to re-work your PK's if they're already integers. 3. Integers are more secure. It's harder to SQL Inject an application with strong typing.
SQL injection generally comes from non parameterised SQL calls, where SQL is constructed as a string.
Your rules, for the most part, look sensible. Rule 4 could become confusing though, especially if you have multiple clashing supertypes. I don't know what RDBMS you intend to use, but using text rather than varchar doesn't sound like a good idea.
Python + MatplotLib /thread
Fair word of warning: Testing the limits of nested subqueries may lead to your murder at the hands of maintaining developers. 
&gt; I don't know what RDBMS you intend to use it's in the thread title :) 
As per rule #1: I take the OOP approach to this. An example of why I choose to use plurals over singular is in the junk drawer that every household has. Do you call your junk drawer your "junks drawer," or your "junk drawers?" No, it is a junk drawer, and it contains junk. The table is a container, just like a junk drawer. The name should describe what is inside, not how much. What if there are no rows in it? How would you name a table for Moose? Rule #2: Would you care to elaborate? I will say, there's only one exception to this rule: lookup tables with no other information. Rule #6: Then the database would be called map, not map_map :) I too am a fan of underscores, mostly because as a developer it just happens to look much better in my code.
I use PostgreSQL, and varchar / text are both the same under the hood. I am curious though, I am trying to think of a scenario where you could have multiple clashing supertypes, and I am drawing a blank. Could you provide an example?
Ah yes, somehow I missed that!
I just meant that if you have multiple supertypes with names beginning with the same letter (person, puffin, potato) and you used a single letter prefix (as per your example) it wouldn't be obvious which subtype belonged to which supertype.
3 seems like a very odd choice. But I have to admit to not being particularly familiar with postgreSQL. In most SQL databases this would be a terrible choice. Perhaps postgreSQL is different? 4 is the one I would have the biggest gripe with though. My rule is to never abbreviate if you can help it. If you want to see what everything does at a glance abbreviating things is usually a pretty bad idea. It saves you a fraction of a second typing person every now and again but it makes it 10x harder to understand whats going on. Just call it person_message rather than p_message. Its so much clearer for such a small cost. Also not sure about 8. Seems like sometimes its ok to actually delete stuff rather than mark it deleted. But I think it really needs context to make the choice so not a big deal. As to how I like to do things. I like to call all my surrogate keys Id and then reference them with tablename_Id in forgein keys. I think it just looks better. for example compare SELECT country.Id , person.Id FROM country INNER JOIN person ON person.country_Id = country.Id WHERE country.Code = 'AU' vs SELECT country.country_Id , person.person_Id FROM country INNER JOIN person ON person.country_Id = country.country_Id WHERE country.Code = 'AU' The second just seems like too much repetition me. The first seems clearer. Plus as an added bonus you can just look at the field and know whether its a FK or a PK. I guess you can also see that I like to make sure I fully reference each field rather than omitting the table name. Since you will need the alias as soon as you have more than one table involved I figure you may as well get used to it and always do it. But really the main thing is consistency. If you have a set of rules and are completely consistent then thats likely to make things 100% easier to follow even if the rules aren't all that well thought out. Any exceptions to the rules and you are heading down a slippery slope so my number one rule is "Try not to make rules that you are likely to break." 
In PostgreSQL text and character varying are the same thing under the hood. I can make every one text and not have to deal with setting some oftentimes arbitrary limit. I agree with the person_message, and I've decided to change that rule after someone else mentioned super/subtypes could collide if they start with the same letter. What about this? SELECT col1, col2, col3 FROM table JOIN other_table USING (table_id) In other words, table has it's PK as table_id, and other_table has an FK called table_id that references table_id PK from table. I like that even better.
I Third Tableau. You can get Tableau Public which is free to use but you limited to only data connecting to text excel and access files locally stored. The full Desktop version can connect live to almost any database.
&gt; As per rule #1: I take the OOP approach to this. You should probably take a DBA approach to this.
Interestingly, I was interested in what DBAs have to say about it, and it seems most agree with singular: http://stackoverflow.com/questions/338156/table-naming-dilemma-singular-vs-plural-names As well, I am a developer and for this tool I am using DBIx::Class, so I think I will stick with the OOP approach since it's much more helpful for me when using an ORM and has no impact on the database whether it's plural or singular.
I use ssrs. It'll do what you need.
a purely cosmetic thing, not specific to your problem: don't be afraid to use underscores in column names, it makes them easier to read. On a similar note, when typing SQL queries, use ALL CAPS for SQL commands, and small letters for column names. It's much, much more readable.
&gt; Stack Overflow is a question and answer site for professional and enthusiast programmers. Almost everyone in that thread is an application programmer, not a DBA. (You can look at their tags.)
[Seems the DBAs agree as well](http://dba.stackexchange.com/questions/13730/plural-vs-singular-table-name). [I'm glad to see even MS DBAs](http://social.msdn.microsoft.com/Forums/sqlserver/en-US/fee44016-617a-4969-bb1c-cbbccd7acdce/table-names-singular-or-plural?forum=sqldatabaseengine) are smart enough to consider the ORM. [This list of best practices](http://architects.dzone.com/articles/20-database-design-best) agrees as well. What do ya know, the first book I pulled off my bookshelf (Database Design for Mere Mortals) agrees as well. In the section, "Guidelines for Creating Field Names" the author states: Guidelines for Creating Field Names • Create a unique, descriptive name that is meaningful to the entire organization. • Create a name that accurately, clearly, and unambiguously identifies the characteristic a field represents. • Use the minimum number of words necessary to convey the meaning of the characteristic the field represents. • Do not use acronyms, and use abbreviations judiciously. • Do not use words that could confuse the meaning of the field name. • Do not use names that implicitly or explicitly identify more than one characteristic. **• Use the singular form of the name.**
Can you recommend some tutorials on this? I'm a self taught programmer but have limited math and statistical analysis skills.
My point in bringing up Quickbooks was that it has a proprietary backend, they do that on purpose. So yeah, Quickbooks was great for me as a consultant because I got to charge through the nose to do anything with it and EVERYONE needed stuff imported/exported. Thanks Intuit, for me... as the DBA that was good, as an end-user I would have found it infuriating.
"Tables should be identified with singular, unambiguous names." (pg. 67, Section Review Data Modeling: Logical Database Design) "Do not use the plural form of the name. As you know, a table represents a single subject, which can be an object or event." (from section Defining the Final Table in Database Design for Mere Mortals).
 SELECT id , tot_hrs , brk_hrs , tot_hrs - brk-hrs AS agg_hrs FROM ( SELECT id , SUM(TOTAL_HOURS) AS tot_hrs , SUM(BREAK_HRS) AS brk-hrs FROM t1, t2, t3, t4 where blahhh... GROUP BY id ) AS sq WHERE tot_hrs - brk-hrs &gt; 95 
Yup, that's precisely it. When I changed it to this: SELECT * FROM [backupcopy].[schema].table WHERE [backup_copy].[schema].table.PKcolumn NOT IN ( SELECT PKcolumn FROM [currentcopy].[schema].table WHERE PKcolumn IS NOT NULL ) It works just fine.
Yea IIF is SQL 2012 and 2014 only. I wish MS Access SQL was able to be copy/pasted into SQL Management Studio, it would make rewriting queries so much easier. [Try using CASE](http://msdn.microsoft.com/en-us/library/ms181765%28v=sql.105%29.aspx) Take a look in the examples section &gt;B. Using a SELECT statement with a searched CASE expression &gt; &gt;Within a SELECT statement, the searched CASE expression allows for values to be replaced in the result set based on comparison values. The following example displays the list price as a text comment based on the price range for a product. &gt;Transact-SQL &gt; &gt;USE AdventureWorks2008R2; &gt;GO &gt;SELECT ProductNumber, Name, "Price Range" = &gt; CASE &gt; WHEN ListPrice = 0 THEN 'Mfg item - not for resale' &gt; WHEN ListPrice &lt; 50 THEN 'Under $50' &gt; WHEN ListPrice &gt;= 50 and ListPrice &lt; 250 THEN 'Under $250' &gt; WHEN ListPrice &gt;= 250 and ListPrice &lt; 1000 THEN 'Under $1000' &gt; ELSE 'Over $1000' &gt; END &gt;FROM Production.Product &gt;ORDER BY ProductNumber ; &gt;GO
Thanks for the reply. I tried using CASE... Unfortunately I need some more direction.
Don't work with meta data, generally a bad bad idea. Parse the data, and store accordingly.
PKColumn null? How?!
Tableau is not limited to small datasets, I use it quite frequently with massive datasets and databases for work. You have the ability run a scheduled extract if your database or flat file (excel, etc) is slow. Live data is where Tableau really comes to life, you should check it out, you'll likely be quite pleased and surprised. Full disclosure: I'm a BI Consultant and Tableau is our weapon of choice. 
Couple things here... Convert your IIF to a case statement. As mentioned, I believe this was implemented in more recent versions of SQLServer Additionally, IsNull() works differently in Access and is simply used to test if a value is null or not. [Source](http://office.microsoft.com/en-us/access-help/examples-of-expressions-that-check-for-null-values-HP001099035.aspx). In SQLServer this function will actually replace a null value with a value you specify (or another column if you tell it to use that). SQLServer does not return a true/false value from this function. Then there's the joins. In Access joining more than 2 tables is a horrendous chore as evidenced by your having to encapsulate the various levels in (). In anything but Access joins are similar to the following (I added aliases because typing out table names can be... time consuming) select 1 from tablea as x join tableb as y on x.col1 = y.col2 join tablec as z on y.col1 = z.col2 All of the above translates your query to the following SELECT dbo.SubSystem.Name AS SubSystem, Sum( case when [dbo].[Deficiency].[FixedBy_ID] is null And [dbo].[Deficiency].[Criticality_ID]=1 And [dbo].[Deficiency].[Vendor]='Ledcor' then 1 else 0 end) AS A_Def, Sum( case when [dbo].[Deficiency].[FixedBy_ID] is null And [dbo].[Deficiency].[Criticality_ID]=2 And [dbo].[Deficiency].[Vendor]='Ledcor' then 1 else 0 end) AS B_Def FROM dbo.SubSystem INNER JOIN dbo.Deficiency ON dbo.SubSystem.SubSystem_ID = dbo.Deficiency.SubSystem_ID INNER JOIN dbo.Criticality ON dbo.Deficiency.Criticality_ID = dbo.Criticality.Criticality_ID you will likely need a group by dbo.SubSystem.Name or something else at the end to make the SUM() work properly, hard to tell without a sample data set to work with. Edit: Cleaned up my post a bit and added a source to the ISNULL() Access bit. Additionally, it doesn't look like you are using anything from dbo.Criticality. If this join isn't being used to restrict your final dataset, then you can drop that last join. Edit 2: Also, the handling of brackets is a bit different. I changed some stuff around in my final query to reflect that. Edit 3: Looks like you had a group by, the different colour made me miss it entirely... my bad :)
Should have clarified.... the problem is in that last line (we think) where it says... ''''+ @DatePlus90str + '''''
Theoretically you could do something like this: SELECT * FROM is_postmeta WHERE post_id IN (SELECT post_id FROM is_postmeta WHERE meta_value LIKE 'name%') I'm tired, hopefully there aren't too many syntax errors in that.
You are correct - change that last bit of code to this: &lt;= ''' + @DatePlus90str + '''' See if that works for you. edit: forgot to mention, you can quickly spot the problem because from your original code @DatePlus90str was colored as part of a string when in truth you want it to be be outside the string so SQL will treat it as the variable and not just the name of the variable.
That's pretty incredible, straight up. It must have been a pain to write all those individual micro optimizations.
That's what I heard.
I didn't try it, but there is [SQL-Ledger](http://en.wikipedia.org/wiki/SQL-Ledger).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**SQL-Ledger**](https://en.wikipedia.org/wiki/SQL-Ledger): [](#sfw) --- &gt; &gt;__SQL-Ledger__ is an [ERP](https://en.wikipedia.org/wiki/Enterprise_resource_planning) and [double entry](https://en.wikipedia.org/wiki/Double-entry_book-keeping) [accounting](https://en.wikipedia.org/wiki/Accounting) system. Accounting data is stored in an [SQL](https://en.wikipedia.org/wiki/SQL) Database Server and a standard [web browser](https://en.wikipedia.org/wiki/Web_browser) can be used as its [user interface](https://en.wikipedia.org/wiki/User_interface). The system uses the [Perl](https://en.wikipedia.org/wiki/Perl) language with a database interface module for processing and [PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) for data storage. &gt;==== &gt;[**Image**](https://i.imgur.com/J0hjFxh.png) [^(i)](https://commons.wikimedia.org/wiki/File:Sql_ledger_login.png) --- ^Interesting: [^LedgerSMB](https://en.wikipedia.org/wiki/LedgerSMB) ^| [^The ^Army ^Rumour ^Service](https://en.wikipedia.org/wiki/The_Army_Rumour_Service) ^| [^List ^of ^ERP ^software ^packages](https://en.wikipedia.org/wiki/List_of_ERP_software_packages) ^| [^Outline ^of ^Perl](https://en.wikipedia.org/wiki/Outline_of_Perl) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cl3lqmn) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cl3lqmn)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Depending on the complexity of the graphs you need, [pgcharts](https://github.com/dimitri/pgcharts) may be suitable.
I finally put together a post on left, right, and full joins. I would love to hear your comments and what I can do to make the post better!
What is your question?
THe stuff above the line. Im not sure whether im right so far and if I am what to put for the where clause
Have you tried using the CONCAT function? http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions026.htm
&gt; SQL Server which is hosted on a different machine. This, of course, poses a problem of connection stability Unless your web &amp; SQL servers are geographically separated and/or your network is really crappy, this shouldn't be happening. **Fix this**, it's unacceptable in a production environment. If you need to set up a failover cluster, or an AlwaysOn AG, or a mirror with log shipping...whatever - if you're **regularly** having connection problems on your network, you have to have a serious talk with your DBA, sysadmin and network admin about it. You're looking at the wrong documentation for your error message. The MSDN link you have there is for SQL Server "internal" errors; the error you're throwing is coming from the .NET Framework because you can't make the connection and the first few hits [on this search](https://www.google.com/search?q=System.Data.SqlClient.SqlException+\(0x80131904\)&amp;oq=System.Data.SqlClient.SqlException+\(0x80131904\)) may prove informative.
Hi, I'm sure there is probably someone who will come along with a more sophisticated solution, but could you just do a simple case statement on your year column and alias it ? select *, case Year when '1Q2012' then '03-31-2012' when '2Q2012' then '00-00-2012' else Year end as Dateformat
If you split it out into quarter number and year, you can do Dateadd (q, 1,'2012-01-01') Edit: this sounded like a fun problem, so here's the full code: declare @qdate nvarchar(6) = '1Q2012' select dateadd(q, cast(left(@qdate, 1) as int)-1, cast(right(@qdate,4) + '-1-1' as date)) QuarterStart ,dateadd(d, -1, dateadd(q, cast(left(@qdate, 1) as int), cast(right(@qdate,4) + '-1-1' as date))) QuarterEnd
Always text because you can have leading zeros that an integer field would drop.
9 digit number needs 4 bytes if stored as unsigned integer while char type needs 8 bytes. So you save half the space and speed up operation during compare/lookup
I hope people respond as an Encrypted Varchar field! PLEASE do't store SS#'s as open values.
Man, that's some Ivory Tower stuff there, lol.
How should I encrypt fields?
It's as if you've encountered this problem before! --- A different problem, but I once had to clean up after a guy who used an integer field to store a shorthand (mdy). He couldn't figure out why everything went south when the dates went from June 30th (63013) to July 1st (7113).
That ended up being pretty much what I used, thanks! Here was my final code: ,CASE WHEN LEFT([Quarter], 2) = '1Q' THEN CAST('3-31-' + RIGHT([Quarter], 4) AS DATETIME) WHEN LEFT([Quarter], 2) = '2Q' THEN CAST('6-30-' + RIGHT([Quarter], 4) AS DATETIME) WHEN LEFT([Quarter], 2) = '3Q' THEN CAST('9-30-' + RIGHT([Quarter], 4) AS DATETIME) WHEN LEFT([Quarter], 2) = '4Q' THEN CAST('12-31-' + RIGHT([Quarter], 4) AS DATETIME) ELSE NULL END AS Trans_Date Your suggestion was what threw me in the right direction, very much appreciated!
Important for multiuse databases, but rather pointless if you are working in an end-to-end encrypted environment like healthcare insurance.
Yeah that's what I started with, I needed something that would be more dynamic as I had multiple years and quarters - I found the solution which I posted in this thread. I do appreciate your help and effort - thanks!
The joke is, not everyone does it. (Nothing about what they *should* do)
This might be a bit advanced for this tutorial, but you might note the possibility of using nested joins. I.e., instead of this: SELECT PC.Name AS Category, PSC.Name AS Subcategory, PM.Name AS Model, P.Name AS Product FROM Production.Product AS P FULL OUTER JOIN Production.ProductModel AS PM ON PM.ProductModelID = P.ProductModelID FULL OUTER JOIN Production.ProductSubcategory AS PSC ON PSC.ProductSubcategoryID = P.ProductSubcategoryID LEFT JOIN Production.ProductCategory AS PC ON PC.ProductCategoryID = PSC.ProductCategoryID ORDER BY PC.Name, PSC.Name try this: SELECT PC.Name AS Category, PSC.Name AS Subcategory, PM.Name AS Model, P.Name AS Product FROM Production.Product AS P FULL OUTER JOIN Production.ProductModel AS PM ON PM.ProductModelID = P.ProductModelID FULL OUTER JOIN Production.ProductSubcategory AS PSC INNER JOIN Production.ProductCategory AS PC ON PC.ProductCategoryID = PSC.ProductCategoryID ON PSC.ProductSubcategoryID = P.ProductSubcategoryID ORDER BY PC.Name, PSC.Name and compare the execution plans. Whether using an INNER or LEFT join, the nested join achieves the same result with a lower cost because from outside the FULL OUTER JOIN the PSC.ProductCategoryID can potentially be NULL, leading the query optimizer to use a NonClustered Index Scan for PC.ProductCategoryID instead of a Clustered Index seek. People sometimes complain about it being weird or hard to understand, or whatever, but I find that when a join to table A is wholly dependent upon table B, and there may or may not be a row from table B in the result (whether table B is in a FULL OUTER JOIN or LEFT JOINed from something else) it tends to work better to isolate the join to table A. Make sense?
I rather take 2x the space and have it be right.
Generally my rule is: If you're going to do arithmetic with it (Sum, Average, Compare) store it as a number, otherwise it's a string. SSN: String Age: Number Numeric confirmation number: String Zip code: String
No, you're right, not everyone does it. Their names are Target and Sony.
I've worked with a lot of EMR's and all rely on full-database and/or OS level encryption. I don't think I've ever seen a case where a single field was individually encrypted to the level that a DBA wouldn't see it without an encryption key.
Preferably with the encryption methods built into your database.
Two reasons: 1. ZIP+4. The ZIP code for Defreestville NY is 12144, but 12144-7050 is *also* valid and more specific for the USPS to use in their routing (easier to process than the street address, at least for higher-level sorting). You *could* store it as two fields, allowing `NULL` for the +4 portion. 2. (And this probably is the real reason) There are large swaths of ZIP codes that start with 0. If you store `02108` as an integer, it'll implicitly drop the leading 0. And then you're gonna have a bad time sending mail and packages.
If you hash it properly, how can you get the value back out when you need to use it again?
This is a micro-optimization that 99.999% of systems don't need. Especially when converting it to a `uint` will drop any leading zeroes, thus making it invalid **or** requiring additional work on the other side to cast as a string, then prepend with the proper number of zeroes to get to a 9-digit number.
Yep. But this guy didn't zero pad single digits so it blew up when the date went to one digit. It would have broken in October too. Which made it similar to the gotcha on SSNs staying with 0.
Are there people who actually believe that? That thought never would have even entered my mind.
It all depends on application of data. If you use the SSN as a primary key in a system that uses the number as part of the calculation for a unique identifier then it is worth it. Lets say you have a system that tracks all the people in this country. It builds event trees and does continuous data transforms on each individuals dataset. 
You can't, but it's possible all its used for is an identity check, in which case that is fine. 
Bingo
I got burned with that one when I first started using PG.
&gt; If you use the SSN as a primary key in a system that uses the number as part of the calculation for a unique identifie Which arguably should never ever be done.
Thanks! I'll address nested joins. I appreciate the suggestion. Kris.
We do this for only the fields needed (finance industry). That way we are not running through decryption for every transaction. We don't want the added time unless necessary.
I've never had to do this because I don't work with sensitive data in that manner... But I think I should know how. Any recommended reading?
Where do you work? 
Also non-us zip codes often have characters (even if they're technically postal codes they generally occupy the same field as a zip code) 
string. Do you need to retrieve it, or just last 4 chars. If the latter store the SS as a hash and use a separate column to also store the last 4 chars. Be careful storing sensitive user data, and read up on best practices. 
I will try to see what can be done to prevent this from happening in the first place. But it seems that the hex error (0x80131904) in the exception is some kind of placeholder, because there are people who are asking questions related to that error about not being able to connect at all, and others with errors like lack of permission, or malformed queries.
&gt; List all staff who have accepted customer orders. The where clause has to decide, if the order was accepted or not. I think you are on the right path with your SELECT statement. But you should rethink the round brackets you have placed. Or at least put in the according closing brackets. 
But in a real world application the difference is probably not large enough to optimize storage for this particular field. If you're working with all 460 million SSNs issued, the difference between integer and string is a few gigabytes for the entire set. (And obviously SSNs would be a terrible primary key if you're working with that)
That depends on what DBMS you're using.
Because a comparable set of servers will cost you about $6,000 in rack space rental fees or a computer capable if doing this yourself will cost close to $1,000. If you think the cert is going to help your career, ask how much its worth. You already paid $50-$60 for a book on the subject, so it is worth something. Plus, the lab will let you keep learning whenever you need it. Value is all relative, I guess.
I understand that is cheaper in hindsight of what his options are this point; but I honestly wouldn't say it's for pennies. Guess I can't relate to this situation, as I've always been able to create the test environments either at home, or on my work pc. 
Thanks! I think that's what I'm looking for. I had considered AWS, but wasn't sure how much it would cost. Obviously, I don't need high capacity machines for a couple copies of Adventureworks.
Thanks! I was looking at AWS, but wasn't too sure about the costs involved. It looks like the other poster addressed that. I haven't seen the Virtual Labs from Microsoft before. I think that's exactly what I was looking for. Some preconfigured machines that I can get some exposure with. It looks like a great resource for something like this. Thank you!
Does MySQL run at a remote hosting facility? I'm not sure they would let you. MySQL listens on Port 3306 and you will probably have to configure your router to accept connections. It is wise to tunnel your remote connection using VPN or SSH. The people at /r/networks may be more suited for this question. 
its on a VPS that I rent I think I have it fixed by opening the port 
I'm not really understanding the design but it looks way too normalized. I'm sure school probably taught you to normalize everything to the nth degree but that is inefficient in the real world. Just my two cents.
so, how would you design something like that? And why is it inefficient? Honestly i tried to make it as simple as possible ( in my point of view ) 
The part that strikes me is all the child tables for cvss. It looks like each one is only one column and is a one to one ratio. Why are these split out from the parent table? For every row, you now store an additional column (the FK) plus the new column, as opposed to just rolling the new column into the parent table. Besides the storage aspect, think of your joins. If you have 1mil rows per table, that's a lot of reads in your query. If it was me, I would denormalize it a little. However, since it is for school, your design may be how they want it. Take my suggestions with a grain of salt - the best solution fits the requirements and not necessarily what works the best. Source: I teach SQL part time but work with SQL for years - highly normalized databases is what the textbooks always want, regardless of practical usage.
My apologies, the only context I have heard a bachelor thesis is in school. Your erd shows a one to one relationship on those tables, but shouldn't the parent table have a many relationship since one confidentiality ID could show up many times in the parent? Or will confidentiality be unique? And the same for the other columns(vector, etc.) If it is unique, I stand by having those child tables rolled into the parent for reasons mentioned above. If it is not unique then the ERD needs to indicate a many to one relationship.
For SQL server, use Database Tuning Advisor (comes with SQL), or also check out SQL Sentry plan explorer (free to use). These will give you index suggestions; I don't know of any tool that rewrites code to make it more optimized. Both of these tools give decent recommendations as far as indexes, the second really shows you where all the cost is so you can focus attention on possibly rewriting it for efficiency. 
Never mind! i m very happy about ur help :) Well actually i m trying to get a construction like this: Every dataset on the CVSS table has an id to the refered table. It seems like 1 to 1 is wrong ( thanks for your advice! ). But how can i change the relation, so that in fact every entry on the CVSS table has exactly ONE entry id, which referes to the childtable.
Thanks for the reply. I ended up highlighting the part of the procedure with issues (it was a loop) and then right clicking on it and selecting "Display Estimated Execution Plan". This recommended some additional indices and showed me where the bottleneck was for the most part. 
Try SQL Sentry plan explorer, they have a free version and it's quite useful.
Sorry I didn't get back to you, crap at checking my mail! I'm in the contract market and there seems to be plenty of Qlikview work with the NHS (are you a britbong?) especially if you have some familiarity with the BI stack and some SQL skills. Advice? Register with every agency and recruiter you can find, every job board, keep your LinkedIn up to date and repost your CV to all of the above every 3 months and you'll get lots of offers. In the meantime read around and see what area of data or dev work interests you, buy a book, read it and if you're still interested when you're finished you're onto a winner. I'm still pleasantly surprised that someone is willing to pay me for something I love to do. EDIT: As for second part of your question, if it falls out of favour, retrain as you go! I used to support Ingres, now long dead. Learned Oracle, got bored with staring at a HP-UX screen all day and the Windows version was shite so retrained in SQL Server which is much more interesting not to mention has a livelier community.
The estimated plan is just that - **estimated**. If you have outdated statistics or table-valued functions used in the query, you'll get a bad estimate. Execute the query &amp; have SSMS capture the **actual** execution plan (CTRL-M). Edit: If you're reading execution plans, get Grant Fritchey's [free eBook on them](https://www.simple-talk.com/books/sql-books/sql-server-execution-plans,-second-edition,-by-grant-fritchey/). Don't blindly trust the index recommendations from the DTA or the execution plan - you need to review them carefully and really understand your data &amp; queries to create the appropriate indexes. The recommendations aren't always right - an index that helps query A may hinder query B..
Good points, I agree. It is hard to explain how to add indexes based on the design, so for someone with limited knowledge, dta would probably get them decent results. But designing your own indexes based on knowledge of the tables would be the most efficient indexes. 
Thank you for the answer, please let me know if the plan below works as you suggest. 1. Full nightly (3AM) backups of all databases to E:\DB_Backups\. With a maintenance plan to clear out any .bak file older than 5 days. 2. Transaction log file backups running every 15 minutes from 5AM to 8PM. With a maintenance plan set to run at 2AM which would clean out all files from E:\DB_Backups\Logs\. This plan seems that it would allow me to run a full restore to any 3AM point for the 5 previous days, and would also allow restores down to 15 minutes of that current business day. Does that make sense?
Thats awesome! Thank you very much, you really helped me a lot. I do understand my mistakes and I m pretty confident that my mentor will agree to my improved work. The db pretty much does make sense for me by now.
That setup makes sense for what you want. Two questions, Why are you only doing log backups from 5 to 8? Do you have any maintenance tasks like index rebuilds, or data imports outside that window? If so, you want log backups running to prevent the tlog from filling. Even if it is low activity, your log backups would be tiny, so I don't see a reason to not have log backups around the clock. Second, are you only keeping one day of log backups due to space? Think of a scenario where someone deletes a table Friday afternoon and you don't find out until Monday? Your only restore of that table is Friday morning at 3AM. Would that be ok with you? 
I was mostly posting the example as a hypothetical situation, to boost my limited understanding. I'd like my production plan to do a few things: 1. Allow for a restore of the full database to the 3AM point for 60 days. (The .bak files are off-loaded to our backup platform via OSSV, and stored for 60 days) 2. Allow for 15 minute point restores for at least the current full business day, but would prefer it to be closer to 15 minute point restores to any point that week. So, to make that happen I would keep the Full Nightly backups the same, and would change my Transaction log backup plans to: 1. Transaction log backups begin after the full backup on Tuesday morning (3AM), and run every 15 minutes until 2:45 the next Tuesday morning. 2. Cleanup task will delete all files in the E:\DB_Backups\Logs\ at 2:45AM every Tuesday morning. This would allow me to restore to a 15 minute point for the last 7 days, and also would cover the situation you brought up about a mistake not being caught until Monday morning, right? Would it also keep the number of transaction log backup files under control, but would not break the LSN chain, since it would be restarted with the full backup? Also, should I be running differential database backups throughout the week, instead of the full? Sorry if all of this seems low level, I've not had a chance to ask anyone most of these questions.
Think of differential kind of like incremental backups in the sense that they are used to lessen backup and restore time. The big difference is they include everything since the last full backup, and are independent of other differentials. If you are fine with your backup and restore times now, then I don't see a need for differentials. As long as your tlog cleanup is set to 7 days in your example then it will do what you proposing. Edit: And for the cleanup of transaction log backups, I would tie that to your full backup subplan so it runs nightly and deletes anything over 7 days every time it runs instead of once a week.
&gt; I tossed primary keys (clustered index) on a lot of my major tables With a few exceptions, every table should have a PK. But not every PK should be clustered. It sounds like your schema may not be very well thought out, or maybe it's just been poorly implemented. Either way, indexes will only go so far in fixing those problems. And that's even more true when you're just "tossing" a PK on a table for the sake of having one - you need the *right* PK and indexes. Truly optimizing the queries is part science and part art. You can't outsmart the query optimizer, but you *can* give it the right information (in the form of indexes and how you construct the query) to produce a good execution plan. There really isn't any software that will do this for you; it's something that comes with knowledge and experience. Or, as I heard in one session at PASS Summit 2012: I can't describe to you what a bad query plan looks like, but I know it when I see it.
Thank you for this insight. I'm using MS SQL on SSMS 2008 v10 (Don't even get me started), so notes about backward compatability and full specification are greatly appriciated. 
Yeah, you could probably do this cheaper than we do it, we just set up some reasonably sized machines so we could simulate some of the more entertaining things that we encounter.
&gt;WHEN LEN(@theDate) = 6 And ISNUMERIC(@theDate)=1 THEN --assuming YYYYMM, parse out dbo.ufnDateSerial(LEFT(@theDate, 4), RIGHT(@theDate, 2), 1) Nothing "wrong" with this, but when you [check](http://use-the-index-luke.com/sql/where-clause/searching-for-ranges/like-performance-tuning) your [data](http://use-the-index-luke.com/sql/where-clause/functions/case-insensitive-search) like this, SQL can't use the index on it (I don't know if you use an index or how big of a .csv this is). Try @thedate like '[1-2][0-9][0-9][0-9][0-1][1-9]' this should check for "good dates" skipping numerics like 301413 (but allowing 201213). ~~Or try try_convert(datetime, @thedate + '01') instead of those substrings and see if you can get some more performance out of the query~~ Never mind doesn't work in 2008 &gt;The ufnDateSerial function I got from this post[1] , which seems like it should work. http://msdn.microsoft.com/en-us/library/hh213020.aspx Also doesn't work in SQL 2008, no chance for starting this app a newer environment? SQL 2014 is already out... ^sorry ^^I'm ^^^^really ^^^^^spoiled
union?
no- thats stupid
i feel like i want to do: from hazmat_codes (left outer join active_users on (active_users.[cost center ref] = hazmat_codes.cc and active_users.[position ref] = hazmat_codes.jc) ) OR (left join hazmat_uid on hazmat_uid.uid = Active_Users.[User ID]) 
I wouldn't necessarily call them a waste, if you have full backups older than the latest full backups transactions logs are still valuable. For example if you need a RPO of point in time for two weeks you could easily run Fulls every night and keep the transactions log two weeks. Also worth noting is if corruption happened before your last full backup and you didn't notice it, you're pretty much hooped without the previous transaction logs. 
http://www.reddit.com/r/SQL/wiki/index
A properly built SP will beat the piss out of a bunch of individual connections. Especially given how likely it is that a bunch of threads trying to the same thing at the same time will have lock contention. Make a proc that takes a list of IDs (how you do this depends on what db you're using) and do them in a batch rather than one at a time.
also w3schools
What are you goals?
Constraint fk1mytable foreign key (superheroid) references superhero 
Good question. I've been spinning my wheels in IT for a while now and I'm just trying to expand my skillset so as to open a few doors. I don't know anything about SQL except that it's everywhere.
Honestly, I started tinkering with it (hoping someone would correct where I'm wrong) but there are just things here that I have no clue what they are. Here's what I started with before just walking away. SELECT DISTINCT au.userid, hz.groupid LEFT JOIN hazmatuids hz ON hz.userid = au.userid LEFT JOIN hazmatcodes hz ON ha.groupid = hz.groupid WHERE au.userstatus = 'active' Full disclosure: You probably won't get accurate results. The provided information is just too jumbled and inconsistent. I have no clue what = what or where some of the elements of your queries came from. Not knocking you, just explaining myself. 
Can you give a data sample from your tables + what the result of 1 query would look like for that data?
There is a web service task in SSIS 2012+. You could also accomplish this in a script task. Security tokens are usually obtained through a call to some authentication method in the wsdl. You can import the wsdl in a script task as a service reference. This will take a bit of c# knowledge. You might want to pickup a book or ask an app developer to help you out.
I like to think of Full/Differential Backups as granular data recovery while Transaction logs are fluid data recovery. Edit - Also with SQL Server 2012 live page restores from t-logs.
Can you show me the table structure?
Database normalization May be a good start
Ewww
How about an update with a join on sql server vs Oracle
Agreed, my head would explode
What's the point of this
My advice assumes you are working with MS SQL Server, there could be material differences in how indexes and keys are handled using other engines. You would need to create two FOREIGN KEY CONSTRAINTS. One would be on SuperheroPower.Superhero (referencing Superhero.ID) and the other would be for the Power. They are totally independent of the Composite Primary Key you speak of. BUT, just to be clear, a Primary Key or even a Composite Key do not have to be unique. Often that happens to be the case, but it is in no way a requirement. For consistency and best practices, I would recommend changing the SuperheroPower table to something like this: CREATE TABLE SuperheroPower ( ID INT PRIMARY KEY IDENTITY(1,1) NOT NULL ,SuperheroID INT FOREIGN KEY REFERENCES Superhero(ID) ,PowerID INT FOREIGN KEY REFERENCES Power(ID) ) CONSTRAINT [UQ_SuperheroPower] UNIQUE NONCLUSTERED ( [SuperheroID], [PowerID] ) I'm not 100% sure of syntax, but this is my recommendation. The CONSTRAINT ensures the unique values you requested. By adding a separate ID column, it ensures that new rows are physically inserted at the end of the table so you don't get page fragmentation from inserts. Also, if you add columns and additional non-primary indexes later, the other indexes are likely to be much smaller and faster to create (all indexes reference the primary key, and a single int column is much smaller than a composite index). That's not so likely in a lookup table like this, but good to know of anyway. 
If you have foreign keys on the table it will need to check all of them. Another possible issue would be if the table is heavily indexed. Dropping and rebuilding May be better
Knowing any SQL is good. One thing you may want to decide on is whether you want to head down the DBA route or the reporting route. There is a lot of crossover in knowledge, but if you're looking for advice it's good to have an idea where you want to head. Good luck!
You should use concat operator as a "binding agent": SELECT staff_no, first_name || ' ' || last_name AS staff_name, street_address || ', ' || suburb || ', ' || city AS staff_address FROM Staff WHERE ... If you want to use brackets, close them properly: SELECT staff_no, (first_name || ' ' || last_name) AS staff_name, (street_address || ', ' || suburb || ', ' || city) AS staff_address FROM Staff WHERE ...
Ah, ok. So I pretty much have to give the SuperheroPower table an Id field? I was kind of thinking that I wouldn't have to; not that it's a big deal for me to add one. I guess I'll do this. Thanks for the explanation.
No, this guy is wrong on a couple points. Primary keys are always unique, be they simple or compound. You don't need the extra column for a pk. You set it up with compound pk like you originally planned then you add two fks that maps each column individually back to its pk table. Only the references side of the fk needs to point at a pk, the other side can be any column or set of columns on your referencing table as long as it fully expresses the pk on the referenced table. 
And it's ok to do it like that? I ask because according to marc_s in [this](http://stackoverflow.com/questions/3996774/foreign-key-relationship-with-composite-primary-keys-in-sql-server-2005) stack overflow answer: "you cannot create a FK relationship to only parts of the PK on the target table - just can't do it."
 CREATE TABLE SuperHero( SuperHeroId int identity(1,1) PRIMARY KEY, Name varchar(50) ) CREATE TABLE [Power]( PowerId int identity (1,1) Primary key, Name varchar(50) ) CREATE TABLE SuperHeroPower( SuperHeroId int FOREIGN KEY REFERENCES SuperHero(SuperHeroId), PowerId int FOREIGN KEY REFERENCES [Power](PowerId), PRIMARY KEY (SuperHeroId, PowerId) ) INSERT INTO SuperHero values('The Flash'),('Superman') INSERT INTO [Power] values ('Super Speed') INSERT INTO SuperHeroPower select h.SuperHeroId, p.PowerId from SuperHero h cross join [Power] p select * from SuperHero h inner join SuperHeroPower hp on h.SuperHeroId = hp.SuperHeroId inner join [Power] p on hp.PowerId = p.PowerId --DROP TABLE SuperHeroPower --DROP TABLE SUPERHERO --DROP TABLE [Power] 
I'd use the script task and do it in C#. WSDLs can be added as a Service Reference.
Yes, it's fine because the `SuperheroPower` table is referencing table in the FK relationship and `Superhero` and `Power` are the two referenced (or target) tables. You could not have the same situation with direction reversed because that would require targeting only part of the compound PK on `SuperheroPower`. In plain english your saying each record in `Superhero` must map to a unique record in `SuperheroPower` by the `SuperheroID` alone. Since you can't uniquely identify a single record in `SuperheroPower` with the `SuperheroID` alone, that statement doesn't make sense in the strictest sense.
yes you can http://stackoverflow.com/questions/11215494/composite-primary-key-foreign-key
sorry- i do that a lot tbl active_users has [user id], [cost center ref], and [position ref] tbl hazmatcodes has [cost center ref], [position ref], and [grpid] tbl hazmatuids has [user id], [grpid] i need to return [user id] and [grpid] where: (active_users.[cost center ref] = hazmat_codes.[cost center ref] and active_users.[position ref] = hazmat_codes.[position ref]) OR --not real sql here; full outer join perhaps? hazmatuids.[user id] = Active_Users.[User ID]
Great! Thanks for the explanation
 INSERT INTO [dbo].[hazmat_uid] ([uid] ,[grpid]) VALUES ('userA','grp1'),('userb','grp2'),('userC','grp3'),('userD','grp4') GO INSERT INTO [dbo].[hazmat_codes] ([cc] ,[jc] ,[grp]) VALUES ('cc1','jc1','grp1'),('cc1','jc2','grp2'),('cc2','jc1','grp2') GO INSERT INTO [dbo].[active_users] ([cost center ref], [position ref] , [user id] ) values ('cc1','jc1','bogart'),('cc2','jc1','humpfrey'),('cc1','jc2','jane'),('ccX','jcX','userA'),('ccY','jcY','userB'),'ccX','jcX','userC'),('ccV','jcV','userD') results: [user id],[grp] 'bogart','grp1' 'userA','grp1' 
Tossed wasn't the right word. I have the correct PK's on all of my tables that have distinct rows.
It's because of your ORDER BY clause. Since you are ordering by COUNTRY, and BRAZIL has 9 entries, it will show them all (there's a 9-way tie for 8th place on the list)
Is this implementation-specific? I don't think I've seen that behaviour in SQL Server; it just brings back the first X rows.
Thanks man!!! I got it working! You are the best!#%@%@%#
I think you are right about MSSQL always returning the top X, but I believe they deviate from the SQL standard in this case.
i think i was thinking wrong.. qry: SELECT grp, grpnm, Active_Users.[user id] FROM hazmat_codes INNER JOIN active_users ON hazmat_codes.cc = active_users.[cost center ref] AND hazmat_codes.jc = active_users.[position ref] LEFT join hazmat_uid ON active_users.[User ID] = hazmat_uid.uid where [user id] = 'usrx' results: 'grp','uid','user id' 'grp1','grp3','usrx' 'grp2','grp3','usrx' create temp table insert all matching from hazmatcodes from active user into temp table insert all matching from hazuids from active user into temp table
What's interesting is if I change it to SELECT TOP 10 * FROM Customers ORDER BY Country, City; it only returns one Rio de Janeiro. By its behavior with Country and returning ties, it should also return the City ties.
Glad you got it all figured out :)
What DBMS are you using? If you're using SQL Server you can load in Microsofts Adventureworks DB and play around with it, there's tons of exercises and such to help you learn using adventureworks out there. Otherwise practice by making a database of all your movies, or books, or friends and play around with normalization and querying.
Depends on the version and edition. At a minimum, you could download adventure works database and practice querying, do backups and restores, test out scripts, etc. SQL express is the free one but does not come with the agent to run jobs so you would need to use task scheduler. It also has limits on DR technologies, and a few other features, but for learning SQL, it should be fine. I recommend getting SSMS (free) which doesn't always download with express, depending on where you get it.
I'm using sql server 2014 at the moment. Will adventureworks work on that?
[Yup!](https://msftdbprodsamples.codeplex.com/releases/view/125550)
Reviewing everything you've provided... this is all over the place. As others have already mentioned, it is really hard to tell what you're asking mostly because of how you're presenting information. easiest way would be to provide data similar to the following... create table #active_users ( userid int, cost_center_ref varchar(10), position_ref varchar(10) ) create table #hazmatcodes ( cost_center_ref varchar(10), position_ref varchar(10), grpid int ) create table #hazmatuids ( userid int, grpid int ) insert into #active_users (userid, cost_center_ref, position_ref) values (1,'cc1','pr1') etc... Then give some example output values, but avoid confusing statements things like &gt; results: [user id],[grp] 'bogart','grp1' 'userA','grp1' I think you meant to say that you expect the following userid and grpid output: bogart, grp1 userA, grp1 consistency is king. &gt; i need to return [user id] and [grpid] where: If the above is true, then you likely just want the following... select b.grpid, a.userid from #active_users a join #hazmatcodes b on a.cost_center_ref = b.cost_center_ref and a.position_ref = b.position_ref union select b.grpid, a.userid from #active_users a join #hazmatuids b on a.userid = b.userid The union will handle all of your "OR" cases.
You can store data. Relationally. What can you do with data? Ummmmm... Well... That's a pretty large question.
Awesome. Thanks again. 
Just yesterday I sautéed up some and had it with green beans and a glass of red wine.
I had mine trampled on by green giraffes.
To add to this, you could easily run SQL Server on a low powered windows desktop. You would only need a few GB of space and very little memory unless you are simulating any type of real workload. If you are going with raspberry pi then go MySQL with phpMyadmin if you go the web route or MySQL Workbench.
Know any languages that you can use to create a UI that can hook into SQLServer? Even if you don't there are some very nice and free development environments, and quick tutorials that can get you up to speed quickly. I did this with Java, but there are many languages to choose from, all with pros and cons. Anywho, the Adventureworks db that others have mentioned coupled with a front-end language, and you can create your own applications. AKA, the sky is the limit.
I haven't been taught any languages to use with sql. I know that php and applications can be used with sql. I would've like to have learnt a language to integrated with sql, sql on it's own is a drag for me after 2 years of it. What would be the easiest way to integrate a database into an application?
Check out [this guy](https://buckysroom.org/videos.php?cat=31) for java tutorials. Once you know a bit of that, a quick google with tell you how to connect your newly created java interface with a SQLServer backend.
Select a.count, count(*) from [yourquery] a group by a.count Based on MSSQL not sure on MySQL but youll figure it out!
https://msftdbprodsamples.codeplex.com/releases/view/125550
For clarification, are you intending to run SQL on the Pi or just have it access SQL across your network? As someone else mentioned, the Pi won't run MS SQL Server, but there are other RDBMS systems that will. You might take a look at Postgres if that's what you're looking to do.
I think the ANSI standard is to use LIMIT 10, like SELECT * FROM Customers order by country limit 10; However, it seems [the w3school site](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_top&amp;ss=-1) doesn't support this statement. 
Postgres is to SQL Server as Debian is to Windows. It's another db server that runs on top of your Debian OS. It uses almost the same query language.
The database engine itself is running as a service or daemon depending on your environment. You need to use a client shell which is compatible with your DB. From the shell you can issue commands directly or create stored procedures with DDL (Data Definition Language) to build any and all database objects (tables, indexes, etc). As far as your second question I've seen it done mutliple ways. I would typcially store standard 7 digit zips as integers. In most of my experience I have seen telephone numbers stored as strings. Hope that helps!
Well, zips here in the USA are 5 digits, except where they are nine. If I specify integers, will it balk at hyphens if someone enters one, like: 34467-2489
I'm looking [here](http://www.w3schools.com/sql/sql_datatypes.asp) for the datatype "strings" but it's not listed. What am I missing (other than years of experience programming SQL)?
It depends on the platform, but usually varchar (var means variable length, regular char is fixed length) for small data sets. Tell us what platform you have and we can be more specific.
You don't *have* to. I would consider it a best practice. There's always exceptions. My general rule is to follow best practices unless you find a specific reason not to (and there are many). In this case, maybe you are just learning and so it doesn't matter a ton either way. But it's good to know the 'right' way to do it and the best way to learn is by practice. If this were a 'real' scenario, I would say there's still not necessarily one specific big reason to add the ID field, but there are lots of little ones and it sets you up better for the future that way. My main consideration is that you definitely want a primary key on your tables in almost all cases, but having a composite index as a primary key is expensive and this is an easy way to avoid that without causing other complications. Other times you still may want to use a composite index as a primary key, but you'd need to know why. Also, just making sure you know that you can add Unique Constraints that aren't the primary key.
If you are learning, there is a ton you can do with SQL Server Express, and since it sounds like you are new to it, it will probably take you a while to find something you can't do with it, that you could do with a dedicated server and expensive licensing. Create some tables, multiple databases, multiple instances, run queries across them, etc. If your PC isn't too shabby, you could also run a VM with Windows Server for free (some limitations, and you'll more or less want to start the whole thing over after 6 months) and that can be a useful experience in itself. You do NOT need a super powerful machine to be able to do this. Then you could run SSMS from your machine but connect to SQL Server on the VM which is a good way to learn as well.
Do zip codes as varchars, and split at 5 and the extra 4, so two variables, zip and plusFour... don't do them as ints, as the leading zero can be cumbersome. 
Good call regarding the zero. Thanks for that!
I'd recommend storing them as two fixed-length character fields, CHAR(5) or NCHAR(5) for the base zip code and CHAR(4) or NCHAR(4) for the extension. Your presentation logic can append them together with the hyphen, so you don't have to worry about how to store and parse values in a single field where part of the value is optional. For a phone number it's trickier because there's no standard, international format. Country code obviously is always a three digit number, but you might be better off storing the local number as a VARCHAR / NVARCHAR. Depending on what your app does, you might want to store the country and a formatting string in separate fields.
Replying so I can find this link later from my work laptop.
Will all data have the plus 4, or will it only be on some? And are you even going to use it? 
Also, do phone numbers as varchars too. 
Perhaps I won't worry about the optional and more specific 4 digits at the end of the zip code for now. I will have some time to add such features later. Perhaps I'll limit the input to 5 digits for now because I don't know for sure that its necessary.
Why save it as an int or number fields? Most data transformations you'd do are string functions not math. 
Think of the hyphens or parenthesis everyone adds to a phone number. Try to store things as ints if you need it for math functions, strings for string function. You are more likely to grab the area code from the string instead of subtracting numbers.
That will work in my sql also. Remember to name your columns in the sub query and assign it an alias
That's an excellent idea. I never thought of creating a virtual server. That would be another great project for me. Thanks. 
tl;dr - one removes duplicate entries, the other one doesn't
I was wondering why we need a whole blog post for something that can be explained in one sentence.
You'd have to type: SELECT TOP 10 WITH TIES * FROM Customers ORDER BY Country
before you store those phones you should strip out '(',')','-' and just store the 10 digits as varchar.
Same results but, I do: SELECT Zip5 + ISNULL('-'+Zip4,'') FROM table
Your observation is right: SQL Server (no matter how you format the execution plan) doesn't allow to distinct between unique and range scan (=seek in SQL Server) like Oracle does. Unfortunately. However, in fact Oracle is the only database that makes this distinction explicit in the execution plan. &gt; Why isn't that information showed? That I cannot answer :/ I wish all databases would.
Thanks for answer. I have another question though: &gt;However, in fact Oracle is the only database that makes this distinction explicit in the execution plan. But what about MySQL for example? Isn't the access type (that's either `const` or `ref`) showing that distinction? And PostgreSQL shows if it is either a Index Scan or a Bitmap Index Scan followed by a Bitmap Heap Scan. Aren't they making the distinction explicit?
You are right regarding MySQL, although it's `eq_ref`, not `ref`: http://use-the-index-luke.com/sql/explain-plan/mysql/operations Sometimes I need to check you my own site ;) Regarding PostgreSQL, it is not so: `Index Scan` can also be used if multiple entries match. The optimizer prefers to use `Bitmap Index+Heap Scan` only if there is a "pretty large"(tm) number of rows expected. It is definitively not that way that if there is a constraint that guarantees there is only one match, it takes `Index Scan` and otherwise `Bitmap Index+Heap Scan`. I repeat: it is **not** that way. 
That's quite true.
Good idea, since people will input them in different ways. What's the command for removing characters before storing? Let's see, it's not a NULL value, and the DELETE command is for deleting rows.
Thats' quite tidy.
Oh my. In this particular application, I don't believe I'll need to account for that possibility, but that's for letting me know.
There are a few ways to do it, but look at the replace function and also be sure to trim your white space. String functions: http://msdn.microsoft.com/en-us/library/ms181984.aspx
Awesome, thanks for the resource link!
I'd do @value = REPLACE(REPLACE(REPLACE(LTRIM(RTRIM(@input)),'-',''),'(',''),')','') You'd likely want to throw in more replace functions to strip other stuff like . and , and / and \. Depending on who's doing your input you could end up with some trash in there. edit: Thanks for the gold, it inspired me to fix the missing '' of the last replace.
Yep. Same results. I like to use coalesce because it's ANSI standard and because it can chain multiple values in a row if I want, but for this situation they're identical.
Why post this? 
Why that is nonsense? Isn't they work even if column names are different in two different result set e.g. emp_name and customer_name contains same data in example but have different column name. I am sorry, but just saying nonsense is not good enough, if you know something is wrong, put your argument against it. We all learn by mistakes and I am ready to correct myself If you give enough detail.
if you had said UNION operates on column names, that would also be dumb each SELECT in a UNION produces a result set consisting of -- wait for it -- rows and columns *of course* UNION operates on those columns
that web site you linked to does **not** describe "exactly what it's like" furthermore, it's a very poor quality web site example -- &gt; What is column? &gt; &gt; A column is a vertical entity in a table that contains all information associated with a specific field in a table. 
My query brings all the boys to the yard... and they're like... DECLARE VARCHAR
&gt;What is row? &gt;A row is a horizontal entity... (͡° ͜ʖ ͡°) 
talk to some real people once in a while.
Why'd I learn Linux? For the chicks
Damn right, it's better than CHAR.
The girl in the back is going in for a casual boob grab.
I'd teach you, but I'd have to DELETE * FROM *
I bet he's dropping objects left and right. dbo.panties
Dalam artikel SQL CREATE Steatment Syntax dan Contoh akan kita jabarkan mengenai syntax CREATE dan contohnya pada SQL. Setiap pekerjaan yang berhubungan dengan database pastinya kita harus mempunyai database untuk menyimpan tabel-tabel kita yang berisi data. Steatment CREATE inilah yang digunakan untuk membuat database maupun tabel. [Artikel Syntax CREATE Steatment](http://www.carangoding.com/in-sql/sql-create-steatment-syntax-dan-contoh.html) 
What about loading the CSV files into a staging table in SQL and query the table? Also, have you considered invoke-sqlcmd? I find that easier to connect to SQL especially if you want to loop through servers. Everytime I have done anything similar I load into SQL server before querying but my powershell skills are very beginner level. Edit: Also, you have your csv variable set before the path variable. Switch these and then write-host the CSV line to see what it is passing to SQL
MySQL 5.5.3 and using the MySQLi interface. PHP version 5.3.15
... just add a comma after staff_name and it should be grand :)
Point taken :)
I suspect the subselects are performance hogs. I'm not familiar with SQLite-specific features, so first let´s bring it to (more or less) ansi sql and eliminate the subselects. Either something's wrong in your explanation or I'm missing something, because the current where-clause doesn't make sense to me. Anyway, you may try something like below. select names.name, ips.ip, max(timestamp), servers.ip, servers.port, servers.description from sightings join ips on sightings.ip = ips.rowid join names on sightings.name = names.rowid join servers on sightings.server = servers.rowid where (names.name like "%:pix%" and ips.ip != '') or (names.name like "%:pix%") group by names.name , ips.ip , servers.ip , servers.port , servers.description order by count(sightings.name) desc limit 1000 How does that do? You may want to consider an index on sightings IP. *edit order by count(sightings.name) is not part of the select list! what if you change it to names.name? 
This actually takes longer and doesn't include results where ":pix" is not in the name, but was used by the IP before. That's what my nested select did: it looks up IPs that used the name and includes all sightings of that IP (grouped by the name-IP-combination) in the results. After your edit, the query still takes a lot longer than my version (possibly because it can cache mine, but not yours with the joining? not sure...). I now realize that my second condition (checking only for name) describes a subset of the first, so it can be omitted. Still, the nesting is needed to go from name to set of IPs and thus can not be eliminated I think.
":pix" is just an example, of course. I want to be able to look up whatever name I want. Also: SQLite doesn't support `select into` :/
JOINing on text/char fields is always going to be a performance hit. Your table design needs to be re-evaluated to make better use of integer based primary keys. Can you explain your problem domain a bit more and I can perhaps recommend a better DB schema for your needs. I've designed quite a lot of databases similar to this for collecting and reporting stats. Edit: My bad, I didn't see the join's were already on int fields. 
If you can pull the whole schema into a visual studio database project you can make use of their code analysis tools there. Keep in mind that SQL is not OOP, and as such duplication isn't always a sign of poor coding practices, sometimes it's exactly the opposite. DRY doesn't always equate to performant set-based queries. That said - looking back at my own code months or years after I wrote it I almost always find a good case for refactoring (what the hell was the guy who wrote THAT thinking... oh, it was me!).
I'm not an SQLite expert, but I think having a preceding wildcard in front of your name text search means the indexes can't effectively be used on that table (assuming you created them, and indexes on all joined fields). Try removing the preceding wildcard and see if it improves performance.
So, I collect information about what player is on what server. Servers consist of IP, port, description. Players means IP plus name. Since a lot of players have dynamic IPs, the same name will (over time) get associated with multiple IPs. I want to be able to look up things like: - all names used by a certain IP (I have a query for this, works well since it's very simple) - other names used by a player that used a specific name before - probably more things in the future, like usage of a name in relation to the time of day, etc. The database is filled by a collector program that scans all servers once a minute. For every player found on a server, a sighting is added (IP, name, server, timestamp). Sometimes players will use a different name once, then switch back to their "normal" name (still using the same IP). Then, when looking for aliases of the player by looking up their "normal" name, I need to first get all IPs that used the "normal" name, then look up all the names these IPs are associated with.
You could also try full text search indexes, I understand there's some add ons for SQLite to support that. Or, split clan tags into a separate field. Edit: another option may be to mirror the field with it reversed and build an index on that. If none of those work for you, then you may be stuck with the performance you have, as indexes generally work like a phone book where text is concerned.
No plans on searching on the server description field, but as the number of entries is very small I don't think it would be much of a problem to create an index on it later in case I need one. I haven't really considered switching the DBMS, because I chose SQLite for the portability and because I must say I like their SQL dialect, simple types, etc. Is 5 million really a lot already? I mean I only filter that huge table by ints which I select from far smaller tables.... I'll take a look into PostgreSQL I think.
No real-time data needed: - the database is filled by a collector program that scans all servers once a minute. - queries are performed by users via a website or an IRC bot. No worries, I'm not writing the FPS game or the backend or whatever (btw, it's http://sauerbraten.org/), it's just a hobby project to reveal fakers and pro players using an alias for now :).
Thanks, good feedback!! We are aware that reality is a tradeoff between a) maintainability/ readability/ DRY and b) performance considerations. As we have this significant amount of code, the exercise is to understand - is the quality of code good/ bad/ consistent? - does the choice of TSQL instead of OOP (and lack of separation between business &amp; data logic) for a large scale project inherently mean sacrifices on maintainability, even if the code quality is good? Aside from code duplication/ commenting/ naming conventions/ general readibility, what characteristics would you be looking out for, if you were in my shoes?
That's not a huge problem - just provide a search for starts with, and a separate one for ends with. Use the field/index combo with the normal name for the starts with, and remove the preceding wildcard, and the field/index combo reversed for the 'ends with', again removing any preceding wildcard. You could also tokenize the input allowing the user to place a wildcard into a single search field to determine the search type, and if they want to throw it in the middle for a 'contains' kind of search (most won't, I bet), then it'll be slow - but at least the other two would be quicker. If you're not going to use indexes with your RDBMS, regardless of which one it is, then there's not much point in using a DB at all over plain text. Grep would probably be faster.
One approach may be to analyze the effects of the code, rather than the code itself - something like SQL Sentry that monitors the database the code is executing against for instance. It's a very in-depth tool, but may require a DBA as an interpreter. There's also plenty of information the dynamic management views can show you - Glenn Barry has a great set of [scripts](http://sqlserverperformance.wordpress.com/2014/09/17/sql-server-diagnostic-information-queries-for-september-2014/) to leverage those. Again, an interpreter may be required. I'm fairly new to health checks and code analysis, but the biggest issues I have been finding are generally pretty well known: * Scalar functions (often implemented as a means of DRY) * Triggers (not always bad, but often abused) * Cursors (same as above) * non-SARGable predicates (forces full table scans) For a more comprehensive watch-list, check out Joe Sack's ['Common Query Tuning Problems and Solutions'](http://www.sqlskills.com/blogs/joe/). 
It appears there is an issue with the openquery function. It works fine when the sql is built and then encapsulated within an execute function but not when calling it directly. Issue resolved but not really solved.
&gt; have you considered invoke-sqlcmd? I find that easier to connect to SQL especially if you want to loop through servers. This could prove problematic if the `test` column in his input data ever contains a single quote. Malformed SQL/SQL injection possibility. `Invoke-SQLCmd` also has the downside of creating &amp; destroying the database connection for each iteration of the loop, whereas the `SqlClient` method used here can create a single connection before the loop and keep reusing it. Of course, the current method has those same two problems, and he really ought to be using a PreparedStatement (created outside the loop as well) instead of string concatenation for the query itself.
why would anyone rebuild an index in tempdb?
http://msdn.microsoft.com/en-us/library/ms188281.aspx
that's sorting while you rebuild indexes on real tables. there is no reason to rebuild an index on any table in tempdb. it's a temp area for sql to use
putting tempdb on SSD's has been shown to be faster. and when it grows it stays the same size like every other db and is full of white space if you can't buy SSD's, you can put it on a RAID1 volume of 15K SAS disks for max speed
Here is a table function that does this as a single set operation (assuming you have another table function that returns a list of consecutive numbers). It performs fairly well and I use it to process CSV parameters for reporting. ALTER FUNCTION dbo.[f_split_to_table] ( @String VARCHAR(MAX) , @Separator VARCHAR(10) ) RETURNS TABLE AS RETURN ( SELECT SUBSTRING(@String, n, CHARINDEX(@Separator, @String + @Separator,n) - n) val FROM dbo.f_nums(20000) WHERE SUBSTRING(@Separator + @String,n,LEN(@Separator)) = @Separator AND n &lt;= LEN(@String) AND LEN(@String) &gt; 0 ) 
I assume its a web app? How do you plan on handling if someone just closes their web browser? You'll do much better by tracking recent activity and making that into a distinct list.
This is an aside as someone who is still relatively new at building databases (and I mostly use SQLite). Is it better to make tables with only one column of data have no integer id primary key, and just use tableName(rowid) when referencing it? It's the first time I've seen that done and I'm wondering if there are any pros/cons to it.
This is not an uncommon request but it is something that is not easily solved. I recommend asking APEX questions on the [official forum](https://community.oracle.com/community/database/developer-tools/application_express). This particular question has no real answer other than whichever means is acceptable enough per your requirements. [Here is a great discussion on the topic](https://community.oracle.com/thread/3605045?start=0&amp;tstart=0). 
[So few upvotes since the last time it was posted](http://www.reddit.com/r/SQL/comments/2avrop/woah_sql_gets_the_girls/)
You may be able to find a vetted tutor on [wyzant.com](http://www.wyzant.com/). 
The biggest thing that helped me was getting a job as a regular data analyst many years ago. It was where i perfected my skills in excel then my next data analyst job actually used a real database (MS SQL Server) and my skills in excel and using that MSSQL really is where i learned the most sql. I watched a series on youtube by "the new boston" he has a great introductory tutorial style series on MySQL. The book are only a good freference if i know what im looking for. I learn by doing and if its for work i can spend much more time doing that. Also I have some good friends who are sql experts, they are a big help. If you want to learn it then start doing it. Always try to challenge yourself with something that is just outside your ability. This will keep you learning.
If you struggle to find a tutor, I will be happy to assist in any way I can. I am merely an amateur, but I find SQL quite easy to understand. Edit: I believe you are referring to stored procedures, am I correct?
Great - Thank you for the offer, I will definitely get back to you. I'm in the same boat, I was asked to do this because I find it very interesting as well.
Yes, this is 100% correct.
Ah. I'm not so hot there, but do have some paltry knowledge.
I take it you're doing this online/over Skype?
I do this for a living. Currently I'm a BI/visualization consultant and to be honest swamped with visualization stuff but feel free to PM me with any snags you are struggling to solve! It would also help to know what Analysis tool you will be using in addition to the underlying DB source (MS SQL Sever 2008 R2/2012/HP Vertica???). When you say you built the schema did you just create it from scratch, use a commonly known modeling method (like Kimball DataWarehouse design)? long story short is I'd need about a half hour - hour of chatting with you before i'd have a solid grasp on what you are trying to accomplish, needs of immediate deliverables and limitations of data source and relational structure. Otherwise I'm happy to take a stab at fixing a query you have working up to a point and offering advice since I don't really have the availability to give you 3 hours at one time. Just PM me with general questions or more details on specific issues.
You cannot use variables inside openquery. Dynamic SQL is your only way, but you will need to define your temp table prior to running. create table #tmpItemCoverage (itemnumber int, etc...) declare @sql varchar(4000) set @sql = ' insert into #tmpItemCoverage select pr.* from openquery(PROGRESS,''select pt_mstr.pt_part as ItemNumber, pt_mstr.pt_desc1 as ItemDescription, in_mstr.in_qty_oh qty_onhand, in_mstr.in_supp_consign_qty as Consignment, in_mstr.in_qty_oh as TotalInventory, mrp_det.mrp_qty as QTRDemandNotSummed, pt_mstr.pt_status as ItemStatus, pt_mstr.pt_pm_code as PMCode, mrp_det.mrp_qty as MRP_Qty, mrp_det.mrp_type as MRP_Type, mrp_det.mrp_due_date as MRP_DueDate, mrp_det.mrp_detail as MRP_Detail from MASTER.PUB.pt_mstr as pt_mstr inner join MASTER.PUB.in_mstr as in_mstr on pt_mstr.pt_part = in_mstr.in_part and pt_mstr.pt_domain = ''''US'''' and pt_mstr.pt_pm_code = ''''P'''' inner join MASTER.PUB.mrp_det as mrp_det on pt_mstr.pt_part = mrp_det.mrp_part and mrp_det.mrp_domain = pt_mstr.pt_domain AND ((mrp_det.mrp_type = ''''DEMAND'''')) and pt_mstr.pt_status = ''''active'''' where mrp_det.mrp_due_date &lt;= '''''+ @DatePlus90str + ''''''') as pr ' print(@sql) 
You cannot use variables inside openquery. Dynamic SQL is your only way, but you will need to define your temp table prior to running. create table #tmpItemCoverage (itemnumber int, etc...) declare @sql varchar(4000) set @sql = ' insert into #tmpItemCoverage select pr.* from openquery(PROGRESS,''select pt_mstr.pt_part as ItemNumber, pt_mstr.pt_desc1 as ItemDescription, in_mstr.in_qty_oh qty_onhand, in_mstr.in_supp_consign_qty as Consignment, in_mstr.in_qty_oh as TotalInventory, mrp_det.mrp_qty as QTRDemandNotSummed, pt_mstr.pt_status as ItemStatus, pt_mstr.pt_pm_code as PMCode, mrp_det.mrp_qty as MRP_Qty, mrp_det.mrp_type as MRP_Type, mrp_det.mrp_due_date as MRP_DueDate, mrp_det.mrp_detail as MRP_Detail from MASTER.PUB.pt_mstr as pt_mstr inner join MASTER.PUB.in_mstr as in_mstr on pt_mstr.pt_part = in_mstr.in_part and pt_mstr.pt_domain = ''''US'''' and pt_mstr.pt_pm_code = ''''P'''' inner join MASTER.PUB.mrp_det as mrp_det on pt_mstr.pt_part = mrp_det.mrp_part and mrp_det.mrp_domain = pt_mstr.pt_domain AND ((mrp_det.mrp_type = ''''DEMAND'''')) and pt_mstr.pt_status = ''''active'''' where mrp_det.mrp_due_date &lt;= '''''+ @DatePlus90str + ''''''') as pr ' print(@sql) 
Is not that they do not understand it. I find SQL boring, most of 95% of your statements are SELECT x FROM y WHERE something is q AND something is w and your scrip is done. You don't actually put brain matter into it, and mostly the SQL is not how well you know the SQL language is about how well you know how your database is structured. At least that is my issue with it and the work that I am doing.
Think up a project for yourself, say an organizational system for your books. Create a table structure for storing all the data about every book. Create procs to populate that table, functions to read from it. Add in a check in/check out system for different people that you lend those books to, thats another table. Then add a history table to track who checked out what and when, thats another proc. Build another proc that can control every aspect of your book system. Start pulling that data into Excel, create some pivot tables and charts to tell a story. You really need to start somewhere. Best to start small and build from that.
Yes
If it doesn't pan out with any other candidates feel free to hit me up. I'm a BI Admin/Dev at my company.
&gt; For example, since business group C and E use the same apps, they would be paired together in some higher level categorization I don't know what you are wanting with this.
Can we assume your table looks like this? App | Admin | Customer Service | Education | Finance | Human Resources | IT | Sales ---|---|----|----|----|----|----|--- Calc | 0 | 1 | 1 | 1 | 1 | 0 | 1 Notes | 1 | 1 | 0 | 1 | 0 | 1 | 1 Mail | 1 | 1 | 1 | 1 | 1 | 1 | 1 Contacts | 0 | 0 | 0 | 0 | 1 | 0 | 1 Music | 1 | 1 | 0 | 1 | 1 | 1 | 1 And that you're looking for an output table that has: CategorizedBusinessGroup|IncludedBusGroups|Apps --|--|-- All Apps|Sales|Calc,Notes,Mail,Contacts,Music Group 1|Admin,IT|Notes,Mail,Music Group 2|CustomerService,Finance|Calc,Notes,Mail,Music Group 3|HumanResources|Calc,Mail,Contacts,Music 
Not entirely sure why I did this but I guess I was interested in the problem :) Essentially the current structure of the table is pretty crazy. It means that every time you add a BusinessGroup you need to add a new column to the table. In the example below there is a slightly better format of the same table which I have called #AppBusinessGroup I have had to use dynamic SQL to convert from the original table format to this more practical format. In practice I don't know if that will be possible in your case as it depends on the names of the businessgroup columns having some consistent portion in order to find them (which I am guessing is not necessarily the case) This solution also relies heavily on the FOR XML trick for concatenating columns. Its pretty clunky but it gets the job done. --CODE: /*build the temp table to store the data in its current format*/ IF OBJECT_ID('tempdb..#AppBusinessGroups') IS NOT NULL DROP TABLE #AppBusinessGroups; GO CREATE TABLE #AppBusinessGroups( [AppName] [varchar](10) NOT NULL, [BusinessGroupA] [bit] NOT NULL, [BusinessGroupB] [bit] NOT NULL, [BusinessGroupC] [bit] NOT NULL, [BusinessGroupD] [bit] NOT NULL, [BusinessGroupE] [bit] NOT NULL ); GO /*fill the table with example data*/ INSERT INTO #AppBusinessGroups VALUES ('APP1',0,1,1,1,0) ,('APP2',0,1,0,1,0) ,('APP3',0,1,1,1,0) ,('APP4',1,1,1,1,1); GO /*build the table with the more standard format*/ IF OBJECT_ID('tempdb..#AppBusinessGroup') IS NOT NULL DROP TABLE #AppBusinessGroup; GO CREATE TABLE #AppBusinessGroup( [AppName] [varchar](10) NOT NULL, [BusinessGroup] [varchar](20) NOT NULL ); GO /*use dynamic sql to get the column names from the original table and turn them into entries in the new table*/ DECLARE @sql NVARCHAR(MAX); SELECT @sql = 'INSERT INTO #AppBusinessGroup ' + STUFF(( SELECT 'UNION SELECT appname,'''+column_name+''' BusinessGroup from #AppBusinessGroups where '+column_name+' = 1 ' FROM tempDB.information_schema.columns WHERE table_name LIKE '#AppBusinessGroups%' AND column_name LIKE 'BusinessGroup%' ORDER BY 1 FOR XML PATH('') ),1,6,''); EXEC sp_executesql @sql; GO /*show the new table format and data*/ SELECT * FROM #AppBusinessGroup; GO /*use the new table format to first find all the groups of apps for each business group*/ WITH AppBusinessGroups (BusinessGroup,Apps) AS( SELECT b.businessgroup ,STUFF(( SELECT ','+appname FROM #AppBusinessGroup WHERE businessgroup = b.businessgroup ORDER BY 1 FOR XML PATH('') ),1,1,'') apps FROM #AppBusinessGroup b GROUP BY b.businessgroup ) /* and then use that to find all the business groups with the same set of apps*/ SELECT STUFF(( SELECT ','+BusinessGroup FROM AppBusinessGroups WHERE Apps = a.Apps ORDER BY 1 FOR XML PATH('') ),1,1,'')BusinessGroups ,a.Apps FROM AppBusinessGroups a GROUP BY Apps; 
 AppName | BGa | BGb | BGc | BGd | BGe -------|---|---|---|---|--- App1 | 0 | 1 | 1 | 1 | 0 App2 | 0 | 1 | 0 | 1 | 0 App3 | 0 | 1 | 1 | 1 | 0 App4 | 1 | 1 | 1 | 1 | 1 returns BusinessGroups | Apps --------------|---- BusinessGroupB,BusinessGroupD | APP1,APP2,APP3,APP4 BusinessGroupC | APP1,APP3,APP4 BusinessGroupA,BusinessGroupE | APP4 
This is AMAZING. Thank you so much!
Thanks for all the responses! /u/extrajoss helped me out, and I managed to get it working! Albeit on a small scale. This is a great sub :)
Thanks for the compliment!
dm me. 
Feel free to shoot me any questions on queries. I'd be happy to solve them and explain, if you can provide a sql fiddle or can show me some basic structure and what your goal is. I am a database developer that works with Oracle and sql server daily. And i have used mysql for personal projects. 
Always account for it you don't know what you'll need in the future. Also if you store as a number you'll lose leading zeros. Edit: seems everyone else has touched on leading zeroes.
IMO, you have an unclear spec. I'd guess they want ORDER BY LastName, FirstName 
No Problem. Like I said, its a pretty clunky solution but it looks like this is really just a once off thing rather than a regularly run query so I figure its ok to be quick and dirty. And it got my brain started in the morning so its a win win :) Hope it all made sense. If you have any control over things at all though I would try and get the structure of the tables changed to something more manageable if you can.
ascending is implied/default, so don't put either (but do put a note that ASC is unnecessary). 
Use @@ROWCOUNT to track number of rows impacted. http://msdn.microsoft.com/en-us/library/ms187316.aspx
If you don't specify DESC then ASC is implied. I think what Mamertine means is you seem to be interpreting the question wrong. From what I read, they want it ordered by last name then by first name, ascending. There is no mention of descending order. Because ascending is implied you simply order by column names as he demonstrated.
`NULL` should not be in quotes. What you have is not the `NULL` value; it's the character string "NULL".
Where x &gt;= TO_DATE('10/01/2012,'mm/dd/yyyy' To_date tells it what format to expect the date string in.
omgosh i would have been running into that wall for a very long time :P thank you very much you've been the biggest help :D
Another option, equivalent to your original, would be Where order_date &gt;= sysdate - 730 Or, perhaps trunc(sysdate), if you're using full timestamps.
Awesome. Thanks!
Yes, thanks for clarifying my thought.
I disliked several of the defaults and changed them to my preference. I can't remember for certain whether your particular scenario was among them, nor exactly where I located the settings, but you should be able to change this fairly easily.
Wow this is exactly the equivalent of the example I gave. Thank you very much for that! Fantastic.
sql-ex.ru is a good tutorial spot if you are trying to learn T-SQL, but it is translated from Russian.
SQLite doesn't support a FROM clause in an update statement. You said update a set varA = b.varB ... but I'm pretty sure you meant to write update a set varB = b.varB ... To do *that* update, use update tA set varB = (select varB from tB where tB.varA = tA.varA);
Thanks for suggestion
I actually just made the move from help desk to data analyst within my company. I agree with most everything that has alreqdy been said but as someone who has just recently made this move ill put in my two cents. First thing I did and really the way I even became interested in SQL was the desire to redo our hardware asset database that prior to me converting it was nothing more than a big spreadsheet in access. What I did was make a copy of the database and inported it into a copy of sql server express I had installed on my local machine. At this point I was googling my way thru everything and the only person in my company that knew I was working on this was a buddy that worked on the help desk with me. This went on for a couple weeks working on it in my spare time and googling my way thru. Then a co worker who heard what I was working who was taking a vmware class at the local community college told me about a SQL Server course starting the next week. I paid around $600 for a 9 week course that covered primarily writing queries. About halfway through the course I eas more confident in my skills to tell more people about the project I had been working on. The real game changer is when I asked an amazing lady from our development team to take a look at my code. Now I feel this part is very important I went to my boss at the time and her boss and asked if it would be ok if me and her met once a week for her to mentor me and sort of code review (at this point my boss was fully onboard with me redoing the asset db). I say this was so important because I believe it showed I was a team player and knew how o handle red tape which their is a lot of in my company. To wrap this story up this arrangement went on for a few weeks till my amazing mentor went to her boss and asked him to bring me on the team. They.actually ended up creating a new position on the dev team for me. I was brought onto the team for a 90 day trial run if I wasnt happy I could go back to my old job and if my new boss wasn't happy he could put me back on the helpdesk. 3 weeks into it thr HR paperwork was filed and I was a data analyst 
730 is a bad choice because of leap years. You should use the add_months function. Where order_date &gt;= add_months(trunc(sysdate), -24) 
 select mygroupcolumns, sum1 = sum( case when mydate between range1start and range1end then RevenuePriceETC end), sum2 = sum( case when mydate between range2start and range2end then RevenuePriceETC end) from mySameTable where mydate between range1start and range1end or mydate between range2start and range2end group by mygroupcolumns 
That's a good way to look at it. I never thought of approaching it this way. I like the incorporation of using another function as well. Thank you!
Could you please clarify this part: *my above code is not working*
Sounds good.
That's because all you're probably doing is basic CRUD apps. There's a lot more to it than that if you really get deep into doing analysis, etc. with your data.
MSSQL **is** T-SQL - T-SQL is the flavor of SQL used by Microsoft SQL Server.
You are looking for the WITH clause. Like this: WITH old_date AS ( SELECT revenue, price FROM my_table WHERE date between &lt;begin_date_range - 30&gt; and &lt;end_date_range - 30&gt; ) SELECT my_table.revenue, my_table.price, old_date.revenue, old_date.price, FROM my_table INNER JOIN old_date on my_table.date = &lt;old_date.date + 30&gt; WHERE date between &lt;begin_date_range&gt; and &lt;end_date_range&gt;; Everything in &lt;&gt; might require some research to get the dates to calculate properly depending on what DB you're using. But this will get you down the right path to where you want to go.
I guess I've just never seen syntax like that. I was also thinking MySQL. My mistake I misread.
Thanks for the help mate.