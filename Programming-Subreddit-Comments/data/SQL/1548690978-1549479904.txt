Dude, ask someone for some help. You shouldn’t be performing updates if you don’t know how. You could screw the whole thing.
Thank you for the assistance. I am not running the query myself. It needs to be done, whether it will help or not, or the software vendor that is requesting the change will no longer help us with this support issue. 
If it's not hundreds of MB, just open it in a good text editor.
I am not performing this update and I am currently asking people for help. 
Yeah I tried opening it in notepad and it's too big. Is there a program allowing me to see a gigantic file?
hes not asking for a query. he wants to run statistics on the tables. usually statistics are run when there is some performance issue with some queries they do. usually statistics are run on 5%-10% of the data on tables. 100% will completely hog the DB and is completely unnecessary. by the way I will also personally fire anyone who goes to a DB and retrieves anything that a "client" asks. "clients" do not have direct select privileges on a DB and they shouldnt. 
It is the clients database of course he has privileges to it.
Notepad is the antithesis of "a good text editor" Get Notepad++, thank me later.
&gt; is the clients database of course he has privil Ok, I dont know your system and company and structure. anyway, statistics are to be ran by a DBA who knows what hes doing. 
How big is it? Notepad++ is the go because it's free
I actually like the format that they have setup for this. Too many Intro to SQL tutorials/etc. start by trying explain SELECT without even knowing what a table is or how to create one yourself and insert data. INSERT/UPDATE usually comes after explaining SELECT. Feel this is a huge oversight with most teaching of SQL. 
I'll look. I was also thinking about MySQL or sth.
Stop. Please, just stop. You're throwing random crap at the wall to see what sticks. Talk to your mentor. Ask them what you're meant to do. What platform. Do you put it on a local copy, or is there a server somewhere. If all they did was hand you a file and say "here, set up a database with this" with *no other instructions* they're doing you a disservice as an intern.
Ok yeah but no need to be rude
I am trying to think if this is the best solution for this problem. Analytic functions can be pretty heavy so I am just not sure if using a rolling sum is the most efficient method. However, I can't think of another way to solve a problem.
Don't use ScriptRunner, use StatementRunner. 
Great, thanks!
Why was I never taught this, that's so simple! Thanks a bunch. I've been looking for this answer on and off for weeks, I can't believe it's that simple 
I'm trying to help you not waste another 2 hours on this task that could easily have been avoided by a 10-minute conversation with your mentor.
Sorry, I guess I mean like someone to be able to physically check your data, not just a bunch of us telling you what to run without knowing what exactly what you need :) All good. Sorry for being vague and good luck!!
The SQL instance hung and we had to force the instance and SQL agent services to be stopped. We've been reviewing the the SQL Server Logs via SSMS and we can see a few different things in the logs, some of which might be the result of other things. We've been scouring the internet all afternoon, but not feeling very confident that we've identified the real cause of the problem yet. One of the main messages that sticks to mind is: "A timeout occurred while waiting for the buffer latch -- type 4 .... Not continuing to wait" 
Thanks lol. Not sure how you responded. I didn’t realize that was possible after I deleted the post.
What type of result set are you expecting back? If you are only wanting a count returned, you could do something like: SELECT count (*) FROM cases c JOIN assessments a ON c. case_id = a. case_id WHERE a.substantiated = true
Hi, we've set a limit on each of the three instances that exist on this server and left space for any operating system items as well. I think it's more likely that the instance hanging or freezing is the result of too many jobs and queries being run on the server, but I can't prove that right now. It seems strange that this has only just happened for the first time as we shouldn't have started doing anything differently today.
Thanks, I'll take a look at this. 
I do wish people would put tables in these questions because it would be easier to visualize. 1. Use row number and order by the substation field and only select the top one. Something like select count(\*) ,is\_substantiated\_flag from ( select case\_id ,is\_substantiated\_flag ,row\_number() over (partition by case\_id order by is\_substantiated\_flag) from tables ) a group by is\_substantiated\_flag &amp;#x200B; 2. Use the Min statement on the substation field &amp;#x200B; select is\_substantiated\_flag count(\*) ( select Min(is\_substantiated\_flag) as is\_substantiated\_flag - assuming 0 is substantiated ,case\_id from table group by case\_id ) a group by is\_substantiated\_flag
&gt; Hi, we've set a limit on each of the three instances that exist on this server and left space for any operating system items as well. Instance-stacking often causes resource contention problems. Have you set memory limits on each instance? How much have you left for the OS? &gt;I think it's more likely that the instance hanging or freezing is the result of too many jobs and queries being run on the server, but I can't prove that right now I doubt that. SQL Server can handle thousands of requests per second. Some things may get sluggish if you don't have enough resources, but it should never crash entirely. &gt; It seems strange that this has only just happened for the first time as we shouldn't have started doing anything differently today. *Something* changed. Someone started running a new/different query. Maintenance wasn't being done and it's caught up with you. You ran out of space. A patch was installed that hadn't been properly tested.
Can you post the rest of the error message for the buffer latch timeout. It should give a database I'd. How frequently do you see that error? I'm not sure if that is the cause but it definitely isn't good. Do you see any sql mini dumps in the sql error log folder? The timestamps would show you when the problem started.
I'm relatively new to SQL, but I would try something like this: &amp;#x200B; COUNT(DISTINCT(CASE WHEN 'Assessment_Status' = "Substantiated" THEN 'Case_ID' ELSE NULL END)) &amp;#x200B; Syntax may vary on your DBMS.
Assuming each case potentially has multiple assessments, or no assessments, consider this Select Count(*) as cases, Sum(case when ..... End) as subassessments From Cases c Left join assessments a On c.caseid =a.caseid And a.substant = 1 I've left the case statements up to you, but this should get you most of the way there. Depending on your experience level, you may need to recheck the left join behaviour to make sure you understand
Do you have query store enabled? You could check top resource consuming queries. Other than that check database dump and error log. If you suspect specific Server logins, you could set up Extended events to do some lightweight monitoring.
If all the records have 0 value except one, you could probably use max(send), max(host), max(upload_id). select id_video, titulo, desc, max(send) as "send", max(host) as "host", max(upload_id) as "upload_id) from TableInit groupy bt id_video, titulo, desc; 
So is u/Tville88 are you look to just count substantiated cases where it is true? Or do you to count substantiated cases first and then if there isn't a substantiated assessment then you count the unsubstantiated assessment? 
 statement0; begin try statement1; statement2; statement3; end try begin catch statement_a; statement_b; end catch statement_Z; So what happens, ms sql reads/executes statement0; sees begin TRY, sequentially reads and tries to execute statements inside the try block. If all of them succeed, ms sql skips the catch block and goes to execute statement Z If a statement in the try block fails, the sequence stops (if statement 2 failed, statement 3 is NOT executed at all), the errored statement is rolled back (and only that one) and ms sql goes to the catch block statement_a, statement_b, etc. If all of them succeed, ms sql goes to execute statementZ. If a statement in the catch block fails, the error is treated as if it happened immediately outside the try/catch block (right before the statementZ). all the other statements and states (transactions, loops, isolation levels, etc.) continue to function as they were before.
I am wondering if this might be an [XY Problem](https://en.wikipedia.org/wiki/XY_problem). You say you want * A Count of the the assessments that lead to a case (of which the maximum seems to be 2) * There can can be two type of assessments that lead to a case (and only if there are both types would the count ever be 2.) * You want one substantiated case to trump a count of unsubstantiated cases. (meaning the count can only ever be one) My question is: * What do you expect the result set to look like. * Can there be more than 1 or each type of assessment?
**XY problem** The XY problem is a communication problem encountered in help desk and similar situations in which the real issue ("X") of the person asking for help is obscured, because instead of asking directly about issue X, they ask how to solve a secondary issue ("Y") which they believe will allow them to resolve issue X. However, resolving issue Y often does not resolve issue X, or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions. The XY problem is commonly encountered in technical support or customer service environments where the end user has attempted to solve the problem on their own, and misunderstands the real nature of the problem, believing that their real problem X has already been solved, except for some small detail Y in their solution. The inability of the support personnel to resolve their real problem or to understand the nature of their enquiry may cause the end user to become frustrated. The situation can make itself clear if the end user asks about some seemingly inane detail which is disconnected from any useful end goal. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
this happens because you put your 'set' statement into the try block. If 'insert into tabDashboard' fails, the set command is skipped altogether and the catch block is executed instead. And it works similarly the other way too - if both insert into and set statements successfully complete, the insert into tabCalculations wont be executed 
Do you know of code academy gives you anything similar? 
is PERCENTILE_DISC not working for you for some reason?
&gt; PERCENTILE_DISC Would this work if the data is already aggregated. So instead of having a table like: Age ID 13 1 13 2 13 3 13 4 14 5 14 6 You have a table that looks like this Age Total 13 4 14 2 And no way to get the raw data 
What is wrong with querying a production db from Excel? Is it a bad idea to do this with a custom connection string or just through an Excel data connection?
Should I be putting a SET inside both blocks, or outside the blocks? I'll play with it. I haven't had any time in the last few days to play with this more than thinking.
I would assume, yes. But only for the paid ones. I will find out soon since I’ve already finished another one for SQL. I’m just going through the free ones that I can find. 
IMO, Excel means the company is either unwilling to pay for power bi or they're stuck in the stone age. I get why people use Excel, but it's often used for enterprise reporting and that's a shame. It's not that the query is less performant when coming from Excel, it's that they're using Excel. I have the same elitist attitude towards access. I'm a snob who wants to force others to use the best tool for the job. I'm of the school of thought that only the production application should be hitting the production box. This isn't possible at small places with a limited server budget. 
So right now I have "nested" commit transactions. For example my "parent" process is named something like SLA.001_Global AS BEGIN TRANSACTION EXEC SLA.002_name EXEC SLA.003_name COMMIT TRANSACTION SLA.002_NAME AS BEGIN TRANSACTION EXEC SLA.007_name EXEC SLA.008_name COMMIT TRANSACTION So what currently happens if any step fails, the entire process rolls back like it had never ran. I have realized this is not a great set up but my "process" is divided into two main parts. 1. Truncating my staging tables, and populating them with new data. 2. Executing dynamic SQL queries. If any step in the first part fails, then the process should roll back like nothing happened. The only way these processes could fail is if something in my parent tables changes -- and this needs to be fixed. On the other hand, my dynamic SQL queries are their own "library". I might have 900 queries being assembled (so a total of 900 loops.) If only one of those queries fail, I want the other 899 to execute and the "parent" process to commit. I also want that (1) loop to be exported to a "logging" table so I can pull it out and determine why this is happening. 99% of the time it's because a cell value is misspelled, or something stupid.
Makes sense. I'm working on a project to create dashboards for our data (some of which is in SQL Server). We don't host our own Power BI setup, so the idea is to write a custom application. In the meantime, I'm prototyping in Excel - which is really fast compared to Power BI, especially with regard to formatting... I also don't have to worry about creating custom visuals to do what I need (ex. plotting multiple series with different x-data on a scatter, for example). 
[removed]
Lol @ you getting downvoted. I don’t understand the reasoning behind downvoting this answer unless it’s the vendor responsible for this mess.
Looks like a spam article trying to sell a product
Do you have to pay for the month-to-month subscription on this? Sorry never done a humble bundle before. Nice find though!
No, thia is a one time payment. You can pay what you want, but there are certain thresholds where paying more unlocks more books. (There is a monthly humble bundle scheme which works a slightly different way)
Thank you. Just realized it was ebooks though. Wanted some more books for my bookshelf :)
You haven't told us what platform you're using. Some have a super magical way of doing this: just add an over (order by) to your sum. Personally I'd build an actual date, but that's optional: SELECT t1.reference, t1.NAME, t1.day, t1.month, t1.year, datefromparts(t1.year, t1.month,t1.day) date t1.action, t1.object, sum(CASE WHEN t1.action = 'Sell' THEN -t1.quantity ELSE t1.quantity as newquantity END ) OVER (ORDER BY datefromparts(t1.year, t1.month,t1.day)) FROM table1 t1 WHERE t1.object &lt;&gt; 'Banana' GROUP BY t1.reference, t1.NAME, t1.day, t1.month, t1.year, datefromparts(t1.year, t1.month,t1.day) t1.action, t1.object, t1.quantity ORDER BY t1.NAME, t1.object, datefromparts(t1.year, t1.month,t1.day) 
&amp;#x200B;
I was able to figure it out using multiple AND statements Thanks for reaching out! &amp;#x200B;
This quiz was before we had learned the JOIN query, but now I have figured it out. I ironically I just started using w3schools to help with sql :)
Thanks everyone. I solved it by setting up 3 CTEs. The first one included everything in the table, the second one selected case ID and case when assessment status = 'substantiated' then 2 else case when assessment status = 'unsubstantiated' then 1 as sub. The third cte selected distinct case id, max (sub) as assessmenttocase. Then I left join the third cte on the first and do a distinct count of case id with assessmenttocase=2, and I do a distinct count of case id with assessmenttocase =1
Thanks!
Most of these if not all of them are free in the library and the library apps hoopla and over drive. 
Dodgy. After following OP's link, you will proceed through 2 further links &amp; 1 forced redirect before landing on a hype page trying to sell you lots of shit for "*just $1 more*"
Direct link: https://www.humblebundle.com/books/programming-cookbooks?partner=domabrat
first of all, you don't need to group secondly, i prefer using a derived table because you're going to reference its results twice i use a CROSS JOIN because the single row from the derived table will be joined to ever row of the invoice table SELECT invoice.inv_num , invoice.inv_amount , derived_table.avg_amount , derived_table.avg_amount - invoice.inv_amount AS diff FROM invoice CROSS JOIN ( SELECT AVG(inv_amount) AS avg_amount FROM invoice ) AS derived_table let me know if there's anything you don't understand
Just go directly to he humble bundle website. 
Well for starters, UPDATE TableToBeUpdated SET col1 = (SELECT TOP 1 col1 FROM #TMP) You've just altered every record in TableToBeUpdated, not just the one you want. I've never seen this approach for single-record edits. I could see the value if you were doing a lot of records at once and you had some tricky calculation to do on it and wanted to do it in a separate transaction to avoid locking the record(s) for the whole duration (but even then you'd have to consider the implications of the data changing between putting the records in a temp table and updating the original). I don't understand the problem you're trying to solve by copying the record, editing the copy and updating the original to match the edited copy. Why not just edit the record?
I just picked this up. Note that of the two books I've dug into a bit, both are from 2013. The SQL book looks useful but possibly a bit dated, it has recipes for: * DB2 v.8 * Oracle Database 10g * PostgreSQL 8 * SQL Server 2005 * MySQL 5 
Thank you so much, I really appreciate it. I will share your wisdom with my group discussion for this class (we were all pretty confused). &amp;#x200B; &amp;#x200B;
What's the advantage of this process over: update TableToBeUpdated set col1 = @col1 where col1 &lt;&gt; @col1 Like /u/fauxmosexual said, you're doing a lot of work copying things around just to do a simple update of a record.
yes, i omitted a where clause from the example, i was trying to use semi-realistic pseudocode... the problem i'm trying to solve by copying the record is to save overhead - fewer UPDATE trips to the table. Instead I make the same number to a #tmp table but I was figuring that no-one else would be doing the same thing, so it would be easier on the db. is this not the case? this made more sense in my head when i wrote this initially as a table variable rather than a temp table. all thoughts are gratefully received! 
i answered him in his question ... 
Start applying. If you can write SQL you've got the main criteria done. It's as good as any other path. Assuming you're in the Microsoft world, try to learn ssis. It's a pain to use, but most places use it. Also try to learn ssrs and ssas. Not that as a bi developer you'll use either. I'm a bi developer. I've never done ssrs it ssas. Both would have opened other doors for me in my career path. I'm happy where I'm at now. Keep an open mind and try to learn as much as you can. Read other people's code and lookup stuff you don't understand. It'll make you a stronger developer.
You're "saving overhead" by creating additional overhead. &gt; i wrote this initially as a table variable rather than a temp table. Table variables aren't as good as some people would have you believe. They have their uses, but generally you'll be better off with a temp table.
Thank you!! This works perfectly!
then Sir I thank you for putting me straight.
I mean, it's humble bundle from the screen my app is showing. So I'd just go check it out directly and ignore a Twitter link. They had a decent one a bit ago with automate the boring stuff. This one looks more textbook focused 
Thank you appreciate the response
There's nothing suspicious about humble bundle. Bought plenty of good stuff before. The link chain is shitty, but the end page is not.
One other thought. A lot of positions will ask for x years experience. Apply anyways. SQL developers seem to be in short supply. If you're at a larger company, ask around see if you can work with the bi developers. Ask questions.
You mean that the parameter could accept any number of @col1, @col2, @col3 arguments? Even doing this the way you're suggesting I don't see that some amount of dynamic SQL is avoidable: you're still going to have to query sys tables to dynamically the actual column names. And if you're going down the dynamic SQL route anyway, you might as well build a single statement that updates all the affected columns at once and apply it to the actual record.
If I were doing this in SQL Server, I would look up "running total window functions SQL server" on Google. 
Yeah, it's quirky like that. 
https://youtu.be/bEtnYWuo2Bw This guy's series was awesome for me.
https://www.microsoft.com/handsonlabs/selfpacedlabs Free labs with virtual machines that run right on your browser. Great for learning SSIS, PowerBI, etc.
https://www.microsoft.com/handsonlabs/selfpacedlabs Free labs with virtual machines that run right on your browser. Great for learning SSIS, PowerBI, etc.
Hi! The first thing you want to do is join the two tables. Then, you will want a WHERE clause specific to finding the "Default". Finally, use a GROUP BY to group to the account level. Your select will be the account number and a COUNT(\*). If you still have troubles putting it together paste what you have and I can provide some more guidance.
Yeah, typically you would check and sanitise all inputs, to ensure that it cant possibly produce invalid SQL. This is not always easy, but then dynamic SQL is never trivial to begin with. Still, it can be done, and its a benefit you are already using a stored proc (so can be amended on the fly rather than having to recompile software or redeploy middleware/web) 
I understand. Naturally its worthwhile making sure it wont fail, but equally important is that if it does fail, you are notified about it (and given full details of the query). Don't neglect working on that, as it can be invaluable and often means any bug can be rectified in minutes instead of trying to isolate it over several weeks. I haven't used SQL server in a long time now, but I kinda remember a condition whereby if you changed the schema of a view, any stored procedure which referenced that view had to be force-recompiled (else it could end up referencing the wrong columns). Maybe it's been fixed now, but if not, you should have a plan in place for this too (as in re-run the ALTER PROCEDURE statement whenever the underlying schema changes). 
Let me give you a basic architectural overview, and then please give me your most brutal feedback. 1. We have dozens of global clients, that use multiple VMS tools, and may use the same tool differently. Much of this "raw" data is being sucked up into a database through an ETL process and then dumped out onto tables. Important to note that if you want to identify something simple it might be `where col1 = n` but for another client it might be `where col2 = n`, or even more convoluted such as `where col1 like %n1%.' Essentially what I'm trying to illustrate is that each client might have their own nuances, and therefore would require their own "query" in order to report on some "metric." So some metrics apply to all clients, others apply to only some, etc. All of this needs to be maintained and the results need to be pushed into a table that can be consumed by Tableau for reporting purposes. My solution was to use dynamic SQL to assemble queries in such a way that 9 stored procedures can handle several hundred clients, where the parameters for the dynamic SQL are being imported from tables that are being populated by a separate ETL job based on an Excel sheet that I maintain.
&gt; You have no idea what you are talking about Funny, I've been a software developer for over 25 years, and a DBA for the past 15. I also have the full gamut of MS SQL Server and DBA certs, aswell as a degree in software engineering and heaps of experience. &gt; If you attend schools like New Horizons Computer learning Centers they charge on average of $1700-$2500/Class. Like I said, you are paying for the training, which is expensive. The actual exams themselves are not expensive to sit. In UK you can take any of the MS exams for a paltry £75 ($100). 
Ok, wasn't sure on your level of technical expertise, but you seem to know what you're doing. &gt; each client might have their own nuances, and therefore would require their own "query" in order to report on some "metric." Is there any way you could abstract these differences into some sort of lookup table, so that each client has their quirks modelled in the database, and you could get to it via some joins? I've dealt with similar highly dynamic systems in the past myself, so no doubt I am simplifying things. But abstracting as much as possible into the database can help a lot in the long run. So instead of it being col1 = 'foo', or col2 = 'foo' Its more like (pseudocode)... FROM clients INNER JOIN attribs ON (clients.clientid = attribs.client_id and attribs.key = &lt;col1 field identifier&gt;) WHERE attribs.value LIKE '%foobar%' and clients.clientid = 436578 
Probably not. If the tables have VERY few records I think it's possible that it might be marginally faster to do one distinct on the end result instead of each of the tables individually: SELECT DISTINCT * FROM (SELECT c1 FROM table) c1, (SELECT c2 FROM table) c2, (SELECT c3 FROM table) c3, (SELECT c4 FROM table) c4
How relavent is the SQL one to Ms SQL 2012+?
This is a followup to a 3 week old [thread \(link\)](https://www.reddit.com/r/SQL/comments/adk2kr/any_serious_mistakes_or_concerns_in_regards_to/). I took the feedback to heart and got rid of almost all the circular references except one. This is a personal database used to keep track of cars and ads that interest me, for the next 2 years. 
It’s not a risk if you design and build it right. 
The SQL cookbook is great for people who are decent at SQL and want to get better.
Have you tried using ROW_NUMBER?
I was an Analyst and moved into a BI Software Developer role a few years ago. If your current company has a BI group you would like to work with, get to know the manager and express your interest in changing roles. Ask for advice of what you can do to improve your skills. As an Analyst you will have a ton of business knowledge that many Developers will not have. That knowledge and proving your ability and desire to learn the software side will help move into a junior developer role. From there you can get the experience needed to move to another company or group when you are ready. Also check out your area for meet up groups and find out what software stack other companies are using. 
&gt; Is there any way you could abstract these differences into some sort of lookup table, so that each client has all their quirks modelled in the database, and you could get to it generically via some joins? I don't really understand this question, but I think that is what I have built out. I have a series of mapping tables that all join together to create a query. I think I understand your pseudo code in the sense to assemble the variables and check to see if they exist, and if so then to run them.
How? I am not sure how I would implement that? I am just not sure where it would fit. Do you mean un roll the data so that it isn't aggregated anymore?
This is great, thanks!
Try saving every combination of the 3 positions into an objects table. You can there filter on any two position to answer the question, "what objects can I make with Block x at position a and block y at position b"? This makes a few assumptions: &amp;#x200B; 1. A Block can only ever fill a single position (because of the Blocks.POSITION relationship) 2. A Block can combine with every other other block within the constraints of #1 **Sample schema** `CREATE TABLE Blocks (` `BLOCK_ID INT,` `POSITION INT,` `B_NAME VARCHAR(45)` `);` &amp;#x200B; `INSERT INTO Blocks VALUES` `(1, 1, 'Block1Pos1'),` `(2, 1, 'Block2Pos1'),` `(3, 1, 'Block3Pos1'),` `(4, 1, 'Block4Pos1'),` `(5, 1, 'Block5Pos1'),` `(6, 1, 'Block6Pos1'),` `(7, 1, 'Block7Pos1'),` `(8, 1, 'Block8Pos1'),` `(9, 1, 'Block9Pos1'),` `(10, 1, 'Block10Pos1'),` `(11, 2, 'Block11Pos2'),` `(12, 2, 'Block12Pos2'),` `(13, 2, 'Block13Pos2'),` `(14, 2, 'Block14Pos2'),` `(15, 2, 'Block15Pos2'),` `(16, 2, 'Block16Pos2'),` `(17, 2, 'Block17Pos2'),` `(18, 2, 'Block18Pos2'),` `(19, 2, 'Block19Pos2'),` `(20, 2, 'Block20Pos2'),` `(21, 3, 'Block21Pos3'),` `(22, 3, 'Block22Pos3'),` `(23, 3, 'Block23Pos3'),` `(24, 3, 'Block24Pos3'),` `(25, 3, 'Block25Pos3'),` `(26, 3, 'Block26Pos3'),` `(27, 3, 'Block27Pos3'),` `(28, 3, 'Block28Pos3'),` `(29, 3, 'Block29Pos3'),` `(30, 3, 'Block30Pos3')` `;` &amp;#x200B; `CREATE TABLE Objects (` `Pos1 INT,` `Pos2 INT,` `Pos3 INT` `);` &amp;#x200B; `INSERT INTO Objects` `SELECT` `p1.BLOCK_ID Pos1,` `p2.BLOCK_ID Pos2,` `p3.BLOCK_ID Pos3` `FROM` `Blocks p1` `CROSS JOIN Blocks p2` `CROSS JOIN Blocks p3` `WHERE` `p1.position = 1` `AND p2.position = 2` `AND p3.position = 3;` &amp;#x200B; **Which objects can be made with blocks 5 and 12?** `SELECT` `p1.B_NAME Pos1,` `p2.B_NAME Pos2,` `p3.B_NAME Pos3` `FROM` `Objects o` `JOIN Blocks p1 ON o.POS1 = p1.BLOCK_ID` `JOIN Blocks p2 ON o.POS2 = p2.BLOCK_ID` `JOIN Blocks p3 ON o.POS3 = p3.BLOCK_ID` `WHERE` `p1.BLOCK_ID = 5` `AND p2.BLOCK_ID = 12`
I've written a solution below so you can skip to that if you want. Pretty much right now you are missing some data so you're going to need additional tables besides the one you currently have. if you look at the start date, end date, ID table, you will notice it only have 1/1, 1/3, 1/4, and 1/6. You're missing 1/2, 1/5. I made a temporary table called dates below to store these dates. What you'll notice in the output, is an ID shows up when the date is postgres between the start date and end date (inclusive). As such, with the dates table below, you can join on this condition and you'll be WITH dates AS ( SELECT '2019-01-01'::DATE AS DATE UNION ALL SELECT '2019-01-02'::DATE AS DATE UNION ALL SELECT '2019-01-03'::DATE AS DATE UNION ALL SELECT '2019-01-04'::DATE AS DATE UNION ALL SELECT '2019-01-05'::DATE AS DATE UNION ALL SELECT '2019-01-06'::DATE AS DATE ) , start_end_date AS ( SELECT '2019-01-01'::DATE AS start_date, '2019-01-04'::DATE AS end_date, 1 as ID UNION ALL SELECT '2019-01-03'::DATE AS start_date, '2019-01-06'::DATE AS end_date, 2 as ID ) SELECT dates.date , start_end_date.start_date , start_end_date.end_date , start_end_date.id FROM dates JOIN start_end_date ON dates.date BETWEEN start_end_date.start_date AND start_end_date.end_date ORDER BY dates.date, start_end_date.start_date Feel free to drop this into postgres and figure out what you're working with. 
If you can use tsql, you could do a fancy cross apply, but not sure if that runs in postgres
Well that was actually very simple. I didn't know you could use the between function that way. Thank you for the help!
SOUNDEX distills every string into 4 tokens and then compares those. Maybe it was useful when it was first invented (early versions were patented around 1920!), but I've yet to find any circumstances where it has given acceptable results in a computing context
I second that. Great resource thank you.
Linkedin Learning may also have some decent classes. 
Wow thank you so much for sharing. This is amazing.
Thanks for your thoughts! The main concern with saving every combination is the total volume. In my example it's fine but in our Production environment we'll have up to 10,000 blocks in each position, and we don't care about every combination. We only really care about all the combinations stemming from specific initial pairs of blocks. If I (redundantly) store the position in the dualblocks table it will make the SQL easier, but I'm hesitant to do that. Just exploring if there is a better way than redundant subqueries or data storage. 
You could save the open position in dualblocks: `CREATE TABLE DualBlocks (` `ID INT,` `BLOCK_ID_1 INT,` `BLOCK_ID_2 INT,` `OPEN_POSITION INT` `);` Then query like so: `SELECT` `p3.BLOCK_ID AS 'Block 3 ID',` `p3.B_NAME AS 'Block 3 Name',` `p3.POSITION AS 'Block 3 Position'` `FROM` `DualBlocks db` `JOIN Blocks p1 ON db.BLOCK_ID_1 = p1.BLOCK_ID` `JOIN Blocks p2 ON db.BLOCK_ID_2 = p2.BLOCK_ID` `JOIN Blocks p3 ON p3.POSITION = db.OPEN_POSITION` `WHERE` [`db.ID`](https://db.ID) `= 1` If you don't need to return columns from the blocks in the dualblock, you could simplify: `SELECT` `p1.BLOCK_ID AS 'Block 3 ID',` `p1.B_NAME AS 'Block 3 Name',` `p1.POSITION AS 'Block 3 Position'` `FROM` `DualBlocks db` `JOIN Blocks p1 ON p1.POSITION = db.OPEN_POSITION` `WHERE` [`db.ID`](https://db.ID) `= 1`
You could save the open position in dualblocks: &amp;#x200B; `CREATE TABLE DualBlocks (` `ID INT,` `BLOCK_ID_1 INT,` `BLOCK_ID_2 INT,` `OPEN_POSITION INT` `);` &amp;#x200B; Then query like so: &amp;#x200B; `SELECT` `p3.BLOCK_ID AS 'Block 3 ID',` `p3.B_NAME AS 'Block 3 Name',` `p3.POSITION AS 'Block 3 Position'` `FROM` `DualBlocks db` `JOIN Blocks p1 ON db.BLOCK_ID_1 = p1.BLOCK_ID` `JOIN Blocks p2 ON db.BLOCK_ID_2 = p2.BLOCK_ID` `JOIN Blocks p3 ON p3.POSITION = db.OPEN_POSITION` `WHERE` [`db.ID`](https://db.ID) `= 1` &amp;#x200B; If you don't need to return columns from the blocks in the dualblock, you could simplify: &amp;#x200B; `SELECT` `p1.BLOCK_ID AS 'Block 3 ID',` `p1.B_NAME AS 'Block 3 Name',` `p1.POSITION AS 'Block 3 Position'` `FROM` `DualBlocks db` `JOIN Blocks p1 ON p1.POSITION = db.OPEN_POSITION` `WHERE` [`db.ID`](https://db.ID) `= 1`
If you're only using SSIS won't that make you an ETL developer? I'm just curious because usually in my projects it's either full MS stack or completely different approach like Qlik.
I'm... Not seeing any circular references here. Aside from some of the fk to the intersection tables being diagrammed as not nullable, I don't see any major issues with the layout. I'd wonder why the interior dimensions are in their own table rather than attributes of the model entity, particularly since they're unlikely to be shared widely across multiple models, but that's relatively minor. What particular part are you concerned about exactly?
Starting to delve into this guidance, thanks very much for this.
I wouldn’t say this approach is either wrong or right - it’s just another way of storing data. It does mean that there is more data to maintain and store, and if your tables are normalised then there would be no need for repeated data. 
Thanks for your reply. When you say this: &gt; It does mean that there is more data to maintain and store, and if your tables are normalised then there would be no need for repeated data. Do you mean if my tables are properly normalised, going the longer route (Complaints -&gt; order_id -&gt; customer_id -&gt; Customer) makes more sense?
I'm not incredibly well versed in this, so take it with a grain of salt, but your "shortcut" design seems like it violates 3NF. The complaint is tied to the order, not the customer, so the customer ID isn't relevant to the complaint. Additionally, maybe it's not always the customer making the complaint? If your business isn't exclusively self-consumption based it could be the recipient of the order that's making contact. Another approach might be to create some views that condense the data into a single object you can reference. You get 3NF compliance and convenience, too. On a completely unrelated note, I'm a big fan of including the table name in your identity column names, so "customer_id", "order_id", and "complaint_id" in their respective tables instead of just "id". In my experience it makes your code easier to read.
I think that being able to go directly from complaint to customer makes sense, always going via order restricts you somewhat. For example, let's say one of your customers makes a complaint that isn't linked to an order, maybe they are offended by something they seen on your website, how would you create this in the database if no order exists? It obviously depends on your business processes but I would have something like: Customer: (Id, Name) Orders: (Id, CustomerId, Widget) Complaints: (Id, CustomerId, OrderId, Complaint) With OrderId on the Complaints table being nullable. I'm guessing that a Complaint always needs to be associated with a customer but may not always be associated with an order.
If someone could help, I'd appreciate it :) I'm stuck with this for hours.
My pleasure. Best of luck!
Most welcome!
Sure thing! 
You’re very welcome!
If you use AVG as an analytic function you can save yourself the join: SELECT invoice.inv_num , avg(invoice.inv_amount) over () - invoice.inv_amount as diff FROM invoice
Ended up having a few more instance freezes/hangs yesterday which is why I didn't respond very quickly. We've been searching through the SQL Server Logs for the instance and scouring the internet for any information about the messages we've seen but not found anything concrete yet. Also ran the SQL Profiler to try and identify what was going through the activity of the instance, but didn't yet find the cause there either. 
Yes, memory limits are set on each instance. 18GB, 4.5GB and 2GB on a 32GB server. It would be good to know how many requests it is handling. Hoping the OZAR tools will help establish that. I agree that it definitely looks like something changed, but we're a small IT team and generally very good about communicating anything we do. My best guess is someone has created some sort of query loop which caused the problems. No patching has taken place, that was one of our first checks, disk drives were all monitored and reviewed for any changes as well.
Is it easy to find the procedure / solution online for setting a timeout for queries? &amp;#x200B;
My experience is that people use excel because it's the only thing they know how to use. Then they create incredibly complicated logic inside multiple worksheets with vlookups against the same query multiple times etc causing Excel to malfunction and slow down processes dramatically, ultimately causing excel to crash and then they complain that it's broken. 
Not sure, I'll have to check, thanks
You'll need to be careful though, if you need to change customer_id in the orders table you'll also need to change it in the complaints table. I dont believe constraints can help you with that.
Your design is fine in my opinion. Also you don't need to normalise everything, denormalization is quite a common thing nowdays. You could use VIEWs to make your queries go a bit faster. 
Question is not what is more efficient or right, but what is your business. Are you recording complaints of customers or complaints on orders?
It should be possible to move the distinct to the outer select, but that might be a de-optimization...
I'd say there are mixed opinions about certificates. Some companies don't care, some of them list as 'preferred' and some of them require you to have one. It depends on them, how much depended they are on MS technologies. But having one is always good, since you can show that you have basic understanding of the topic. If you want to take one, there are numerous online courses you can attend or if you trust yourself you can just study by yourself from MS docs or books they suggest on the MCSA's page. 
You could try to use hierarchical query :) [Like this one here](https://stackoverflow.com/questions/20215744/how-to-create-a-mysql-hierarchical-recursive-query)
Unfortunately I don't know how to do that. I'm a developer and that's firmly in the DBA world. I know it's a solution as it's been floated as a solution to analysts bringing down a server by starting a poorly written query then leaving for the weekend, hoping they'll get results before Monday. I think there's an /r/DBA they could be a resource.
Here's a sneak peek of /r/DBA using the [top posts](https://np.reddit.com/r/DBA/top/?sort=top&amp;t=year) of the year! \#1: [Fell into a DBA role, need some advice](https://np.reddit.com/r/DBA/comments/aec2ta/fell_into_a_dba_role_need_some_advice/) \#2: [How to create a database from a backup file in oracle?](https://np.reddit.com/r/DBA/comments/9myinm/how_to_create_a_database_from_a_backup_file_in/) \#3: [Does anybody actually use SqlServer .NET / C# CLR?](https://np.reddit.com/r/DBA/comments/909m2v/does_anybody_actually_use_sqlserver_net_c_clr/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
Yep, and no one understands how it works but it gets passed around to everyone, and some executive considers it gospel even when it's demonstratably wrong. The last shop I was at, we migrated servers. The DBA spent months figuring out what used this server. Two weeks after the migration was complete I got a call from a vice president asking why my reports didn't match his. Eventually we discovered he was using an Excel document that queried the now offline server. That was a frustrating place to work as they were very set in their ways and not using any remotely current technologies. They did get a new CIO and he was trying to modernize, but when nothing has been addressed for 10+ years, it takes time. 
I had that title at a different shop. In a way that basically what I do now. I'm in a big shop now, front end and back end people have the same title. I'm a back end guy. 
I think you might be overthinking this. Just do it as you wrote: SP_1: your data prep. SP_2: your analytics queries. Use the try/catch to avoid errors stopping your SP_2 the way we discussed previously. SP_3: your wrapper - to make sure your analytics dont execute if sp_1 failed. begin begin transaction exec sp_1; commit transaction; exec sp_2; end;
Looks cleaner. Definitely easier to read and understand. It's a little odd that special tools are keyed to by damages, maintenance, *and* design flaws, but you could probably justify those references.
&gt;On a completely unrelated note, I'm a big fan of including the table name in your identity column names, so "customer_id", "order_id", and "complaint_id" in their respective tables instead of just "id". In my experience it makes your code easier to read. I've been struggling with this. On one hand it seems redundant to include the table name in the column name, on the other hand it seems every table has columns like id, name, datecreated, datemodified, etc. And then there is the fact that a lot of column names you want to use are restricted keywords if you don't prefix them with the table name.
In this post, I show how to do anomaly detection with some basic statistics and SQLite. Hope some of you find it interesting, or even useful! [https://www.donaldmellenbruch.com/post/groupwise-anomaly-detection-with-sql/](https://www.donaldmellenbruch.com/post/groupwise-anomaly-detection-with-sql/) 
I us AS to alias columns in my SELECT, and do not use it when aliasing columns in my FROM. Example: SELECT u.user_name AS username ,u.user_id AS userid ,ud.first_name + ' ' + ud.last_name AS name FROM users u JOIN user_details ud ON u.user_id = ud.user_id;
I wouldn’t lose sleep. As long as it’s got a good FK constraint then it’s not likely to get orphaned and changes would hopefully cascade down. Reporting again would be quicker too so I guess it’s about the design requirements. 
Try to look up CTE. It stands for Common table expression. With this, you can do recursive query. To get into that, write your top level query first. Then add another child level sub-query (with the IN operator). Now you can rewrite your top level query to be the 'anchor' of your CTE and rewrite your subquery to be the recursive part. Let me know if you made some progress. I'll help more whwn I'm not on the mobile. 
ouch, that smells like oracle 
&gt; What is your preferred method for aliasing columns?
&gt; My best guess is someone has created some sort of query loop which caused the problems I consider that a change. But again, a user query should *not* crash the entire instance (slow it down, kill performance, sure. But an outright crash? no way). If someone was able to do that, you may have an underlying problem that needs to be addressed.
I'd say it's T-SQL :D I'm doing the same exact thing in it.
I'm using 'selected\_column \\t alias\_name'. Tho it's a bit less readable when you write 40 column selects but a bit quicker to type and I think it's worth it.
I do the same thing.
If the column is going to be regularly used as a join, and it's a generic name like "ID" or "KEY", I include the table name. I like it to be as clear as humanly possible which columns can join together. Especially if that row is something like an integer where you can easily end up joining things that aren't actually related. 
Thanks! That's exactly what I was thinking as well, and I appreciate the query with the third join!
... You cannot use 'as' in the column alias list in the table alias definition to begin with. this might be the joke tho...
It also makes it possible to use the `USING` syntax in Oracle (`select * from orders left join order_items using (order_id)`). Only works if the columns in both tables have the same name. 
I misstated my comment, and edited it. AS for aliasing columns, no AS for aliasing tables in FROM is what I meant. 
I do both Oracle and SQL Server. This however is SQL Server. The concatenation operator in Oracle is `||` not `+`.
`column_name = ...` is a syntax error, so clearly `as column_name`
CROSS APPLY is Microsoft's not-standard implementation of a LATERAL CROSS JOIN - I fail to see how that would help here. 
AS, even where it's optional.
What's the difference between `mov_sum` and a normal `sum()` window function?
Those are just the column aliases, the query uses `sum()` and `count()` (see below). The aliases, which can be anything, are short for for "moving sum", "moving count". ``` SELECT t1.ts, t1.group_name, t1.metric, t1.value, count(t2.value) AS mov_n, sum(t2.value) AS mov_sum, sum(t2.value*t2.value) AS mov_sum_sq FROM ... ```
Are they different values than you'd get by using window functions though? At first glance you seem to be overcomplicating things but I might be missing something important.
Dude, this is incredible! I have never heard of this before and have been searching high and low for something like this. I must be bad at Google but this is incredible and you are awesome for sharing this. 
Haha my pleasure. It used to be called Microsoft Virtual Labs when it launched. It’s been great for our team! Glad I could help! I found a link to it last year on the channel 9 feed. https://channel9.msdn.com/ 
SQLite (my version at least) does not have windowing functions that perform the necessary calculations, nor does it have basic built-in functions like `sqrt()`, `pow()`, etc. I wanted to demonstrate how simple anomaly detection can still be done despite these limitations. 
Sqlite has had window functions for a while now... https://www.sqlite.org/windowfunctions.html
Okay cool. One more question if you don’t mind. Does sololearn have opportunities to practice outside of right when you learn the section? Like if I go through a whole chapter (like using AND, LIKE, etc) does it give us a chance to practice it or just go on to the next section?
It will ask to move to the next section. There is a quiz at the end of every section. 
Yes, SQLite supports `row_number`, `rank`, windowing functions, but not windows of `sum`, `count`, `avg`, `std`, etc. Regardless, the logic involved is the similar. One could easily translate this logic to use windowing functions in, say, MySQL. But by keeping it organic like this, this query should work with minimal modifications across SQL dialects. An added benefit of this approach, IMO, is that the window is defined by an exact time duration (e.g., 10800 seconds), *not* by a number of points (as with windowing functions).
It has built in `count()`, `sum()` and `avg()` window functions...
1. If it did, it changes nothing. 2. Read the article you referenced; it does not. 
Oh, I assure you the documentation is correct and every built in sqlite aggregate function has a window function version.
Row Number over (Order By Date Desc) and Where Row Number = 2 in the syntax of your sql flavah
would this method require the query to be run every time a new data set is appended to the table?
i found the exams/certifications to be more of a motivator and guideline to study the fundamentals. i used the official exam references and it teaches you all the core fundamentals of database administration. i have yet to do it on the job professionally but i know when the time comes, i'll be prepared for it as i understand the principles. i found those two exams (764/765) much easier than the two before it, 761/762. i just read the exam references for each in a week or two and then took the exams, passed all. the exam references are great for the exam
I'd have to see the data set how it's formatted. I guess depending what you're doing I'd do something like select \* from table a where [a.Date](https://a.Date) = (select z.Date from (select a.Date, ROW\_NUMBER() OVER (Order By a.Date DESC) rowNum from table a) z where z.rowNum = 2) 
 select top 1 Date from ( SELECT distinct TOP 2 Date FROM table order by Date desc) as a order by Date asc &amp;#x200B;
Hi there! Thanks so much for your help. This is what I have so far. &amp;#x200B; SELECT \* FROM loans JOIN applications WHERE loanstatus = 'Default'
Cool, thanks for the info. If I ever get time, I may create a rendition of this that uses windowing functions.
or more like this? &amp;#x200B; SELECT COUNT appid FROM LOANS JOIN applications WHERE loanstatus = 'Default' &amp;#x200B;
Do you have a link to your data set in the article?
Which database are you using?
Not previously, but I just created a temporary link to the database "observations.db", [here](https://www.dropbox.com/s/r2qzf5rga11y1tm/observations.db?dl=0).
This is rather important, as I would just do this in MS SQL and convert the time to the proper format once it's isolated: declare @Time1 int = 125456 SELECT CAST(MSDB.DBO.AGENT_DATETIME(19700101,@Time1) as time) declare @Time2 int = 83451 SELECT CAST(MSDB.DBO.AGENT_DATETIME(19700101,@Time2) as time) declare @Time3 int = 145448 SELECT CAST(MSDB.DBO.AGENT_DATETIME(19700101,@Time3) as time) but this obviously won't work in any other DBMS...
Can anyone vouch that the SQL book in this bundle. I am worried it is too out of date. Thanks
TIL
Are these actual books or just ebooks?
Just the eBooks. Can download in PDF, ePub, and MOBI format. They're stored in your Humble Bundle account's library.
I see. I can google and download the pdf for free but whatevers
Yeah, fuck authors for trying to make a living. 
Last one I came up with. Am I on the correct track? &amp;#x200B; SELECT COUNT \* appid FROM loans JOIN applications WHERE loanstatus = 'Default' GROUP BY actid &amp;#x200B;
Thanks. The 3 hours previous part is proving more difficult to implement than I'd thought using window functions. hmm.
Folks don't like it when you point out how screwed-up the arrangement is. This is a classic game of telephone with a splash of buck-passing, with the punchline potentially being "your database is hosed, can't/won't help you, have a nice day!"
LOL. They could have at least asked to turn it on and off again. That would have cleared the plan cache.
For example: unpivot by abcdefgh category, do a regular seg# = rownum() - count(*) unbound preceding partitioned by category, get count(*) grouping by category, seq#, get max of the counts by category, pivot. 
Yeah, they're similar, yet different approaches. The `JOIN` approach allows defining the window size by time (-10800s), regardless of the observation interval. A windowing function defines the window size by the number of preceeding points. So, to do three hours in a windowing function, you'd have to consider the time interval (5m in this case). So the appropriate window would be something like: ``` sum(value) over (PARTITION BY ... ORDER BY ts ROWS BETWEEN 37 PRECEEDING AND 1 PRECEEDING) ``` If the desired window size is 3h, and you have 5m intervals, you want the window size to be 36 points (assuming no missing values). The "1 PRECEEDING" is important because we only want to consider points that occur *before and not including* the current point. Of course, these values would need to be changed if the data appeared in different (or irregular) intervals. Thanks for looking in to it!
Thanks, although I had already googled this and kept getting results on using partitions - which don’t seem to work with the server I’m on...
Yup, that's what I'm doing.
Ah, interesting. I assumed 37-1=36 points would give exact results as the original method. So, I guess the desired window size is actually 36-1=35 points, since we're not including the current point in the window. If you get to a point where you want to share your work, send it over and I'll append it to my post.
Relational Databases rely on Transactions. A transaction is composed of one or more operations on the database, whether that is an insert or delete or changing something already in the database. When a transaction is committed, it means that the change made into the database has been saved and made permanent and available to all users. An "uncommitted transaction" can be undone with a rollback. Furthermore, the state changes caused by an uncommitted transaction can only be seen by the user who owns that transaction. An uncommitted transaction can also lock tables, and thus, cause your application to wait on the lock to be released or maybe crash. How to handle that? I don't know. Are users changing data on your production directly instead of through your application? Is that allowed in your organization? Or are they running untested scripts directly on the database? Does your application have a bug? There's a lot of questions.
 SELECT ts, group_name, metric, value, mov_n, mov_avg, (mov_sum_sq - (mov_sum*mov_sum/mov_n)) / (mov_n - 1) AS mov_var, ((value - (mov_sum/mov_n)) * (value - (mov_sum/mov_n))) / ((mov_sum_sq - (mov_sum*mov_sum/mov_n)) / (mov_n - 1)) AS mov_z_sq, CASE WHEN ((value - (mov_sum/mov_n)) * (value - (mov_sum/mov_n))) / ((mov_sum_sq - (mov_sum*mov_sum/mov_n)) / (mov_n - 1)) &gt; 3*3 THEN 1 ELSE 0 END is_anomaly FROM ( SELECT ts, group_name, metric, value, count(*) OVER w AS mov_n, avg(value) OVER w AS mov_avg, sum(value) OVER w AS mov_sum, sum(value * value) OVER w AS mov_sum_sq FROM events WINDOW w AS (PARTITION BY group_name, metric ORDER BY ts ROWS BETWEEN 36 PRECEDING AND 1 PRECEDING) ) ORDER BY ts, group_name, metric; Results are the same in my samplings except for the last couple of decimals on some rows, thanks to the vagaries of floating point math.
It's not clear from your post, are you the dev of said app, or a user?
I thought the SQL cook book was actually pretty darn good. It's basically "how can you use window functions to solve business questions" from what I remember with lots of examples.
Thanks for reply and I appreciate your explanation. We have a number of developers with database access, it could have easily been one of them. ( note to self to tighten up a change control process! ) I’m the server admin ( but I’m not a DBA, with little knowledge of databases). I just wondered if there was some log file I could search through around the time of the outage to see what transactions where uncommitted or what caused the uncommitted transaction to occur in the first place. Would a standard SELECT query be contained as a ‘transaction’ ? I assume it has to be an INSERT, UPDATE or DELETE for it to be considered as such ? 
I'm wondering what's new in SQL though? I haven't used it day to day since August 2016, but still do the basics occasionally in my current role. Here's the Amazon link to checkout the review scores: SQL Cookbook: Query Solutions and Techniques for Database Developers (Cookbooks (O'Reilly)) https://www.amazon.com/dp/0596009763/ref=cm_sw_r_cp_apa_i_iJnuCbBS4T84X Mostly positive and FWIW, the Kindle version is $17.27. I was already considering buying the bundle, now I'm leaning even more towards purchasing.
Sorry, I am the server admin but not a DBA. The application is (supposed to be) fully managed but they just identified the problem as being an uncommitted transaction, then ( I presume ) killed that transaction without committing it which resolved the issue. I’d like to try understand what caused the issue, was it one of our developers trying to update items in a table and didn’t write his query correctly or was it the application itself. Where could I go to see a history of transactions ? Thank you for your reply. 
Everything has to be in a transaction. An ordinary SELECT won't exclusively lock a record though, so it's not likely to cause the problem. A badly written select statement can monopolise the server's resources though, which would result in the transaction not completing, but that doesn't sound quite like what has been described.
Whoa! I'm no stranger to windowing functions, but I've never seen a window defined and reused like this. Way cool.
You'd probably have to query system tables or the management views. A bit beyond my pay grade but this looks like a good start: https://blog.coeo.com/inside-the-transaction-log-file-using-fn_dblog-and-fn_full_dblog
my comment was in regards to your use and non-use of AS so why bring up the concatenation?
I had a number of Excel reports that users kept asking for so I set up a webserver running PHP and wrote a simple front end that would get some basic requirements such as dates and what report. The PHP script would then extract the data and display it on the screen for the user to look at and a button to export it to a CSV file (or they normally just copied and pasted from the screen). The advantage was I learnt PHP &amp; Javascript and managers and their managers loved it thus making me look good.
So I have to ask the JavaScript guy
With this concise definition of the moving window, I was able to implement this procedure without a subquery. ``` SELECT ts, group_name, metric, value, avg(value) OVER w as mov_avg, case when ((value - (avg(value) OVER w)) * (value - (avg(value) OVER w))) / (( (sum(value * value) OVER w) - ((sum(value) OVER w) * (avg(value) OVER w))) / ( (count(*) OVER w) - 1)) &gt; 3*3 THEN 1 ELSE 0 END as is_outlier FROM events WINDOW w AS (PARTITION BY group_name, metric ORDER BY ts ROWS BETWEEN 36 PRECEDING AND 1 PRECEDING) ORDER by ts asc ``` I wasn't concerned with performance in my original write-up, but since you brought it up, I'm curious if you could answer a question I have. Does SQLite reuse, or recalculate multiple calls to a windowing function? e.g., `avg(value) OVER w` is called multiple times, but is is *calculated* multiple times? I'm thinking the above rendition will be fastest since it does not rely on sub-queries, but could the duplicated calls to the same function unnecessarily slow things down? Now that we have three ways of doing essentially the same thing, I want to put 'em head-to-head. Perhaps that can be motivation for a future post. ;)
Because it’s an easy way to tell the difference between the two systems and determine which system it is.
I recently got this at work and properly started reading it yesterday. So far so good! Have learnt one new thing so far and had good revision on a few others. I think if you are already pro it probably wouldn't not be that useful. But if you are new or just really getting into writing your own quieres then it can be useful. 
Partitions are different than the PARTITION BY clause in a windowing function. Sorry I don't have more time for a better response. 
https://modern-sql.com/blog/2017-06/whats-new-in-sql-2016
Don't bother with the partner links, [go directly to the site yourself](https://www.humblebundle.com/).
That book is fantastic. The name does not do it justice. I recommend it wholeheartedly. 
thank you
Do an inner join on the date to a query that only returns the 2 dates: Select x,y,z From table Inner join ( Select top 2 scan_dates from table group by scan_date order by scan_date )a on a.scan_date=table.scan_date Syntax might be slightly off but the concept should work. Not sure how it will perform for your data set since I don’t know how it looks
Assuming MSSQL here, and also assuming you want two distinct most recent scan dates -- one way to do it: SELECT \* FROM \[acas\_master\_test\] where \[scan\_date\] IN (SELECT DISTINCT TOP 2 \[scan\_date\] FROM \[acas\_master\_test\] ORDER BY \[scan\_date\] DESC) &amp;#x200B;
Does it bother anyone else that the animal on the Python book cover is not a snake?
This might work in MySQL. Simple concept for the query is to finding out the latest 2 dates in WHERE clause through a self joining. select * from [acas_master_test] where [scan_date] in (select t1.[scan_date] from [acas_master_test] t1 left join [acas_master_test] t2 on t1.[scan_date] &lt;= t2.[scan_date] group by t1.[scan_date] having count(distinct t2.[scan_date])&lt;=2); &amp;#x200B;
Yoh should be able to just drop the outer query and SELECT * on the inner for the same results.
Why not just a row_number () over (partition by A order by ID ASC) as rn? Make it a cte and then select max(rn) for output.
I was so close to buying these, and then realized I would never touch them. I like paper copies of books, and have the Python cookbook already. Good deal tho if you don't mind using an E reader. 
O'Reilly publishes many books about Python, they can't all have pythons on them? 
Or [this link, more specifically for the programming cookbooks](https://www.humblebundle.com/books/programming-cookbooks)
What you could do is perhaps add a row number, and then write a function that selects the 5th row as first, and then then you run a while from the 7th row to the end? ROW_NUMBER ( ) OVER ( [ PARTITION BY value_expression , ... [ n ] ] order_by_clause )
I was just kidding around, but yeah...
You're getting there, but I highly recommend watching a YouTube video or two on intro to SQL syntax. Your join is incomplete, and your select statement syntax is off as well. 
The only problem with that is that if there are two rows with the same max scan_date, then you'll only get the rows with the max scan date.
Something like this should work for you. [GL1] = SUM(CASE WHEN [Account] = ?GL1? THEN [Amount Field] ELSE 0.00 END)
Off the top of my head if pivot were aggregating (and I see why it would, since record ID spans multiple rows), I would add a unique identifier such as `row_number()` to prevent or fine tune aggregation.
It's not quite that it ***is*** aggregating, but that it ***requires*** an aggregate statement. I might be misunderstanding, but all the info I found showed that it required an aggregate statement, however, the row_number() identifier I've never heard of, I might need to find some info on that as it might be helpful for other things. 
That's not a bad idea, I'll have to give that a shot. Thanks!
Maybe something like this? CREATE TABLE #recordAccount ( RecordID int not null, GLAccount money not null ) INSERT INTO #recordAccount (RecordID, GLAccount) VALUES (123, 1234.0000) INSERT INTO #recordAccount (RecordID, GLAccount) VALUES (123, 5678.0000) INSERT INTO #recordAccount (RecordID, GLAccount) VALUES (123, 4455.0000) INSERT INTO #recordAccount (RecordID, GLAccount) VALUES (321, 1223.0000) SELECT RecordId, [1] GL1, [2] GL2, [3] GL3 FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY RecordId ORDER BY (SELECT 0)) num, RecordId, GLAccount FROM #recordAccount ) s PIVOT ( MAX(GLAccount) FOR num IN ([1], [2], [3])) AS PivotTable; 
`PIVOT` needs aggregation because what if you have multiple rows per your grouping. In fact, you can't pivot the data you posted, using `PIVOT` or any other method as it is right now, because how is the engine supposed to know which of the values (1234, 5678, 4455) goes where? So, in order to actually make this happen, you need some other sort of identifier, like this GL1, GL2, GL3 items, unless you actually have them, like so: RecordID | AccountID | Value ---|---|---- 123 | GL1 | 1234.00 123 | GL2 | 5678.00 123 | GL3 | 4455.00 321 | GL1 | 1223.00 Given something like that, whatever you decide to use for the aggregation function is fine, as long as there is of course just 1 value per record and account; the aggregation function only matters if you have multiple. So you go with: select * from reddit_aldk1n r pivot (sum(value) for AccountID IN ([GL1], [GL2], [GL3])) p [And here it is](https://i.imgur.com/JdjOBf6.png) 
Hey thanks! I think this worked. To follow up to my previous question... now that I have a filtered down list, is there a way to "compare" specific values that are in the "older" scan date and the "latest" scan date? &amp;#x200B; i.e. 5 "IDs" that were in the previous scan are not in the new scan or 3 "IDs" that are in the new scan are not in the "old" scan date?
Thank you, I'll look into this!
Thank you for the information!
I have group by for that but distinct works too
You can dump everything into a temp table and do an outer self to find old v new. Here's a modified sample I threw together, it was the first thing that came to my mind. It may not be the best way though and its from memory so syntax may be off. SELECT * INTO #temptable FROM [acas_master_test] WHERE [scan_date] IN ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) CREATE INDEX ix_scan ON #temptable (id, scan_date) --depending on volume of records --New records SELECT new.* FROM #temptable new left join #temptable old ON new.id = old .id and new.scan_date &lt;&gt; old.scan_date WHERE old.id is NULL
If the aggregate partition statement is unique, then your problem is solved (because it's no longer aggregating). So create a unique identifier. That's how you think in SQL, you nest your logic, using a view or a common table expression.
You could use built in aggregate function eg count(*) then GROUP BY it by user_name
This would be better answered in /r/Oracle. [https://docs.oracle.com/cd/B19306\_01/server.102/b14357/ape.htm](https://docs.oracle.com/cd/B19306_01/server.102/b14357/ape.htm)
Hey thanks! I forgot to mention, but I'm trying to avoid using temp tables. If not using temps, is it at all possible to do the "calculations" on one table? I can't think of a way to do it in one, so I might default to using 2 if necessary.
You can install the instantclient, plus the "tools" package [https://www.oracle.com/technetwork/topics/winx64soft-089540.html](https://www.oracle.com/technetwork/topics/winx64soft-089540.html)
Are you looking for a list of unique users that logged in? You can do that by just selecting the column that the username is in and throwing the `DISTINCT` keyword in front of it: SELECT DISTINCT User_Column WHERE EventType LIKE 'BROKER_USERLOGGEDIN' ORDER BY time DESC If you're looking for something more complex, you'll need to include some sanitized sample data for us to see
the links on that page appear to be dead
Pretty much. Right now it just gives users who have logged in with multiple entries. I'll try to run it with DISTINCT and see if that results unique users so I don't have to trim on the back end. Thanks for the reply.
Thanks for your input, I'll have to try that as well to see if it'll result what I'm looking for, thanks!
Can you just schedule the Python job to run via the SQL Server Agent? Or is it a semi-manual process and not fully automated? How are you / your peers with PowerShell? Without seeing the original code and a sample input/output, it's hard to provide additional help
https://www.w3schools.com/sql/sql_create_table.asp Create the table, making sure your `id` is set to `auto_increment` Lookup two main difference between a table and a view and write them down. Then run two insert statements, one for each of the answers you wrote above and run the insert statement for just the answer column, since the `id` will auto-increment https://www.w3schools.com/sql/sql_insert.asp
The python Script is simi-automated right now. I have to manually run the script and it spits out a sql file with all the INSERT statements in it. Here is the python Script that I run If it helps. NUMBERS = '0123456789' invalidCount=1 Count=1 data=[] #CONNECT TO DATABASE pw = getpass.getpass("Password?:") conn = pyodbc.connect( r'DRIVER={{SQL Server Native Client 10.0}};SERVER=;PWD={pw}'.format(pw=pw) ) c = conn.cursor() # IMPORT LIST OF PHONE NUMBERS IN DATABASE CalledNumbers = set() c.execute('SELECT [Phone(Billing)] as [Phone] FROM [OutBound].[dbo].[Leads]') for row in c.fetchall(): OfficePhone = '' for char in row[0]: if char in NUMBERS: OfficePhone = OfficePhone + char #print("Called : '{OfficePhone}'".format(OfficePhone=OfficePhone)) CalledNumbers.add(OfficePhone) #CREATE TUPLE OF PROCESSED RECORDS (PHONE:TIMESTAMP) ProcessedRecords = set() c.execute('''SELECT ISNULL([Phone(Billing)],'999-999-9999') as [Phone], convert(datetime,case when ISNULL(OrderDate,'') = '' then '1970-01-01' when len(OrderDate) = len('2018-10-09 16:07:16.483000') then left(OrderDate,LEN(OrderDate)-3) else replace(OrderDate,'CDT','')end,120) as [OrderDate] FROM [OutBound].[dbo].[Leads] ''') for row in c.fetchall(): for char in row.Phone: if char in NUMBERS: ProcessedRecords.add((row[0],row[1])) PhoneNumbers = set() Duplicate_Numbers = set() # IMPORT DATA FROM EMAIL TICKET c.execute(''' SELECT [BodyHtml],[DateReceivedUTC] FROM [SmarterTrack].[dbo].st_TicketMessages tm JOIN [SmarterTrack].dbo.st_Tickets t on t.TicketID = tm.TicketID WHERE TicketStatusID = 1 order by DateReceivedUTC ''') with open('parsed_Tickets_{t}.sql'.format(t=t),'w') as fp: for row in c.fetchall(): data=row[0].split('&lt;br /&gt;\r\n') Name = data[1].replace("'"," ") name=Name.split(' ') FirstName = name[:2] FirstNameFormatted = ' '.join(FirstName) LastName = name[2:] LastNameFormatted = ' '.join(LastName) OfficePhone = '' for char in data[2]: if char in NUMBERS: OfficePhone = OfficePhone + char datereceived = row[1] stateAndCountry = data[5].split('&lt;br&gt;') stateAndCountryFormatted = stateAndCountry[0] stateCountryData = stateAndCountryFormatted.split(',') state = stateCountryData[0] country = stateCountryData[1] # CHECK IF EXACT RECORD ALREADY EXISTS skipRecord = '' if (OfficePhone,datereceived) in ProcessedRecords: skipRecord = OfficePhone print('Record Already Processed') if len(OfficePhone)!= 10: OfficePhone = '' for char in data[3]: if char in NUMBERS: OfficePhone = OfficePhone + char if len(OfficePhone) == 10: if OfficePhone not in CalledNumbers and OfficePhone not in PhoneNumbers and OfficePhone not in skipRecord: PhoneNumbers.add(OfficePhone) fp.write("INSERT INTO [OutBound].[dbo].[Leads] VALUES ('','','{datereceived}','','{FirstName}','{LastName}','','','','{state}','','{country}','','{OfficePhone}','','','','','','','','','','','','','','','','','','','','','','','','',null,null,null,-1,-1,null,-1,null,null)".format(datereceived=datereceived,FirstName=FirstNameFormatted,LastName=LastNameFormatted,state=state,country=country,OfficePhone=OfficePhone)) fp.write('\n') print("lead : '{OfficePhone}'".format(OfficePhone=OfficePhone)) else: # Duplicate Numbers {Completed = -3} Duplicate_Numbers.add(OfficePhone) Count = Count+1 #print(Count) fp.write("INSERT INTO [OutBound].[dbo].[Leads] VALUES ('','','{datereceived}','','{FirstName}','{LastName}','','','','{state}','','{country}','','{OfficePhone}','','','','','','','','','','','','','','','','','','','','','','','','',null,null,null,-3,1,null,-1,null,null)".format(datereceived=datereceived,FirstName=FirstNameFormatted,LastName=LastNameFormatted,state=state,country=country,OfficePhone=OfficePhone)) fp.write('\n') # Place Invalid Numbers {Completed = -2} else: fp.write("INSERT INTO [OutBound].[dbo].[AltarVampireOutbound_Leads] VALUES ('','','{datereceived}','','{FirstName}','{LastName}','','','','{state}','','{country}','','{OfficePhone}','','','','','','','','','','','','','','','','','','','','','','','','',null,null,null,-2,-1,null,-1,null,null)".format(datereceived=datereceived,FirstName=FirstNameFormatted,LastName=LastNameFormatted,state=state,country=country,OfficePhone=OfficePhone)) fp.write('\n') invalidCount = invalidCount+1 #print(invalidCount)
Thanks, I played with this and I'm going to keep it in mind for future things. In this specific case, because I don't have the ability to modify the data itself, it's not going to do the trick. Thank you though!
Unfortunately I can't modify this table though.
haha, you're asking reddit to do your homework? "tHW1\_Q1" is pretty clearly "table HomeWork1\_Question1" and that question is right off of a homework assignment. Dude.
Your solution seems to do exactly what I want, but it contains some features I'm not super familiar with. Working on learning my way through this now. 
I don't mind helping with homework, but no doing it instead. What have you tried so far?
The main issue I see with your approach in SQL is you're replacing `&lt;br /&gt;` with a comma, but your data also contains commas, so you're going to run into trouble. Sadly 2008 doesn't have `SPLIT_STRING` (available in SQL 2016), so it's going to be manually doing the sub-string parsing on `&lt;br /&gt;` (or designate an infrequently used character like `~` to replace `&lt;br /&gt;` with in your TSQL)
Yeah it is part of a hwk assignment..its clear. Im just asking for help or where i can get started. But its cool “dude”
Cool thanks
You could do it with a CTE probably. I don't know if its a good idea or comparable to temp table w/ index. ;WITH cte AS ( SELECT *, ROW_NUMBER () OVER (PARTITION BY scan_date ORDER BY id) as rn FROM [acas_master_test] WHERE [scan_date] IN ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) --New records SELECT new.* FROM cte new left join cte old ON new.id = old .id and new.rn &lt;&gt; old.rn WHERE old.id is NULL Another option is to use two queries (one for each date) with an EXCEPT. --Most recent SELECT DISTINCT TOP 1 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC --Second most recent SELECT TOP 1 [scan_date] FROM ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) as d ORDER BY [scan_date] ASC
 $sql_tickets .= ", SUM(CASE WHEN hotspot_rate.name = '".$rates[$rate]['name']. "' THEN 1 ELSE 0 END) AS \"".$rates[$rate]['name']."\" "; There's your Bobby Tables moment. If the rate name has a single quote in it (`'`), your query will break at best, leave you exposed at worst. Use parameterized queries anytime you need to "insert" data into a query.
Where is your effort?
Homework?
Awesome, thanks for the input.
Oh! Completely missed the "group by". My bad. :)
Awesome, thank you. I'll try this out!
&gt;SELECT \* FROM \[acas\_master\_test\] WHERE \[scan\_date\] IN ( SELECT TOP 1 \[scan\_date\] FROM ( SELECT DISTINCT TOP 2 \[scan\_date\] FROM \[acas\_master\_test\] ORDER BY \[scan\_date\] DESC ) as d ORDER BY \[scan\_date\] ASC ) This worked so well, thank you so much! 
"Student" is a role played by an individual. "Employee" is also a role played by an individual. An individual hasMany roles sqlfiddle is timing out so I can't see what you're doing I would use something like this: create table individuals ( individual_id int primary key, name text, ... ); /* use SQL Table Inheritance here */ create table roles ( individual_id int references individuals(individual_id), role_type text check (role_type in ('student','employee')), student_number text null, employee_number text null, primary key (individual_id, role_type) );
google "subtypes and supertypes"
That seems like a good solution. I could query the roles table with the user ID, check the role_type on my backend server, and present the relevant information returned. If I added many more roles though, that would create a giant table wouldn't it? I could separate it out to multiple tables per role, but that would create more queries. Which one would be better for performance, or is it negligible?
Single Table Inheritance (one big table w all the types) is faster, but requires the use of nulls Class Table Inheritance (one table per type linking to a parent table) is slower but doesn't use nulls You're probably fine just using a single table. Good luck!
No worries!
Yes, It's my homework, but I dont't know which test cases I can do.
It's worth it to familiarize yourself with window functions, in particular ROW_NUMBER, LAG and LEAD. It's very useful for things like deleting duplicates, paginating results.
I'll check into them. Do you have any preferred resources? Or just the general MS SQL pages on Microsoft's website?
Thanks, I ended up with a pretty sexy process.
what type of mail notifications ? If it is alert then Azure managed instances have some monitoring that takes over traditional SQL server alerts and some have become obsolete. or you can look into Powershell Send-MailMessage commandlet. [https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/send-mailmessage?view=powershell-6](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/send-mailmessage?view=powershell-6)
As a simple example I have a process that runs and pushes data into a log table, and at the end of the process it counts how many errors there were during the run. If this value is greater than 0 I would want an email to be sent telling us so we can fix it and rerun the process
something like this would help in powershell. [https://stackoverflow.com/a/44244241/3872659](https://stackoverflow.com/a/44244241/3872659) [https://stackoverflow.com/a/47939177/3872659](https://stackoverflow.com/a/47939177/3872659) &amp;#x200B; you have to figure out how to configure your smtp with the link in my previous reply. The default value is the value of the $PSEmailServer preference variable. If the preference variable is not set and this parameter is omitted, the command fails. &amp;#x200B; you can also look into sendgrid for azure [https://sendgrid.com/partners/azure/](https://sendgrid.com/partners/azure/)
Can you post some samples of the table and an idea of volume?
Its around 100,000+ rows of data. I don't have access to the data until I go in for work, so I can't get a good example for a few days. The input of this data is not regulated so their are a lot of random patterns. Basically it looks something like: Inc. Benard Dr. 100st San Fransisco, CA or 2503 North Davis st., Salt Lake City, Utah 
There is no simple way to do this. What you need to do is start looking at the distributions of your data, and then creating "cases" where you will change the data (or clean it.) For example you might run a select statement to see what % of the data uses `ST` instead of `Street` instead of `St.`. Your ultimately goal is to write an algorithm to clean a significant portion of your data. You may end up with some % that is uncleanable, or which needs to be manually cleaned by hand, but that isn't the purpose of your SQL work. Your SQL work is to try and whittle that % down until it is statistically insignificant for what you are trying to achieve.
This really isn't my space, but what you're saying is that Azure can send email notifications?
This is such a massive pain in the ass and so common an issue there's plenty of services where you can send your shit addresses to to get cleaned up and geolocated. I've done home-brew address cleaning and it's frustrating and you just end up identifying case after case after case and never quite get what you want. Avoid if possible.
I worked in direct mail marketing for 10 years. The gold plated version of doing this would be to run your data through at CASS engine to have it standardized by the USPS database. There are relatively inexpensive services for this online. A few that I remember are Accuzip, Smarty Streets, and Melissa Data. Once you get your data back from them it’ll all be uniform and you’ll be able to see if any of your addresses are invalid. Good luck!
Thank you so much! 
We use Smarty Streets at work. Not sure on pricing but it’s pretty easy to use to clean up and verify address data 
That's what I was thinking. I don't want to go down the rabbit hole. Almost had to do that with phone numbers because people think 1-800-555-FORD is a valid phone number. 
OK. I'll give that a go. What is the best way to get the state and city data into a different column? 
If you need help let me know. Accuzip customer service is better than Melissa Data. SS might have an automated solution online. Good luck!
Replace St to St*, Street to Street*, Drive to Drive*, Road to Road* after Substring everything after * and move to City column etc, use pathindex to identify street number and move everything before number to street name column etc. Like notasqlstar mentioned it's case by case until you eliminate all possibilities. It all depends on data you have to work with.
You don't need to modify the table. I spelled it out: you use a view or common table expression to synthesize a unique key, then you can avoid aggregating on your (non unique) columns.
So lets say you have (1) column with an entire address, and it looks like this: | Address | | :--- | | 1234 Cherry Ln., NY, NY, 12345 | | 1234 Cherry Lane, New York, New York, 12345-1234 | | 123466 Boardwalk Street, New York, NY, 12345 | So this would represent the worst case scenario, and you would definitely want to try to manage this at the layer where the data is collected to normalize it, but if that is all you have to work with then there are a variety of techniques you can use to deconstruct these values, parse them, look them up to standardize them, and then produce a clean set of data. What you need to start doing is running queries to look at the distribution of your data so that you can begin to wrap your mind around the different steps you will need to do in terms of isolating certain rows, and then changing them.
The first step is to look at it and try and come up with something simple that can account for like... 60% of the data. Then you create a second rule that comes up with like 20% of the data Then another rule that comes up with about 7% Another rule to come up with about 3% ... 2.5% In the end you feel confident you have about 95% of the data in a perfect world It isn't ideal, but it will let you do what you want You can probably live with even less than 95% for a lot of statistical processes. Start by coming up with queries like: select count(*) from table where blah blah can be a very complex set of sub processes, where you're "testing" the population of data to see what you can put in a bucket, and then you select top 1000 where not equal to blah to look at a sample of what isn't being put in the bucket to come up with the next rule. You keep doing this until things become nonsensical and too difficult / not worth factoring in. 
Just in case anyone reads this, I've found the correct answer to my question is to use views, there's no need to copy the data to table, just create a view to query the linked server. CREATE VIEW [dbo].[WO1_WorkOrderMaster] AS select * from MAS_CER...WO1_WorkOrderMaster GO 
I recently had a report to do in SSRS and exactly the case of what you are doing. Pivot is going to be your answer for this one.
Another user provides a pivot implementation example that did the trick and taught me a few new things in the process! Super helpful. 
SELECT first_name, last_name, employer FROM users u JOIN employees e ON u.id = e.user_id UNION ALL SELECT first_name, last_name, institution FROM users u JOIN students s ON u.id = s.user_id
aint nothing wrong with a good ol' union SELECT first_name, last_name, employer as E_or_I FROM users JOIN employees on users.id=employees.user_id union SELECT first_name, last_name, institution as E_or_I FROM users JOIN students on users.id=students.user_id;
You don’t need a third party to clean up your data. The USPS offers a free API. Study up a little on XML and you will be able to clean the data yourself. I wrote a VB.Net class to automate this process. All I have to do is send each part of the address, and the class returns the cleaned up version. I have used it to validate tens of thousands of addresses.
The two queries in your code box are exactly the same. Is this a trick question?
which RDBMS are you using?
SQL Server, updated the OP to reflect that
Check the last line. I guarantee they return different results.
This sounds great. Could you help point me in the right direction?
Are you proficient in VB.Net?
Share your create table and any constraint statements. 
Updated OP w/ information. Those blocks of code are what I wrote to create the tables.
SQL server stores each bit column side by side with up to another seven bit columns inside of one byte. So you essentially have a column of one bit and it can store seven more but fields in that space if you added them. It’s in the internals documentation books https://docs.microsoft.com/en-us/sql/t-sql/data-types/bit-transact-sql?view=sql-server-2017
post the sql scripts (table creates and insert statements). print out sp_spaceused on them. I believe the answer lies in the fact that sql server (being optimized for real world conditions if you will) doesn't treat a single bit column as a true bit in the table data. table data isn't written raw to disk, its all contained with the 8k pages and so forth. I pretty a single bit column is still a byte on disk (and you essentially can get 8 bit columns for the price of one). THe .5 mb difference is still kinda interesting but curious of what that relates to % of table size difference. 
Not at all
Thanks for the insight, I edited the post to include sp\_spaceused.
Thank you. So this explains why the bit column is much bigger than you'd expect, but I'm still wondering how the character column is smaller. The SQL docs say that char(n) is n-bytes long, so theoretically 1 bit (+7 more = 1 byte) should equal the char(n = 1 byte) column.
The only thing I can offer would pretty much require an understanding of VB.Net, XML, and SQL. Considering the time value of money, your best bet is to farm out the cleanup to a third party. I would be happy to do it for you, but it would probably be more expensive.
It has to do with how the data is placed on the page and can be affected by other rows inserted/updated and plenty of other things. Use DBCC PAGE to dump a raw data page in hex for each of your tables and you can look at how it is stored physically. There used to be a cool plugin called internals viewer that did this automatically but when SSMS moves to being Visual Studio based in 2012 this plugin never got updated https://blogs.msdn.microsoft.com/sqlserverstorageengine/2006/06/10/how-to-use-dbcc-page/
Alright, well thank you. I need to learn VB and XML. But I also want to learn python. There are a lot of things I want to learn.
well, I can see you true size diff is 560 KB not .5MB. so we're really talking like 5% difference. this is probably just due to how data pages are on disk. I'd be curious if you scripted this out, and repeated the whole process 1 thousand times. is the bit table always bigger. 50/50 . random???
I will check that out, thank you!
This is just spitballing, but could disk fragmentation cause an issue like this? Maybe the data isn't filling the 4 KB disk allocation, so multiple pieces of data, which you could otherwise fit several into a single 4 KB block, are occupying more 4KB blocks than are necessary, causing it to take up more space.
[https://rmtwrk.com](https://rmtwrk.com/) aggregates remote job posts from all over the internet and displays them all in one place. So by checking just one site you can stay on top of things. You can also subscribe to receive daily emails that have the new job posts for the job types you're interested in.
yea, these tables are heaps which I think allows sql to get more "creative" with how it tosses the pages on disk. data on disk is really managed in terms of extents (8 8kb page, 64 kbs). when tables are small they are using mixed extents (pages distributed across multiple extents). larger table get uniform extents so all pages in the extent are for that table. I'm thinking that is probably the likely scenario here. this breaks down the how data is stored on disk. the answer is somewhere in her: https://docs.microsoft.com/en-us/sql/relational-databases/pages-and-extents-architecture-guide?view=sql-server-2017 
&gt; Replace St to St# And hope none of your addresses are catholic churches.
That sounds like it requires addresses already split into street/suburb/city/state/country fields?
The class itself splits up the fields. Yes, currently setup for US only.
Oh, thx much for the gold. Not necessary tho - i am really a very basic reddit user.
This will cause a lot of fun with things like "West", "coast", "Broad", anything with "St" as the prefix and a whole lot of other words that contain any of the contractions you have identified. 
&gt;worst case scenario &gt;correctly spelled addresses using common abbreviations with all of the elements of an address present and in the same order oh sweet summer child
You can always specify and look for '% St %' 
What are you working in?
Where is "the following form"? 
In some databases like MS SQL Server there is no difference. In SQL Server a transaction is started and committed implicitly if you don't explicitly create one. In e.g. Oracle Database if you omit to start (and commit) a transaction it will start a transaction implicitly and wait for you to explicitly commit it. I.e. nothing will be committed until you commit it, or issue a DDL command such as create table, drop table, alter table etc. DDL commands in Oracle Database commit earlier transactions implicitly. That's a really weird design choice but other than that I prefer Oracle's approach. I've been saved by it a few times when I did "delete from" on the wrong database. Then it's just a matter of issuing rollback; 
 SELECT * , Rank1 + Rank2 + Rank3 AS Total FROM ( SELECT Cashier , COUNT(DISTINCT ticket_id) AS Transactions , RANK() OVER (ORDER BY COUNT(DISTINCT ticket_id) DESC) AS Rank1 , SUM(Gross_Line_Total) AS "Gross Sum" , RANK() OVER (ORDER BY sum(Gross_Line_Total) DESC) AS Rank2 , SUM(Gross_Line_Total)/COUNT(DISTINCT ticket_id) AS Avg , SUM(Quantity)/COUNT(DISTINCT ticket_id) AS UPT , RANK() OVER (ORDER BY SUM("Quantity")/COUNT(DISTINCT ticket_id) DESC) AS Rank3 FROM bi_tickets WHERE DATE_TRUNC('WEEK', CAST(bi_tickets."Date" AS timestamp)) = DATE_TRUNC('WEEK', CAST((NOW() + INTERVAL '-1 week') AS timestamp)) GROUP BY Cashier ) AS d ORDER BY Transactions DESC
check pm pls! &amp;#x200B;
I did, there is nothing.
check in the live chat
What have you tried so far? 
So, ultimately, you want to compare the digits of each number, and their position, left to right? And in the event that there's one number off, everything is okay, but more than one number and it's a reject? If so, your existing logic has a gap if someone typos a number like: 123456789 1123456789 Because then only the first number would be similar when in reality, all numbers are similar. 
Yes, that's the ultimate goal. So if you end up with two telephone number that are 07901234567 and then 0790123456**8** the formula will find that 10 digits are similar in structure and that we should check the veracity of the second number.
Nothing so far; I imagined this must be a standard kind of question people ask of their data and maybe someone would have something pre-packaged!
LOL, I was just trying to give a rough example of 2 same addresses that are dissimilar in terms of street names, etc.
Not the right subreddit I'm afraid!
https://oracle-base.com/articles/11g/utl_match-string-matching-in-oracle Try this site.. You can look up algorithms for string similarity. 
Are there any other not nullable fields?
we'd need to see the Shipments table and it's variables/constraints to give a good answer. 
All the other fields are boolean. The default value is set to No. I think this might be the issue. 
Spoke too soon. They are already set to null. No other not nullable fields.
In MSSQL, I will wrap multiple data-altering statements (INSERT, UPDATE, DELETE, etc.) in a BEGIN TRAN / COMMIT if I want to ensure that the block of statements is an "all-or-nothing" block of statements. That way, if I encounter an error somewhere in the block, I will catch it using a TRY/CATCH, and then a ROLLBACK TRAN will ensure that I don't have a state where only a part of the block of statements made changes to the database. In your example, there's no difference in MSSQL, because like /u/reallyserious said, there's an implicit transaction for individual statements like that.
try it with single quotes instead of double quotes for your strings
Try r/ssrs
&gt; I want to pull all the categories with their assigned products and product information use LEFT OUTER JOINs in case some category has no products SELECT c.category_title , p.title , p.other_columns FROM categories AS c LEFT OUTER JOIN products_has_categories AS pc ON pc.categories_category_id = c.category_id LEFT OUTER JOIN products AS p ON p.id = pc.products_id ORDER BY c.category_title , p.title 
you're looking for /r/PhotoshopRequest
You are a god, thank you so much.
&gt;parameterized queries You're right. I was only running a check to see if date fields were empty or not. I've beefed up the form validation and implemented parameterized queries. Here's my revised code: //Check to see if date is inputed correctly. function validateDate($date, $format = 'd. M Y') { $d = DateTime::createFromFormat($format, $date); return $d &amp;&amp; $d-&gt;format($format) == $date; } if(isset($_POST["submit"]) &amp;&amp; validateDate($_POST["min"]) &amp;&amp; validateDate($_POST["max"])) { $startDate = new DateTime($_POST["min"]); $endDate = new DateTime($_POST["max"]); //Fetch available ticket rates. $sql_rates = "SELECT hotspot_rate.name ". "FROM hotspot_rate ". "WHERE hotspot_rate.name LIKE :GB OR hotspot_rate.name LIKE :MB"; //pdo-prepare, followed by pdo-execute allows us to parametersize the query, preventing various bugs and SQL injections. $pdo_rates = $pdo-&gt;prepare($sql_rates, array(PDO::ATTR_CURSOR =&gt; PDO::CURSOR_FWDONLY)); $pdo_rates-&gt;execute([':MB' =&gt; '%MB%', ':GB' =&gt; '%GB%']); $rates = $pdo_rates-&gt;fetchAll(PDO::FETCH_ASSOC); //Start of SQL Ticket query. $sql_tickets = "SELECT hotspot_account.username, hotspot_account.id"; //Iterate through available rates. for ($rate = 0; $rate &lt; sizeof($rates); $rate++) { $sql_tickets .= ", SUM(CASE WHEN hotspot_rate.name = :".$rates[$rate]['name']. " THEN 1 ELSE 0 END) AS \"".$rates[$rate]['name']."\" "; } //Count total amount of data. $sql_tickets .= ", SUM(CASE WHEN hotspot_rate.name= :".$rates[0]['name']; for ($rate = 1; $rate &lt; sizeof($rates); $rate++) { $sql_tickets .= " OR hotspot_rate.name = :".$rates[$rate]['name']; } $sql_tickets .= " THEN hotspot_rate.traffic ELSE 0 END) AS \"total_data\" "; //Count total amount of tickets, regardless of rate. $sql_tickets .= ", SUM(CASE WHEN hotspot_rate.name = :".$rates[0]['name']; for ($rate = 1; $rate &lt; sizeof($rates); $rate++) { $sql_tickets .= " OR hotspot_rate.name = :".$rates[$rate]['name']; } $sql_tickets .= " THEN 1 ELSE 0 END) AS \"total_tickets\" "; //Rest of SQL Ticket query. $sql_tickets .= "FROM hotspot_ticket ". "INNER JOIN hotspot_rate ". "ON hotspot_ticket.rate_id = hotspot_rate.id ". "INNER JOIN hotspot_account ". "ON hotspot_ticket.account_id = hotspot_account.id ". "WHERE hotspot_ticket.timestamp ". "BETWEEN '".$startDate-&gt;format('m/d/Y')." 00:00:00.000' ". "AND '".$endDate-&gt;format('m/d/Y')." 23:59:59.997' ". "GROUP BY hotspot_account.username, ". "hotspot_account.id"; //Now we will prepare this query, replacing the :rateNames with proper values. $pdo_tickets = $pdo-&gt;prepare($sql_tickets, array(PDO::ATTR_CURSOR =&gt; PDO::CURSOR_FWDONLY)); //One last iteration through rates. Binding values to parameters. for ($rate = 0; $rate &lt; sizeof($rates); $rate++) { $pdo_tickets-&gt;bindParam(':'.$rates[$rate]['name'], $rates[$rate]['name']); } //Execute this mess. $pdo_tickets-&gt;execute(); //Fetch tickets into an array. $tickets = $pdo_tickets-&gt;fetchAll(PDO::FETCH_ASSOC); }
In your worksheet function you are referring to cells. In SQL like Oracle SQL you will refer to items within tables. So selection of the item is really the hard part. Are the rows in your tables Static? 
I've managed to try it on my work PC now. This should work properly on SQL server at least. Not sure about MySQL create table student ( rollno int, name varchar(50), course_no int ) create table course ( course_no int, course_name varchar(100) ) insert into student values (1,'James',1), (2,'John',1), (3,'Michel',2), (4,'Paul',2) insert into course values (1,'Engineering'), (2,'Medicine') select s.summed, c.course_no, c.course_name from ( select count(rollno) as summed, course_no from student group by course_no ) as s join course c on s.course_no = c.course_no
[https://docs.microsoft.com/en-us/sql/t-sql/functions/logical-functions-iif-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/logical-functions-iif-transact-sql?view=sql-server-2017) [https://docs.microsoft.com/en-us/sql/t-sql/functions/substring-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/substring-transact-sql?view=sql-server-2017) If they're guaranteed &lt;=11 digit numbers then it's probably not worth getting into loops or any dynamic SQL.
 select i.id ,p.program ,i.allocation*p.total as total from id_table i full outer join program_table p on 1=1
SELECT [x.ID](https://x.ID), Y.Program, ([x.total](https://x.total) \* y.allocation) as Total FROM Table1 as x JOIN Table2 as y on y.Allocation &gt; 0
Can you explain what your join is doing `on 1= 1`? I was thinking you'd cross join the tables and then join back or something similar to that.
 CREATE TABLE table3 ( ID VARCHAR(9) , Program VARCHAR(9) , Total DECIMAL(7,2) ) ; INSERT INTO table3 SELECT t1.ID , t2.Program , t1.Total * T2.Allocation FROM table1 AS t1 CROSS JOIN table2 AS t2 
There are a lot of 'it depends' answers for DB architecture questions. It usually depends on workloads. I tend towards only denormalising if it has a material performance benefit - I'd expect all of the ID columns to be indexed, so I'd normally just write the extra join. If the DB needed to support queries of complaint by customer every minute, then I'd use the shortcut.
implied inner join? puhlease you're thinking of CROSS JOIN
full outer join? puhlease you're thinking of CROSS JOIN
potayto, potahto
It is redundant, but it's a nice quality of life thing. I just do it for ID columns though. Restricted keywords are only an issue insofar as you need to wrap references to them with qualifiers, e.g. \[Name\] in T-SQL. But again I'm really anal and qualify with aliases everywhere.
the "on" bit of the join is required (unless you use the cross join syntax as suggested elsewhere.) 1=1 always evaluates to true, making the full outer equivalent to a cross join. I just like writing all my joins with an "on", but to each their own.
you're just jealous you couldn't think of the actual best solution
Be careful with `CROSS JOIN`s (which this is). The row count can get a bit out of hand.
you may have a required field probably some identifying id field in that table that needs to be included in your insert statement. if an auto incrementing key isnt setup, you would need to add that key column with proper value to your insert.
Or even try adding every column to your insert statement and supplying a ‘’ or NULL value for all the other columns which you aren’t inserting data. It will either work or you will at least be told the offending column from the insert.
but... but... this is *exactly* what was required!! 
Seriously, who passes up the rare opportunity to use THE COOLEST JOIN.
Yes, Azure **can** send mail with runbooks and powershell. and Yes, Azure sql instances **cannot** send mail alerts.
You're completely correct. I'm just warning /u/antiphony that this sort of thing can blow up if one isn't careful.
This would be an Azure SQL instance, so I'm not entirely certain what you're saying.
I understand now. Thanks.
As other explained, in your example there is no differences. Think of a transaction as a validation. It gives you control to say "yes, this is correct, apply it", rather than "Salty mackerel! I forgot something in the where clause" and restoring a backup. You can do several queries in a transaction, and check a result later, and decide if you want to commit your changes or not. I often have to go in production database and modify live data, either because a bug in our application caused something to be in a state that is not correctly handled, or because a former process migrating data between systems had a bug and I need to correct it. I use a transaction (with XACT\_ABORT ON) to design a select that show the data that are wrong, then write an update, and then run the same select again. As long as my 2nd select don't show the data the way I need them to be, I'm fine, as I rollback everything. In a sale recording for example, our application must save data in approximately 12 tables (article, snapshot of data valid at the time of the order, financial data, ordering... There is a lot going on). If something went wrong while saving, let's say, the financial data, we don't want to have records related to other fields of the sale to be present, that would make no sense and just pollute the database. Transactions allow you to do that. If the application have an exception that is returned from a statement, it can do a rollback and remove everything that was inserted before, leaving the database unchanged. That's the "A" (Atomicity) of the [ACID](https://en.wikipedia.org/wiki/ACID_(computer_science)) acronym, and transactions are how it's enforced. 
Or in your case, to each their "on".
Use this resource, it's well explained and simple. It also allows you to try writing queries. https://www.w3schools.com/sql/default.asp It should guide you through the steps required for this query quite quickly.
Not a video but it covers everything you need to get started with SQL querying. https://www.w3schools.com/sql/default.asp
SUM (x) OVER (PARTITION BY y ORDER BY z) This is the syntax in TSQL (Microsoft Server) The PARTITION BY can contain multiple fields (comma separated if required) as can the ORDER BY.
Left join Table_name Send ON v.id_video = send.video_id AND send.upload_id = 'send' Left join Table_name host ON v.id_video = send.video_id AND send.upload_id = 'host' Left join Table_name Send ON v.id_video = send.video_id AND send.upload_id = 'upload_id' Then SELECT V.*, Send.
Azure SQL instance does not have dbmail so the sql Instance cannot send mails. but you can send out mails by running powershell and runbooks against the database. &amp;#x200B; If you don't understand then just go with it is not possible.
Have you used Qlikview? You could model the data and as it is in memory it's fast. We use Qlikview frequently rather than writing SQL for ad-hoc queries. Our users used Qlikview but we neglected to build an app for ourselves (BI department). When we built our own app it greatly reduced the time taken for ad-hoc queries. It's not cheap though and requires a server with a lot of memory.
We have a reporting layer that does this but we have such a lot of data our adhoc function is to present that data not in that layer. Repeat topics get pushed into the reporting layer for self service. What we deal with is a high volume of unique requests that may got 8-12 months before another ticket repeats the same content. Or it’s a brand new feature / function they want to do. I’m hoping that I can take a step back and see some high level patterns that could go into the self serve layer. Thanks for the suggestion though 
I don't think you understand completely It isn't my job to understand anything more than, "is it possible for me to get an email?" If the answer is, "yes," then my IT team doesn't understand, and that's a problem which needs to be fixed. 
i'm using SAS so no worries about row count ;)
Very nice using cross join, which I was not aware of! I spent a long time thinking about which join to use and I wish I had known about this
I like this 1=1 method, thank you
I think this would be a pain in SQL. I'd bring it into R or Pandas, remove anything that isn't a digit, convert to a string and calculate the Levenshtein distance. https://en.wikipedia.org/wiki/Levenshtein_distance
This really isn't a SQL specific question, but anyways. Is there some requirement to use Visual Studio? Microsoft Access has an entire visual front-end portion of the application that can be built that does exactly what you're looking for. [Here](https://support.office.com/en-us/article/create-a-form-in-access-5d550a3d-92e1-4f38-9772-7e7e21e80c6b)
This doesn't sound like an SQL issue, SQL will quite happily query multiple fields. Possibly ask the subreddit for your programming language about how to build queries to be submitted. Generally there will be ways to safely bind variables so injection doesn't work, or existing solutions to checking a variable for injection risk. Also, if this is a local DB used by a few internal customers you don't have to worry about SQL injection. If this is public facing you shouldn't be using Access as a backend. And if this is a small setup for a few internal users, why not just build the forms in Access?
Can you explain to me what the benefit of working with/learning PowerShell is? I realize this sounds stupid, but can you automate things with it? For example, I have Python jobs that I have written... are you saying I can do something in the PowerShell to get them to run on a schedule? Is PowerShell similar to a Linux command line?
I don't think injection will be an issue; it is literally just going to be her using this program to log observations of trees. That said, I figure it's better for my development as a programmer to practice healthy habits. I'm looking into setting it up in Access. Thing is, will I be able to just send her the completed Access file where viewing the data, adding/editing the data and querying the data is all fairly straight-forward? &amp;#x200B;
Yeah sorry, I'm still green to the point where I'm not quite sure what category my 'fuck up' falls under. Thanks for your help regardless. This looks really helpful, thank you. My only concern is whether, when all is said and done, I'll be able to send her this Access file wherein she can easily view/edit/query the data without having to learn any complexities. She's fairly attached to her Excel spreadsheet, so the simpler the better. &amp;#x200B;
You'll be able to provide an Access file that will be a fully bespoke application in terms of functionality. I'm not sure what you mean by querying the data (it doesn't sound like you expect this person to be able to craft SQL queries), but you will be able to provide reports with filter options.
Like others pointed out you may want to do this in Access forms. \&gt;&gt; Employee ID = textbox1.text AND Salary = textbox2.text. But in the code you posted why do you think that textbox2.text matches Salary type? try Salary = Val(textbox2.text)
She wants to be able to view the data in more elaborate circumstances ("Show me all Elm Trees with Height between 30 - 40 ft" etc.); as far as I know this is not something Excel can provide but Access can. Maybe I'm wrong here (not an Excel expert) but that seems to be what she wants me to do.
Sorry that was more-or-less Pseudocode; just trying to relay that I wanna be able to enter search values for the database that stack.
Putting filters on an Excel spreadsheet is literally as simple as going to Data &gt; Filter, then the column headings turn to drop downs where you can select 'Elm' from the species column and Number Filters &gt; Between on the height column. 
This is all you'll need - &amp;#x200B; SELECT uniqueID, MIN(Reason) AS Reason, MIN(Subreason) AS Subreason, MIN(Date_of_Creation) AS Date_of_Creation FROM Table GROUP BY uniqueID &amp;#x200B;
You just need to build the sql command, but if use Access it will be even easier as you can use Access queries and nest them, each with a different criterion 
If I am following your issue it sure sounds like a correlated subquery would do the trick. If you still can’t get what you need after doing web searches for “correlated subquery” let me know and I will try to give you more specific help. FYI, with SQL there are usually many ways to get the same result. Some are more efficient than others. If the tables aren’t large the time savings between the most efficient and least efficient is minimal. I say this because if the tables are small and you feel more comfortable using multiple sub queries, go ahead and use that method. It is always good to learn and try multiple methods but don’t get too hung up on finding the most efficient method while you are still learning the basics. My $0.02
SELECT uniqueID, Reason, Subreason, MIN(Date\_of\_Cr)ation) First\_ Date\_of\_Creation FROM Appointments GROUP BY uniqueID Reason, Subreason,
&gt; As other explained, in your example there is no differences. There *is* a difference if OP is using e.g Oracle Database. He didn't mention what database he's using. 
If your data is pretty big, you may get better performance using a window function. WITH cte_Appointments AS ( SELECT UniqueId, Reason, Subreason, DateOfCreation, ROW_NUMBER () OVER(PARTITION BY UniqueId ORDER BY DateOfCreation) RowNumber FROM Appointments) SELECT UniqueId, Reason, Subreason, DateOfCreation FROM cte_Appointments WHERE RowNumber = 1 &amp;#x200B;
 select * from employees where id = (your thing) and salary = (your other thing)
I think you want this Select id, reason, subreason, date of creation from (select id, reason, subreason, date of creation, row_number() over(partition by id, reason, subreason order by date of creation asc) as earliest from appointments) as A where A.earliest = 1;
There is no join between your main query and subquery, so your subquery always returns same employee record for all your main query rows. You can do it by adding where clause or even better would be to move your subquery out from select statement and make a join.
Both queries use the same table. I'm not sure how a join like that would look. Do you have an example query? Greatly appreciated
If your table has a primary key, join them using that. This way you'll be certain both queries are talking about the same record/row. If it doesn't have a primary key, maybe you can try joining by all the columns, I think... But it's not guaranteed to work.
&gt; SELECT valid_from employees ORDER BY valid_from ASC LIMIT 1 select a.*, b.start_date from employees a join ( SELECT max(valid_from) as start_date, employee_id from employees group by employee_id )b on a.employee_id = b.employee_id
&gt; If my title changed from A to B the following changes would happen in the table: why would your `is~valid` column be 0 though? as for your problem, you need to join to a "max subquery" SELECT e.the_columns_you_need FROM ( SELECT employee_id , MAX(valid_from) AS latest FROM employees WHERE department LIKE '%XXX%' AND is_valid = 1 GROUP BY employee_id ) AS m INNER JOIN employees AS e ON e.employee_id = m.employee_id 
The group by always says the employee_id is ambiguous. Even if I put e.employee_id or m.employee_id
For some reason this yields; &gt;Column 'employee_id' in field list is ambiguous Similar to my response to /u/r3pr0b8
I got it working, thank you
I got it working, thank you
i don't see any sql here whatsoever try /r/javascript 
Anyone else doing this? I like the idea but the website is sketchy as all hell.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/datascience] [Hey guys !! Is anyone of you an SQL pro ?](https://www.reddit.com/r/datascience/comments/am2hay/hey_guys_is_anyone_of_you_an_sql_pro/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
any ideas to improve the website ? open to any 
What this homework from a textbook or online resource? If you can link me to the website or the name of the book that would be awesome! Thanks
Did you make it? I'm not trying to sound offensive when I criticize it. * There's no background to the text, making it difficult to read. * There's no explanation or example as to what will be received. * There's very little explanation period, it feels more like a data grab than anything. * When you click the Sign Up button, the form loads, but it's a little wonky with it snapping back and forth when you scroll up and down, again with the text difficult to read. * There's no privacy policy, though it does say information will only be used for the list. * There's no info about ***you***. If it's yours, tell people why you think they'll benefit, what skills you have to share, who knows. Just ideas!
I tried javascript but was told /r/learnjavascript where I was told jsp is too old. I just don't have many other places to show jsp .
Thanks. Not offense taken. I just got started with it so im very open to suggestions.
Excellent! So it your your site? Can you tell us more about what will be received if we sign up?
Honestly, the way you've posted it here comes across very spammy.
Found this in my SQL notes. Try this.... [https://forums.asp.net/t/1965445.aspx?How+to+split+a+comma+separated+value+to+columns+in+sql+server](https://forums.asp.net/t/1965445.aspx?How+to+split+a+comma+separated+value+to+columns+in+sql+server)
First of all you need to use a string split method or function that will transform your column into multiple rows based on your delimiter. Then you need to pivot the data from rows to columns. There's loads of information out there on these topics and this should keep you busy for a while.
In Postgres you could do something like this: If there is exactly one comma: update your_table set new_column = split_part(existing_column, ',', 2); If there might be multiple commas after the first one: update your_table set new_column = substr(existing_column, strpos(existing_column, ',') + 1); &amp;#x200B;
This is a SQL subreddit.
What is the question/problem you're having here?
use substr() and instr() ?
in MySQL, SUBSTRING_INDEX()
Is there an easier way then doing multiple BEGIN / END? &amp;#x200B; How do I update the field instead of inserting?
What's with people not mentioning what database they use? If you have just one comma in there and want to have one column with everything to the left of it and another one with everything to the right of it, you can just search for a comma and then in one column take the substring leading up to it and another one with content after it. In MySQL: select id , substring(t.whatever, 1, ifnull(nullif(instr(t.whatever, ','), 0)-1, length(t.whatever))) , substring(t.whatever, nullif(instr(t.whatever, ','), 0)+1, length(t.whatever)) from myshittydata t [Here's a working example](https://www.db-fiddle.com/f/2BtCCkPo9D2faTdV9KEqVJ/1). For SQL Server you'd have to change functions, like: * `ifnull(x, y)` = `isnull(x,y)` * `instr(x, y)` = `charindex(y, x)` * `length(x)` = `len(x)`
If the contents of your `IF` block are identical (it looks like they are), you can do this: IF (@Stn_ID in ('3160','2760',2600'','1860')) BEGIN SELECT TOP (1) Order_Nbr FROM MES_data_Job_Ctl AS ctl INNER JOIN MES_data_Job_Stn AS Stn ON Ctl.Job_ID = Stn.Job_ID Where Stn_ID = @Stn_ID ORDER BY Ctl.Inserted_DT DESC SET NOCOUNT ON; END
I'll try this, what about updating the Last\_Broadcast table instead of the insert? &amp;#x200B; UPDATE Last_Broadcast SET Test = Order_Nbr FROM (SELECT TOP 1 MES_data_Job_Ctl AS ctl ORDER BY Ctl.Inserted_DT DESC INNER JOIN MES_data_Job_Stn AS Stn ON Ctl.Job_ID = Stn.Job_ID) Where Stn_ID = @Stn_ID &amp;#x200B;
Or what about this? IF (@Stn_ID IN ('1860', '2600', '2760', '3160')) BEGIN @Order_Nbr = SELECT TOP (1) Order_Nbr FROM MES_data_Job_Ctl AS ctl INNER JOIN MES_data_Job_Stn AS Stn ON Ctl.Job_ID = Stn.Job_ID Where Stn_ID = @Stn_ID ORDER BY Ctl.Inserted_DT DESC UPDATE Last_Broadcast SET @Stn_ID = @Order_Nbr SET NOCOUNT ON; END &amp;#x200B;
There are a lot of cool solutions in here, but to toss out a non-sql answer, if the data set isn't millions of rows and it's a one off request, use excel. Most SQL pivot functions take a lot of work, and doing this in excel is literally a one click and your done level of effort. If the data is too large and/or you have to do this multiple times xpath in sql server has some awesome string functions that can get the job done with a minimum of self-flagellation. 
&gt;can you automate things with it? Yep! &gt;I can do something in the PowerShell to get them to run on a schedule? You can schedule the scripts to run via Task Manager (or a 3rd party scheduler), just like you would schedule bash scripts with cron &gt;Is PowerShell similar to a Linux command line? Yes, PowerShell Core also runs _on_ Linux :) The main reason I suggested PowerShell in this instance is SQL Server allows you to schedule jobs with the SQL Agent that's built into SQL... and it directly supports running PowerShell scripts from it. It also supports easy querying of SQL via `Invoke-SQLCmd`
this is the old school way, not using window functions for the top 2 row number, but gleefully using WITH syntax -- WITH maxes AS ( SELECT category , rows FROM ( SELECT category , COUNT(*) AS rows FROM yourtable GROUP BY category ) AS m ) , toptwo AS ( SELECT MAX(rows) AS max2 FROM maxes UNION ALL SELECT MAX(rows) FROM maxes WHERE rows &lt; ( SELECT MAX(rows) FROM maxes ) ) SELECT t.category , t.words FROM ( SELECT category FROM maxes WHERE rows IN ( SELECT max2 FROM toptwo ) ) AS x INNER JOIN yourtable AS t ON t.category = x.category ORDER BY t.category if your platform down not support WITH syntax, then you can do like i did -- take the text of the subqueries defined by the WITH clause, and substitute them into the sql of the actual SELECT... i tested it this way because i'm still on MySQL 5.7 and here are the results category words B Hello B Hey B Howdy B Hi D Pear D Orange D Grape D Banana D Apple 
 SELECT Category, Words FROM YourTable WHERE Category IN (SELECT TOP 2 CATEGORY, COUNT(*) FROM YourTable GROUP BY Category ORDER BY 1 DESC) I think this one is a bit more easier to understand, and gives you the same result. (The TOP 2 is T-SQL specific syntax I believe, so in your platform it might be LIMIT 2) 
At a glance it looks like this is being done so that two datefields that are stored in different formats match so that they can be compared. 
I like this way the best. It's possible to do as one query with a cross apply or row_number(), but I don't really think you gain anything except being more confusing. I would write the variable select as this though: SELECT TOP (1) @Order_Nbr = Order_Nbr FROM MES_data_Job_Ctl AS ctl INNER JOIN MES_data_Job_Stn AS Stn ON Ctl.Job_ID = Stn.Job_ID Where Stn_ID = @Stn_ID ORDER BY Ctl.Inserted_DT DESC
&gt; What does TRUNC(datefield) + 1 evaluate to if the date is null? NULL 
well, sure, if your platform supports TOP or LIMIT, yes by the way, you have an error in your syntax -- an IN subquery is allowed to return only one column
I was able to mess around with this query and eventually get it to do what I needed it to do. Thanks a lot!
I'll be honest but I have intentionally avoided anything PowerShell related over the last few years because it is kind of outside of my official "swim lane." I've read and heard things, but in general it's not a tool that our DBA's would like me to be using, and not one that they themselves are comfortable using.
Well that’s good news for our data’s validity then lol thank you. What might the nvl2(...trunc(date) +1 expression accomplish then? Want to understand for my own knowledge.
Well that’s good news for our data’s validity then lol thank you. What might the nvl2(...trunc(date) +1 expression accomplish then? Want to understand for my own knowledge.
You need to be using the value method instead of the query method. &amp;#x200B; [http://www.sqlservercentral.com/articles/Stairway+Series/+Querying+XML+Data/92784/](http://www.sqlservercentral.com/articles/Stairway+Series/+Querying+XML+Data/92784/)
 from the docs -- NVL2(expr1,expr2,expr3) NVL2 lets you determine the value returned by a query based on whether a specified expression is null or not null. If expr1 is not null, then NVL2 returns expr2. If expr1 is null, then NVL2 returns expr3. so your code is saying "if `date` is not null, use `trunc(date) + 1` (which gives the day after the date), otherwise repeat this logic with another date, presumably until it finds one of those dates that's not null, and returns the day after that 
 SELECT SUBSTRING_INDEX(url,'/',3) a textbook classic example of this fabulous function
Ahhh I see. Yep 👍🏻 thank you totally makes sense now. 
You are an absolute legend! Thank you. I've been pulling my hair out over this. 
Oh yeah, you're right, sorry. 
 WITH MAX_W(W, NUMBER) AS ( SELECT TOP 2 RESOURCE_TYPE W, COUNT(*) NUMBER FROM TABLE_NAME GROUP BY CATEGORY ORDER BY 2 DESC ) SELECT CATEGORY, WORD FROM TABLE_NAME WHERE CATEGORY IN (SELECT W FROM MAX_W) Just in case, and sorry for misleading you previously :D 
You can see why posting it here doesnt make sense, right? 
substring instring combo
Maybe try, set feedback off. Been a while since I oracled though...
Set feedback off but test to make sure there's no additional feedback that's getting ommitted that you didn't want to get ommitted. 
That worked! Now i just have one single line break in between each of the query results. Any ideas on how to remove that?
This should be what you need if you're version of SQL Server supports CTE's. &amp;#x200B; DECLARE @table TABLE ( ID INT NOT NULL, DT DATE NOT NULL ) INSERT INTO @table (ID, DT) VALUES (1, '2018-01-01'), (1, '2017-01-01'), (1, '2016-01-01'), (1, '2013-01-01'), (1, '2012-01-01'), (2, '2017-01-01'), (2, '2016-01-01'), (2, '2013-01-01') ;WITH DATES(DATE, ID) AS ( SELECT DISTINCT DT, ID FROM @table ), GROUPS AS ( SELECT ROW_NUMBER() OVER (ORDER BY DATE, ID) AS rn, DATEADD(YEAR, -ROW_NUMBER() OVER (ORDER BY DATE, ID), DATE) AS GRP, DATE, ID FROM DATES ) SELECT ID, MAX(consecutiveDates) AS [COUNT] FROM ( SELECT COUNT(*) AS consecutiveDates, MIN(DATE) AS minDate, MAX(DATE) AS maxDate, ID FROM GROUPS GROUP BY GRP, ID ) AS a GROUP BY ID
You can do something like update to left (CustID,1) + "0000" + right(CustID, 3). With a where CustID like "[a-z]*" It should get you started
the code works as is but I need to call it within a VIEW. :( 
Your logic is off on your query. I don't use MySql, but I believe you're just doing a cross join on your sub select. Can you try a left join from trials to entries on t.id = e.trial_is AND e.user_id = 59 Then same select but use whatever the ISNULL(count(user__id), 0) equivalent is.
Yeesh! That’s a beast, but I’ll give it a try. Thanks!
Just ''UNION All'' the sets together.
Can you just `UNION ALL` it? SELECT e.EmployeeId, COUNT(EmployeeReward.EmployeeId) As Count FROM dbo.EmployeeReward RIGHT OUTER JOIN dbo.Employee AS e ON e.EmployeeId = EmployeeReward.EmployeeId WHERE (EmployeeReward.Date &gt;='2018-01-01 00:00:00.000' AND EmployeeReward.Date &lt;'2019-10-01 00:00:00.000') OR (EmployeeReward.Date IS NULL) GROUP BY e.EmployeeId UNION ALL SELECT e.EmployeeId, COUNT(EmployeeReward.employeeId) As Count FROM dbo.EmployeeReward RIGHT OUTER JOIN dbo.Employee AS e ON e.EmployeeId = EmployeeReward.EmployeeId WHERE (--CURRENT YEAR AND CURRENT YEAR BONX BEFORE JAN MONTH(EmployeeReward.Date) BETWEEN 10 AND 12 AND MONTH(GETDATE()) BETWEEN 10 AND 12 AND YEAR (EmployeeReward.Date) = YEAR(GETDATE()) ) OR (--CURRENT YEAR AND CURRENT YEAR BONX AFTER JAN MONTH(EmployeeReward.Date) BETWEEN 1 AND 9 AND MONTH(GETDATE()) BETWEEN 1 AND 9 AND YEAR (EmployeeReward.Date) = YEAR(GETDATE()) ) OR (--CURRENT YEAR AND PREVIOUS YEAR BONX MONTH(EmployeeReward.Date) BETWEEN 10 AND 12 AND MONTH(GETDATE()) BETWEEN 1 AND 9 AND YEAR (EmployeeReward.Date) = YEAR(GETDATE())-1 ) OR (--return no bonx employees EmployeeReward.Date IS NULL ) AND (EmployeeReward.Date) &lt;= (GETDATE()) GROUP BY e.EmployeeId 
DECLARE @user\_id INT = 59 DECLARE @event\_id INT = 12 ; &amp;#x200B; &amp;#x200B; SELECT \[Event\] = [E.name](https://E.name) ,\[Trial\] = [T.name](https://T.name) ,\[Enrolled\] = CASE WHEN ent.\[user\_id\] IS NULL THEN 0 ELSE 1 END FROM dbo.\[EVENT\] e INNER JOIN dbo.\[TRIAL\] t ON t.event\_id = [e.id](https://e.id) LEFT JOIN dbo.\[ENTRIES\] ent ON ent.trial\_id = [t.id](https://t.id) AND ent.\[user\_id\] = @user\_id WHERE [e.id](https://e.id) = @event\_id
Give this a shot. &amp;#x200B; WITH top_two_categories AS ( SELECT TOP 2 Category, COUNT(Category) AS number_of_rows FROM table GROUP BY Category ORDER BY COUNT(Category) DESC ) SELECT table.Category, table.Words FROM table INNER JOIN top_two_categories ON table.Category = top_two_categories.Category &amp;#x200B;
UPDATE CustID SET CustID = Right(‘000000’ + CustID,6) 
set feedback off &amp;#x200B; Maybe?
Following on from u/14IS4 but maybe less to code: &amp;#x200B; DECLARE @table TABLE ( ID INT NOT NULL, DT DATE NOT NULL ) INSERT INTO @table (ID, DT) VALUES (1, '2018-01-01'), (1, '2017-01-01'), (1, '2016-01-01'), (1, '2013-01-01'), (1, '2012-01-01'), (2, '2017-01-01'), (2, '2016-01-01'), (2, '2013-01-01') ; WITH cte AS ( SELECT ID ,DT ,[ctr] = 1 FROM @table UNION ALL SELECT Tbl.ID ,Tbl.DT ,ctr = ctr + 1 FROM cte t INNER JOIN @table tbl on tbl.ID = t.ID AND tbl.DT = DATEADD(YEAR,1,t.DT) ) SELECT ID ,MAX(ctr) FROM cte GROUP BY ID &amp;#x200B;
Quickest way would be to add the target into the SQL result set and then add a secondary measure. If you wanted a fixed value then you would just go to chart data and under values Add.. Expression. State your target value. Then right click the newly created series, change the chart type for that series to be line and there you have it.
The only other consideration would be that the time is likely datetime format, so if you want unique users per day, you would likely need to do some sort of aggregation at a date level. So a similar query to what u/ihaxr has said but with a group and no DISTINCT &amp;#x200B; SELECT User_Column,user_Date = CAST([time] AS DATE) WHERE EventType LIKE 'BROKER_USERLOGGEDIN' -- Performance, look at indexing this field. GROUP BY CAST(time AS DATE) &amp;#x200B;
Try set trimspool on and set sqlblanklines off https://docs.oracle.com/cd/B19306_01/server.102/b14357/ch12040.htm
Have you checked out PowerShell AzureRm save context? [https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/save-azurermcontext?view=azurermps-6.13.0](https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/save-azurermcontext?view=azurermps-6.13.0) Also: Import-AzureRmContext and Select-AzureRmSubscription 
&gt;\_ This worked great thank you! Totally different question, is it possible to capitalize the a column header? I'd like it to look like UPT instead of U Pt (how it comes out now?) Thanks again!!!
You've double-checked the path to make sure it's correct?
Quadruple checked. I did noticed Windows keeps on resetting the permissions of my IS 470 folder where the file is located 
Maybe test with another directory? One you know you have full control over?
I'd refer to any course materials displaying a schema, and use a similar layout. Barring any existing examples like that, I'd probably just list all the create table statements needed to create an empty database.
*This makes perfect sense.* I was looking in the wrong diection. As its the same number of columns from both instances of the table, the UNION will work. I have only used UNIONs to combine data from multiple tables , however I see it can also be used to combine data from the same table. Especially as only one date condition can exist at a time **Thank you very much!!!!!!!** 
Thank you very much. Makes perfect sense. Will use this !!!
Make a CTE with your current solution and query it just for the fieles you need 
No other way? I don't have much time to learn CTE's 
Its pretty easy though, And will help you a ton ! Lets say you have your current query call it query A. Don't know which flavour you are using but in PostgreSQL you go like this: With (my_new_cte_name) as ( query A) Select (manager_id, employer name, count(...) ) From my_new_ctw_name... Basically you create a table in memory and query it, For that count you will have to group by though
Write a keyboard macro using auto hot key. If you have a fancy keyboard there mighy already be macro functionality built in.
Yes! Thank you this worked. I tweaked it but once I saw your code I pictured Legos being taken apart and the zeros being put back in.
mate CTEs if rather easy, it just sounds complicated. Just google it (don't forget to include SQL flavor; oracle, posgres, ms, mysql...). It is just query to create temp table like and then you filter what you need.
1. you don't need "with as", and you are actually not using it in your query. 2. I don't get this "punctuation" error, but in general, query looks fine. If no values then either you have a incorrect value for "from_address" or there are no records for this address. Exxecute again query no 2 but remove where and order by parts. Copy value of any random address you get, and then ran again original query no 2 using this address you copied. If there are results back, then your query is good but there are no records for address you want to use. 
&gt; I'd like it to look like UPT instead of U Pt (how it comes out now?) please show the actual line from your query like this -- , SUM(Quantity)/COUNT(DISTINCT ticket_id) AS UPT
what you want is **foreign keys**
The reason to partition a table is to generally speed up the write (insert or update) processes which generally lock the table until the writing is over. Partitioning works based on the value of a column (or a computed column). For example, if Age is one of your columns, you can partition based on Age by saying that the Ages 0-25 will be in partition 1, 26-50 in partition 2 etc. &amp;#x200B; For your case, you have to look at [indexes](https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html). Querying a partitioned table vs a normal table won't have much performance improvements. But creating the indexes based on the important query columns will definitely improve your performance. There are two types of indices that you can create Clustered index (index that will reorder the way the entire table is stored based on the columns used for indexing. There can only be on clustered index.) and Non-clustered index (this is sort of like a second list with the columns (specified to partition on) in a specific order as pointers to the actual memory locations which could be in a different order. More than on non-clustered indices can be created).
 CREATE TABLE users_roles_t ( user_id int NOT NULL, role_id int NOT NULL, PRIMARY KEY (user_id, role_id), FOREIGN KEY (user_id) REFERENCES users(id), FOREIGN KEY (role_id) REFERENCES roles(id), );
thanks a lot. How I didn't know this as a developer, worries me. Definitely good to practice. All I've been doing for years is just reading from Dbs. &amp;#x200B; Did this in the end: &amp;#x200B; CREATE TABLE users_roles_t ( userId INTEGER NOT NULL FOREIGN KEY REFERENCES dbo.usersp(id), roleId INTEGER NOT NULL FOREIGN KEY REFERENCES dbo.rolesp(id) ); &amp;#x200B;
lool! Thanks. Just noticed you used the same name as the table. Did your 't' at the end mean 'test' also?
I'm still getting errors. Here's the screen shot of the site and my solution. [https://imgur.com/a/WhEORGa](https://imgur.com/a/WhEORGa) &amp;#x200B; The question is here: https://www.testdome.com/questions/sql/users-and-roles/20598?visibility=1
Yes, I meant `users_roles` for the name of the table.
that works too however, the syntax that /u/krugarkali gave is my preference, because it allows you to *name* the foreign key constraint, which makes it a lot easier to remove if you have to CONSTRAINT user_roles_user FOREIGN KEY (userid) REFERENCES users (id)
It won't help in that case. Are you making sure you are using your partition key in the query? Can you give us a shortened sample of the table and partition ddl and a sample query?
Constraints should come after all columns.
Hi there, I appreciate the reply. I have tried again with another from_address and I still get no results. I can see from any Ethereum block explorer that the first address has over 200,000 sent transactions, so there must be values. Regarding the punctuation error, what I meant was the address itself starts with 0x. When I enter this after the =, I get an invalid hex syntax error, so I arbitrarily added ‘ on either side, without knowing whether that was correct or changed the meaning of the query at all. 
Hey! Thanks for helping! I'm a noob in SQL, and I've never had seen that kind of code, I've only worked with much simpler queries, and when I try to run yours it gives me many errors, but I will try to understand your logic and do it myself, many thanks!!
Yea, I figured I would have to do some joins, I will try it, thanks!!
It’s likely just the field names. I built the tables locally to test it all out. Happy to help if you get stuck. 
I’d say a gridview for basically viewing data in its raw form or a dashboard. People like colors, charts, graphs, lines, etc. Both with an export to Excel option.
After a lot of attempts, your solution did not work either. That is a screenshot showing the solution and error [https://imgur.com/a/9VkTx2N](https://imgur.com/a/9VkTx2N)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/AWJhnHI.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20efljje6) 
Ask them what they need and throw a website out that gives them that data. Are you a full stack guy or what do you do exactly? 
Power BI ? You can create views and access it with Power BI for easy dashboards.
Try removing the single quotes around the binary criteria. Binary values aren't actually string so they don't need to be single quoted. Your first query doesn't limit anything because you're essentially saying "give me an interim table with a single column called from_address and a single row in it with your value, then give me everything from the transactions table" because you don't use the interim table with a WHERE clause in you query.
Well I do GIS and I create all the reports. I do have a website guy but he's always busy, my last request he wasn't available for 2 weeks, this week he flew here to London from New York but he had too much work to do this 
I searched gridview and it sent me to an Android thing?
Looks like that one is paid but will investigate
The desktop version is free, I think.
The solution that worked in the end was: &amp;#x200B; CREATE TABLE users_roles ( userId INTEGER NOT NULL , roleId INTEGER NOT NULL, Foreign Key (userId) References users(id), Foreign Key (roleId) References roles(id) ); &amp;#x200B; Now, one test out of 4 is failing. \-A user can only exist within a role once: Wrong answer &amp;#x200B; Any assistance will be appreciated.
Metabase is FOSS (though it's AGPL), web based. Mostly for charts but shows tabular data too
Userid and roleid need to be part of a primary key to avoid having the same user in the same role more than once. 
What are they going to be using it for? Do they need pretty graphs, a dump of the raw data, tailored reports or something else? 
 CREATE TABLE users_roles ( userId INTEGER NOT NULL , roleId INTEGER NOT NULL, Primary Key (userId, roleId), Foreign Key (userId) References users(id), Foreign Key (roleId) References roles(id) ); Mate, perfect! Thanks a lot. &amp;#x200B; It appears there are some 'syntactical sugar' that can be done in T-SQL but won't work in SQLite, based on my findings. Hopefully I'm correct.
PowerBI is an option but you'll likely need to get them a pro licenses (since most users will likely want to share their info) or setup a premium instance depending on how large an audience you're serving. A tool like Domo could work as well, though it's a bit costly. &amp;#x200B; have access to SSAS or SSRS? For the data guru, linking to a cube through excel will give folks the familiarity of a pivot table with the ability to do a lot of their own digging through the data (and build dashboards and such over the top). You'd likely have to use a ODBC connector to make it work. SSRS if you want to do some more of the actual report design work. Similar constraint with the connector as well. &amp;#x200B; Sharepoint has some limited functionality as well, where you can link to sql server, but I don't think that would be ideal. &amp;#x200B; If nothing else, you could create a database which has limited views and restricted access and use that account to allow folks to connect an excel pivot or other tools to it which could manage the pulling of the data.
Depends what language you want to use. There are free gridview controls out there for ASP.Net. There are controls you can pay for as well. DevExpress has a lot of nice products.
Thanks for the reply. When I remove the single quotes from the term beginning with 0x, I get an “invalid hex” syntax error. How would I change the first option to make the where clause apply? I know there are more than 200,000 transactions from that from_address. The purpose of the WHERE clause was to narrow down to those 200k transactions but I am clearly not doing it right. 
Budget is getting tight.. Never heard about SSAS Database is postgresql not SQL server The data is updated constantly so another database doesn't work. And users are all illiterate in sql
Dump of my report views works, or dashboard like I mentioned on OP Access from PC in internal website 
&gt; The data is updated constantly How up to date does the data they get need to be? Once they get a download of some kind, they're already going to be out of date... One thing you might consider is building a set of views that provide commonly needed data, then make excel files that have refreshable links to those views. Then they can download the excel files, build their stuff into them, then have the data refresh when they need updated information from the database. This of course assumes they're primarily doing their work in Excel.
Look into NOT EXISTS or NOT IN. Your goal is to essentially find any employee records who do not have their ID as another employee's managerId.
Are you asking for a website? An application? Are you going to develop it or want something out of the box? What's your budget and/or time constraint? This is a rather broad question.
I made connected Excel documents that would refresh a view upon opening, or clicking refresh. They can then create tabs for pivot tables and whatnot for individual use.
'Tis. It's what I use for my non tech folks. It's great because you can connect to a ton of resources including databases, and even Excel (they love their Excel). I created beautiful dashboard reports and use slicers to change the dates etc. To take it user friendly. I store the report in a shared drive and my team can access it no problem. Create back ups though. My team managed to break reports in ways I didn't know was possible. Back ups allowed me to be cool as a cucumber about it.
Is that done purely inside Excel?
Are these Excel files connected to the db without any other software needed?
Just trying to figure out what's the easiest/best solution A tab on the website can work. I know SQL and another dude knows JavaScript. Budget is try to keep it minimal so boss won't get upset
Yeah, they like theirs Excel... :p
Easiest and best are often at odds with each other ;) If you have Excel you can set up data sources and the users can pull data directly into it. That's probably as simple/easy as it gets (assuming you have Excel/Office already). 
Try left joining on the table. You can then filter down the results from there. Didn’t include the entire statement but hopefully the portion below will help you out. From employees e Left join employees e2 
Yea I was going to say Excel or (gasp) Access. 
I have no idea what gis is but I would recommend creating a simple web app and binding the data to a table. 
&gt; Are these Excel files connected to the db without any other software needed? I think so, but it's possible there would need to be some kind of ODBC connection driver for your postgres database. Here's an idea of how it works: https://support.office.com/en-us/article/connect-to-a-postgresql-database-power-query-bf941e52-066f-4911-a41f-2493c39e69e4 and https://www.youtube.com/watch?v=z5DrJM7qsEk
Cube it. Or if it’s specific datasets, you could create a user group who can access views of the data in read mode. Then they can do stuff in excel or access to consume and manipulate. 
What is `2` supposed to be? I don't really understand using `group by 1` but what does `having 2` mean?
I can't stand using numeric representations of columns to begin with, so I'd say both are "wrong". :) But at least the second explicitly defines the column. Referencing aliased column names isn't quite as egregious, at least.
It's not accepting the column number as a valid reference in your HAVING clause. I'm not sure if that's a version specific feature.
SELECT user_id, SUM(watch_duration_in_minutes) AS 'total_duration' FROM watch_history GROUP BY user_id HAVING SUM(watch_duration_in_minutes) &gt; 400;
You can reference columns by their index in the select. So where the second column is greater than 400.
Those referencing the selected columns. "1" being user\_id and "2" being SUM(watch\_duration\_in\_minutes).
HAVING is essentially a where clause used for grouped items. 'HAVING 2' is in reference to the second column. So in other words, it's asking where the sum of watch_duration_in_minutes is over 400. Hope that helps! 
What do you mean by their index? Do you mean if I `SELECT ID, Name, State` that I can `GROUP BY 1, 2, 3` to reference the order of the items I selected?
So as I progress here, should I assume I will be using full column names or what I set them AS instead of these numeric column representations?
I understand what `HAVING` does, but `2` makes no sense to me, so the OP is trying to say `2 = sum(watch_duration_in_minutes)`?
I guess my question more directly is: should it be accepting it or am I applying it wrong. Upon some response it sounds like numeric column references arent the way to go. 
Correct. Which is why in my example it worker for my "correct" example. My issues lies in why it wouldn't apply to my HAVING statement. 
Puke. If I saw someone writing `GROUP BY 1, 2, 3` I'd punch them in the throat.
that is legal, and also produces terrible sql, because reordering the columns in the select sends everything directly to hell
@PiedPifer, so shouldn't the first example I gave be correct then?
2 is the column number. Sometimes you can use it in lieu of the column name (eg. 'ORDER BY 2 DESC') but in this case the HAVING clause is not recognising 2 as a valid reference. OP is using '2' as a shorthand for sum(watch_duration_in_minutes).
duly noted haha, sounds like the general consensus. 
Look, forget why it's right or wrong. The answer is there's a technicality about SQL's 1950s grammar that means it can't prove that you mean the column number. Focus instead on why it's a bad idea to use column numbers. Now you can't really change the select, because you have to go through and maintain the indices. At two of them, it's already a little churlish. Real queries tend to return ten to fifteen things. Use names because they produce more maintainable code. 
yes. a thousand times yes.
Again, it may be version specific. I work with PSQL almost exclusively so I couldn't say definitively. Generally it's bad practice to use column numbers as shorthand for anything but very small queries. It seems easy at first, but it can become a pain very quickly as you rewrite things.
I always use full column names. It's mostly preference. But I think you can only use the position in ORDER BY clause. Here's my preferred way of writing your query: SELECT user_id ,[total_duration] = SUM(watch_duration_in_minutes) FROM watch_history GROUP BY user_id HAVING SUM(watch_duration_in_minutes) &gt; 400; Lately I've become a fan of using `=` with square brackets `[` &amp; `]` rather than `AS` for column representation, as it clearly shows what the name of ALL the columns in your SELECT list because they will always be the first. Otherwise, if you put your expressions first, the column names end up being all over the place. Same with explicitly writing the column names in GROUP BY, ORDER BY and HAVING. No confusion and no extra work needed to look up the aliases or ordinal positions in other parts of the code. Hopefully that made sense!
Ha, it took me a long time to get over the habit. Looking at old stuff makes me want to punch myself.
yes. you have to enable developer options in the settings in Excel, then a Data tab will appear. from there you can connect directly to a database and either write your own query or just plug in a view. you can even get fancy and have it prompt for a date range or something for the user, too, with a little extra coding.
Thats very helpful! Im working on getting more organized within my work so this is going to be very useful. Thank you!
What do you mean you need to retrieve 10 million rows? Is your select statement returning 10 million rows to the code invoking the query?? If so, you need to ask why it is fetching such a large number of rows.
Looks like this has been solved but just as a PSA, please never ever reference your columns by their index like 1 or 2. Write out the full column name each step of the way and people will have an easier time reading your code :)
Maybe I missed it in the comments, but I think no one has actually explained why this is wrong in the first place. The evaluation order is: FROM WHERE GROUP BY HAVING SELECT ORDER BY Thus, referencing “2” in HAVING doesn’t mean anything, as the second item in the select list doesn’t even exist yet. Yes, you shouldn’t use numbers in lieu of column names, but the more important thing here is to understand what order the code is processed in.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/learnsql] [\[PostgreSQL\] Noob question about returning multiple rows of data](https://www.reddit.com/r/learnSQL/comments/amhugw/postgresql_noob_question_about_returning_multiple/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
^^This needs to be upvoted as the top answer
It really seemed awful at first, but it’s kind of grown on me. Two reasons: writing quick queries doing exploratory work it’s really convenient. If you make a change to your select statement, you don’t also have to change it in your group by and order by statements since they’re already referenced by position. I guess this could potentially cause issues but it never has for me. The other is that I think when you have lengthy expressions as columns (like case statements, nested functions, whatever) it actually makes your sql a lot more compact and readable. 
I mean, if you're just exploring and writing quick I get it and think it's cool. But if you put that shit in production we have a problem.
Get adventure works. Open Excel fill it with names and addresses. Save it as a .csv. Load that to the customer address table. Next make a flat file of all the customers who spent over x dollars in 2005. This is a common real world request. I used to make a lot of files to be sent to a printer who would send out marketing flyers to customers. Do a similar thing to extract files of people who ordered things in the past week. Look into how you'd FTP this file in ssis. Also most printers like a summary email. With row counts and a total dollar value. You can do all that in ssis. You'll need to fudge the dates because the data is stale.
Geographic information systems, stuff like utilities, roads, housing, bridges and that sort of thing. The stuff that local government looks after. 
If you are writing sql to be used later (by yourself or someone else) you should definitely use column names directly. But if you are working with ad hoc queries for yourself, writing group by 1,2,3,4 order by 4 desc is so fast and easy that it's impossible to avoid the habit ... (This is coming from someone that worked daily with SQL server for 12 years and then moving to Amazon Redshift)
Couple of things. Don't use the ordinal position of a column to try and reference it. It's tough to understand and not all DBs accept it. Second, I really like writing queries like this with derived tables like the following: SELECT * FROM (SELECT user_id, SUM(watch_duration_in_minutes) total_duration FROM watch history GROUP BY user_id) AN N WHERE total_duration &gt; 400 Why do I write my SQL this way. 1. Not all DBs will allow you to use the actual column alias in the SELECT statement in the HAVING clause 2. In order to ensure it works from DB to DB you have to write the entire formula in the HAVING clause 3. Using a derived table ensures I don't have to worry about the two issues above 4. The HAVING clause isn't as well known or used by many folks who program in SQL (it should be) but the above follows an innermost execution so that people understand, "OH SUM FIRST, then I should only grab the rows above 400. 5. The SQL Query Optimizer for the DBMS SHOULD treat both of these queries the exact same in terms of execution. 6. Both queries will give the same results.
I give in. I've spent hours on it and still have no idea. I tried this : select e.name from employees e where e.id NOT IN (Select e.managerId From employees e where e.managerId = 0) The above leads to test results: * Example case: Wrong answer * No managers: Correct answer * Workers have managers: Wrong answer * Managers have managers: Wrong answer And this, select e.name from employees e where e.managerId != 0 of course leads to: * Example case: Correct answer * No managers: Wrong answer * Workers have managers: Correct answer * Managers have managers: Wrong answer I can't figure it out.
Appreciate the clue. But still couldn't solve it. I've never really seen the benefit/use of left and right out joins. I have always been using inner joins. Now, using it here, I can't se how I can.
What RDBMS. I could see this being a gigantic problem with the Oracle. 
Can you show an example of what's happening?
It's Oracle. I'm using Oracle pl/sql. 
The date format is incorrect in your insert. The database is expecting 1997-01-01.
That's all it was. Was missing the ticks thank you!
You need to change the date to TO_DATE('01-JAN-1997',DD-MON-YYYY)
You're Welcome!
I hope this is more helpful than confusing: SELECT e.name FROM employees e LEFT JOIN employees e2 ON e.id = e2.managerId WHERE e2.id IS NULL This is what I was thinking. Inner Joins are great for when you want to rerun results where both tables have both a match on the columns you are joining. By doing a left join you return all results from the first table and only results that match from the second table. What I did was take the first table band left joined it to employees on id to manager id. Selecting from the second table e2 above will give you results for employees that have managers. By filtering the where clause where e2.is is null will remove employees to have no manger because they are the manager. I’m sure there is an easier way to tackle the problem but this is how I would do it. Here’s a link to an image that shows the different types of joins: https://i.stack.imgur.com/VQ5XP.png 
I hope this is more helpful than confusing: SELECT e.name FROM employees e LEFT JOIN employees e2 ON e.id = e2.managerId WHERE e2.id IS NULL This is what I was thinking. Inner Joins are great for when you want to rerun results where both tables have both a match on the columns you are joining. By doing a left join you return all results from the first table and only results that match from the second table. What I did was take the first table band left joined it to employees on id to manager id. Selecting from the second table e2 above will give you results for employees that have managers. By filtering the where clause where e2.is is null will remove employees to have no manger because they are the manager. I’m sure there is an easier way to tackle the problem but this is how I would do it. Here’s a link to an image that shows the different types of joins: https://i.stack.imgur.com/VQ5XP.png 
To my knowledge, it will either create it with the user being able to access it, or with all users being able to access it. What type of synonym were you looking for?
I’m sorry, I‘m not sure what you’re asking. I’m very new to sql. I guess I would need one user to access it, but it would be from different machines. Not sure if that would make a difference or not
Do you want to be the only one that can see the synonym, or do you want everyone that can logon to the database to be able to see the synonym?
I would be accessing the table from some java code, but I’d be using the same user details everyone to access the table, so I guess having that user being the only on that could see the synonym would be best
Do you have a service account that will logon and use the synonym?
I need a poster of this.
You can do a lot with SSIS. What do you like?
I don’t have a separate account that I’m accessing it from. The account that’s logging on through the java code is the same one I’m using to access the table personally
I think that account just needs access to the synonym. It can likely be private if you create it under that schema context.
Thanks for the help! Your questions are helping me understand it better
The same goes for linked servers/connections.
Looks like you haven't added (user_id) and (role_id) after FOREIGN KEY statement. If it still says syntax error, look into the docs of your SQL server for FOREIGN KEY syntax.
Hi, I recently a SQL bootcamp on udemy and found it very helpful for my database course which I'm taking now at my college. The instructor for this course utilizes PostgreSQL and it's only $11. You can look through the contents to see if this may be of some use to you. https://www.udemy.com/the-complete-sql-bootcamp/
The synonym is an object just like everything else belonging to the user (eg tables, views, triggers). It stays with the user you created it under which sounds like the same user the table belongs to. 
How was it?? Is this like the YouTube videos??
I have this book but I haven't gotten past a few chapters as my job is using MySQL instead of Postgres. &amp;#x200B; [https://nostarch.com/practicalSQL](https://nostarch.com/practicalSQL) &amp;#x200B;
I actually gave that a shot once with HTML/CSS. It wasn't bad. I may check it out again. Thanks!
Seriously, do you think people who come to /r/SQL need to know what data is?
Just looking at the index of course content I would call this "SQL Essentials", certainly not complete. It lacks basic and advanced stuff like: * CTE * indexes (there are many types, kinds and uses - it is really another course) * transactions * materialized views (possibly) * windowing functions * intersect + except (he covers only union, why?) * possibly no full, natural and cross joins (they fall into inner/outer joins category, but are often ignored for no good reason). * partitioning * access control (grants, grants per column, rls, inheritance, groups) * domains (kinda like check constraints that got covered) * range data types * grouping sets (there is more than just group by) In addition, no mention of PostgreSQL specific ( in part because of PL/pgSQL): * triggers * other types of functions (and procedures in PG 11) * support for functions written in perl, python, java, c * cursors * data types like arrays * JSON support * XML support * a few popular modules shipped by default * composite data types * sharding
Nice, I didn't know that the TRUNCATE would empty the table without touching the structure.
To add even more to it, there is an option for TRUNCATE. That is TRUNCATE REUSE STORAGE. This tells the DB software to keep the storage allocation on disk, so when you go to re insert a similar sized data set, it doesn't need to re allocate disk space thus making the insert faster.
it's not working because of the syntax error SELECT 'prop' ( SELECT ... but there's an easier way to do this -- SELECT COUNT( CASE WHEN score &gt; 50 THEN 'ok' ELSE NULL END ) / COUNT(*) AS prop FROM movies 
With something like SQL which is tied in so closely with a computer/internet, you're much better off looking into articles and working through free examples/classes/tutorials online tbh &amp;#x200B; There are loads of options, really the only limiting factor is your time and patience &amp;#x200B; Good luck!
thanks :-) &amp;#x200B;
Unfortunately, saying that the difference between DELETE and TRUNCATE is that "the structure of the table remains the same with TRUNCATE" is wrong (or at least not precise enough). The Wikipedia article explains the difference much better:
Make a social media site. That will get you familiar with writing all the basic types of queries. 
[Practical SQL](https://m.barnesandnoble.com/w/practical-sql-anthony-debarros/1126049058) 
tried it, i'm getting a result of 0... it's not working 
ah, good old integer arithmetic try it like this -- SELECT 1.00 * COUNT( CASE WHEN score &gt; 50 THEN 'ok' ELSE NULL END ) / COUNT(*) AS prop FROM movies 
It’s likely down to the field data type. Tend to put 0.00 * (whatever the sum is) and it’ll apply implicit conversion. Bit of a hack mind. 
Thanks, that helps a lot!
yess! you rock, thank you!
one thing though- prop isn't showing in my output. i need it to show like prop .652
are you sure? because i use 1.00
it isn't showing? so, what exactly is showing? there's only one column!!!
just the .652. prop is just what i need showing in the output- it isn't an actual column in my table or anything. 
I’d need to test, but I upvoted yours because it’s just something to apply to convert, couldn’t remember if it was 1.00 or 0.00. Happy to be correct. 
I second this. No Starch Press has published some excellent books for beginners, including this one.
found the answer- for posterity: to see column names, you have you turn .headers on in the command line 
but it *is* a column in the query result (which is a table)
sqlite peccadillo
think about it for a second... no matter what the result of the calculation is, what do you suppose you get if you **multiply it by zero**
you're right- sorry i'm new to sql so i didn't fully understand the structures in the output. anyway, you need need to turn .headers on, and it will show up appropriately in the cmd prompt
no worries learning is why we have these subreddits, where we can help each other
What about a book not for learning SQL, but for learning database administration?
Are you raising your hand in class and asking for clarification during the lectures? If you don't understand a concept, say something.
I'm very fond of [A Curious Moon](https://bigmachine.io/products/a-curious-moon/). It's a quirky approach to make a technical book a novel, but I found it provided good context for why you would do certain things. You also learn a lot of good practice in that book.
BCP? Neat little tool that I used to build a wiki. Basically results to file, but it's pretty powerful. https://youtu.be/jhu3h3VYOJQ 
Will check it out! Thanks
&gt; This test may cause you to second guess yourself 100% true good luck with 762, i found it much easier than 761
That's good to hear. Lol. This wasn't a fun experience, but hitting finish.. seeing PASS throwing my hands up and saying fuck yea in a dead silent testing room... worth it.
Why doesn't the second item exist? Is it because indexing starts at 0? 
What website are you practicing from? I would like to try practicing too. Thanks!
Of course I'm asking for clarification? I'm not just sitting there twiddling my thumbs. I even mentioned that I went forward to the instructor and expressed my concerns. The class isn't even supposed to cover writing statements yet. It is supposed to be covering the very basics of what SQL even is. Keywords, normal forms, what a database is, how tables can relate, etc. The instructor has taken it beyond that. Imagine if the course had four classes before it could be complete. SQL I, SQL II, SQL III, and SQL IV. Instead of starting at SQL I, it feels as if we've started at the end of SQL II, or possibly the beginning of SQL III.
That's perfect mate thanks!
How much is the fee for the exam scheduling?
TBH in terms of SSIS i'm not sure, based on the tutorials it looks like it's just moving data from one place to another and maybe perform some cleaning on it but I'm assuming there's more to it than that. I'm looking for real world problems I can try and solve.
I used Microsoft com which rerouted me to pearsonvue?. 165 USD was the total cost, as specified on microsoft. Then I picked a community college that had a Saturday test (since I'm out of pocket, not company sponsored) and took it. 
Wow, $165 is a bit pricey and how many times can u retake the test if you fail? Is $165 one time deal?
I dont think it came with retake vouchers. There is a site that will get you 2 retake vouchers and the original test for 200 something. I just want you to think about that though. I was also worried about retake prices... but that just means you're not that confident. Worst case scenario, you take and fail, you'll have an idea of what you missed. Work on that and try again. Or pay for almost 2 tests, and pass it on the first attempt... then you're out. I went with the "I'll pay for the real test, treat it as a practice test, and if I pass then that is that." Mentality. My biggest concern was not knowing what they were going to ask.
Ok, really appreciate your answer for questions. Good luck with the next one!
No, it's because it literally doesn't exist yet. When we reach the HAVING part of the code, all we have done so far is defined what table we are querying (FROM), which rows should be filtered out (WHERE), and defined any grouping (GROUP BY). We have not made any declaration as to which fields we want. Thus, asking for the groups "HAVING 2 &gt; 400" means nothing in the current context, as there is no "second field" yet just as much as there isn't (and likely can't be) a 1,000th field. Just cause the field of interest *will be second* (or, as is another common error, *will be called* something after being aliased ) doesn't change the fact that it isn't defined yet. This is why we *can* use an alias or column index in the ORDER BY clause, as by this point we have already selected (and aliased) our fields.
Maybe it'll take the edge off if you realize you could take this test 10 times, or pay for one term at a community college. I also would not suggest just passing 1 test. Go all in, get that mcse or mcsd. It just shows that you love the subject and are willing to invest in yourself to prove it.
Congrats, can you share how you studied? I'm watching Phillip Barton's udemy course 70-461, and reading through Itzik Ben-Gan exam ref 70-761, hopefully this will be enough.
Yeah, that's a general basic idea, but yes you can do more than that. 
Can you give examples of social media site?
Of course. Like your own Facebook. Reddit. Or instagram. You just need users. Posts. Comments replies and likes. And be able to post. Delete. Edit. And of course get all these. Really drives home how to use a relational database. Especially ordering comments posts and replies by time and how many likes they have. 
I took and passed (barely) the 70-461 exam and it was th hardest test I've ever taken. The way they word the questions and sometimes I thought I knew exactly what they wanted but re-read the question to see it was totally opposite. I'm not the sharpest tool so I took three months to study but would love to take the 761 to show myself I can do it. Congrats on this amazing achievement!!!
Master these. Transactions (tran, try catch, raiserrror, throw and hints) Joins (cant stress this enough, except, union, intersect, outer inner full, and especially apply) ...which return distinct lists, which return nulls, etc Aggregate functions (sum, avg, count) Adding nulls (isnull,coalesce,iif) BETWEEN and other opperands Temporal querying (for system_time [as of, contains, etc) Rank, dense rank, grouping, group by, rollup, and whatever those functions are called What @ # ## and ctes are, when to use them. IN() and exists () COLLATE (not options, but what it does) Update delete insert, possibly merge but I didn't get any merge questions. And you'll be good. I believe in you. I didn't get any XML or json questions. It's not so bad, but I had the same thing. One question required me to choose something in the select clause that made no sense / wasnt in the request. Then I freaked myself out because I got a series of questions on one data set and the question used the word unique... another one used the word distinct... and that messes with my head... cough joins cough.
The XML was 30% of the 70-461 and overwhelmed the shit out of me. Is no XML in the 761 common or did you just get really lucky?
I didnt study. On friday... I said I want to take this test on a saturday.. i had a choice. The 2nd or 23rd. I looked at my wife and she asked if I was going to study, and if I thought it would make a difference... I said no and took the test 12 hours from that moment. I love sql. I've been in this industry for 7 years. I figured, my first attempt would be a practice test (not actually paying 99 for a practice) and if I passed, then yay. If I didn't then I would know where I was weak.
Someone else took it recently, posted it on reddit. I messaged them, they said they didnt get XML or json questions. This is the 2016 version. It might be that these technologies are not "normal form"... because they are not and break normalization.. and microsoft wised up, skipped them.
About 2 hours before the test I refreshed myself on Apply and temporal tables and did some XML json queries on my dev machine (I do sql dev youtube vids)... that was the extent of my studying.
R/datasets ! Find something you like, model it, ssis. My wife wants me to make a meal planner. I need to find food data sets and actually do that project. If you are passionate about the subject, it will help you make sense of what and how you want to load it.
Ah that makes sense. Is this possible if all I know is SQL, I can't imagine where to start. i.e should I know HTML to be able to even begin making the website
Your NOT IN is close. First you can remove the e.managerId != 0 check. It's redundant for the query. Removing that won't fix it though. In this example John has a NULL managerId. So `Select e.managerId From employees e` will return 2 rows: |NULL| |:-| |1| &amp;#x200B; Since != NULL is false, NOT IN (NULL, 1) will also return false. You need to handle the NULL: `select` [`e.name`](https://e.name) `from employees e where` [`e.id`](https://e.id) `NOT IN (Select COALESCE(e.managerId, 0) From employees e)` Keep in mind anything = NULL or anything != NULL is always false. That's why we have the IS NULL and IS NOT NULL syntax. &amp;#x200B; This is how the query would look using NOT EXISTS: `SELECT name` `FROM employees e1` `WHERE NOT EXISTS(SELECT 1` `FROM employees e2` `WHERE e2.managerId =`[`e1.id`](https://e1.id)`)` &amp;#x200B; &amp;#x200B;
I really dont understand why they just dodnt make the language standard be like this, its so much better. Helps thinking and helps getting started on sql.
&gt; I'm not just sitting there twiddling my thumbs. Well for most students on /r/college, that's not the case. What specific topics are you covering which you're having problems with?
Ohhh that's right, HAVING comes before SELECT in the order of logical query processing. Thanks so much for your answer and explanation. This helps the OP and me as well :)
Did you get any questions on XML? Thanks!
Nope. None on XML or json. Yes on querying temporal tables. 
If you haven't discovered it yet, `.mode columns` is nice too.
Based on your SQL videos, it seems clear to me that you are an advanced SQL user with experience in building real databases. Why did you decide to take the 70-761 cert? Thanks!
Good question. Few answers. Is about investing in yourself. I wanted to do it. I wasnt asked to do it. I wanted to prove to myself that I had an understanding. 761 is just the start. I don't have a degree. I went to college and took java, actionscript, vb.net, c#.net, html/javascript/css, and finally sql. I loved sql, but there wasnt a class that went deeper than the knowledge I started to find online/from experimentation. So... this is how I'm getting my degree. Self taught and career learned. "Schools not a place for smart people" - Rick Sanchez Taking 761,762 and 743 (I think its 743) allows your current employer get a silver membership from microsoft. Which gives them access to a ton of things. Having two people with those credentials gets them the gold, a better deal. From the perspective of a future employer: I did this, I took my exams out of my own pocket. I love what I do, I'm confident in my decisions, and Microsoft can vouch for my understanding. I've met people with a 4 year that are not at my level. I've always been a hungry to learn. SQL is my craft. 
Why it's this not checking for updates? Only inserts and deletions. 
I'm not understanding your question. It is updating. Only inserts and deletions are how I was taught when it came to large sets of data.
If you insert when the data doesn't exist in the target and you delete when the data doesn't exist in the source, what happens when the key exists in source and target but the attribute fields are or of sync?
Thanks for the reply! Your videos or reply never disappoint. What does it mean when a company has "silver membership" from Microsoft? Can a person earn the gold membership solo? Thanks again, and best wishes!
I don't thinks so. I remember they used to have some sort of subscription that would give you access to office, visual studio, sql, basically the whole kit... but I think that's gone. Even the MSDN subscription got morphed into the visual studio subscription. And sql developer edition is the full version, but free (I wouldn't as knowledgeable as I am without this edition). I dont fully understand everything that's in the Microsoft partnership network. From what little I know, Its not only recognition by microsoft, but access to any tools you need for your team, link: (core benefits) https://partner.microsoft.com/en-us/membership/core-benefits So a company can get a silver or gold compentcy, link: https://partner.microsoft.com/en-us/membership/competencies And you get a badge for your company's site. In order to get this badge, as a company, you must have a certain number of individuals that have passed specific tests. Here is the data platform one I'm trying to do. Someone would get a silver membership (compare previous core benefits of a silver member) some other stuff (on this next link) if I completed the tests (same page, check requirements to see the tests you need). https://partner.microsoft.com/en-us/membership/data-platform-competency For the company, they get software, and a badge that says... hey.. we have certified people creating our products. For yourself.. you become the individual that allows them to obtain these things. You are the 1 of 2 required for data platform gold, or... you become the 1 of 1 for the data platform silver membership. The other compentancies have different requirements. The app dev (c#) compentancy requires 2 people take certain tests to get silver. And 4 people for gold. If I owned a small tech shop, I'd be looking for someone to save me cash. Obtaining these certs, if the company knows about MPN, makes you the person that saves them a ton of money on software. 
Thanks btw, for the video comments. Just something I want to do. Show people how I think and code. New video drops tomorrow, I'm ripping apart adventure works into my master data management system. Trying to communicate how to think. And how to use it. 
It'd be much easier if you had a one-to-many (Or whatever) relationship using a table with one row per address. Or if you're using a database engine with array types, that instead of a string...
I don't know if I understand your question. If I have a jar of 100 marbles, and 40 of them are blue, 30 of them are green, 25 of them are purple, and 5 are yellow. Now we receive another jar of marbles which has the normal 40 blue, 30 green, 25 purple, and 5 yellow, but now it has 7 lavender. Some weeks we might only receive a jar that has 60 marbles, it might be 9 blue, 3 green, 2 purple, and 0 yellow, but there will be at least a minimum of 1 new color, with an infinite number of colors being possible. When the two colors match, the attributes of the new marbles are imported, and the attributes of the old marbles are no longer relevant. However, very regularly the attributes between marbles are identical and no changes have been made, of if a change had been made it is only to (1) out of many attributes, or columns. But any time we find a new color marble we straight import it. So delete the marbles we have, then grab all the new ones. Simpler.
Most places use hash () or checksum() does the same thing. https://docs.microsoft.com/en-us/sql/t-sql/functions/hashbytes-transact-sql?view=sql-server-2017#see-also You use hash to compare all fields. If nothing has changed don't update that row. If it has changed update it. That's a traditional data warehouse thing. Hashbytes(columnb + column c + column d) You have to call them as varchar then contact them all together. It's ugly as heck. Also if you've got that many rows ditch merge. Merge is slow with tables over a million. Statement 1 is an update. Statement 2 is the insert if not exists.
We aren't using anything like that. I rewrote it to delete the rows that do exist, and then dump any of the rows in. The process has gone from about an hour and a half down to... 8 minutes?
And I'm suggesting delete isn't the best path imo. There's many ways to solve problems in SQL. If it works, go. There will be downsides. Is it ever possible a user would be reading when the rows are missing? 
No possibility, this is a nonstransactional database from your definition of the term.
You on sql 2016? Select count(1) cnt, value From ( Select value From string_split(@email,',') )x Group by value
The marble analogy really didn't make the explanation clearer.
&gt;Scale of 1-10 how bad is this? I'd give it a 4: dumb, but eventually functional. 
Typically the joining table would be on the right of the '='. This makes things much easier to follow when doing left or right joins later. 
Depends on the database. For SQL Server, the order doesn't matter but you can force it to evaluate JOINs in order. For Hive, the order does matter.
What about the ON 's. Does the Left side of the = have to be from the right side of the join? Or can it all be on their own side respectful? Hopefully that makes sense...
When you just put 'join', without specifying the type, it does an inner join. For an inner join - no, there's no difference. If you were doing a left or right join, then there's a huge difference if you start with 'from people' or 'from salary', or 'from address'. 
It should work either way. Mostly it is just for organisational standards. And to not be cursed by the person who maintains your code. But from a technical perspective, you should be fine.
Doesn't matter for inner joins (which is the default join). It kind of makes a difference for outer joins, which you have to specify as being left or right depending on the table order.
Oh that's good to know - a while ago the old 70-461 had way too many of those questions, probably because those features were new.
SQLite implements joins as [nested loops](https://www.sqlite.org/optoverview.html#order_of_tables_in_a_join). It will freely reorder INNER joins if it sees necessary but can't for outer joins. If there were an index on `s.salary` theoretically you can get a big performance improvement by having that as the outer loop. 
I had 2 questions on XML on mine. 
First of all, what is a 'week'? The first day of the week is a regional setting. Secondly, what platform are you using? Depending on your answers to those, this might work: AND Yourdate &gt;= dateadd(day, 1-datepart(dw, getdate()), CONVERT(date,getdate())) AND Yourdate &lt; dateadd(day, 8-datepart(dw, getdate()), CONVERT(date,getdate()))
&gt; First of all, what is a 'week'? The first day of the week is a regional setting. Third, what is considered the "current week"? I'm having flashbacks to a reporting project I did where the "week" started on Tuesday, and the "current" week was the most recently completed full Tues-Monday "week." So if you were running the report today (Monday, February 4th), the "current week" would be January 22 through the end of January 28th. Dates &amp; times can be hard. Screwball business rules make them mind-bending. I committed some unspeakable SQL horrors on that project trying to satisfy the requirements.
I'm able to auth to Azure, but when I try to run commands within that context against SQL I get a windows auth isn't supported on this edition of sql server type of error. 
upvote for "screwball business rules"
I understand that. I'm nearly 30 years old, so I'm attending school probably close to a decade later than most. I don't have the time nor the opportunity to waste my classes like a lot of these kids do. I need to get in, get my degree, and get a job that doesn't suck 20lbs of sweaty ass every day. I am going into class in a moment and when my classes are over for the day, I'll reply to your comment again with information that my professor attempted to cover today, and how it is in far left field compared to what the course is supposed to be covering at the moment. 
WHERE DATEDIFF(WK,fieldname,GetDate())=0
Maybe XML is the varied one. Did you get any questions about how many index lookups happen, given a query?
Not that I can remember, one of the XML questions I had to free write a query that pulled data from an XML column. 
It is the best way I can explain our data model. These are not transnational records, per se, and the database is not the master system of record. We will always have 40 blue marbles, and never 39 or 41, because I can count 40 of them today. In the future the attributes of those marbles might change (dates might be updated) but the marbles themselves will not. Each week we get new marbles, and each week is a blend of new colors we dont' have, and colors that we do have. Our goal is to have (1) jar with all the marbles in it, with all the attributes of those marbles up to date.
I was reading that a lack of a PK or index for merges like this might actually result in faster performance. Not necessarily faster to drop the index, run the merge, then recreate the index, but that the index itself may not help the merge process in any significant way that I had thought it might.
If you remember some of the subjects asked about, could you list them or message me. I think I might grab my 762 in a couple weeks, if the sat appointment is still available. Thanks in advance.
If you didnt know... this is valid. ON a.id =b.id And b.fk = @fk Same data is returned if you do.. ON b.id = a.id and b.fk =@fk 
Not OP but I still don't think that makes it any clearer... What I understand if you have a jar of marbles that you're adding to every week... you're questioning how you handle maintaining only the *colors* in a table? Or do you keep a record of how many there are? You said you update based on Primary Key....what value in your marble analogy are you updating? The total count? If you're only storing the total count....how do you have 2.5 million rows? In your analogy...you have 2.5 million unique marble colors? If there's a row for each "marble" then what are you updating on your merge? You mention when colors match other attributes are imported and updated...so you're effectively throwing away your initial marbles when you add new marbles to your jar? And you said you will **always have exactly 40 blue marbles**... that doesn't really make sense even within your own analogy because *"Some weeks we might only receive a jar that has 60 marbles, it might be 9 blue, 3 green, 2 purple, and 0 yellow"*. How does that jive with *"We will always have 40 blue marbles, and never 39 or 41"*. Not trying to be snarky - you have piqued my interest and I wanna help, this one sounds interesting.
&gt; Or do you keep a record of how many there are? Not exactly. Every week we either add a new set of data, which would either be a mix of existing marbles we know we had in the past, and then updating any attributes which may have changed, or new marbles we're expecting to receive. Sometimes we load the entire data. &gt;And you said you will always have exactly 40 blue marbles... that doesn't really make sense even within your own analogy because "Some weeks we might only receive a jar that has 60 marbles, it might be 9 blue, 3 green, 2 purple, and 0 yellow". In a perfect world we would only receive information on all 40 marbles if all 40 had an attribute change, or if we were to run a full load of data. In most weeks we might only receive a file with information on 8 of the 40 we had, but we will receive 32 new marbles. Does that make sense?
So you have 40 blue marbles, and you may only get a report on 9 of them this week. 3 of those 9 reported this week may have nothing changed. The other 6 may have reported weighing less this week? So you'll never get any new blue marbles, but you may get a new cyan marble next week?
Correct on all counts.
Sorry, I should have specified. I would be hoping to show results for Monday -&gt; Sunday. I am using Presto, and pretty new to SQL in general so I'm finding it tough to find presto specific resources online. Thanks for the help
Okay. Fun. Unless something's changed in the last couple years, indexing really isn't an exact science, it's always a "Your mileage may vary" art. Especially when you're working with inserting lots of rows at once - you never really know how much impact and index and its stats will have. My initial reaction is that it's definitely an oversight that there aren't any indices set up, but it's entirely possible it's been checked and any index that was added slowed down the inserts too much and weighed down the whole process too much. Your suggestion of bulk deleting and reinserting is a good jumping board, too - especially without any indices. My guess is that was the plan when they designed it if it isn't indexed. Updates are expensive - inserts are cheap (generally). If you match up your marbles and *know* these marbles are on your report - dropping them and re-adding them from your new report might be much, much faster. The problem is that matching them up first to see what you can delete might need indexing to be efficient. These are classic growing pains, and good problems to have. If it were my data - first step I would do is set up a temp table with a random day's report and see how efficient it is at matching up your records, and check your plans for any bottle necks. If you can get your new day's data matching up easily to delete them - then this might be your cheapest and easiest win. 
To add some context that might make this easier to understand We have a total of 5 jars to hold marbles: 1. R-jar 2. O-jar 3. C-jar 4. T-jar And so let's ignore the past for a minute and say we get (1) new turquoise marble today from Client XYZ, it might end up looking like this: 1. Insert (1) turquoise marble 2. No marbles yet 3. No marbles yet 4. No marbles yet Next load it might look like this: 1. Insert (1) turquoise marble 2. Insert (5) turquoise marbles 3. No marbles yet 4. No marbles yet Next load: 1. Insert (1) turquoise marble 2. Insert (5) turquoise marbles 3. Insert (4) turquoise marbles 4. No marbles yet New load: 1. Insert (1) turquoise marble 2. Insert (5) turquoise marbles 3. Insert (4) turquoise marbles 4. See note Note: Each week we would expect to be receiving new turquoise marbles for T-jar, which are "special" or unique compared to the other jars. The final change we can receive for the turquoise marble is when it is is closed, and at that point no more T-jar marbles can be created ever. Now each week we load lists of marbles for each jars, but the information in those lists can be totally unrelated. For example, we might have a load that looks like this: 1. No information for the turquoise marble. 2. Information on 3 turquoise marbles, with 1 marble changing. 3. Information for all 4 turquoise, with 0 marbles changing. 4. Some of the turquoise marbles we already have here, but some new ones. This pattern can continue where turquoise marbles are included in the load files even after turquoise has closed. This example is then in the real world scaled up for multiple clients, and hundreds of thousands of different colored marbles. There is no real reason to "update" our jars because we can simply remove any of the marbles from the jars when they exist in the new file, and then dump the marbles back in to replace the ones we deleted, which will then reflect any updates that exist, plus we'll also dump the new marbles in. This process will always equal a MERGE/UPDATE, but because of the row count and number of columns involved the new technique currently seems to execute in a much more efficient way. Bear in mind that about once a quarter or so we will load 100% of the data for all clients for all jars even if only 5% of the attributes have changed for the marbles we have. This is done to "sync" our data with the systems of record as new fields are added, or administrators/developers in those systems make changes based on our requests/recommendations.
Well, because these tables are not "transactional" the PK is really irrelevant and only being used to make deduping easier. They don't really relate to other tables in the same way as I've seen in other environments. Which is totally fine, so I guess I can see how the index would slow these processes down. My thoughts is that creating an index on the PK (its not a real PK, its just called the PK) isn't going to help the merge because it is still going to do have to do a full table scan to see which rows match and need to be updated, and which rows don't match and need to be inserted. Now with a delete I would create an index to make the delete faster, and then from there we just dump the new table into the old table and call it a day.
a right join is the same as a left join, reversed
Here's some information that may help you out. &amp;#x200B; * Test your MERGE statement in comparison to Updates / Inserts / Deletes. Merge typically performs worse than making something a little lengthier, but of course, it depends. So measure and see which is better, but most of the time in a production setting, doing a IF THIS ELSE for Updates / Inserts / Deletes is best. * Don't Update when there is nothing to Update. Even in Merge you can control what you'll update and the where clause should be checking this. It essentially becomes where ISNULL(SourceColumn,'') &lt;&gt; ISNUL(DestinationColumn,''). * Truncate and reload is a viable strategy and may be advisable for what you are doing. * If you are going to do Updates / Inserts / Deletes, I would recommend to evaluate and fine tune the indexing strategy here. If you can identify inserts / deletes easily, you should look at partition swapping. This may or may not be relevant. &amp;#x200B;
You can start here. &amp;#x200B; [https://blogs.msdn.microsoft.com/walzenbach/2010/05/20/how-to-insert-binary-data-like-imagesdocuments-into-a-sql-server-database-with-sql-server-management-studio/](https://blogs.msdn.microsoft.com/walzenbach/2010/05/20/how-to-insert-binary-data-like-imagesdocuments-into-a-sql-server-database-with-sql-server-management-studio/) &amp;#x200B; I don't recommend using SQL Server as a document store.
On your server side, you can build a where clause dynamically by checking for a type of undefined on all possible parameters, then put it all together. I did this on a project (lines 36-83 for an example): https://github.com/metzgerb/little-library-site/blob/master/app.js
It also won't reorder `CROSS JOIN`, which is otherwise equivalent to `INNER JOIN`.
That's a cool subreddit mate, cheers!
This is a great idea, thanks!
&gt; &gt; Truncate and reload is a viable strategy and may be advisable for what you are doing. This is where my mind is at as well. There is no real reason to merge or try to do incremental loads based on our data model. &gt;If you can identify inserts / deletes easily, you should look at partition swapping. This may or may not be relevant. I'll check out partition swapping, never heard of it and have no opinion on the matter.
That's interesting. Was it because of the normal index overhead on insert, or was it something specific about the merge operation? Or possibly something to do with the change detection?
If you are not the authoritative data holder and you are receiving a data set that should be the primary and you don't care about storing the past data set, truncate / reload is pretty common. You see this most frequently with data warehouses, but perhaps you are populating a table that has employee contact information for an application and it needs the latest authoritative list. That's two examples of times when you'd want to use this method. Using this method can become extremely difficult to maintain as the data set grows. This is when people begin to look towards incremental processing, or if you want to build something from the start that will scale. This is where Change Tracking can come into play, but if you are only the recipient and not the holder of the master data set, you are powerless. You can do your own process of incremental processing, but it's going to be painful. I had a recent scenario where People A have a data set and told us it would be the master data set. Our data is transactional and we tie other data sets to this data set they send us. Well, turns out the data set they are sending us changes daily and they do not tell us what changes. There is also no primary key or master record holder. We can only discern what is different by trying to match on every single column and hoping certain combinations of columns adhere to a primary key's duty. This is especially dangerous because we can't control what they input on their side, so if they change anything with the data in the data set they provide, it could have extremely painful repercussions on our side. The scenario I described is not a good scenario and is one you want to avoid at all costs. Since they are giving you a primary key, you can easily index this column and you might have the ability to create a solution that would scale. The issue is, you need a quick way to tell what data has changed. If there is no quick way to do this, truncate / reload can be a faster approach. 
Today was starting week four of classes. The software my school requires instructors to use to post assignments, deadlines, notes, syllabi, etc. has an easy to read/access chart that lists the weeks of class. It simply shows "Week 4" in a box, and every week of class after that in a following box. It is up to the instructors to fill in the information regarding that week. Again, information such as what assignments are due, any resources to reference, etc. Well, my professor didn't even have anything posted for this week, so upon getting to class no one knew what we were doing for today nor the rest of the week. After the instructor came in, we learned that we were picking back up where we left off last week, which is creating a basic help desk for a mock company using postgresql and dia diagram. I believe he is attempting to teach us the possible relations between different elements. For instance, the diagrams we created are **department, computer, software, employee, and ticket, each table with it's own attributes. Simple stuff that no one really has a problem with. Where I get lost is when he jumps from our basic intro class to what we would learn next year and starts writing tons of code on the projector, but just mumbling as he goes through it. All this while skimming through the 40+ pages he wants us to read this week, briefly speaking in terminology that all but one of us are familiar with and going off on tangents totally unrelated, or not directly related, to what we are trying to learn. For example, we were doing very basic things today like: CREATE TABLE employee employee_id NUMERIC (7,0) NOT NULL PRIMARY KEY, department_id NUMERIC (7,0) NULL, computer_id NUMERIC (7,0) NOT NULL, manager_id NUMERIC (7,0) NULL, ); This is just an example. This syntax is very basic, but he doesn't elaborate, even when asked, on things like NULL and NOT NULL. I know what they both mean, but in instances I believe one should be used over the other, he doesn't explain why I am incorrect. For example: He may say that manager_id should be NULL, when my thoughts are that it should be NOT NULL because what if someone needs to contact the employee's manager, but they need the manager's ID to look their name/extension up. He just says, "no, that's wrong," followed by something (possibly an attempt at an explanation?) totally over my head. Then, randomly, he will edit the code the class attempting to write with syntax that isn't even I'm the chapter we are in. For example, he will change the above code to something like this: CREATE TABLE employee employee_id NUMERIC (7,0) DEFAULT NEXTVAL ('sq_employees') PRIMARY KEY, department_id NUMERIC (7,0) NULL, computer_id NUMERIC (7,0) NOT NULL, manager_id NUMERIC (7,0) NULL, ); I have no idea what DEFAULT NEXTVAL ('sq_employees') even means, other than I suppose it **somehow** references the drop sequence, but then I have absolutely **NO** idea what a drop sequence even is, nor why it has to be in a specific order. He doesn't explain anything at all. [Here is the practice code we've created thus far.](https://asub-my.sharepoint.com/:u:/g/personal/austin_melvin_asub_edu/EbwoCdLThnFIttB2km1NbFoB1X9tmoIz5n7wct9736_yKg?e=wd6kUj) [Here are the inserts, but I have no idea what an insert is either.](https://asub-my.sharepoint.com/:u:/g/personal/austin_melvin_asub_edu/ESEyJp6wLK5MmmFMqCRsnWIBqR2BoDg6jeEPQCVoK1R5yQ?e=xTzwud) It isn't one specific thing that I'm having trouble with, really. It's the fact that he doesn't explain anything to those of us with no prior coding knowledge. 
That's above my paygrade, I did some Googling and found some resources. So at this point I'm willing to ignore the lack of an index on the PK that isn't a PK, but the MERGE still sucks.
I just say "i'm an IT guy":)
BI/Data Architect?
What are you trying to do. Both sqls make no sense. The first one just displayes max salary and thd count is 1?
SELECT months * salary, COUNT(*) FROM Employee WHERE months * salary = (SELECT MAX(months * salary) FROM Employee) GROUP BY months * salary
Computer Operator or Lead Database Developer, probably the latter. 
they'd call you a principal systems analyst at my company or if you are in a specific group something like "principal analyst, data strategy" 
Did you every have to work with XML in a professional setting? Did you learn Xquery as part of your certification prep? Thanks again!
I havent really used XML in a professional environment, as a recommended part of a solution. XML is not a solution outside of data that is not consumed. Constructing XML and json via recursive procedures from normalized models is glorious, and I do that regularly. Sometimes there was legacy XML data, but I quickly put a view over it. I dont like blob/LOB storage, I work with highly normalized data... the big data of sql (my youtube vids). Xml and json are limited and painful when stored as is... that being said.. Retrieving values is pretty easy. I can do that. The WITH statement is similar to json. Plug in your root, ../ to back up a level (json, always $.the.full.path) ezpz. It's the update and insert and the little things that I cannot remember. Things like "insert first into..." etc. I did spend a few moments in the morning attempting to update some sample XML, using sp_preparexmldocument and other things. Just so it was in the front of my head. Json is pretty cake. There are only a few things you can do with it. Append or not, lax or strict. If I was asked to write an XML update in the free code section, I'd most likely get it wrong. If It was a multiple choice, I'd probably get it right. 
Do you have your own library?
It's funny you asked that. My adventure works videos are on a scheduled release, 3 hours 15 minutes away for the first 2. The first table is person.person. in that video I said. I'm skipping this XML stuff... if you really want to see it comment and I'll consider it. Use a horizontal storage solution, not an as is solution lol. Its soooooo much cleaner.
One I learned from my first CS prof. If it's going to be quicker to solve the problem manually than programmatically, don't make an atomic bologna slicer.
Data Integration Specialist Team Lead, Database ETL architect/Engineer Database Architect/engineer BI/Data Warehouse Architect/Engineer You and I are, for the most part, very similar. I also support the database portion of application development as I’m a Development DBA, but it sounds like with the Python journey you’re on you’re doing the same.
The first query works because both columns you're selecting are aggregate functions so they don't need a group by (as it's only returning 1 row) &amp;#x200B; The second query has one normal column (well calculated) and an aggregate column so it needs a group by statement in there (as it returns more than 1 row so you have to specify that the COUNT(\*) for every row is grouped by your calculated column (months \* salary))
I personally had hundreds of reports both automated and adhoc but parametize and as I get asked to do the same adhoc more than once I would automate it and add it to the report server for the user. 
You need to specify the precision and scale for the decimal data type.
It still doesn't work :/ CREATE TABLE Obsolete\_Titles( partnum nvarchar (5) NULL, bktitle nvarchar (40) NULL, devcost money (40) NULL, slprice money (40) NULL, pubdate datetime (40) NULL ) CREATE TABLE Obsolete\_Titles( partnum nvarchar (5) NULL, bktitle nvarchar (40) NULL, devcost money NULL, slprice money NULL, pubdate datetime NULL ) &amp;#x200B;
You want the term "architect" in your title. Something like "Analytics Architect, Senior Developer" Everything else doesn't matter. To give you a military analogy you have reached your final rank unless you decide to become an officer and go into management, with titles such as Director, etc. Data Analytics Architect, Senior Developer sounds pretty sexy to me... Maybe you prefer BI Architect, but it's all just blah at that point. My advice to you is to think about where you want to be in 5-10 years. Want to be the head of BI somewhere? Go with BI Architect. Want to get into predictive analytics and oversee those people? Analytics Architect is more applicable. 
I actually noped out of a managerial role recently because it would have taken me out of the day-to-day tech too much. I think in 5-10 years, I just want to have even more experience doing what I'm doing, but with newer tech. Maybe presenting at small scale conferences (like a local PASS group or something) and teaching. Basically, when I walk by I want people to say "there goes asariel, she knows her shit" lol
nevermind... I snagged some lecture for 70762 from udemy for $12. This is the test that I wanted to take... this is the easy test! procs, triggers, views, cluster v ordered... woot!
the bottom one works.
If there is a clustered index on the table, then the table is not a HEAP... I would definitely non cluster the PK and non cluster the ON (if its more than the pk). It might be a heap due to the amount of data being dumped in.... but it shouldnt be a heap if you are comparing keys. if you are always comparing Keys... WHEN MATCHED BY TARGET (ON clause matches data in the table) Then Update WHEN NOT MATCHED BY TARGET Then insert WHEN NOT MATCHED BY SOURCE Then delete (kills all records not in the merge's source... pretty dangerous but it sounds like you could do it) 
[removed]
I'm not smart enough to tell whether you're a data genius or a madman reinventing a wheel.
Porque no los dos? I'm taking control back. Making things more customizable than mds or dqs or temporal tables. I'm giving people real normalization techniques and consumable shapes... not just "non transitive property blah blah". This is how hyper normalization looks, and works. And on production hardware it'll blow your mind.
Do you mind sharing how you organize your library? I'm a PM, but I find myself supporting my engineering team/stakeholders with adhoc and sometimes project based scripts, but haven't found a system of organization that works for me.
Did you understand y the first one wasn’t working though? 
because money(40) isnt a thing. and Decimal isnt a thing. money is, and Decimal(precision,scale) is
Very cool, 
A beginner step that is often overlooked is to use the built in snippets feature in SSMS. Anytime I write something useful I try to add it to my snippets. My work keeps a shared folder in out DB project got repo.
Hi, Tnx for your reply! I tried it with GROUP BY and it indeed works. I'm having a little troubling understanding why it doesnt work though. Isn't the 2nd query, in essence, the same as this (replacing 'months * salary' with 'earnings', and assuming max earnings = 5)? SELECT Salary, COUNT(*) FROM Employee WHERE salary = 5; 
`datetime` isn't a valid data type in standard SQL, which DBMS product are you using? 
In Postgres you could turn the comma separated value into an array and return the length of that array: select cardinality(string_to_array(email_address, ',')) as frequency from the_table; But the correct solution would be to properly normalize your data. &amp;#x200B;
Why not something like this? SELECT DISTINCT Schedule.Units, '2018' AS [start date] FROM Schedule WHERE year(Schedule.[start date]) = 2018; If start date is an actual date and time then yeah, you're going to get lots of records.
This is a bit weird. If Access can't find Schedule.[start date], it will treat it as a prompt for a value and ask you to input it - which is what you're seeing in your second example. But in your first example, it clearly can find that field and returns that date. Looking at them I'd expect that if the first one didn't prompt for a value then the second one wouldn't either. So, assuming the problem is 'Access is just fucky for no reason sometimes', I'd suggest changing the WHERE clause to WHERE Schedule.[start date] &gt;= #01/01/2019# Failing that, try refreshing the ODBC link from the linked table manager. Sometimes that just fixes things for no reason. tl;dr second query looks fine but Access is fucky sometimes.
I believe this did in fact give me all the unique Units from 2018 and after, though all the start dates are just that 2018 string we set. Regardless, I changed the date a few times and it's pulling different numbers of records the further back I go, so I think this is doing what I needed for my purposes. I'm confused why this worked yet similar ones didn't, but ah well! Thank you :D
I tried this one as well and it was still asking for the date. Yeah, I know Access is a pain. I honestly hate it sometimes.
I am not sure but why not use a count distinct for the output of the current quarry ?
Shite
Shite
Just SELECT * FROM Table
Bad idea. If someone changes the schema of the table afterwards, your query will break. I run into this problem sometimes when migrating applications to new versions, there's always some piece of code somewhere that nobody remembered that's doing SELECT * and now the whole application is broken because we added some columns.
What am i supposed to do then? I’m inside a plsql block and the only way of outputting the whole row is by writing each and every column. Let’s say I’ll never update this schema. How do i get rid of the cumbersome task of writing every column? 
Does your application *actually need* every column on the table? This is pretty rare in my experience. Select only what you *need*. There are potential performance problems if you ask for more than you need. 
I do! Each and every column but a single row. 
Ideally you would use an editor that can generate code snippets and write this boring stuff for you. I'm a MS SQL Server DBA and I know Visual Studio can do this for SQL Server. You might want to find a editor with similar functionality for the database you are working with.
Select every column by name and address them by name in your application code. That's the correct way to do it.
GitHub is great, we use it as a query repository. I’ve shared it with folks on other teams looking for similar data and they are blown away by the organization/simplicity 
Damn. My boss gave me this. I didn’t realise it would be this hard. Anyway thanks for the input guys, appreciate it. 
If writing out a few column names is "hard", I've got some bad news for you... You could write a query to get the column names so you can just copy and paste it and throw some commas between them. Heck, in some RDBMSs you don't even need to do that (if you have `STRING_AGG()`). SQL Server 2017+ example: USE master; SELECT string_agg(quotename(column_name), ',') FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'spt_values' Replace the table name with your table, and make sure you're using the right database. Copy &amp; paste the result.
%rowtype might be what u want?
im confused why you cant just select * into a rowtype variable. i do this all the time. 
rowtype bro
thats really poor advice to learn. 
If columns are added, you'll get more data than you expect. If columns are removed, downstream code may break (looking for columns that suddenly disappeared) If columns are rearranged, downstream code that depends upon the ordinal position of columns (instead of names) *will* break. If you don't need all the columns in the first place, it's a performance problem.
Yeah, because doing SELECT * because you're even too lazy to learn how to use the tools provided by your development environment is a better idea? You don't have to take advice from some random dude on the internet. More job security for my fellow DBA's who get to clean up the mess later...
TBH - 25 columns isn't that bad.
In mssql there are a couple ways. String_splot(column, ',') depending on the version you are in. Also... Left(column, charindex(',',column)-1) , right(column,charindex(',',reverse(column)))
Good gut check! What would the manual solution look like for sharing or curation for you? Does a shared spreadsheet count as manual?
Check this out. Might depend on mssql version. Declare @I'd bigint Declare @XML XML Select @XML ='that file' Exec sp_preparexmldocument @I'd output, @XML Select * from openxml(@I'd, '/s:Schema/s:ElementType/s:AttributeType',2) With ( Name nvarchar(50) '/@name' --same level as above , content nvarchar(50) '../@content' --move one level up. --expand accordingly )
Maria Butina, dat you? 
yeah dont handicap yourself by letting a GUI do what u could do in 1 or 2 lines. shrug. i was a dba once too. 
It goes both ways - yeah learn how to write it (the notion of using information\_schema to create the concat string is nifty) - but I've saved myself tons of times by just using SSMS and right click &gt; script as. You should learn how to code and how to best use the tools to improve productivity. 
i really would reccommend looking into %rowtype and see if you can come up with a clever solution to your problem. using rowtype is like setting an array to the exact columns that exist in the table, at run time. you can take it a step further, and do TYPE's TYPE MAIN_REC_TYPE IS TABLE OF MAIN_CURSOR%ROWTYPE; then u open &gt; bulk collect from cursor into that new temp-table. and skys the limit~
trust me -- my days as MSSQL DBA, i too, utilized that button.. but seeing hes talking about PLSQL &gt; which is ORACLE. you dont have SSIS.. you need somethign like TOAD or something to do that sort of heavy lifting for you based on GUI's. im now a programmer and even if i DID have a generate button, id look at the script and learn wtf it is doing etc. instead of blindly following it. but i do understand what u are saying because ive done it in the past too, and yes, its saved time and totally helped my learning curve. 
I'm going to re-write that article to use a lot fewer words: Use some sort of source control and be vaguely organised. Basically you should be doing this already. I also completely disagree with #3. Storing a spreadsheet containing sample results for every view and proc you write is a waste of disk space and chances are people will be searching for column header names and code - not specific data. 
I'm strictly a DB Dev, haven't touched Oracle's PL/SQL but I've been picking up Postgres's flavah recently and doing most of my coding for that in PGAdmin. The tools are about the same for quick snippet creation. Really, we're talking about saving time on generating a list of columns, nothing is going to speed up logic generation. 
The way my current employer does it: DB/ObjectType/Schema/ObjectName E.g. MyDatabase/StoredProcedures/Finance/GetOutstandingInvoices We have 30+ databases each containing lots of objects and this works fine when people follow naming conventions. The specifics of how you organize are really not that important so long as its consistent.
Putting the query in the excel sheet is a practice that has saved me a ton of headaches. Every time an adhoc request was made the query would go on a sheet inside the excel file, so inevitably when a month had gone by and the stakeholder was asking for refreshed data I could just rip it and send it back updated. 
&gt;I don't recommend using SQL Server as a document store. Agreed. Store a path to a file stored on a NAS or something. Much cleaner.
literally do this.... --decalre rowtypeVariable schema.table%rowtype; --select all into the rowtype variable based on condtions select * into rowtypeVariable from table where blahblah; --print the rowtype variable. u can even do an IF based on the return of some of the columns at this point if rowtypeVarible.columnIcareAbout = 'whatever im looking for' THEN dbms_output.put_line(rowtypeVariable); end if; that will print the entire contents of the variable... easy. 
And when your local disk blows up you lose your queries. Or you store it on a NAS and get file lock issues cause some other dev wanted a peak. Or you know a column name but have 10 spreadsheets for 10 different databases and have to open each manually and search cause an .xlsx file is not plaintext. Just use some source control. Don't bring excel into this, it has its place for data analytics but thats not for storing code. The difference between a business user and a database professional is that the latter knows excel is a data GUI and not an IDE or code repo. If excel has any part of writing or storing your database code then you are doing something very wrong. 
I think I just lost some marbles reading that. Analogies are supposed to bring clarity not more confusion!!
Mate it’s not some columns. It’s 25-30 columns in the table. Writing each of them felt cumbersome or not efficient of a query, if you will. 
Probably will end up doing that. I’m really new so wanted to know if there is another way. 
Hmm... that's a tough question and one I haven't really thought much of. &amp;#x200B; I suppose, related to sharing and curation... &amp;#x200B; Say I have found a solution to a problem and wish to share it to the broader community. But I'm not really the Pinal Dave, Brent Ozar, or Kimberly Tripp type whose full time job it is to publish blog posts about SQL Server, I just have one solution I thought was kinda neat-o. &amp;#x200B; Maybe I'd reach out to some of them and see if they'll give me a post on their site, since they already have the distribution channel, I then don't have to hack up my own site just for one little article. Else I might find it fitting to post it on Reddit or StackOverflow. &amp;#x200B; Shared spreadsheets (like google sheets) have been wonderful for coordinating small groups. Everything from volunteer coordination (days and times you'll be signing up to work during an event), to who is bringing what to a pot-luck. &amp;#x200B; Caveat emptor, a not-small part of my job is killing Access and Excel solutions which *are* being repeated and *do* want something a bit more enterprise (like SQL Server).
Am I missing something? Why don't you just copy and paste the column names? The problem with something like: select * from t1 Is that if your schema changes, your application will break either because a column you think is there is no longer there or you'll be pulling in more columns than you want. Most interfaces I've used (DBeaver, SSMS, Toad, etc.) have a 'generate' function that will pull out the column names and you can simply copy and paste them into the script.
Even ignoring the issues mentioned earlier, dumping everything into a temp table using a cursor will get you real bad performance on the database side. Google for "set-based vs row-based", plenty of articles explaining the difference. You might want to consider moving some application logic into the database itself. I managed to reduce the execution time for a job that took 40 minutes down to 2 seconds by using set-based operations.
Past experience (in research/data) thought me this one: \- If you have to do it &gt; 5 times, just automate it. However, \- The majority of one-time problem solutions will be faster if done manually. However, \- Problem solutions are very rarely one-time deals. Bonus: One of the biggest advantages of programmatic work is that the code is the documentation. You can go back to it 5 years after you wrote it and it will tell you what and how (and, if hopefully annotated, why) you did. 
Still bad design. One case I had to fix did exactly that. Except they had some huge nvarchar(max) datatypes set on some of the columns. Which cannot be indexed in SQL server. So instead of just grabbing what it needed from the indexes it had to do a lookup for each entry in the base table. Remember that allot of times your DBA doesn't have a clue about how your application works. All he sees is what queries are coming in from your application. If the only thing he sees is SELECT *, there's nothing he can do for you after your database starts growing and performance starts to suffer.
I think you should if you have time and wanting to do it. Putting in on youtube would be most logical because it has wider audience then everywhere else.
I'll be honest, your Master Data Management series on YouTube absolutely blew my head off, I followed along and worked most out (although I'll probably never use it) but in a number of instances had absolutely no idea what was going on. I loved your style of working through things rather than just talking about it or giving best case scenarios. The idea of being able to work through with you would be awesome. Thanks for your effort thus far!
Time is all we have. I love sql. I love to teach.
Thanks for the compliment. That series is pretty complex. The model i describe is in production in a ton of companies. You'd be surprised who uses it. I like live development because it's natural. You make mistakes, you resolve them. I dont cut my vids... perfection doesnt exist in the wild. This series would be way easier to understand. I would start with the select, move to aggregates and wheres, move to all 7 joins, null handling (joins and iif), XML, json, @ # ## ctes, transactions, error handling... probably a good 10 or so videos, separated accordingly.
Are you saying that you have no opportunity in your current role to exercise SQL without proving that you're already competent? I tried self learning SQL a few different ways, but the process of actually learning to query real data in a corporate environment was so much more insightful than any self study approaches I took.
Now I'm thinking... "best case scenario"... I should be showing how to break these things too. Not just, this is an intersect, but how to get errors from it.
I would suggest that one of the best ways to speed up analyst work is by learning some basic Python, with a touch of Pandas and Jupyter Notebook. It's more of a data scientist with setup if you add some science libraries. The reason it's so good is because it retains the displayed result of every instruction you send, so you can scroll through your work with ease without losing results. You can plot the data easily without exporting data to Excel for that (plus it's much faster than Excel for larger data sets, and the plotting libraries are just generations ahead). If you have a more complex environment you can also create a reusable function set in Python behind the notebook that you can include in the notebook. Everything you make can easily be managed in version control. Fancy text editing is possible with markdown. Data can be imported/exported using 1 single command. For anything bigger than 100 rows with sums, averages, and growth graphs, this approach is better than Excel. For anything now complex than 2 queries, this is better.
At my current role, we have software that sits on top of our cubes and provide the end user a nerf ball way to get the data (which is great, it allows Sales to actually pull their own data). The IT guys who manage the cubes use SQL to grab data from that cubes. That being said, I could probably ping the databases myself using SQL instead of using the package software to find the data that I am looking for.
For my gig, we have a number of different views of "production data", none of which are the actual Production database. They're just copies of the prod data with a short delay behind real time. Experimenting on that is pretty safe and good for training. I also have full access on a non production database where I can experiment with just about anything. You should see if you have similar opportunities For training
Try this hack: &gt; SELECT DISTINCT Schedule.Units, Schedule.[start date] FROM Schedule WHERE year(Schedule.[start date]) &gt;= 2018 GROUP BY Schedule.Units, Schedule.[start date];
Yes, I think there would be a lot of interest. I would definitely be interested. &amp;#x200B;
&gt;How much interest is there to have a series dedicated to a cert. My website is pretty small and niche, I have had nearly 7k views since creating it. Out of those views, 22% of my traffic has been people reading my 70-761 exam experience. 19% of the traffic has been people reading about my 70-762 exam experience. Apart from articles that get picked up by newsletters or other bigger names, those articles account for essentially 40% of what people read on my website. So there is definitely demand out there. &gt;I've written down every subject that I remember from the exam, but cannot provide actual questions... Non disclosure. Be exceptionally careful what you say about the exams. You don't want your license pulled. The NDA actually has shifted how I do my posts. I wanted to convey more information to help people than I can, but at the same time, you can't give anything away. So I like to do two posts now, a beforehand and an afterwards post. The before hand is how I studied and explicitly the resources and topics I spent time on. The second post is how difficult I felt the exam was, where I struggled in general on the exam, and my results. You cannot disclose almost anything about the exam. &gt; Any disclosure of the exam or information related to the exam, including exam questions, answers, **content**, computations, diagrams, drawings or worksheets (“***Exam Related Information***”) is strictly prohibited. You will not disclose, distribute, copy, display, publish, summarize, photograph, record, download, transmit or post the exam or any Exam Related Information, in whole or in part, in any form or by any means now known or hereafter devised. [NDA Source.](https://www.microsoft.com/en-us/learning/certification-exam-policies.aspx) So to me, if you posted, "make sure to study temporal tables as I had some questions like that", that is breaking the NDA. You cannot disclose any information regarding any of the content. Basically, if you can't read it online by Microsoft, you can't disclose it. &gt;The series would demand that you have SQL dev installed, a decent enough knowledge about syntax (because you shouldn't take this cert as a green dev), and I will provide .sql and .xls dumps of my table so you can follow along. I think a lot of users would find this exceptionally helpful. &gt;This series would be free. Although I have contemplated getting paid by udemy... my conscience wants me to lift people up, and not take their money. What is the purpose of your online portfolio? Are you doing this to make money on the side? Show off as a resume? Be a personal shared archive of resources to assist yourself? Helping contribute back to the community? Working towards educational careers? Using this as a self learning tool? I don't think all of those goals are a perfect mesh, some go together well, but not all at once. If you are doing this to make money on the side, I probably wouldn't use this much as a resume piece or a portfolio. Sure you have things you can present to clients and customers, but if it's riddled with ads, pop ups, or nuisances, it's not going to look professional and they aren't going to appreciate it. If you offer paid services or paid trainings, it could mesh with a portfolio. Likewise if you are a big name, you can probably get away with terrible ads. (Pinal Dave is a good example there. I love his site, but those ads are awful.) Likewise if you are trying to contribute to the community, it doesn't mean 100% of your work needs to be free and accessible. My personal reasons are to give back to the community while building myself a portfolio. My expectation is that the work I'm doing for myself is an investment and it will get paid out in the long run. I also know what it's like to be new in this realm and not knowing where to go and struggling. I learned very much trial by fire, I want to help other people in similar circumstances not get burnt like I did. So my content is free and professional looking (as professional as a database backend guy can make a frontend...) without being riddled with popups or ads. So I'd try to decide what niche you are filling and focus into that, I wouldn't try to dance around hitting it lightly in each category.
pivot (any way you do it)
I used the microsoft official online video on demand stuff for my SQL 2016 MSCA, and it was definitely lacking in substance. If you provided this service to others i'm sure it would be miles ahead of what MSFT offers for huge dollar amounts. 
My situation is as follows: We have contracts with payers for our ACO. There are 4 that send us "Claims" data. I say "claims" because some of them are NOT claims data but rather they are rolled up by the payer into a more readable format. What I do: After cleaning each dataset, i find the fields that are common across the payers datasets. They tend to be a personID, event type (Inpatient, Outpatient, professional, SNF..etc), sub event type (short term acute care, long term care, etc), claim from and thru date, a DRG if inpatient, Facility, and provider. There are some calculated fields that are in there too, for instance if the claim started out in Emergency department then i considered the IP or OP event to be unplanned. This is not perfect and i am running into an issue now where we need to reliably group IP and OP events. For instance, things in healthcare that were always done Inpatient are now starting to be done as outpatient. So when the CFO is complaining about less Inpatient events, what they could be seeing is the transition from IP to OP in that specific DRG. Being able to group all these events under an umbrella like Cardiac or Ortho is challenging to say the least. Like everyone else is saying, it may sound trivial but deciding what an Encounter or Event is really does help. For instance, if you were trying to figure out an ADK (admits per thousand) you may want to combine Inpatient events if there was a transfer from one facility to another. This means in claims you will have two claims, two facilities, possibly two drgs, etc...but technically thats all 1 event. On the flipside, you may want to see how many encounters a patient had outside your network and you may want to throw that transfer back into the mix (hopefully identified as a transfer). &amp;#x200B; It really gets complex and the problem i suspect you will face is the same problem i face; No one wants to make the decision on measurement and when/if they do it will take a lot of time to come to an agreement. To boot, once the measurement is in place, they will want to change it. &amp;#x200B;
I just dealt with this exact thing with SNF. The amount they go back to IP or OP facilities and then back to the SNF is somewhat troubling. If you count it as a new SNF visit everytime they go SNF -&gt; IP -&gt; SNF you could end up with a much higher count than what is really going on.
Yes times one million!
Hey, This was a different project in the original post. But funny enough, I’m actually working with ACO data as well (as of right now). The problem that we are running into, is defining the business requirements. And from what I read, they want to change measures and they don’t know what they want. Same boat here. If you figure out what works, let me know 
I bet Dave is really cool... but capitalizing on temporary solutions for underlying issues irks me. Dave and stack are the same beast. Even r/sql (dont hate me guys) falls into that category sometimes. "Dont use cursors blee blahbbity what's a physical model, I only use Primary." How about... all of us ... create an industry standard design? Utilize codds reasons and rationality for increasing normalization while future proofing a model and create something that fits ANY company's need? That's my niche. Substance. Live, imperfect, dgaf about clicks, substance. Wanna sub and watch an ad, get me a nickel? Do it. Gold me? Sure w/e. Ignore an introvert? Hah. Offer a contradicting opinion? Better submit a dataset and your test code because if you're right, I'll be incorporating that. It's the messages from greens and 20 year sql vets that say... I've learned something... that's priceless. I'm elliot Alderson insane. I and love it. I have and will continue to disembowel MSSQL and wear its insides as a hat. Unbridled passion. That's my niche, I'm me, and nobody else is.
For sure. Goes both ways. This stuff is wild. Especially without defining those types of things. At some point you have to just pick a way, document why, and keep moving on. If you dont, you will NEVER finish haha.
im well aware of the overhead that comes with it -- but thats not what OP wants. i was giving him a solution that will work. shrug. 
I don't think it's possible, but you could try: ```sql SELECT ID, GROUP_CONCAT(EMAIL) FROM T GROUP BY ID; ``` assuming: 1. you use MySQL; 2. the first column name is `ID`, and the second column name is `EMAIL` and table name is `T`;
I've been slowly working on a custom gitbook off-and-on for a few months now covering all aspects of Data Architecture (a very broad subject, I know). Let me know if you decide you're looking for anyone to review the content prior to release. As far as the YouTube series for the certification - yes, I think its worthwhile. The severe lack of content, period, when it comes to enterprise database work is astounding to me (part of the reason I've started on working on it myself). 
&gt;The severe lack of content, period, when it comes to enterprise database work is astounding to me (part of the reason I've started on working on it myself). Tell me about it... before I was mentored... master data management on youtube was pictures and a dude from sap talking about ideology. No implementation or substance. That why I started. I dont envy people that have to wait for a chance. Let them learn and master on their own. Dont bogard the sql snacks.
_Generally speaking_, SQL is SQL. There are some feature and syntax differences and "gotchas" between the various platforms but they're all based on the same concepts; relational algebra transcends specific databases. Anything claiming to make you an "expert" with *eight and a half hours* of videos is suspect IMHO.
The basics are the same between all the different relational database servers.
It’s really quite insane at the lack of resources. It took me around 4 years of exhaustive extracurricular study to pick up enough to move from Engineer to Architect. That was scouring through reference manuals, official documentation, books, articles, poorly made YouTube videos, etc. It was frankly asinine, and I unfortunately didn’t have anyone to mentor me based on the job settings I’ve been in. With the huge push towards even smaller companies using Machine Learning and Business Intelligence to drive their decision making - Data-centric roles are even more important now than in the past. Unfortunately, the last few environments I’ve found myself in has been large data environments with little-to-no standards, fragmented architecture and some of the poorest design decisions you could imagine. Job security, I guess.
It’s just a gimmick that all these courses say. If you look around on these subreddits, that specific course is pretty popular. Feel free to drop some sources you have used.
Do you have a link to the youtube channel? I would love to check it out!
https://www.youtube.com/user/psylense
&gt; If you offer paid services or paid trainings, it could mesh with a portfolio. Likewise if you are a big name, you can probably get away with terrible ads. &gt; How about... all of us ... create an industry standard design? Utilize codds reasons and rationality for increasing normalization while future proofing a model and create something that fits ANY company's need? At /u/FoCo_SQL and /u/AbstractSqlEngineer: I believe you can get away with hosting something for fairly cheap out of pocket that could still reach the masses. It would need to be something modern, to ensure the inclusion of the new generation of learning - much like how the JavaScript community has embraced such. At the same time, bigger companies such as Microsoft, Google, Facebook, etc. have been more involved in bringing on content creators into their company on salary just to evangelize technology in general. Creating a great product/service could hopefully leverage that. Every company out there has databases, and yet look at the number of subscribers for /r/sql, /r/database(s), /r/datascience, etc. compared to that of the likes of /r/python and /r/javascript. The first problem is there isn't a sole resource where you can learn this stuff front-to-back. It's fractured. Every day there's someone posting in here: Where's the best place to learn about SQL and databases and the responses have to be "It depends, if you're wanting to learn about X then there's this 4 year old article that's awesome, if you're wanting to learn about Y then pick up this $50 book and read Chapters 3, 7, and 9." For someone just starting out, they're going to take the path of least resistance and slowly just transition into a general programming role. Where's the community's one stop shop for all of the following: * Relational Database Theory * Database Administration * Data Security * Relational Database Design * Data Warehouse Design * Data Governance and Standards * etc, etc, etc. There isn't one. The other problem is that university's and other higher learning aren't really helping the issue either. Look at any 4-year tech/computer related degree and you usually only find one required class - Introduction to Databases - and it's near the end of the 4 years. To bootcamp's, databases are simply whatever you can SELECT, INSERT, UPDATE and DELETE into and don't move into anything past that. A modern, sophisticated, properly curated resource on Data Architecture and Database technology would be something that just hasn't been done before. 
I'm currently working through T SQL Fundamentals, udemy 461 series (I forget name from memory) and through the exam reference guide. What would your YouTube series provide where you see gaps in those courses? I ask because I'm intrigued and would follow, even though I'm shooting for 761 this summer.
If your a beginner learning things like select, group by, join, where, having, functions like row\_number, left, charindex, etc, it should be mostly fine. Some of the more 'advanced' features like CTEs cross over too. A strength of SQL Server is the ability to write stored procedures and use dynamic programming. I say 'strength' but depending on who you work for, the latter may or may not be something they're comfortable with you doing. As you move along, T-SQL becomes a bit differentiated. At the level of the course you linked to, most things will indeed be transferable.
Honestly, I dont know. Maybe a teacher analogy... you had many teachers in the same subject, yet you remember one you liked and one you hated. I might be the one you hate, I might be the one you liked. Only you can determine those gaps, or my gaps. Here is my plan so far. 1. No PowerPoint, just code code code. See it, download it, do it. 2. I will be incorporating good development techniques into this series. Formatting, naming conventions. What's common in the wild, what you should be doing instead. 3. I will break it. Someone mentioned people teach in best case, but... if you know what doesn't work, you can deduce what does. 4. Not one person has all the answers. Watch everything you can. 5. How to think. The more someone understands why and how, the easier it is to make subconscious relationships to other patterns and functionality. 
thanks, i'll try it, but I've read somewhere where they said sp\_preparexmldocument was the wold way to do it. i managed to extract the data with powershell, but if i could do it with SQL it would be much easier to maintain. now i got an extra step in my ETL with powershell.
Oracle?
If you're using 2012 or later, you can use this example: CREATE TABLE #temp (id INT IDENTITY(1,1) ,stringToSplit VARCHAR(1000) ) &amp;#x200B; INSERT INTO #temp (stringToSplit) VALUES('From ''2/28/2017 5:00 PM'' to ''3/31/2017 6:38 PM''') &amp;#x200B; SELECT \* FROM ( SELECT id, Split.a.value('.', 'VARCHAR(100)') AS splitString FROM ( SELECT id, CAST ('&lt;M&gt;' + REPLACE(stringToSplit, '''', '&lt;/M&gt;&lt;M&gt;') + '&lt;/M&gt;' AS XML) AS Data FROM #temp ) AS A CROSS APPLY Data.nodes ('/M') AS Split(a) ) t WHERE TRY\_CAST(splitString AS DATE) IS NOT NULL AND splitString != '' &amp;#x200B; If you're using anything before 2012 you can't use the TRY\_CAST piece
I can do anything in sql. Send the file my way and I'll RIP it apart and send it back. 
I like your method better than what I came up with. I just extended what OP was doing. It's pretty ugly honestly and not really extensible to eg more then two quoted values. **create table #temp(comment varchar(MAX))** **insert into #temp** **VALUES** **('From ''2/28/2017 5:00 PM'' to ''3/31/2017 6:38 PM''')** **select \* from #temp** **select** **substring(comment,charindex('''',comment),charindex('''',comment,charindex('''',comment)+1) - charindex('''',comment)+1)** **,case when len(comment) - len(replace(comment,'''','')) &gt; 2 then substring(comment,charindex('''',comment,charindex('''',comment,charindex('''',comment)+1)+1),charindex('''',comment,charindex('''',comment,charindex('''',comment,charindex('''',comment)+1)+1)+1)) else null end** **from** **#temp**
Great idea!
That's a pretty big jump. What do you know about SQL and database administration already?
*Hey just noticed..* it's your **6th Cakeday** mkingsbu! ^(hug)
Nearly there, but you missed a bit. &gt;Order the results by product line **and the descending order of the difference**
Could I ask you more explicitly what steps I need to take to get started on something like this?
I'm sorry, but how exactly would I write the descending order of the difference? would I have to quote the MSRP - buyPrice again?
Knowledge is very basic about SQL. I have used a couple of SQL tutorials to help teach me basic queries to get through using TOAD and Neteeza. Not much about Database Administration. I have to run P2 calls occasionally that have DBA's on them. While what they say goes over my head, it sounds interesting to me.
Yes, Oracle. 
Yes you'd put that calc into the order clause.
Are you okay entering the variables at runtime?
I’d prefer to enter them, then hit run, so that I can save the query each time and have a reminder of the dates used last time. 
So the order by line should be ORDER BY productLine, MSRP - buyPRICE DESC; &amp;#x200B;
Try this: DEFINE lower = 'date'; DEFINE upper = 'date'; SELECT * FROM table WHERE date BETWEEN &amp;lower AND &amp;upper;
Looks good to me. Just fyi, and it shouldn't matter, in the ORDER BY the default sort order is ascending, so what you've got there is equivalent to ORDER BY productLine ASC, MSRP - buyPrice DESC;
Thanks for your help!
You can also use a number table. It's about a 1/3 down in the following article: [https://www.red-gate.com/simple-talk/sql/database-administration/creative-solutions-by-using-a-number-table/](https://www.red-gate.com/simple-talk/sql/database-administration/creative-solutions-by-using-a-number-table/) &amp;#x200B; I haven't done any testing on performance, but I just tend to use the xml way because it's pretty straight forward and doesn't need an additional table.. 
I would recommend completing a course and take intern opportunity in field related to database. You won’t land a career with Helpdesk experience. 
A join is going to join all things the joining rows have in common. So since iPad is listed on both twice, both those rows would be duplicated. 2 iPad rows + 2 iPad rows + 1ipod row = 5 rows
Why are they duplicated? I thought the point of join ons were to remove duplicated?
I will consider this. Thank you
Nope, joins risk adding more duplication. Since you are joining on name, it joins each instance of iPad on table 1 to each instance of iPad on table 2, making 4 rows.
ok thank you ! can i pm you with a few more questions ? i have an exam in 4hrs 
lol, take me forever to see Ipad vs Ipod. &amp;#x200B; 2 Ipad x 2 Ipad + 1 Ipod x 1 Ipod
Sure
One thing to note is that it is immaterial that this is a left join because there are no names on the left table that aren't on the right table. An inner join would produce the same result with these tables / correspondence. To think about, consider that the first step of any join is the cartesian product. &amp;#x200B; So you have table 1: IPAD\* IPAD\*\* IPOD\* &amp;#x200B; table 2: &amp;#x200B; IPAD\*\*\* IPAD\*\*\*\* IPOD\*\* &amp;#x200B; I've added stars to make it easier to follow, but consider them the same value as without the star. &amp;#x200B; A cartesian product or cross join takes all possible combinations. &amp;#x200B; (1) IPAD\* (2) IPAD\*\*\* (1) IPAD\* (2) IPAD\*\*\*\* (1) IPAD\* (2) IPOD\*\* (1) IPAD\*\* (2) IPAD\*\*\* (1) IPAD\*\* (2) IPAD\*\*\*\* (1) IPAD\*\* (2) IPOD\*\* (1) IPOD\* (2) IPAD\*\*\* (1) IPOD\* (2) IPAD\*\*\*\* (1) IPOD\* (2) IPOD\*\* &amp;#x200B; Then we apply the on clause &amp;#x200B; (1) IPAD\* (2) IPAD\*\*\* (1) IPAD\* (2) IPAD\*\*\*\* ~~(1) IPAD\* (2) IPOD\*\*~~ (1) IPAD\*\* (2) IPAD\*\*\* (1) IPAD\*\* (2) IPAD\*\*\*\* ~~(1) IPAD\*\* (2) IPOD\*\*~~ ~~(1) IPOD\* (2) IPAD\*\*\*~~ ~~(1) IPOD\* (2) IPAD\*\*\*\*~~ (1) IPOD\* (2) IPOD\*\* &amp;#x200B; That leaves 4 rows. In a sense having an 'on clause' in tables where both sides have duplicate values of the correspondence field doesn't 'create' duplicates, but it 'leaves' duplicates in a way that an on clause with uniquely specified values would not. &amp;#x200B;
Sure. What parts are you most interested in understanding? It’s not all sql ofc. 
&gt; take me forever to see Ipad vs Ipod. Same. Bad scan and a bad font.
amazing explanation thanks for your time!
All right it's in the works.
Its not a left join. It's an OUTER join. OUTER is optional. This means every thing that is in the OUTER table will be returned REGARDLESS of the match in the ON statement Select * from a LEFT OUTER JOIN b on a.name = b.name Will return every row from a, and every row from b that matches. Select * from a RIGHT OUTER JOIN b on a.name = b.name Will return everything from B and records from A that match. When you think about it, row by row... IPad appears twice in A and twice in B. What I am about to describe is known as EXPANSION. During the join, sql gets the first row from A, checks the ON clause and looks for a match (2 rows in B) So that's 2 rows. Now sql moves to the second row... same thing,iPad, 2 rows in B. That's 2 more rows (4 total) Lastly, ipod matches only one record in B. That's 1 more row. Totally 5.
Yes. They are useless. Mssql doesnt have lateral, neither have natural, postgre has built in functions that sql doesnt. Dont do it. You'll learn things that are irrelevant. Get something like a 70461 or 70761 exam from udemy. That will give you a run down.
Looks interesting. I'll read that write up tomorrow. Thanks!
Thanks for this input. It seems one could get their IP banned as well from the website.
Select I'd, [1],[2],[3] From (Select id,email,row_number() over (partition by id) as Rownum From table )x Pivot ( max(email) For rownum in ([1],[2],[3]) )pvt
Does your current employer have Jr DBA positions you could apply for? Sounds a lot like me 15 years ago. Started in help desk right out of college. Spent 2 years there until the production DBA left and they figured I could learn it, it was really a JR DBA position... just keeping the lights on. After learning all I could there, I took a position on the dev team which gave me a different perspective on how SQL is utilized by applications. After 4 years, back to DBA (at a different firm), but with a much better idea on how a DBA can really be helpful to company. That being said, report writer is a good way to get started with the application side of being a DBA, while jr DBA (maybe after taking some certification courses?) is a way to get started with the production side of being a DBA. 
The various SQL dialects are about 90% identical, but there's more to working with a database than just knowing its SQL dialect. Your much better off working through some MS SQL Server courses or books.
https://imgur.com/a/aoYeWTo is the answer 6?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/oSMtdCW.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20efuggot) 
https://imgur.com/a/aoYeWTo wuld the answer here be 6??
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/oSMtdCW.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20efugh8b) 
https://imgur.com/a/aoYeWTo is the answer 6?
Tell me why it is 6.
there will be 4 2s. a 3 and a 7
You are correct.
PRAISE THE LAWD
Inner are matches only. Instead of full outer, if it was inner, it would be 4 2s and that's it.
ok thanks. can I live chat you with some potential more questions? 2hrs to my exam ://
Why did you procrastinate 
I'm doing a summer unit. they condensed 12 week course into 4 weeks. I submitted the course project worth 40% 3 days ago and my final exam is today. I didnt procrastinate, it's just not much time 
This is pretty advanced stuff. Where you from?
Answer= Pi is a reduced set. The X is a natural join Take a distinct ac over ad. Ac 14 24 21 Ad 35 24 21 24 Pi ac natural pi ad 14 24 remove 2 rows 21 remove 1 row. 35
Damn son. You're boss.
University of Western Australia. This is a 1st year databases course lol
and the second question? (only if you have time )
 Take a and d from r and s I'll edit.. I need to see the picture 1 sec
Isn't X a cartesian product? BOWTIE is natural join maybe?
Yes. Sorry. Dunno that key. I meant ]X[
You basically ignore C and D. Work with A and B, because projection is on the outside of the parenthesis You are forcing a join on the inside given that the right table (s)B = 8. A6b8 are the only matches, then its asking you for A and d... D being a 6. Columns are important. Different answer if both matrices were abc instead of ABC and ABD.
This is wrong. I havent done this in years You have to match the correct letter for each side. 
omg dude... union is not a join. Union would be "U". You're getting the "correct" final answer 4 because 2+2= 4 and 2x2 = 4.
where did i say union?
Yea man. It's too late for this stuff. I havent had to do this for years. Shits all gone. Codd help us all.
Its fucking distinct on each side |X| 24, 21 x 24, 21 Just like Abc 78 68 Abd 58 68 6, 6 for 1 join.
I'm good now. Lol. Its back.
Gimme another one. No answer
I said union I'm screwing up my codd math because, in the states, 90% of our DB guys dont know rel algebra. You stop communicating in it and its lost.
This would be a great on going series for a YouTube channel. As each new test comes out you could create a new series for that test. Have you thought about Oracle?
[https://imgur.com/a/8BmM8Gp](https://imgur.com/a/8BmM8Gp) not RA but not sure is it 23? 
I have never professionally worked with oracle. TSQL all the way. I would probably have to limit this series to Data Platform compentancy exams.
No, it's not 23. Just try to think through all possible sequences of these transactions and their results.
18.
Key words, running approximately at the Same time.
Does that make sense? Possible fire order Count, Insert, Delete Count is 20 Delete, count, insert Count is 20 Insert, delete, count Count is 21 Insert, count, delete Count is 23 
not really. whats the insert count delete mean
The instructions before the commit. Select count(*) commit Inserts commit Delete commit
thanks for your help! i'm off, wish me luck D:
If you do not want to affect the database you are currently working with, you should write only SELECT queries. If you do not write \`ALTER\`, \`INSERT INTO\`, \`UPDATE\`, \`DROP\`, \`DELETE\` statement, then there is no change that you would alter any data. 
It's quite possible to acquire a decent level of SQL on your own and get a job in that field. SQL is a high level programming language. It means that you do not write custom code. So it's quite limited in the sense that you just describe what data you want to get and not how to get them. Therefore it takes much less time to grasp in compare to actual programming language like Java, C++, etc. Go for a practice oriented tutorial. The following (free) resource may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Please do let us know if you really decide to make this series!
Can someone point me in the direction of his videos? Lol 
Thousands of duplicate units and dates. The only criteria it seems to keep is that nothing from before 2018 was pulled. I managed to get all the unique Units from 2018 and beyond using the response from u/UseMstr_DropDatabase it just doesn't give me the dates. Works for my purposes, although I'd like to know why the dates are refusing to cooperate with DISTINCT Units. :/
guess what! my exam was recheduled to next week due to a printer error!!!
Check to see how the underlying date is formatted. DISTINCT (or GROUP BY (same thing basically)) likely isn't doing what you want it to because the actual field is storing the date *and* the time. A sample dataset would help me/us troubleshoot it a bit better. Also, don't forget you can stack queries in Access. You can create a query for all 2018 orders/units that will bring in everything, save it off, create a new query, and instead of bringing in a table, bring in the saved query and then do your DISTINCT/grouping there. Also, I think it's probably better to use the DatePart function in Access than what you're doing with year(schedule... https://www.w3schools.com/sql/func_msaccess_datepart.asp Replace it with this DatePart("yyyy", [Schedule].[Start Date]) SELECT * FROM Table WHERE DatePart("yyyy", [Schedule].[Start Date]) = 2018
Ask for access to a development database. If your IT department/DBAs are competent, it will be a separate server from production with production-like data that doesn’t have sensitive data like PII or HIPAA-protected information in it. And even then, it should be read-only. 
Yes. Btw, can you give me the link to your channel?
Got a link?
&gt; Its not a left join. &gt; &gt; It's an OUTER join. OUTER is optional. &gt; &gt; Not sure if you meant this, but a `LEFT JOIN` and a `LEFT OUTER JOIN` is the same thing. 
https://www.youtube.com/user/psylense
I'm doing it, I've been sketching out a flow from subject to subject.
Great! Do you have a YouTube channel or something?
Just trying to communicate the OUTER functionality in case there was a RIGHT or FULL question. 
https://www.youtube.com/user/psylense
It appears someone else answered your question. Sorry, I went to bed. I hope your exam goes well.
Thank you! I'm definitely going to follow your channel
Can you please link us to your gitbook? It would be great to know what I don't know. Thanks!
Depends on SQL but in MS you can just export using a query with bcp in a stored proc. I’ve set this up quite a bit.
It’s in SSMS. Could you give me an example or a bit more clarification? It would save me a bunch
Downvotes because it's a basic homework question and you're talking about how you have an exam in 4 hours. These are basic SQL elements that you should have taken the time to understand earlier than 4 hours before an exam. &amp;#x200B; Just explaining why others would. I get it if you're just starting and SQL isn't your focus, but next time take a little time to understand the content a little earlier. You can't possibly understand SQL if you don't even understand a join (the fact it's a left join is irrelevant here, this is basic join logic).
Doesn’t seem to work. DBeaver does nothing, SQL Developer says “Missing IN or OUT parameter at index:: 1”
http://www.sqlfiddle.com
Try this: DEFINE lower = 'date'; DEFINE upper = 'date'; SELECT * FROM table WHERE date BETWEEN '&amp;&amp;lower' AND '&amp;&amp;upper';
It doesn’t seem to like the define statements, throwing an error at position 1. 
What's the error?
SQL Error 900 42000 ORA-00900 invalid sql statement Error: 900 position 0 define......
Try running it in SQL Developer or SQL*Plus instead of something third party. 
Try running it in SQL Developer or SQL*Plus instead of something third party. 
I don't believe I am using a DBMS? I downloaded MySQL Community Server and am trying to create a database based on a course I took. When I try to execute I get a syntax error message. Based on that I've been trying to change the code to figure out what's going wrong. I thought it might be an issue with an old data type so i changed "smalldatetime" to "datetime" thinking that might be a factor. Below is the original from the instructor: \--STEP 4 Highlight and Execute lines 17 through 260 CREATE TABLE Customers(custnum nvarchar (5) NULL,referredby nvarchar (5) NULL,custname nvarchar (30) NULL,address nvarchar (25) NULL,city nvarchar (20) NULL,state nvarchar (2) NULL,zipcode nvarchar (12) NULL,repid nvarchar (3) NULL) CREATE TABLE Obsolete\_Titles(partnum nvarchar (5) NULL,bktitle nvarchar (40) NULL,devcost money NULL,slprice money NULL,pubdate smalldatetime NULL) CREATE TABLE Sales(ordnum nvarchar (5) NULL,sldate smalldatetime NULL,qty int NULL,custnum nvarchar (5) NULL,partnum nvarchar (5) NULL,repid nvarchar (3) NULL) CREATE TABLE Slspers(repid nvarchar (3) NULL,fname nvarchar (10) NULL,lname nvarchar (20) NULL,commrate float NULL) CREATE TABLE Titles(partnum nvarchar (5) NULL,bktitle nvarchar (40) NULL,devcost money NULL,slprice money NULL,pubdate smalldatetime NULL) 
good to know
The lower and upper variables are prompting me for input instead of using the defined values. 
Is there another way to try this? Everything I see on the web uses DECLARE statements, but as I said I’ve never been successful in implementing them for my own uses. 
Why did people downvote you? I think this is a better route to go.
Why do people do anything? What they do, is dissuade people from commenting and trying to steer individuals in a better direction. Thus decreasing the value of this subreddit and the quality of data professionals as a whole. Jokes on them. Dgaf. Most likely it's because I either mentioned a cert, or said useless.
If you want to learn java, get java videos. Not c# ones.. even though the similarities are almost plagiarism. If you want to learn PG P My or T sql... get that specific video.
[The wiki has items for review](https://www.reddit.com/r/SQL/wiki/index?utm_source=reddit&amp;utm_medium=usertext&amp;utm_name=SQL&amp;utm_content=t5_2qp8q)
https://www.tutorialgateway.org/ssis-flat-file-destination/
And you input the variables into the define?
Yes, I input the lower and upper dates in the ‘date’ segments of the define statement. The main query is ignoring the define and prompting me for inputs on those variables called. 
You're highlighting the entire query and running it all at the same time, right?
Pretty sure MySQL is free, IDK about for business use
Isn't the function DATE_ADD()?
I did it like you suggested and got an SQL Syntax Error
SSRS is significantly cheaper (included with your SQL Server license, even!), but it's not as powerful. PowerBI is really good at making dynamic reports that can be adjusted on the fly, while SSRS is really only for static reports that must be reloaded whenever a parameter changes. They both have their pros and cons, but SSRS is what we use. We're considering PowerBI, but unless their online license calculator is faulty, they've priced themselves way the hell out of our price bracket -- and we're government.
PowerBi is incredibly expensive, like /u/pmbasehore said. SSRS is good, that's what we used at my last company. MSSQL2016 has some nice features that are PowerBi like, but not true as fluid.
Not sure what the error could be either but have you tried declaring it as a variable and using that instead of the function? DECLARE @18YOCHECK DATETIME = DATEADD(year, -18, CURRENT_DATE) DELIMITER $$ CREATE TRIGGER Altersgrenze BEFORE INSERT ON spieler FOR EACH ROW BEGIN IF new.gebdat &gt;= @18YOCHECK THEN SIGNAL SQLSTATE '02000' SET MESSAGE_TEXT ='Altersgrenze ist 18 Jahre !'; END IF; END$$ DELIMITER ; &amp;#x200B;
ty
Thank you. I started doing sqlzoo and I really like how the language works. But I am only a few sections in.
Thank you for that ! &amp;#x200B; How can I check what is in the variable, cuz I inserted someone who should be 12 and the Trigger didn't go off so something is still wrong with my Trigger. &amp;#x200B; Sorry, I am still a beginner :(
Not sure if I'm missing something but I think you need a 'where' clause to stop the recursion. Like in this example: [https://www.essentialsql.com/recursive-ctes-explained/](https://www.essentialsql.com/recursive-ctes-explained/) They have n &lt; 50 in the example where they generate 1-50 with a CTE. It looks like you framed that logic with your level column but never added the where clause.
Looking at the script again it looks like you are throwing an error when someone is older than 18 not younger (I don't speak German so I couldn't tell what the error was, my bad). Flip the IF statement to IF new.gebdat &lt; @18YOCHECK and that should work.
Sadly flipping the IF Statement didn't do the job. But Thanks for your help. At least I learned something new. (Declaring Variables)
Like everyone else said - PowerBI report servers are super expensive (per user license, needing enterprise edition and software assurance). All depends on your size and need for it. If you can afford it and want professional looking graphs/charts and dashboard, go for PowerBI. If you are looking for something cost effective and just a general need of resorts, go for SSRS. 
Yes, I’m executing the whole thing, just as I would if I was executing a query using CTEs. 
Thanks to any and all. I've been pulling hair out over this for the last 24 hours. 
If you have some systems experience, enterprise level solutions can be had on as little as $100/year on a host like vpsdime.com with free MySQL offloading ( up to 20gb, unlimited databases). Then using an excel add-in tool like MySQL for Excel, you can load/update/query data from Excel. The team of 10 can use the MySQL database as a central location and single source of data. I'm not affiliated with vpsdime, but I have been a subscriber and feel like they provide a good value. 
Shouldn't you be joining parent object name to child object name? Presently you're joining obj (which in the base records is parent object name) to parent object name - it's just going to keep going forever.
Assuming TransId is the first column, and that you want to return all data for a given TransId where that TransId appears multiple time in the table, then I'd use a CTE to identify the records that are duplicated and then join to that to return all of the related records. So, something like: ;with dups as ( select TransId from table group by TransId having count(*) &gt; 1 ) select table.* from dups inner join table on dups.TransId = table.TransId &amp;#x200B;
Power BI Report Server is free if you have the Enterprise Assurance plan.
Power BI is more for a end user to have the ability to create their own dashboards. If you are going to be creating the reports, SSRS is a little more clunky, but is tied directly to MSSQL. I prefer Power Bi. 
so... yes... we had SSRS EE under SA using old server CALs... PBIRS required us to purchase/convert to core licenses... the documentation used to say "PBIRS uses SQL EE licenses" - now says "uses SQL EE *core* licenses" - because of us and *lots* of emails with our account rep. depending where you were - we were on RS 08 R2... 2016 has the revamped installer (splits RS from the rest of the SQL installer) and UI... the UI also brings backend changes - now a webapp executable, API service layer executable, etc... not a huge load on server, but a tad larger. Also, these don't run as independent services, they're all spun up from the core RS win service. PBIRS also adds the PBI components to the backend... mashup.exe to perform the data refreshes via PowerQuery / PowerPivot, as well as msdrv.exe - aka ANALYSIS SERVICES (tabular / powerpivot mode)... and this too is NOT installed as a service, but rather run in the background from the rest of the RS EXE's... consider the memory usage of having each PBI file open on the server (the files are opened when people are interacting with the report, so that the UI can issue the TAB queries to SSAS, and also for data refreshes). whereas SSRS still follows the traditional SQL lifecycle, PBIRS follows the "Modern Lifecycle"... aka 3-4 updates per year, and only the two most recent releases/updates are supported... also, KEEP DOWNLOADED COPIES OF THE UPDATES - the download links might actually change - APR 2016 was released twice, same downloadId... if you have multiple environments that need to be consistent, KEEP YOUR OWN COPY (i create subfolders for each release, name it to include the version reported by the download link, the build version as reported internally by the product, and the date of release as reported by the DL link). also, be aware that we're STILL waiting for a STABLE release of the product... i've had several emails and conversations with MS acct reps, prod team, etc... *every* release has had *something* wrong that affected us, that wasn't broken previously... mostly small stuff and edge cases, but still... the latest issue is a caching/concurrency bug when multiple users are sharing the same PBI report - it's supposed to be fixed with the Jan update, i've just not yet had a chance to install it... prior to that it was stuff like "if your report happens to include a reference to an *object name*'s value, when the object renders on another page, #VALUE instead of actually functioning" - again, edge cases, but we've been burned. that said, we're running a fair amount of activity on just 4 cores... we have other RS boxes running a ton more (50k+ RDL report executions per day) - unsure what we'll want to do about upgrading these... we watch some of the RS metrics - memory usage, etc... but so far it's more or less on cruise control. feel free to ask something specific.
To be fair, you can also create an SSRS report using any data source that has an ODBC driver, though it's not super pleasant to do. I've done them using OracleDB and SqlLite before. It sucked, but it worked.
The choice depends on your budget and requirements. SSRS is far better for paginated tabular reporting, generating invoices etc. PBI is better for everything else. If you mean Power BI Report Server, we use it. You can host .rdl reports (SSRS or Report Builder, though I've not used the latter or migrated any reports myself) on it.
I'm not sure whether you're using T-SQL, but have you wrapped the login and user with square brackets? Eg: use [AppDB] create user [DOMAIN\username] for login [DOMAIN\username]
Can't you create dashboard with SSRS? Static ones I think
Oh my, thank you so much. That worked! 
Not \*pointless\*, but I expect you'd get a lot more value out of a course specifically tailored for MSSQL e.g. 70-461 or whatever the current equivalent is. You don't want syntax and function differences adding to the confusion of learning a new platform.
glad it helped
If you are going from Excel to SQL I would recommend using MS SQL because it will make your life easier in the long run. Set up a local computer on the network that you can all connect to, or set up local host databases on everyone's machine using something like the Northwind set. No cost, easy to set up, and if you are going to be taking your team away from Excel and into SQL then this might be your best solution overall. 