Whoever is down voting this is a twat.
How many of these groups do you have? If it needs to be reusable, you can make a view like the following. May take some time upfront, but would be easy to maintain afterwards as you add brands: SELECT brand, CASE WHEN Brand IN ('Stouffer's', 'Stouffer's Simple Dishes', ' Stouffer's Classics', 'Stouffer's etc') THEN 'Stouffer's' WHEN Brand IN ('Craft', 'Craft Singles', 'Craft etc') THEN 'Craft' /*Keep repeating for all groups*/ AS ReportingGroup, price, quantity, etc.... From Table This will give a view with the brand and ReportingGroup as defined by you and your team. Then use this view instead of your brand table in queries. 
You can actually do this in a table instead of a case statement, and use master data management to fix the problem. the table has one column for the source brand, and a 2nd column for the conformed brand. Undefined brands are inserted into the table with the 2nd column being the same as the 1st. The 2nd column is then updated by someone doing data entry. Join to the table on the source brand name and display the 2nd brand name, and you're done! 
If only we all had data entry interns!! Certainly an option, though. In this instance, I feel a view is more scalable, but that is simply personal preference. Writing update statements /editing a separate table seems more tedious in the long run.
Hey, if you work somewhere you can deploy your code every time you have to change this, then knock yourself out! Just know, you are now a data entry person! Every time it changes it's now your responsibility. Good times. I work on enterprise software, so seeing a solution like the case statement gives me the heebie-jeebies. This would never pass a code review, but it doesn't sound like you have that problem. Just be sure to put a comment in your code for the poor sucker who comes after you! 
Your assumption is correct - MS SQL. I'll look into Soundex more. It didn't accurately group what I needed when I tried last time but perhaps it will work with a little more TLC in the code. Thank you!
Haha, I have to agree with this - hard coding that much into a statement is pretty much the equivalent of what I'm trying to avoid by not using a lookup or helper table. There is no way to maintain it other than manually and ain't nobody got time for that. Perhaps if my data set were much, much smaller it would be an option. Really the issue is that the data is stored one way to show "correctly" on our ecommerce site, but for our reporting needs, it should be stored under the larger brand. Ideally we would have a reporting category, and that's where we're trying to get to.
So, just curious, what's the difference between maintaining a view statement and a helper table? Especially if you have a data entry person doing either? I'm in BI. I receive relational databases and query/report from there , just trying to learn, not disagreeing. 
Just my assumption playing around with your query on [SqlFiddle](http://sqlfiddle.com), that enum declaration isn't supported in your version of oracle (or maybe at all, idk not too familiar with ora). Looks like you need to use the CHECK constraint on the field as a varchar2(2). [This](http://thinkoracle.blogspot.no/2005/05/enum-in-oracle.html) goes over the syntax for you.
thanks that fixed it
1. Executing DBCC CHECKDB and DBCC DBREPAIR statements to recover lost data is one option - "http://nalavadebela.wordpress.com/2014/04/05/repair-sql-database/" 2. Restore from a good backup (SQL Server) 3. MS SQL Database Recovery software 
Glad I could help!
On that first question my first thought would be to use a windowing function. 
To me the maintenance would be quite similar. The only real benefit I would see from using a helper table versus a view statement is that reuse would be easier and more reliable from a helper table. It's not great practice to have a view statement floating around in several reports - more places to change code if part of the view statement were to need updating. That's just my 2c, though.
This might work in the OP's initial example, but this doesn't see dynamic enough if lots of items are compared or new items are introduced. My only suggestion to the OP is to create a CLR procedure or function to do this using C#. Review Levenshtein Distance (or Edit Distance) which is a string metric to calculate the similarity between two strings. There is no native TSQL implementation of this, but Id id find a C# implementation if you'd like to wrap this into a CLR and use within MS SQL: http://www.dotnetperls.com/levenshtein Just hit-up Google, and you might be able to find more. This is really the best bet I can think of so it's as dynamic a possible.
The performance of the view will scale terribly with more cases. This is a data problem that you're trying to program around. The thing you need to know is what the reporting category is for each product. The correct way to solve this is to capture that data, be that manually or via some automated process. What you definitely don't want to do is some expensive operation on every row every single time you want to look at that data. It makes much more sense to store the result of that expensive operation and reference that stored value, especially given that the result should change much less frequently than the data is read. This is explicitly the thing that databases are for, storing data.
Happy to help. Give an example table structure(s) and example results please. 
You should be able to SUM on the column you want and then GROUP BY the persons name. All in one query.
Ugh. I can't really. It's against policy. its a timeclock db obviously. We have and emp#, firstname, lastname, time in. The weird thing about my goal is that our timein is the only logical way of getting whether or not they're in, and you can only tell if they are in if they have an odd number of clock ins. Right now I have select logtime, count(*) as timein, user_id from (Dbname) where user_id = 'Uniquenumber' and log_time between 'todays date' and 'tomorrow' group by user_id, log_time. So the result I get is 1 - Log_time | timein | user_id 2 - Same. 3 - Same These are all for the same emp number. What I would prefer to happen is just 1 - logtime | Timein(sum of all for the day) | user_id Just one entry. I'm going to turn this into an SSRS at some point and have it on the current date and they only have to enter the employee name or number, haven't decided that last bit yet. was that good enough? 
Operand data type datetime is invalid for sum operator. Unfortunately, I cannot sum on that.
I'm sorry that I don't know how to say this in plain english, but i think you're after SELECT PERSON, COUNT(*) FROM TABLE_NAME GROUP BY PERSON That will count how many times each PERSON's name appears plus count the number of times it appears.
haha, that's just fine. No it's not that, I'm totaling the amount of times they clock in for TODAY. If it's 1 that means they're here, if it's 2, that means they are not, if it's 3, that means they are here... so on and so on. I tried to sum log-time, but it's a date field.
What question are you trying to answer with this query? Is it 'when did john last clock in today?'
Did you try COUNT() grouping by the employees ID or whatever? Edit: I saw your post above, try to group by every column you want to see
My question is, "How many times did he clock in today?" because depending on whether or not it was even or odd, I'll be able to determine if he's here currently, or not. (Lunch/doneforday/etc)
I take it that you're also wanting it to say "Here, Not Here"? Having a counter by date is also possible with the query i provided, just add the date column to the SELECT as well as adding it to the GROUP BY.
Maybe try adding a where clause so you are counting only clock in times greater than or equal to midnight today (the beginning of the current day).
1 2014-11-18 08:02:00.000 400049 1 2014-11-18 12:00:00.000 400049 1 2014-11-18 12:59:00.000 400049 That's the result of my current query. the 1's are just a 1 for each time that particular userid has used the timeclock system today, so as you can see, this person is currently here because they clocked in, out for lunch, and back in. I just want it to say: 3 -- then the date/time then the emp number. Whether they are here or not is going to depend on whether that 3 is an even or odd number.
I guess the count by user would work because it registers every time the clock in device is used. One second.
What version of SQL are you on? MySQL, SQL Server, ..etc? If it's SQL Server, check this out http://sqlfiddle.com/#!3/c1d1d/3
Yes. It's super silly, there is no identifier like 1 or 2, for in or out. I have to use the last time they used the machine, hence why 1 = in 2 = out 3 = in... etc.
server.
This is difficult without giving an outright answer... but the below could help. sum(case when cast(current_timestamp as date) = cast(clockintime as date) then 1 else 0 end) This wouldn't work for people that are working overnight shifts (say 7pm to 7am) but that is an entirely different question than the one you're asking.
I don't know if you saw the fiddle link as i editing the post afterwards, http://sqlfiddle.com/#!3/c1d1d/3 i think thats what you're after from your other posts' description.
Luckily we don't have those kinds of people! Thank you. I just got handed a brand new project so I have to take my leave from here. When I get back to this, I'll make sure I make you aware of my attempts and results.
I'm not following how you think the number of check ins helps you reason about whether John is in or out. With both pieces of information, we obviously know all we need to know. John checked in at 09.30 John checked out at 11.45 John checked in at 12.45 But remove the outs and you only ever know when john was last in. Are you saying the clock in column actually represents both the user's clock in and clock out behaviour?
The system itself doesn't have an indicator, I'm making the indicator basically in the form of a report. Before this, you'd have to sort through and count the amount in the DB manually.... which not everyone has access to. Secondly, I'll be adding details to it later such as their names and what not and the location the device is in. Yeah, whether they are clocking in or out, that information goes in the same column as a time.
When I've had to solve these kind of things the approach has generally been to match the ones you can with a high confidence and do the rest manually. Sometimes that means me slogging through a bunch of data entry, and other times there are subject matter experts that have to do it. Either way, there's going to be pain for some one, so make it count. Hopefully there's not going to be a lot of flux in your data that requires recategorizing things.
Sounds good. Obviously you could replace current_timestamp with a reporting variable (so that you could check for clock ins on dates other than the current date). Hope everything works out!
We just want the current date as it's just going to be used to tell where people are and if they're there, but thanks. I have to create a new report now! Luckily for me I think I know how I'm going to do this one.
I'm not quite sure I understand the criteria you're trying to meet, but is this close to what you're looking for? DECLARE @table TABLE (Brand NVARCHAR(50), Price MONEY, Flag BIT); INSERT INTO @Table (Brand, Price, Flag) VALUES ('Pepsi',2,1), ('Coke',1,1), ('Pepsi',1,0), ('Coke',2,1); SELECT * FROM @Table WHERE Flag = 1 AND Price &lt;&gt; 1; Results Brand Price Flag Pepsi 2.00 1 Coke 2.00 1 
Cast the datetime datatype to match the output datatype, also cast it same in your group by.
Have you indexed the fields you are sorting on? Such as HeaderDate especially?
format 101 = mm/dd/yyyy here's the list: http://msdn.microsoft.com/en-us/library/ms187928.aspx
HeaderId is probably indexed where headerdate isn't, is headerId always increasing so that a higher headerId can't have a lower headerdate? What does a row in #TMP represent? One line for each distinct item or just an arbitary list of 500 items?
cheers! this was spot on, thank you!
I hadn't thought of that. Thanks, I'll try it out.
No, currently there is no index on HeaderDate, only on HeaderId (PK, clustered.) Would one index on both HeaderDate and HeaderId help?
Indeed, HeaderDate is not indexed. #TMP contains a distinct list of Items.
Has she recently had a patient called Bobby Tables?
Can an invoice header with a lower HeaderId have a higher HeaderDate? If so then you already have all the indexes you need since headerid is the clustered index.
Call me lazy but I will always prefer CASE mainly because I *always* forget the syntax for PIVOT
Example: SELECT CONVERT(VARCHAR(10), GETDATE(), 101) would result in 11/18/2014. Just change the VARCHAR data type length and the aforementioned date/time styles per your requirements.
You could try something like this: This would work for any number products. Not sure how big your dataset is or how well this will perform, but I just used a lot of window functions. If your using sql server 2005 or higher you should be ok with these functions. ; with cte as ( SELECT x.SalesDate ,x.ProductCode ,(x.Retail/nullif(x.Quantity,0)) as Current_Retail ,x.Quantity ,x.Retail as total_sales FROM x x WHERE x.ProductCode = 1264 ) select distinct productCode, totalSales2 - TotalSalesAll Loss from ( select *, max(Current_Retail) over ( partition by productcode) maxCurrentRetail ,sum(quantity) over (partition by productCode) totalSales ,sum(total_sales) over (partition by productCode) totalSalesAll ,max(Current_Retail) over ( partition by productcode) * sum(quantity) over (partition by productCode) TotalSales2 from cte )x
Thank you kindly for your help. Im not sure if you noticed but i ammended my original question. Instead of assuming the right price is the max price, i need to join it to the retail table to get THAT retail price at the time. Then do the calculations from there.
If you've got PHI on a pc that's infected with spyware you have much bigger problems than a missing database.
Thanks! Really thought I had that, must have overlooked it
Thanks for the example, I'll try something like it and update
Ditch the CTE, move the logic directly into your update and also ditch the use of the row number function. Instead create a non clustered index on the InvoiceDetails table, using the HeaderID and ItemID. Now join your query to a derived table that selects the ItemID and MAX(HeaderID) from InvoiceDetails. Join using that max value and you will get (what I assume) will be your very latest record for the ItemID. On another note, CTEs are great for recursion and multi-level data aggregation. Other than that don't use them, as they often cause more issues than you realize.
Your error is coming from the fact that your date does not have a preceding 0. You are passing in a string of 12-1-2012, when it needs something like 12-01-2012. Look at using the STUFF function to add preceding 0s then wrap that into a RIGHT function to only grab the last 2 characters.
Which RDBMS?
that works, but if i want to do this for say 10 brands... will i have to do 10 inner joins?
Using the first solution, yes. Using the second, no.
lag() and lead(), depending on implementation, work by looking in the already existing and possibly already ordered set. This makes it so that the data only has to be read once.
Thanks Qaudman, this morning I remembered the Lag() function from a totally different context, so I had this idea to use it, without really looking up for the theory behind that. I checked also the lead() function you suggested and I found this Oracle-base page that helped me to understand a bit better how these functions works http://oracle-base.com/articles/misc/lag-lead-analytic-functions.php
If you continue to use the ROW NUMBER function ordered by HeaderDate then yes you should index it, it would speed up the sort. I think some of the other suggestions here are probably going to be better in the long run though.
Declare @now DATETIME Set @now = Getdate() Do the count like he did and do WHERE Clock_Date = @now
An MDF file isn’t a backup file isn’t the main data file (or master data file depending on who you ask) hence the file extension MDF. You can attach the MDF to the database by right clicking on the databases folder in Enterprise Manager (SQL 2000) or Management Studio (SQL 2005) then selecting all tasks and then Attach Database. When you do this you will get an error about the log file missing. This shouldn’t matter as SQL Server will simply create a new log file for you. This will only work if the SQL Server was stopped when you copied the MDF file from the old SQL Server. If you copied the file while the SQL Server was running this file probably will not attach either and you will need to get a valid backup of the database. Download help tool for sql database - SQL Server Restore Toolbox. http://www.sqlserver.restoretools.com/ You can read up more on SQL Server here. http://www.filerepairforum.com/forum/microsoft/microsoft-aa/sql-server/498-creating-a-new-database-using-an-mdf-file?_=1416149856104
Thanks so much! This worked perfectly! :)
Can you post any DDL statements showing the layout of your tables? I'm having a hard time seeing what data you're trying to pull from. Also what RDBMS are you using?
My first instinct in Oracle would have been to `CROSS JOIN` to a subquery that just has 1 row per 0-3 in it... SELECT level-1 lvl FROM DUAL CONNECT BY level &lt;= 4 and then do some string/date manipulation to get the other dates. But strangely, Oracle doesn't allow 'YYYYWW' as an input date format (only works with to_char), so I ended up with [this horrible abomination of a calculation](http://sqlfiddle.com/#!4/2b5f0/6). Suffice to say, regardless of efficiency, `LAG` is a much better method than that.
So long as you understand why it worked, that's the important part! You're welcome.
Or, because I'm bored: select * -- ALWAYS NAME YOUR COLUMNS, * FOR EXAMPLE ONLY from PART_COST c CROSS JOIN PART_INFO i WHERE c.PART_NAME='SCREWDRIVER' AND i.PART_NAME='SOMETHING ELSE' UNION ALL -- OR UNION IF THERE MAY BE CROSS OVER BETWEEN SELECT's select * -- ALWAYS NAME YOUR COLUMNS, * FOR EXAMPLE ONLY from PART_COST c INNER JOIN PART_INFO i ON c.PART_NAME=i.PART_NAME WHERE c.PART_NAME!='SCREWDRIVER' edit: formatting 
Wow. I'm bored. Okay, this way uses a CTE to "fix" the name, before joining. Could also do newPART_INFO and change 'SOMETHING ELSE' to 'SCREWDRIVER'. Apples to apples. WITH newPART_COST ([PART_NAME],[col1],[col2]...etc) AS ( SELECT CASE WHEN c.PART_NAME='SCREWDRIVER' THEN 'SOMETHING ELSE' ELSE c.PART_NAME END,c.col1,c.col2...etc FROM PART_COST c ) SELECT * FROM newPART_COST c INNER JOIN PART_INFO i ON c.PART_NAME=i.PART_NAME edit: realizied the UNION really was not needed in the CTE
Very nice use-case for `LAG()`. `LEAD()`, `LAG()`, `FIRST_VALUE()`, and `LAST_VALUE()` are indeed very awesome functions. [I've recently blogged about them](http://blog.jooq.org/2014/11/07/dont-miss-out-on-awesome-sql-power-with-first_value-last_value-lead-and-lag/). They've come in handy on numerous occasions.
You'll need to build a Date look-up table with at least Jan 2014 through April 2014 then another look-up table with the possible Ticket sources, Email and Browser being the two in there for this example Then you'd join the first table to both of these tables with a grouping to get the desired results. I'll work on an example when I can, but you'll need at least these other two tables for this to work. 
You almost have it. You want to translate your column in the case statement or if it isn't the case you want to change then just return it as is. Join the result of the case statement to the other table. (As a side note, this can be ill-performant as another poster said. However, since you noted you are in Oracle you can create a Function Based Index which will you would create on the column in question doing the same case statement. This would be the equivalent to creating an index on a persisted computed column in SQL Server.) Should be like this: CASE WHEN i.Part_Name = 'PHILLIPS_SD' THEN 'SCREWDRIVER' ELSE i.Part_Name END = c.Part_Name
I built this, but my results are a bit different from yours. You said if a ticket is opened one month and closed in a future month it's counted 1 time each month, but your example data shows no values for January though there are tickets opened in January that were closed in future months. For this wouldn't there be data for January? In my example below this is true, but hopefully this example is enough for you to tweak to get what you're needing. If a ticket that's opened for say three months is only counted two months excluding the first month this can be factored in easily enough. Just DECLARE @lkDt TABLE (FullDate DATE); INSERT INTO @lkDt (FullDate) VALUES ('2014-01-01'),('2014-02-01'),('2014-03-01'),('2014-04-01'); DECLARE @lkTopic TABLE (Topic NVARCHAR(25)); INSERT INTO @lkTopic (Topic) VALUES ('Email'),('Browser'); DECLARE @tbl TABLE (Ticket INTEGER, [Open] DATE, [Close] DATE, Topic NVARCHAR(10)); INSERT INTO @tbl (ticket, [open],[close],topic) VALUES (1,'2014-01-01','2014-01-02','Email'), (2,'2014-01-01','2014-01-02','Email'), (3,'2014-01-01','2014-01-02','Browser'), (4,'2014-01-01','2014-03-03','Email'), (5,'2014-02-01','2014-03-01','Browser'), (6,'2014-02-15','2014-05-15','Browser'), (7,'2014-02-26',null,'Email'), (8,'2014-03-01','2014-03-02','Email'), (9,'2014-03-15','2014-03-16','Email'), (10,'2014-03-29','2014-04-01','Email') SELECT Dt.FullDate AS [Year-Month], Tp.Topic AS [Source], COUNT(1) AS [Count] FROM @lkDt Dt CROSS APPLY @lkTopic Tp LEFT OUTER JOIN @tbl Tk ON Dt.FullDate BETWEEN Tk.[Open] AND COALESCE(Tk.[Close],'9999-01-01') AND (YEAR(Tk.[Open]) &lt;&gt; YEAR(Tk.[Close]) OR MONTH(Tk.[Open]) &lt;&gt; MONTH(Tk.[Close])) GROUP BY Dt.FullDate, Tp.Topic ORDER BY Dt.FullDate, Source Desc Results Year-Month Source Count 2014-01-01 Email 1 2014-01-01 Browser 1 2014-02-01 Email 2 2014-02-01 Browser 2 2014-03-01 Email 3 2014-03-01 Browser 3 2014-04-01 Email 2 2014-04-01 Browser 2 
Think this will get you started: DECLARE @t TABLE ( [TicketID] INT NOT NULL IDENTITY(1,1) PRIMARY KEY CLUSTERED ,[DateOpen] DATETIME NOT NULL DEFAULT(GETUTCDATE()) ,[DateClosed] DATETIME NULL ,[Topic] NVARCHAR(450) NOT NULL ); INSERT INTO @t([DateOpen],[DateClosed],[Topic]) VALUES (CONVERT(DATETIME,'20140101',112),CONVERT(DATETIME,'20140102',112),'Email') ,(CONVERT(DATETIME,'20140101',112),CONVERT(DATETIME,'20140102',112),'Email') ,(CONVERT(DATETIME,'20140101',112),CONVERT(DATETIME,'20140102',112),'Browser') ,(CONVERT(DATETIME,'20140101',112),CONVERT(DATETIME,'20140303',112),'Email') ,(CONVERT(DATETIME,'20140201',112),CONVERT(DATETIME,'20140301',112),'Browser') ,(CONVERT(DATETIME,'20140215',112),CONVERT(DATETIME,'20140515',112),'Browser') ,(CONVERT(DATETIME,'20140226',112),NULL,'Email') ,(CONVERT(DATETIME,'20140301',112),CONVERT(DATETIME,'20140302',112),'Email') ,(CONVERT(DATETIME,'20140315',112),CONVERT(DATETIME,'20140316',112),'Email') ,(CONVERT(DATETIME,'20140329',112),CONVERT(DATETIME,'20140401',112),'Email') ,(CONVERT(DATETIME,'20140331',112),NULL,'Email'); WITH [closedSameMonth] ([TypeOfTicket],[TicketID],[MonthsOpen]) AS ( SELECT 'closedSameMonth',[o].[TicketID],CONVERT(INT,NULL) FROM @t [o] WHERE [o].[DateOpen] IS NOT NULL AND [o].[DateClosed] IS NOT NULL AND [o].[DateOpen]&lt;=[o].[DateClosed] AND ( DATEPART(YEAR,[o].[DateOpen])=DATEPART(YEAR,[o].[DateClosed]) AND DATEPART(MONTH,[o].[DateOpen])=DATEPART(MONTH,[o].[DateClosed]) ) ) ,[opened] ([TypeOfTicket],[TicketID],[MonthsOpen]) AS ( SELECT CASE WHEN [o].[DateClosed] IS NULL THEN 'openedAndNotClosed' ELSE 'openedAndClosedLate' END,[o].[TicketID],DATEDIFF(MONTH,[o].[DateOpen],COALESCE([o].[DateClosed],GETUTCDATE())) -- NUMBER OF MONTHS SINCE OPENED FROM @t [o] WHERE [o].[TicketID] NOT IN (SELECT [csm].[TicketID] FROM [closedSameMonth] [csm]) AND [o].[DateOpen] IS NOT NULL AND ( ( [o].[DateClosed] IS NOT NULL AND [o].[DateOpen]&lt;=[o].[DateClosed] AND (NOT ( DATEPART(YEAR,[o].[DateOpen])=DATEPART(YEAR,[o].[DateClosed]) AND DATEPART(MONTH,[o].[DateOpen])=DATEPART(MONTH,[o].[DateClosed]) )) ) OR ( [o].[DateClosed] IS NULL ) ) ) ,[joined] ([TypeOfTicket],[TicketID],[MonthsOpen]) AS ( SELECT [o].[TypeOfTicket],[o].[TicketID],[o].[MonthsOpen] FROM [closedSameMonth] [o] UNION ALL SELECT [o].[TypeOfTicket],[o].[TicketID],[o].[MonthsOpen] FROM [opened] [o] ) SELECT [o].[TypeOfTicket],[TotalOpenMonths]=SUM([o].[MonthsOpen]) FROM [joined] [o] WHERE [o].[MonthsOpen] IS NOT NULL -- PREVENT GROUP BY [o].[TypeOfTicket] ORDER BY [o].[TypeOfTicket]; edit: slightly clean code using COALESCE
You'll have a better time with putting the rn=1 row into another temp table. Especially if the number of rows that are NOT rn=1 is very large. IF ( OBJECT_ID('tempdb..#TEMPTABLE') IS NOT NULL ) BEGIN;DROP TABLE #TEMPTABLE; CREATE TABLE #TEMPTABLE ( [ItemId] INT NOT NULL PRIMARY KEY CLUSTERED ,[TotalCharges] MONEY NULL ,[HeaderDate] DATETIME NULL ); WITH [base] ([ItemId],[TotalCharges],[HeaderDate],[rn] AS ( SELECT D.[ItemId] ,H.[TotalCharges] ,H.[HeaderDate] ,ROW_NUMBER() OVER (PARTITION BY D.ItemId ORDER BY H.HeaderDate DESC, H.HeaderId DESC) FROM dbo.InvoiceHeaders H INNER JOIN dbo.InvoiceDetails D ON D.HeaderId=H.HeaderId ) INSERT INTO #TEMPTABLE ([ItemId],[TotalCharges],[HeaderDate]) SELECT [ItemId],[TotalCharges],[HeaderDate] FROM [base] [o] WHERE [o].[rn]=1 ORDER BY [o].[ItemId]; -- COL IS PK IN TEMP TABLE UPDATE [t] SET [t].[LastAmount]=COALESCE([t].TotalCharges,0.0000),[t].[LastDate]=[tt].[HeaderDate] FROM #TMP [t] INNER JOIN #TEMPTABLE [tt] ON [t].[ItemId]=[tt].[ItemId];
So an example correct result would be something like * 111| 79 * 222| 12 And what I have been getting is * 111| 23 * 111| 56 * 222| 8 * 222| 4
There is always the ISNUMERIC function that you can use. Add a where clause like WHERE ISNUMERIC (Action_Day) = 1
Assuming this is SQL: WITH cte AS ( SELECT table_b.data FROM table_b INNER JOIN table_c ON table_b.key = table_c.key) SELECT table_a.data, cte.data FROM table_a LEFT JOIN cte ON table_a.key = cte.key
Add `table_b.key` to your inner `SELECT`.
What's the result you're getting, just no rows? If so, comment out the `FROM` clause from the `LEFT JOIN` onwards. If you still get no rows, the problem is with `table_a`. You don't happen to have a `WHERE` clause you're not including in your example by any chance, do you?
Thank you sir, I have actually figured this out in my free time due to another one of you fine peoples help. I appreciate it.
 select * from part_cost c, part_info i where decode(c.part_cost, 'SCREWDRIVER','PHILLIPS_SD') = i.PART_NAME
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**SQL**](https://en.wikipedia.org/wiki/SQL): [](#sfw) --- &gt; &gt;__SQL__ (^i/ˈɛs kjuː ˈɛl/, or ^i/ˈsiːkwəl/; __Structured Query Language__ ) is a [special-purpose programming language](https://en.wikipedia.org/wiki/Special-purpose_programming_language) designed for managing data held in a [relational database management system](https://en.wikipedia.org/wiki/Relational_database_management_system) (RDBMS), or for stream processing in a [relational data stream management system](https://en.wikipedia.org/wiki/Relational_data_stream_management_system) (RDSMS). &gt;Originally based upon [relational algebra](https://en.wikipedia.org/wiki/Relational_algebra) and [tuple relational calculus](https://en.wikipedia.org/wiki/Tuple_relational_calculus), SQL consists of a [data definition language](https://en.wikipedia.org/wiki/Data_definition_language) and a [data manipulation language](https://en.wikipedia.org/wiki/Data_manipulation_language). The scope of SQL includes data insert, query, update and delete, [schema](https://en.wikipedia.org/wiki/Database_schema) creation and modification, and data access control. Although SQL is often described as, and to a great extent is, a [declarative language](https://en.wikipedia.org/wiki/Declarative_programming) ([4GL](https://en.wikipedia.org/wiki/4GL)), it also includes [procedural](https://en.wikipedia.org/wiki/Procedural_programming) elements. &gt;SQL was one of the first commercial languages for [Edgar F. Codd](https://en.wikipedia.org/wiki/Edgar_F._Codd)'s [relational model](https://en.wikipedia.org/wiki/Relational_model), as described in his influential 1970 paper, "A Relational Model of Data for Large Shared Data Banks." Despite not entirely adhering to [the relational model as described by Codd](https://en.wikipedia.org/wiki/Codd%27s_12_rules), it became the most widely used database language. &gt; --- ^Interesting: [^SQL ^Server ^Compact](https://en.wikipedia.org/wiki/SQL_Server_Compact) ^| [^Microsoft ^SQL ^Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) ^| [^Data ^definition ^language](https://en.wikipedia.org/wiki/Data_definition_language) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm7enij) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm7enij)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Use an inner join. 
You are probably in text mode. Switch to results in grid.
Whoa definitely trying this approach when I get back to work tomorrow! Makes sense with the join. Thank you very much!
Hmm, my coworker uses text mode and when he runs his query, the results stay at the top while the rest loads. Someone had set it up for him years ago though, so he doesn't know how to change my settings.
Results tab and there should be a way with text mode. Hmm, I'll post a screenshot tomorrow.
The website i want to make lets users make pages for different products from a template and allows users to also search for products with tags and genres. ie Minecraft is a game so it is in the games genre, but its a sandbox game, FPS and survival game so i want people to be able to search it up just by those tags. On top of that i want users to be able to create the pages for these products aswell. i hope this helps. :)
You might want to look into web frameworks such as Django or Ruby on Rails that might help with modeling and presenting the data.
I bought the book ["Sams Teach Yourself SQL in 10 Minutes"](http://amazon.rkstar.com/0672336073), where each chapter teaches a new idea and should take about 10 minutes to read. I found it pretty handy and informative.
Thank you, do you know any tutorials that could help?
CJ Date's Introduction to RDBMS. This is the book which taught me about the logics behind those SQL sentences.
You may want to hit http://www.w3schools.com/ 
I think if you want to go into the reasoning and motivation then you should ask "why relational database" before you ask "why SQL".
Thanks, exactly what I was looking for! I'm curious why my nested join expression didn't work since this article seems to state it does, but I'll look into it further myself.
I do; but, before you go off and attempt to learn a new framework, you should first do your own research to (a) figure out if a framework will help solve your problem, (b) decide which is best for you.
Thanks for the prompt reply. I don't think I was clear enough. The EOMonth thing, I can do. It's the creation of the subsequent records themselves. It's the insert clause I'm trying to figure out. There might be only 1 physical record (the initial sale) but I want to create 2 additional records on the fly to fill in the gap in time. Edit for clarity: Presume I already have the 1st record.
I don't really mean to be a dick but I'm betting you have no responses because your run on and/or unpunctuated sentence is hard to follow and your question is not at all clear. You need to give more info
I don't think that was a run on?... Anyways, more info after lunch
"How exactly would I go about doing this properly? I suppose I could just do a subquery for table_b joined on table_c, but I'm wondering if there's a simpler way." It is the right way. If there's no need to have anything from the Table_C, you can express exactly what you look for with 'exists': select a.data, b.data from a left join b on b.a_key = a.key where (b.key is null or b.c_key in (select c.key from c where c.key is not null) ) 
Would you guys surgest fuelPHP? Heck, I don't even know what I'm looking for in a framwork.
You could use a CTE to dynamically create your list of dates, but I would suggest creating a calendar table. I have an example table at http://www.reddit.com/r/SQL/comments/2l5brf/ms_sql_date_grouping_query_performance_and/ Add columns, as needed.
I'm unsure if this is possible even in a CTE. The only way I know of to create a date table is to loop through the dates, and such a loop isn't possible within a CTE as far as I know. If using table variable is okay, like my example, this could be done with a loop to populate it, but to do this solely in a single Select is I think not possible.
The problem has to do with Peoplesoft's HRMS creating job combo codes. the client has too many of them and needs to only have active codes (they have like 14000 and only need 2500.) Is there a way to delete these codes from within Peoplesoft in a batch automatically? Or would I have to find and delete them 1-by-1
That's about as efficient as you're going to get from a performance standpoint. Would it help to set default values for these columns and disallow nulls or is there some meaning to the nulls? Edit: `WHERE ColumnName IS NULL` This eliminates the function from the predicate and makes it sargable.
I generally use COALESCE instead of ISNULL, but either way works the same. SQL Magazine has a great write-up about COALESCE vs ISNULL that may be worth reading: http://sqlmag.com/t-sql/coalesce-vs-isnull But I'd say the way you have it is fine. 
Ah so you are suggesting I might see a bit of a boost by doing this instead? UPDATE dbo.SomeTable SET ColumnName = 'N/A' WHERE COALESCE(ColumnName, '') = '' You may be right, but I was looking for a completely different way to do it. It would be a lot of effort to update all of our code to do COALESCE instead of ISNULL for only a potential for a very minor improvement. Thanks for the suggestion though!
We have A LOT of NULL data, which saves a lot of storage space. Replacing NULL with blanks would actually be pretty expensive as far as storage is concerned. It is a good idea though, I will look into it. No, NULL does not have any specific meaning (most of the time). I am not really seeing this as a major problem for us, I was just curious if there was some cool way to do this that I was not aware of. 
No problem, have fun. 
Normalisation is about breaking the table down to multiple tables to reduce repetition of data. In your example it may be safe to assume model AR34S is always going to be a black Lenovo with with 4Gb / 512Gb / 1080p / 14" / i5 / 200-watt HR / Bluetooth. Note it is not safe to assume it is always $1200 because order 3256 has it for $1030. So, I would make a new table that separates out the model specifications from the order table. The order table would still store the model number, quantity, price, order number, order date and est delivery date but would not have the CPU and RAM etc specs any longer. That's the idea behind normalisation. As to the specific normalisation forms try the top answer on this StackOverflow question -&gt; http://stackoverflow.com/questions/723998/what-are-1nf-2nf-and-3nf-in-database-design Note that the table you supplied already has a primary key and no duplicate rows so part of the task has already been don for you.
Is it a lot of data? That's the quickest way of getting it done, but if you're worried about locking, you can use a while loop to handle a batch of them at a time. As long as one NULL exists, load a temp table with top (n), update table on matched IDs, repeat. Edit: just noticed you said you have may have a lot of tables across multiple applications. You can get table names and column names from the system tables and use dynamic sql to update any that are of varchar. Might it be better to coalesce reports/user visible fields?
Does anyone actually Google anymore, or is it now the antichrist?
Just keep writing more `insert`s into `ShipPort` with the values you require on the table. It's not clear what the purpose of two ports are - a ship can only be in one port at any given time, and your image doesn't describe what the second port means. Don't just arbitrarily add fields to `ShipPort` to cram something in to fit - understand the relationship and only add the field if needed.
Normalization is based on functional dependencies. What are the functional dependencies?
You would run a SQL script in the DBMS to delete the values in the database, determined by the criteria that you deem them useless. Assuming they have not been used in anyones records, thus deleting relationships for historical data relationships. 
It can be done... create a dynamic table and use cartesian join where dates are greater than first sales date and less than todays date. This is a bastardisation of MS ACCESS code and MS SQL code to show you how it can be done.. it will not work as is in either...just an example of HOW TO. SELECT dateadd("m",id, DateAdd("d",Dateserial(2000,1,1),-Day(Dateserial(2000,1,1)))) AS SalesDt FROM (SELECT row_number() OVER (ORDER BY PrmKey) AS id, col FROM table); &lt;== generate count using row_num to add those months. where dateadd("m",id, DateAdd("d",Dateserial(2000,1,1),-Day(Dateserial(2000,1,1)))) &lt; now() &lt;==limit to todays date order by 1,2 I have been trying to put together the whole query example for you, its doing my head in, so long since I did it for a similar Oracle report some years ago. I suspect you will need a UNION to get the first sales date for the client... I will post when it comes to me... 
Not sure of the purpose for the data, but two things spring to mind. First, don't you need to record the sequence in which Ships enter Ports? So that you can deduce what route the ship took? This would need yet another table called something like TRIP as a parent of ShipPort - and add a field Sequence_ind to ShipPort. Secondly, you may add a field EventId to the ShipPort table, since apparently at every shipport combination just one event takes place. In other words, does the assignment say anything about recording the process at all? If it does, you may want to facilitate this by both additions. 
OK, this is it. SQL for MSACCESS with a smattering of MS SQL for using the row_number(). To mimic row number for the query, I created a table with a single column with numbers 1 thru 100. - so it will not work correctly in either DBMS, it needs to be tweaked to work in either. The first query is for the clients first sale. The second query uses a cartesian join on the dynamic date data set to return all dates for the last day of the months upto today, form the first sale. The last day of the month if calculated by taking todays day count -1 from the current calculated month. You can change how far back the date calendar goes by editing the Dateserial value. Currently set at 01/01/2000. There is not reason that value could not be determine by a min(date) sub query !. The sub query to create the date data set uses any old table with at least as many records as months you need to cover for the whole time period. Whack this in a view and query via clientID. Select clientID, min(date) as Date from tblSales group by clientID union select clientID, Date from (select client, Date from tblSales s1, where s1.Date = (select min(s2.Date) from tblSales s2 where s2.clientid = s.clientid) ) t2 , (SELECT dateadd("m",MONTHCOUNT, DateAdd("d",Dateserial(2000,1,1),-Day(Dateserial(2000,1,1)))) AS SalesDt &lt;==get last day of last month FROM (SELECT row_number() OVER (ORDER BY PrmKey) AS MONTHCOUNT, col FROM table) &lt;== generate dynamic date range data set ) t3 where t3.SalesDt &lt; now() &lt;==restrict date to less than today order by 1,2 desc
What is your database? Or are we just doing your homework for you? If you don't have a helper function, you need to manually calculate the current day of the week then make a CASE statement to subtract the appropriate number of days back to Sun/Mon.
Create a join table with this information. Also, if you haven't already, create a join table for other useful dates you wrongly use functions to generate (such as weekdays, holidays, every single day for the next 10-20 years, etc)
They don't need to HAVE only active codes. They only want to SEE active codes. A more useful solution would be to add a boolean column/field to determine if the code is active/closed and then adjust all your reports to only collect "active" data. This way, you don't lose historical records. You should default them all to 'closed' and make your users go in and open the codes they need (or they need to provide a list of open codes). They will also need a work process to close codes in the future.
I like the way you think stranger
Use an OR. The use of the function here may prevent SQL Server from using any index present. If it's a small table, you won't notice the difference. If it's a big table, you will... If this is one off, who cares?
&gt; A LOT of NULL data, which saves a lot of storage space You may want to double-check that - depending upon your version, there may be little to no savings unless certain conditions are met.
Yeah, setting a default value as blank would probably be the best solution. But if you want to keep nulls, about the only thing I'd do different is "WHERE ColumnName IS NULL", using the ISNULL function instead uses a *TINY* bit more power comparatively and is smaller in syntax. Usually if you want to keep nulls I just do something like "SELECT ISNULL(ColumnName, '') AS ColumnName" in my selects and let them be.
What am I missing? Each week has ONLY one of each day so why are you trying to identify the first WhateverDay in a week?
Loads of new date functions were added in SQL 2012, EOMONTH being one of them. http://www.mssqltips.com/sqlservertip/2588/new-date-and-time-functions-in-sql-server-2012/ 
Dont mind those spaces, they don't exist.
Add c.DEVICE_LOCATION_NAME to your Group BY. Also c.DEVICE_LOCATION_ID is in your Group By but not in your Select. You may want to remove this too.
Some of these fields are spatial data types, and we are on SQL 2012 and are utilizing compression, so it does make a big difference.
thanks- another good idea
It happens to all of us :-D Cheers on finding the bug though.
I think you need to change c.part_cost for c.part_name in 1st parameter and add 4th parameter as c.part_name, because in other cases it will return null value.
Do you need every record from ACCESS-user table? If so, start your 'from' with that table and left join other tables. You can technically do with right joined tables but it is always more confusing, IMO.
I'll give this a shot, thanks.
 select b.USER_ID, a.FIRST_NAME, a.LAST_NAME, CAST(LOG_TIME as DATE) as TimeIn, c.DEVICE_ID, c.DEVICE_LOCATION_NAME, COUNT(*) as [counter], case when count(*)%2=0 then 'not here' else 'here' end as [Here_status] from ACCESS_USER as a left join USER_TIME_LOG as b on a.USER_Id=b.USER_ID left join DEVICE as c on c.device_id=b.DEVICE_ID where b.log_time between '11/21/2014' and '11/22/2014' group by b.user_id, a.FIRST_NAME, a.LAST_NAME, CAST(log_time as DATE), c.DEVICE_ID, c.DEVICE_LOCATION_NAME So I did that, and got the same results. Many Anger. Such Frustrate.
&gt; Does anyone know how to accomplish this task in ansi sql? you really want ANSI SQL? for starters, you have to declare your own function to determine the day of week the following is taken from chapter 29 of Joe Celko's *SQL for Smarties* -- CREATE FUNCTION Zeller (IN z_year INTEGER, IN z_month INTEGER, IN z_day INTEGER) RETURNS INTEGER LANGUAGE SQL DETERMINISTIC BEGIN DECLARE m INTEGER; DECLARE d INTEGER; DECLARE y INTEGER; SET y = z_year; SET m = z_month - 2; IF (m &lt;= 0) THEN SET m = m + 12; SET y = y - 1; END IF; RETURN (MOD((z_day + (13 * m - 1)/5 + 5 * MOD(y, 100)/4 - 7 * y/400), 7) + 1); END; are you sure you want ANSI SQL?
~~Did you try joining using ISNULL? That way if the childId is not there, use the motherId.~~ I took a second look and understand it better now I think. Try using UNION, like so: SELECT * FROM Referrals R JOIN Client C on R.Client_Id = C.ChildMotherId UNION SELECT * FROM Referrals R JOIN Client C on R.Client_Id = C.ChildId
It's hard to know without more information about your schema and data, but does this work for you? SELECT DISTINCT r.Client_ID ,ISNULL(cm.ChildMotherID,c.ChildID) AS HouseholdID FROM Referrals r LEFT JOIN Client cm ON r.Client_ID = cm.ChildMotherID LEFT JOIN Client c ON r.Client_ID = c.ChildID
Would like to know in particular what you would like to know more about?
Well I guess that's why I am so stumped, I do not know where to begin. So I understand it is database software so I guess the most practical way for me to start would be to create a database to contain data I am used to handling(event log?) and then trying to manipulate that data. Then maybe also querying an SCCM database? Sorry if I seem all over the place but I do not even know if I am asking the right questions. 
You would only need to bother with a transaction if you needed to execute a series of commands that either needed to all be "committed" successfully as a whole or be "rolled back". If you have a web form that updates two tables when posted, you might want to ensure that either BOTH tables were successfully updated or NEITHER were updated. In that case, it would be wise to begin a transaction beforehand so you can rollback the transaction should something go wrong (like you can't update one of the tables). 
Everything - even trivial queries have transactions. Transactions can be explicit or [implicit](http://technet.microsoft.com/en-us/library/ms188317\(v=SQL.105\).aspx), so you don't need to always state a "begin transaction" or a "commit" - it will just do it for you automatically. Even if you aren't changing anything, you still have to (implicitly, usually) tell the database - don't drop the database - I'm using it. So at a basic level, the transaction tells the database how long to keep a shared database lock. And so on to more granular locks (don't change the data while I'm reading it) etc. So transactions are always needed in a relational (acid) database. Do you need them in web dev? Maybe. Web transactions could be anything, from a balance transfer from your bank, an order from Amazon, to a Facebook profile update. Some of those need relational transaction support. Some don't. Is it imperative that your Facebook update is committed to disk? There are those who disagree, but it doesn't need full ACID. So Facebook doesn't use an ACID database to store that data. Long answer, but it depends on what kind of web app you are working on. Concurrency? Same thing. Higher ISO levels can get you there, but so can other techniques. Many much simpler. Higher ISO levels will ensure consistency, but offer at the cost of concurrency. 
When you write "the relationship structural elements-book is many-one", do you mean to write "the relationship structual elements-**problems** is many-one"?
Oh, no, that was a brainfart. The relationship is actually many-many, since every book has many structural elements, and every structural element might belong to many books (e.g. "Chapter 1. (Introductory Foo)", which is a structural element, belongs to both the 1st and 2nd edition of "Basic Foo".) Thanks for pointing that out. I guess I could create a junction table that associates books with structural elements, but I'd need to make sure that only structural elements in the same tree are mapped to the same book, and there is still the problem of how to associate problems to both structural elements and books. 
&gt;* Many books might share the same structure, for example the 1st and 2nd edition of "Basic Foo". I don't want to duplicate data for books with the same structure. This is a mistake that will make things harder. Just because the data looks the same does not make it same in terms of your model. As soon as you ditch this idea all your problems go away and your schema becomes obvious. 
What you are calling a mistake seems to be ordinary normalization? When the database will contain multiple editions and reprints of the same text with identical structures, much of the data is in fact the same. As an extreme example, I have a set of literally dozens of reprints of the same exact text, with identical structures but some minor changes to the problems. I am not sure I see why I have to denormalize? 
Good, your question makes more sense now. How about an approach like this? Create a many-many join table `books_structures`, then use that table's id in a many-many join table `books_structures_problems`. Thus a problem is connected to a (book, structure) pair, rather than connected directly to a book and/or a structure.
Any columns from left joined tables could be null in the result set of a left join. Either use isnull( column, default) or do (column is null or column ....) for those. where (b.log_time is null or b.log_time between '11/21/2014' and '11/22/2014') and
&gt; The website i want to make lets users make pages for different products from a template and allows users to also search for products with tags and genres. ie Minecraft is a game so it is in the games genre, but its a sandbox game, FPS and survival game so i want people to be able to search it up just by those tags. On top of that i want users to be able to create the pages for these products aswell. If your technical knowledge is possible to judge accurately from this post, you are literally years away from being equipped to do a project of that sort. Don't bite off more than you can chew. There are many sides to being able to implement something like that. If you want to do it all by yourself: * You need to program the user front-end. This includes learning html and css in order to format content, and possibly tools for client-side scripting like JavaScript/libraries like jQuery. * You need to program the server backend: This means that you need to learn a server-side scripting language like php, python/django, ruby on rails or asp.net. As part of this you might have to learn SQL and database design. * You need to be able to administer the web backend. This means that you need to learn how to configure and administrate a web server, a DBMS, maybe cache technology like varnish, firewall configuration, backup automation, etc. Do one thing at a time. Try, for example, to start out by learning html and css. You'll find tons of tutorials online. 
Let alone pay attention in the class giving him the information.
OP read a butt-hurt article about not using CASE statements because they can be inefficient and smart people should do the work to make pure boolean logic. Honestly, I do whatever makes sense at the time. If they ask me to add a filter on a column that's really a CASE statement, it's getting copy/pasted into the WHERE clause. 
&gt; it was talking about tools "ontop of SQL" - which I understood it to be like additional applications that you get AFTER you get sql server I have no idea what you're talking about.
&gt;My company uses Microsoft SQL for a lot of things and I really hate when a user asks me a basic question and I can't answer them Where are the people who *do* know the database? The DBA, the developers...can you redirect users to the appropriate people to answer the questions?
WHERE ([**STUDENTS**.StudentID] = @StudentID)
To describe why OP is getting this error: more than one table has the column name 'StudentID' so when you are choosing column 'StudentID' the query doesnt know which table you are trying to reference.
&gt; putting "Students." in front of StudentID in the WHERE clause just causes another set of errors. Students not being in front of StudentID is the source of this error. So that's the first problem to fix. *Then* what are the other errors that appear when you do that? I'm not seeing anything integral to the SQL that's obviously wrong so we may need further background to figure out your other errors. 
A many to many table connecting problems to their location in various editions of the book. So, you have a table, let's call it Inclusions (lousy name) with only these elements: Edition: a reference to a book, or edition of a book Location: a reference to where in the structure of this edition this problem is included Problem: a reference to the problem The Book table and the Structure table contain no references to Problems and the Problem table contains no references to books or structure. Keep the edges of your digraph away from your nodes.
/u/dpenton makes a very good point, if you tried putting the table name inside the square brackets you'd have made things really ugly and could cause your other errors. Unless I actually need them (i.e. a table name with a space in it) I get rid of the square brackets so I would end up formatting your query something like SELECT CourseName, Grade FROM STUDENTS JOIN GRADES ON STUDENTS.StudentID = GRADES.StudentID JOIN COURSES ON GRADES.CourseID = COURSES.CourseID WHERE ( STUDENTS.StudentID = @StudentID) ORDER BY CourseName;
I agree, case is easier. would be nice if microsoft came up with an easier syntax for pivot and a way to do it dynamically without too much effort
Meh.. when I am maintaining someone else's code and I see things I like that I automatically add a character in front of it to make it *not* a reserved word. Using a reserved word in that manner is just plain bad coding practice if anyone else has to maintain your code. The software may recognize you're using a reserved word in that manner but the person maintaining your code later is very likely to totally miss that you're doing that and spend an unnecessarily long time debugging problems until they make wholesale changes to make your identifiers *not* reserved words.
Always alias your table names. And then use the alias. SELECT c.CourseName, g.Grade FROM STUDENTS s INNER JOIN GRADES g ON s.StudentID = g.StudentID INNER JOIN COURSES c ON g.CourseID = c.CourseID WHERE (s.[StudentID] = @StudentID) ORDER BY c.CourseName; Some guesses on the alias uses, but should point in a vaguely correct direction.
For speed, though: WHERE [ColumnName] IS NOT NULL AND [ColumnName]!='' Assuming [ColumnName] is a NULLable column and stupid people use blank ('') as a default or say (and I've actually heard this) "I don't like NULL". Uggh. Note that SQL Server considers (for most Collations) the string '' (blank) and SPACE(x) [where x BETWEEN 1 AND 8000 (read the BOL)]to be the same. SELECT CASE WHEN NULL='' THEN 1 ELSE 0 END ,CASE WHEN ''='' THEN 1 ELSE 0 END ,CASE WHEN SPACE(1)='' THEN 1 ELSE 0 END ,CASE WHEN SPACE(8000)='' THEN 1 ELSE 0 END
ACK! NO! NO! NO! NO! NO! NO! NO! NO! NO! Never edit the bloody system databases, even if you can. Bad DBA! Bad! Go sit in the corner and use MS Access for an hour to think about what you did. DROP DATABASE is the only way you should do this. Read other ppl's entries. /u/samalex01 has a good script.
And they didn't include NULL/NOT for any columns. Or primary keys. Or foreign keys. Nope.
Sometimes. Not always. That's the key. But, to each their own. :)
Select table1.product, table1.quantity as table1qty, 0 as table2qty From table1 Union all Select table2.product, 0 as table1qty, table2.quantity as table2qty From table2 (excuse formatting, on my mobile) 
Just a general comment unrelated to the actual solution for this specific question: What you _really_ want to do here is normalize your table structure a bit. Not sure what the purpose of the two identically structured product tables is, but I think you'd be better off pulling the products themselves out to a separate lookup table. A structure like the following might suit you better: tProduct productId productName tProductQuantity1 productId quantity tProductQuantity2 productId quantity Such that your tProduct table contains the complete universe of all products and your tProductQuantity* tables contain whatever data they have in them. This way you'll be selecting from tProduct and LEFT OUTER JOINing the other tables in and remaining sure you're queries are returning the full universe of products. The other suggestions to use a UNION ALL would also work, but there's plenty of inherent benefits to a more normalized structure. SELECT tP.productName, COALESCE(t1.quantity, 0) t1_ct, COALESCE(t2.quantity, 0) t2_ct FROM tProduct tP LEFT OUTER JOIN tProductQuantity1 t1 ON t1.productId = tP.productId LEFT OUTER JOIN tProductQuantity2 t2 ON t2.productId = tP.productId ORDER BY tP.productName, t1_ct, t2_ct ; Just my two cents.
thanks for the response. I'm watching it now!
thank you, I'll try that today along with the other 3 responses. None of that will modify any of the original tables right?
This is something I recently had to look up so it's fresh in my mind. Basically both are pretty much the same thing, as long as the columns you are working with are not nullable. If they are nullable I do know that the general feeling on it is to use not exists because of the way each of the statements handle null values. Sorry that it's not a more detailed answer, but hope this helps!
 [column] NOT IN ( [Select or comma separated list]) Compares the column value of that row to a list of things. Have to be returning something that matches that column's value. NOT EXISTS ( [Select]) lets you compare a number of columns in the where clause and upon finding the first row in the select that matches stops retrieving that select. You're not actually doing anything with the results of the select so most frequently it's in the format NOT EXISTS ( SELECT 1 FROM [remainder of select and where clause] ) I have also frequently seen NOT EXISTS ( SELECT 'x' FROM [remainder of select and where clause] ) It's also a matter of clarity. you can say.. SELECT [a whole bunch of stuff] FROM Table_One WHERE Table_One.Table_One_id NOT IN ( SELECT ASTS.Table_One_ID FROM Already_Showed_this_stuff ASTS WHERE ASTS.Table_One_ID = Table_One.Table_One_id ) OR you could do it as: SELECT [a whole bunch of stuff] FROM Table_One WHERE NOT EXISTS ( SELECT 1 FROM Already_Showed_this_stuff ASTS WHERE ASTS.Table_One_ID = Table_One.Table_One_id ) I think the second version makes it clear that you're only selecting rows from Table_One that aren't in the table Already_Showed_this_Stuff and the first version is just something of a puzzle. Both would work, if Table_One_ID is an indexed field, both would be equally quick, but the second will make more sense when you, or someone else, comes to maintain this bit of SQL later. edit: typo
Access is a great entry point for a small business looking for a database structure to manage their data more efficiently... But, SQL and Access is a very loose partnership. Get a stack (i.e XAMPP) and experiment, that would be my suggestion. Incorporate WordPress into the stack, along with MySQL, is a powerful tool for any SME.
Don't stress the database portion as much - you'll still have a whole website to build and host. I reccomend you start looking at a webhosting option first; the reason why I jump there is becuase they will provide you with the database infrastructure you need. You can then build/purchase/borrow/modify whatever you need for the DB portion to build the site. Webhosts also provide you with those tools, so your not doing (as much) work to build each component from scratch - they are practically drop in and run modules. Consider what you want to do. *What you REALLY want to do.* Short of a run down on project mangement: if all you're looking for is store info a list of what's for sale, great. If you want an eCommerce portion as well, also great, but takes more work (time) and a commitement from the friend to look at his email, printer, or whatever will output the order queue. I don't work for them but I'd reccomend HostGator which uses MySQL: they are cheap, good feature to price tradeoff, easy to work with, and usually have a very helpful support staff. You can research it or ask a hundred people and recieve two hundreded asnwers, but there is one, and you can just get going on it. tl;dr: don't think about just the DB, get a webhost and those options available that you are looking for edit: wordspeak
They produce the same result, but in a different manner. NOT IN compares a value in a query result NOT EXISTS simply looks for any return value from the subquery. This requires less computing for the SQL engine, and is quicker.
Agreed and it gives you practice of configuring a server properly ever 180 days. 
Yes SQL Server 2012 Express is free. http://www.microsoft.com/en-us/download/details.aspx?id=29062
&gt; the date ranges will never overlap. The date ranges you gave us overlap on 1/26. create table projects ( project_id char(1) primary key, start_date date not null, end_date date not null, check (start_date &lt; end_date) ); insert into projects values ('A', '2014-01-01', '2014-01-26'); -- Changed your start date for 'B', so it doesn't overlap 'A'. insert into projects values ('B', '2014-01-27', '2014-02-09'); create table costs ( cost_id integer primary key, cost_date date not null, cost_amt numeric (14, 2) not null, check (cost_amt &gt; 0) ); insert into costs values (1, '2013-06-06', 5.00), -- Out of range (2, '2014-01-01', 3.00), -- Proj A (3, '2014-01-26', 2.50), -- Proj A (4, '2014-02-01', 3.95); -- Proj B select p.project_id, p.start_date, p.end_date, c.cost_id, c.cost_date, c.cost_amt from projects p inner join costs c on c.cost_date between p.start_date and p.end_date order by project_id, cost_date; This is standard SQL. For MySQL, delete the CHECK constraints. There's no good way to enforce the business rule that every cost be assignable to a project. For example, your third project might start on 2/15, and there might be costs on 2/10. But there's no project running on 2/10. Better table design is your only hope.
Try doing a "not in" on a set with nulls.
It's probably a number of reasons combined that make any company choose for DB platform X or Y, but I'll try to answer. &gt; What do you think it will take for Large companies to embrace sql server more and possibly make a switch from Oracle to sql server? For one, SQL Server solely runs on Windows platforms. A LargeCo may have a number of reasons to prefer Unix/Linux over Windows: available internal platform knowledge, security implications, connectivity to other application platforms, etc Second, the whole application stack has to be taken into account. It seems quite logical to develop C# applications on Windows platforms using SQL Server as DBMS back-end; just as it seems logical to combine Java applications with an Oracle back-end. What do the software developers prefer? Re Business Intelligence, if you want to get the most out of MS BI, you will have to integrate it into Sharepoint, which is not free. Third, (as you mention Cognos) if we're on the subject of BI (my field) Microsoft has quite a different view on Business Intelligence from IBM Cognos with their emphasis on Self Service BI. Microsoft doesn't cover all bases as clearly as Cognos just yet. One feature is where the Semantic Layer is defined (Physical layer/Relational layer/Business layer), as opposed to Microsoft which envisions to provide end users with (a bit more) lower level data and leave it up to them to perform analyses and gain insight (with Power Pivot/Query/View). I am by no means saying that the MS BI stack is the wrong choice (in fact, I am currently advising 2 companies to migrate to it), but it does start in generating buy-in via adopting MS's vision on BI. 
It also give you a reason to learn to use Puppet or something else to configure it for you every 180 days.
Thanks, as it relates to BI, do you think Microsoft is comfortable where they are in the market or do you see them gaining ground on IBM in the near future? 
Disclaimer : anecdotal + personal observation I've seen a tremendous level of adoption of sql server in Big Financial for really large data (hundreds of T). Is oracle really that market dominant?
I think Oracle is a great product, but it just seems very complicated. I've also been slightly turned off by some of our oracle DBAs who seem to pretend that SQL server is inferior and they pretend as if it doesn't exist. I guess I'm hoping that Microsoft regains some respect especially at my company which houses tons of transactional data. I just find SQL server to be more user friendly. 
&gt; I am currently advising 2 companies to migrate to it If I may, what kind of job do you have, is it BA? If yes, what technical skills should I acquire as a student planning to apply for an internship around spring-summer? 
This might be a job for [COALESCE](http://www.sqlteam.com/article/using-coalesce-to-build-comma-delimited-string).
interesting, thanks. but, we're not allowed to use cursors in this environment. will i need a cursor for this? 
Won't do your homework for you, but start with this: figure out the set of people who **have** been Treasurer (or currently are), then select the inverse of that. I can think of three ways to do it in a single query: * Correlated subquery * Join to a subquery * Subquery in the `WHERE` clause
Allright, thanks! Edit: Found it, thank you. 
&gt; They produce the same result, but in a different manner. this is not always true... it depends entirely on what the subqueries contain 
dear OP... please see the sidebar and identify which dbms you're using nobody works in a "pure ansi" environment because there is no such thing
cisco composite
take a look at the not exists subquery
I recently left a subsidiary of IBM that used MS SQL Server exclusively. But I know what OP is talking about. I used to work for Blue Cross and Wells Fargo. Both of their backends were embarrassingly antiquated. I also used to work for the State of Oregon and holy shit what a trainwreck. I think what will take is a refusal to bail out companies "too big to fail" that are also too greedy to upgrade. I think as more organizations outperform their competitors, either the antiquated will finally evolve or they'll fizzle. These companies spent billions on IT solutions back when billions was actually a lot of money. A lot of that old guard is still on the board of directors and they're reluctant to do a major upgrade again. Even if its crucial to streamlining costs and evolving to embrace new technologies. 
Honestly I'd look into crafting some Business Intelligence reports so he can enhance the shopping experience and boost sales for his existing customer base. Once that template is in place, THEN build a website with the goal of enhancing their experience similarly. Don't just build a website with address and hours of operation. That's silly. That's the 90's model. Build a website with a purpose. Use SQL to determine what that purpose is first. SQL = strategy website = execution Sun Tzu said that strategy without execution is the slowest path to success. Execution without strategy is the noise before defeat.
I find this interesting. I work for a relatively large company (30 billion a year in revenue) and we are migrating to SQL Server and away from Oracle. We are also moving away from Cognos and to the MS BI stack. I think Oracle is okay from the BI perspective but I much prefer SQL Server. It is much more responsive in our environment. I would love to hear some reasons as to why Cognos or TM1 is superior. I'm genuinely curious. I just feel like I can do everything I need to with MS and SQL that I could do with the Cognos GUI. 
I'm currently working for a healthcare company: i'm not trying to be a jerk or put down a product that people use, but honestly, I find Cognos to be pure garbage. Garbage for 4 reasons. 1. INSANELY EXPENSIVE. 2. Horrible resources online, need to pay for every little thing. 3. Overly complicated. 4. Just down right ugly looking. The amount of money my company probably pours into Cognos is enough to make my stomach ill. The return on that investment is well...nothing to be desired. Could we achieve the same result using microsoft BI or something cheaper. Absolutely. I guess i'm more frustrated at my company for pouring so much into this product. Yes, here is another problem with Cognos. List reports. Suppose you want a report rolled up nicely where someone can drill down through groups like in SSRS. Well, nope. You get this ugly long list that someone has to sort through in order to find what they are looking for. Anyway I don't mean to offend anyone. I just know how we use cognos and it is a waste, maybe other people use it differently and are able to leverage its strong points better, but not me. 
Seems you and I are on exactly the same page. I would like to hear the opposite point of view. 
I think Microsoft &amp; Qlikview will increase their market share over Cognos and Business Objects. At the end of the day, Cognos and BO do have quite a bit of a learning curve and it is my experience that knowledge of those tools mostly remains within the IT department, where they prepare the business reports. Business-only staff, esp Finance, is still most comfortable in Excel. Excel 2013 provides the end user with such powerful analytical power (the SSAS engine, including tabular data format, is now part of Excel) that there hardly remains a business case for Business users to learn yet another software tool. Btw, for discussion's sake I left out SAS and Microstrategy: two very decent software offerings in their own right. 
I appreciate your thoughtful response. This was where I was leaning toward. Measuring business metrics for him to understand his business better. When I asked him about what his inventory system looked like, he didn't seem too confident answering that question for me. He seems like an old school guy. I wouldn't be surprised if he does most of the stuff in his head (indeed much of what he sells has no set price and is often a negotiated price). My idea was to initially build a rudimentary database with all of his inventory and pricing information, and then monitor that over a period of time. Once you have the data from the time frame, that's powerful information that is much easier to interpret than a month to month guesstimate of how you're doing based on whether you took home some money.
Generally you would have your building table, your player table and your player_building table that you can use to link up both and add some additional details, such as quantity. If players don't share buildings ever then you could just as easily have a player_id field in the building table and populate that directly - no need for the extra relationship table. The alternative route would be to have something more dynamic in the player table, such as an array or hstore/json so you have a field that contains all the data. The difference is how you plan on gathering/using the data, and what other data needs to be found during the same queries. Do buildings have a building material or other additional attributes associated with a players specific buildings or is there just a simple relationship where a player has a building and that's that. It also depends what database you are actually using. Some of them suck pretty bad at indexing arrays so that wouldn't be a great way to go, just as one simple example. Just as a silly plug, if you want to see how my game was put together, it sounds kind of simple to what you did. Check out The Schemaverse. Site is here http://schemaverse.com or you can head right to the source https://github.com/abstrct/schemaverse 
I'm owner of a company providing independent BI advice. You may want to invest time in Microsoft's Self Service BI offering, with Power Query &amp; Power View. You can create great looking reports and analyses with tooling readily available in Excel 2013, and as a free add-in in Excel 2010. Microsoft allows you to tie internal data to openly available external data, e.g. Geographical data, enabling very nice heatmapping which will impress. If you prefer to dive into the BI data end of things, you may want to invest in getting to know how to generate ETL processes using BIML for SSIS, or examine how Pentaho's XML structure can be generated. It may not land you an internship right away, but it will be worth it. But don't take my word for it, they're just suggestions. 
Buildings will have information such as resource costs (which could be a number of different resources), population max, workers, etc etc. The table that I want to create to link to players would have both the ID for the user and the ID for the building and a third column for the quantity. From there, I'd be able to reference the ID for the building to extract any information I'd need for calculations. This is to ensure that any changes I make to buildings are made across the board. If I want to balance a building's population max increment, then I'd be able to do so and run an update script to change the player's values. I intend to create the variables (such as population max) inside the player's table and increment/decrement them as necessary so that real-time processing of caps isn't required. This should prevent a lot of long processing times. Arrays, as you say, is an option, but how does it affect processing time? Would it be faster to use a linked table to gather quantities, then use the building ID to reference the base table for building actions?
A couple of things are wrong with the diagram: It does not follow a standard - it's no crowfeet, no idefx, no uml. I cannot detect the granularity of it either.
Thank you for replying. How about this one: http://i.imgur.com/rzXCN5I.png ?
Since there are added details unique to a specific player's building, it definitely sounds like the linking table, i.e. player_buidling, would be the best way to go. Linking up a couple tables here and there won't be a problem at all - assuming they aren't completely huge. Do you usually just look up the details of one building at a time or do you do lookups and aggregate stats of multiple buildings?
I cannot think of an example where the result would differ across the two methods using the same subquery result. Perhaps you could expand on your statement with an example.
I think we're misunderstanding each other. A building "house" may support 10 population. This figure will be the same for every player. It may cost 10 wood and 150 stone to build. This will also remain the same. The player will have x number of house. Each time they build a house, their maximum population will rise by 10 (a variable stored directly in the player table) and each time one is razed, it will lower by 10. Does that make it a little clearer? So there will be a table to store the different building's static information. There could be 10 different buildings for argument's sake. The values in this table won't change. The player's quantities of these buildings will change.
This worked perfectly, thank you!
Oohhh, so a player owning a specific house doesn't matter, the detail is that they have x number of a particular type of houses. Is that a bit closer?
 SELECT person.name FROM person INNER JOIN member ON member.pid = person.pid GROUP BY person.name HAVING COUNT(*) = ( SELECT COUNT(DISTINCT groupid) FROM member ) note: your requirement to show all the tuples is kind of useless because you already know that there will be one tuple for every group... so this simply shows just the names
I'm an oracle DBA and our shop has SQL DBAs as well. No one thinks the other product is inferior. Our choices were based off of familiarity and application dependencies. I have just as much respect for MSSQL as I do for mysql and oracle. They are different products that can serve different purposes. We went from crystal reports on a MS back end to OBIEE because we migrated to ebiz and siebel. Sometimes the change is a chain reaction. 
okay, here's a simple example SELECT * FROM t1 WHERE c1 NOT IN ( SELECT c2 FROM t2 ) SELECT * FROM t1 WHERE NOT EXISTS ( SELECT c2 FROM t2 ) same subquery, right? in the first example, you get all the c1's that aren't also a c2 in the second example, you either get everything from t1 or else nothing 
I did absolutely not think of employing count()! Thank you very much! This was getting a bit frustrating. :)
I see where you are coming from.. However your sub queries are not equal in the scheme of things. You have not tied the EXISTS to the main query via the common value to equal them out - which I supposed confirms you first comment, but then we are comparing apples with oranges - my comment was *same subquery result*, not *same subquery*!! - edit: which is maybe ambiguous! e.g SELECT * FROM t1 WHERE NOT EXISTS ( SELECT 1 FROM t2 where c1.t1 =c2.t2 ) is the same as SELECT * FROM t1 WHERE c1 NOT IN ( SELECT c2 FROM t2 )
Hmm let me ask you: what is that you're trying to achieve? Are you trying to follow a specific notation? You could look up "entity relationship diagram examples" on google and just throw in whatever general notation standard you want to follow (UML for example).
What is the 'Azure route' ? How can I design &amp; deploy a database from MS Access 2013 to an azure sql server? 
i think you know enough to construct an example on your own just make a couple of the c2 values in t2 NULL and run it
Remove that comma. I'm being played aren't I? Someone's going to jump out of a bush and scare me. Oh, and by the way, why are you creating a FK on a field in a table that references the same field in the same table? It'll probably work, but will serve no purpose.
&gt; Can anyone explain what is wrong in my syntax? the comma, in that position
&gt; The study material we received contained this piece of code as an example of how to create a database with 2 tables and a foreign key. &gt; &gt; http://puu.sh/d3tGh/9f7e812ebb.png &gt; &gt; So I assumed the comma there was properly in place. in the example you linked to, the commas were all in the right place you copied it wrong 
I'm talking about an SQL server installation, too, but with a MS Access 2013 front end. I'm talking about setting it up fresh and from scratch but dont know how to go about it. The MS docs are very confusing for someone new to Azure and MS Access. 
I totally can but its more just for myself, I am seeing how valuable SQL is and it is striking my interest. 
IT systems. What first sparked my interest is a project that required me to mine data from our SCCM database and I was completely lost. It may not be part of my job to support users, but I still want to be able too. TRANSFERABLE SKILLS!!!
Awesome! Thank you so much I will start here and see what I can accomplish.
Annnnd that's how I made bank as a consultant for five years lol.
&gt;My main issue is that people that call in sick/vacation/etc are not showing up on my results even with a left join. Do you have something in the where clause (some criteria against emptime) ?
I do where b.log_time between '11/24/2014' and '11/25/2014' and a.STATUS &lt;&gt; 'terminate' or LOG_TIME is null I test it every day to make sure I'm not just lucky and it's actually working. As long as you use the timeclock, I can tell where you are and if you're here. Snap, I should probably remove that is null, because those will be there anyways.
&gt; where (b.log_time between '11/24/2014' and '11/25/2014' or LOG_TIME is null) and a.STATUS &lt;&gt; 'terminate' Nothing much has changed, the employee that isn't here is still not showing up. I don't understand, by the definition of a left join, this should be working. I'm going to personal message you my entire query.
Awesome!! You were really on point, my company does indeed use the process you described. I will definitely check out sqlzoo.net Since it is interactive and asks you to correct things that sounds to be very helpful. Thank you again!
Breaking up the DBs - think of the huge administrative overhead for even the slightest piece of work required. The overhead and cost would far outweight the cost of a single high cost database solution. Not to mention getting effective reporting across the whole business would be all but impossible. Never let developers get involved with the technology. Get database admins with the proper experience. Do not be a cheapskate at this crucial juncture of your business. Better to spend your money on database experts to tweak performance across the application. Create views for complex queries, get proper indexing happening. Get performance stats from your host. Examine traffic, examine troublesome queries. Ask your hosts why its so slow... but do not split the database. I'll also add; if you cannot afford the required infrastructure, then maybe you are not charging enough for your services. Performance across a large application with many users does not come cheap. There will always be high spots in usage times; typically early morning, lunchtime, and end of day. These are the expensive times for usage and the app infrastructure has to be built around these usage stats.
My first question would be what level of effort has already been put into index/code optimization and [de]normalization? Splitting up the DB is a very large effort that I wouldn't even consider until completing a reasonable level of analysis. I'd assume some has been done given no one is typically ready to jump so quickly on spending more $. If it were me, I would get a DBA to determine wait state information, index hits, and sql even tracks and suggest indexes based on existing code, etc. In parallel, I would identify and prioritize any key areas that are causing customers issues and start working through them 1 at a time with profiler to find your weak points.
Here is an alternative using EXISTS. With the GROUP BY Method as a comparison. In fact the exists method is way more complicated and harder to follow so I would never recommend using it in this case... but it can be done, and I think its always worth looking at the alternatives :) http://sqlfiddle.com/#!6/4fd54/1
I'd say SoftTree SQL Assistant is head and heels above Redgate here. To works on virtually any windows editor for coding/refactoring/formatting/debugging etc. The pro version also has version control and other features built in that are not in SQL Prompt. Also supports MySQL, Postgres, access, sybase etc. 
When I was learning SQL I thought comma first was crazy. After my first week with a job where I used SQL, I was putting commas first in my C# code. (which I've since stopped doing because that's absurd)
There are a few ways to do this, here are a couple. WITH cte_LastBMA AS ( SELECT DISTINCT e.UserSID, MAX(e.EventID) EventID FROM eventlog e WHERE e.EventType = 'Broker_Machine_Allocated' GROUP BY e.UserSID ) SELECT cte_LastBMA.EventID, e.Time, cte_LastBMA.UserSID, e.EventType, e.ModuleAndEventText FROM eventlog e JOIN cte_LastBMA ON cte_LastBMA.EventID = e.EventID Or using a subquery, SELECT EventID, Time, UserSID, EventType, ModuleAndEventText FROM eventlog WHERE EventID IN ( SELECT EventID FROM ( SELECT DISTINCT e.UserSID, MAX(e.EventID) EventID FROM eventlog e WHERE e.EventType = 'Broker_Machine_Allocated' GROUP BY e.UserSID ) z ) The CTE will probably be more efficient.
Can verify, sql prompt is the bomb.
Yeah, nothing says "fuck yeah" like running a format command on a 10,000 line SQL file. Fuck I hate the people I work with.
/u/r3pr0b8 is talking about [this](http://stackoverflow.com/questions/129077/not-in-clause-and-null-values) If your data does not allow null values, then you likely would never notice a difference. Edit: I know this was a day ago, I'm just catching up, apologies if you've already looked into it.
full outer join
Are we building an ERP system? What is the scope?
What's the best open-source solution? 
The schema completely depends on the requirements to your application. For example: * Do users have managers or should they only exists for your authentication layer? * Do you have hierarchical departments or teams of users inside them? * How is the relation between different departments in different companies, do you've shared service centers (one department work for multiple companies)? Sometimes you can use predefined schemas but if you don't write an ERP-Suite which are extremely normalized, you should think of your own.
maybe one of these? http://www.databaseanswers.org/data_models/index.htm
I really enjoyed and was able to build a strong foundation with Head First SQL http://www.amazon.com/Head-First-SQL-Brain-Learners/dp/0596526849/ref=sr_1_1?ie=UTF8&amp;qid=1416913925&amp;sr=8-1&amp;keywords=head+first+sql&amp;pebp=1416913929647 Orielly used to publish the first chapter online but I can't find t on their site right now.
[Apex Refactor](http://www.apexsql.com/Download.aspx) isn't too bad in it's feature set.
I think it's very useful to learn SQL prior to learning a procedural programming language. It will change the way you approach larger projects. Also SQL is incredibly efficient, easier to debug, and in many cases easier to write since it's a declarative language. I suggest Lynda.com's tutorial on SQL, it's amazing. For brushing up and reference, I suggest the book SQL in 24 hours. Keep in mind that both are database agnostic. If you want to specialize in MySQL, T-SQL, Oracle, etc you need another book. The rabbit hole goes deeper
And MY BOW!
I second Red Gate SQL Prompt, I live by this product. As for comma placement, I honestly loath putting them in the front so I always reformat to move them to the back. Just my personal opinion it looks cleaner and is easier to read. But to each his own.
The target is a service for IT shops in the 50-100 employee size * user heirachy: Boss-&gt;Supevisors-&gt;workers (each level 1 to many) * I think the dept issues will be folded into the user scheme * they may work for multple companies but I am envisioning this as a "internal" app for now. I am fine with writing my own, it is just with greenfield implementations you fix all the mistakes you know about and not having a lot of mistakes under my belt as an app dev I am spinning my wheels here. At one level I do not care if it is a close fit as much as by using this I start making progress in generally the right direction, and then when I figure out where the shoe binds I can fix it. 
thanks looking at it 
Scope will be SAS website for multiple companies(I hope), think startup targeted at midsize it shops
http://poorsql.com/ is what I used to use and it was open source. I found it all on github, but it hasn't been updated in awhile, so you'd have to work with it yourself.
See existing data model patterns here: http://dba.stackexchange.com/questions/12991/ready-to-use-database-models-example/23831#23831 Use The Party Model
This is perfect, you're a lifesaver.
Heh, this is just too funny :-D Also note, the chances of getting duplicate GUID's is by no means zero, though it's pretty close: http://math.stackexchange.com/questions/33610/probability-of-duplicate-guid 
&gt; I need to somehow make emptime show people from users, even if they did not clock in. I'm not clear on this. I'm assuming you're saying you need to get a report out of these two tables that shows what employees are at work today vs what employees are not? Can I redefine how you're going at it with something like SELECT User.Name, CASE WHEN ( SELECT COUNT(*) FROM EmpTime WHERE User.User_ID = EmpTime.UserID ) % 2 = 1 -- Use Modulo function THEN 'Employee is here' ELSE 'Employee is not at work' END EmployeeAttendance FROM User WHERE User.Status &lt;&gt; 'terminate' edit: commented badly.. fixed
First, you're trying to define a foreign key when neither table *has* a foreign key. You have two individual tables, each with a Primary key and no column existing to link the two. Notice in your example, table Componist has a column School_ID in it's definition and that column is defined as the exact same data type as the primary key in the Muziekschool table. You didn't put that into either of your tables, so you there is no reason to build a foreign key relationship between the two because you **did not include a foreign key**. To make what you have syntactically correct, just model the Movie table's primary key from the way you created a primary key for the Person table. Then go back to your example and look at the foreign key they made and add the column and the foreign key definition following that example.
Skynet, erm The Google hasn't, and it knows everything: https://www.google.com/?gws_rd=ssl#q=E4C4614B-31A6-47BD-99B1-706A89345F81 Your search - E4C4614B-31A6-47BD-99B1-706A89345F81 - did not match any documents. 8-D
Grab a free 180 day evaluation copy of SQL Server 2012 or 2014: http://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2012-sp1 If you're learning MS SQL it never hurts to reinstall every 180 days if not more often. And these are full Enterprise equivalent versions, so you'll have everything you need to work with clustering, HA, SSRS, SSAS, SSIS, the works. You can even get a 180 day eval copy of Windows Server 2012 to install it on. I rebuild my test servers so often at home this is really the only way I do it.
Nope, just trying to interject some jovial conversation into the thread. Sorry it wasn't taken as such. 
All these answers assume a continuously increasing numeric EventID (BIGINT or INT, probably). Something like this can be done for non-numeric EventID's (like GUID's): WITH cte_LastBMA ([UserSID],[MAX_TIME]) AS ( SELECT DISTINCT e.UserSID, MAX(e.Time) FROM eventlog e WHERE e.EventType = 'Broker_Machine_Allocated' GROUP BY e.UserSID ) SELECT e.EventID, e.[Time], e.UserSID, e.EventType, e.ModuleAndEventText FROM eventlog e INNER JOIN cte_LastBMA ON cte_LastBMA.[Time] = e.[MAX_TIME] AND e.[User_SID]=cte_LastBMA.[User_SID]; If more than one EventID can happen for a [Time] slice you'll need to do some more querying, though.
Each select in the union has to be identical, so you could do something really scarey like SELECT Beast, ID, Name, Strength, TheRest FROM ( SELECT 'Lion' Beast, Lions.ID, Lions.name, Lions.Strength, &lt; some big ugly concatenated string of everything else&gt; as theRest FROM Lions UNION SELECT 'Elephant' Beast, Elephants.ID, Elephants.name, Elephants.Strength, &lt; some big ugly concatenated string of everything else&gt; as theRest FROM Elephants UNION SELECT 'Tiger' Beast, Tigers.ID, Tigers.name, Tigers.Strength, &lt; some big ugly concatenated string of everything else&gt; as theRest FROM Tigers UNION SELECT 'Dragon' Beast, Dragons.ID, Dragons.name, Dragons.Strength, &lt; some big ugly concatenated string of everything else&gt; as theRest FROM Dragons ) ORDER BY Strength Desc But overall I would re-evaluate why exactly you want to get a full row for each beast when it's just going to end up an ugly mish mash that's not usable as a source by any automated system and not readable as a report.
 Cam confirm, just searched for E4C4614B-31A6-47BD-99B1-706A89345F81 and got nothing. Some guy at Google Analytics is like "Why are people searching for this? WHAT DOES IT MEAN?"
will take a look thanks alot :)
thanks alot :D will take a look
&gt; SELECT DISTINCT Employee &gt; FROM Location &gt; WHERE NOT EXISTS ( SELECT 1 &gt; FROM Location LocToday &gt; WHERE Location.Employee = LocToday.Employee &gt; AND LocToday.Work_Date = Sysdate &gt; ) issue i'm having now is it filters it fine but brings up EVERYTHING that isn't equal to today (tomorow,friday etc. etc.) 
nevermind reran it and it's fine now :) thanks so much this is perfect
not sure if/how to mark as solved but this is solved :)
First off, understand that SQL is a set based language and an iterative\procedural approach like that you have (the loops) is usually not the best way. Try breaking down the requirements into smaller parts: 1. Can you write a query to get all of the staff_no with more than 5 years of experience? 2. Can you write a query based off of the previous that shows the staff records with more than 5 years experience? 3. Can you write a query to get all of the staff_no that received a new qualification in 2013? 4. Can you write a query based off of the previous that show the staff records?
Apologies all on this side of the keyboard. No harm no fowl, unless it's deep fried with some gravy :0D
Do you have a sample of the data?, it is not clear from your description.
I'm not quite sure what your asking for Table: Class Column: Days Data: Mon, Tues, Thurs
Ahh, I see. This is not a good way to store the data BTW. But a solution is to take a count of the length of the data minus the length with the commas removed, plus 1. LEN(DaysData) - LEN(REPLACE(DaysData, ",", "")) + 1. This counts the number of values in the list.
I love that you didn't give the answer :-) 
OP: Do you have access to an environment where you can try this out?
How would you suggest storing this kind of data?
Alright, thanks. I'll give it a try
Indeed. Working in an oracle environment for school. 
Hey that's a pretty interesting way of breaking it down. I'm going to give it a go by that methodology tomorrow when I am back in the lab. I think, based on your comment, I would be working with a series of nested statements?
Please tell him that nothing should be done on a production environment unless it's been tested on a backup first
Shouldn't developers know this from the get go? I'm taking a class in database design and sql. I swear everything we've learned is to design a fast database and to keep it lean and mean. Normalization, clustered and non-clustered indexes rebuilding them as well. Using stored procedures to execute code and prevent race conditions. Transactions to make sure a series of changes are processed or not at all to prevent data from being applied incompletely. The reason why I state this is that there seems to be a big animosity between the roles of a sql developer and dba. What gives?
All which can be automated with scripts to ensure consistency and fast application. There is no undo button for critical hanges done via the gui at least in sql server there aren't learning T-SQL will save anybody massive amounts of time due to the reusability of the scripts. 
Never heard of northwind before thanks for dropping that knowledge sir
This method would also allow you to record any day specific class data for that day, such as room number, teacher, etc.
If you are using MSSQL you would use [DBCC CHECKIDENT (table_name, RESEED)](http://msdn.microsoft.com/en-us/library/ms176057.aspx). This will reset the next identity value to be the current maximum plus 1. In MySQL it is probably something like this: DECLARE @NewSeed INT; SELECT @NewSeed = MAX(Value) + 1 FROM tablename; ALTER TABLE tablename AUTO_INCREMENT = @NewSeed; Each DBMS is different. 
Use autogenerated identities and let the database engine handle it in transaction isolation.
Union them dynamically. Example below. -- build up for testing the example if object_id('tempdb..#tablea') is not null begin drop table #tablea end; create table #tablea ( ID int , SOMEDATA varchar(25) , SOMENUMBER int , [DATETIME] datetime ) insert into #tablea values (2,'text',1123,'2013-11-06') , (5, 'text', 1123, '2013-11-06') , (7, 'text', 1093, '2013-12-01'); if object_id('tempdb..#tablename_2') is not null begin drop table #tablename_2 end; create table #tablename_2 ( otherid int , aflag int , anumber int , bnumber int ) insert into #tablename_2 values (205, 0, 1725, 175), (201, 1, 1145, 188); if object_id('tempdb..#tablename_5') is not null begin drop table #tablename_5 end; create table #tablename_5 ( otherid int , aflag int , anumber int , bnumber int ) insert into #tablename_5 values (505, 0, 1725, 175), (501, 1, 1145, 188); if object_id('tempdb..#tablename_7') is not null begin drop table #tablename_7 end; create table #tablename_7 ( otherid int , aflag int , anumber int , bnumber int ) insert into #tablename_7 values (705, 0, 1725, 175), (701, 1, 1145, 188); -- What's above was prep for testing. -- What's below is the example dynamic statement. if object_id('tempdb..#tablelist') is not null begin drop table #tablelist end; select distinct [id], currid = cast([id] as varchar(10)) into #tablelist from #tablea; declare @MySQL nvarchar(255) = ''; declare @CurrID varchar(10); while (exists(select * from #tablelist)) begin set @CurrID = (select top 1 currid from #tablelist); set @MySQL = @MySQL + 'select a.*, b.* from #tablea a, #tablename_' + @CurrID + ' b where a.[id] = ' + @CurrID + ' ' delete from #tablelist where currid = @CurrID; if (select count(1) from #tablelist) &gt; 0 begin set @MySQL = @MySQL + 'union all ' end end drop table #tablelist; exec sp_executesql @MySQL
This looks promising thank you. I tried to test it but I am getting an incorrect syntax near the keyword 'while' . Looked up the [while exists syntax for transact-sql](http://msdn.microsoft.com/en-us//library/ms178642.aspx) but it looks correct. EDIT : nevermind that . Fixed it was typo 
Could you help me understand these parts : set @MySQL = @MySQL + 'select a.*, b.* from #tablea a, #tablename_' + And : set @MySQL = @MySQL + 'union all '
You're just setting the value of @MySql to itself plus the new part after the plus. You want to append that stuff to the string you've built so far. 
Ok have it partially working and understanding more now thanks . Only it looks like the query partially executes untill nvarchar(255) is full. I tried setting it to 4000 (max? ) and it will then run until the 4000 full. I tried debugging using PRINT @MySQL and the line just before the 4000 is reached will run fine but the next won't EDIT : Fully working now. Thanks for the help. Fixed using nvarchar(max) 
I always use a `nvarchar(max)` for building dynamic queries to avoid those sort of problems.
why woould you want to do this? also, what if your auto_increment numbers start at 1 and go all the way up to 12345, and then you delete the row with id=3 -- are you gonna want to renumber all the rows from 4 up to 12345? i think you should just forget the whole idea, and leave the numbers exactly as they were assigned, even if you delete the last inserted row
&gt;&gt;And if I delete a row, the value of the ID continues to increment based on the value of the deleted row. This is only true if you delete the last row that was inserted. You could do it using a reseed command in a trigger, but I wouldn't recommend it. Not sure what your reasoning is but there'll be better options. If you need to know how many rows there are use Select count(*), if you need consecutive IDs for a loop then dump into a temp table and create a tempID, if you're worried you'll run out of IDs then convert you ID column to BigInt etc. If you can advise your reasoning for wanting to do this it'll be easier to give you a better alternative.
&gt; What raise do people who've been employed for 6 years and got a new certification in 2013 get? That's a very good point and it would be interesting to see if there's overlap between the two queries. If there is, then that should be a clarification question to the prof.
Get the thinking right and the answer will fall in line. :)
Race Condition Defined A race condition is when two or more users or processes compete for a shared resource (e.g. the next highest ID in a table for an insert) and the outcome depends upon the order of execution
Sounds like you've got too many rows in Table A for the SQL statement to fit in the variable. You should create a temp table and cursor though the rows in table A. I'm on my phone, so it's a bit much to type the code in, but if you google cursor and temp table, it shouldn't be too bad. Pretty much you'd need: Create table #temp (... Declare Cursor... Fetch @id Cursor while @SQL = 'Select * From Table' + @id Insert into #temp Exec @SQL Fetch next @id End Select * from #temp Drop table #temp
Even if there isn't overlap between the two sets **today** it's one of those things you have to account for. Yes, it's only an academic exercise, but it's also a good time to learn that when your work starts having an impact on peoples' salaries, you **have** to get it right.
Hey man. Just saw your comment. We completed the diagram 2 days ago. Thank you so much again! Have a good day.
This. 100x this. Forcing an identifier can cause you all kinds of update problems or a trigger tree larger than a redwood. Edit for clarification request via message - Update problems would most likely occur with lock contention and serialization performance related problems. 
I hope you're asking this because you're just a little OCD and like your numbers to match up, but I am going to suspect that there is more to it than that. Can you please elaborate a little on *why* you would want to do this?
Is this a one time combining of multiple tables, or is this something you'll need to do on a regular basis? Does your solution need to handle any number of arbitrary tablename_*N* tables, or are you limited to the three you've listed? I would do something similar to the example below, but instead of unioning them I would just add them all to a master table. That way it does not matter how many tablename_*n* subtables you have because your dynamic string never includes more than 2 tables (the subtable and your master) IF you have just 3 static tables you just do something like [this question from yesterday](http://www.reddit.com/r/SQL/comments/2nepkc/ordered_querying_of_multiple_tables/) that had 4 static tables. 
My biggest question is whether or not you're running Replication on this SQL Instance. If replication is setup on this instance I believe the sp_dropserver/sp_addserver will fail. But if this isn't the case your steps are correct. Just run SELECT @@SERVERNAME afterwards to verify the name change is correct. Also if you're running Reporting Services you'll need to possibly manually update this in the SSRS report config file. I think the biggest issue I ever had with renaming a SQL Server was with the Kerberos double-hop between the SQL Engine and Reporting Services. If you start getting ‘NT AUTHORITY\ANONYMOUS LOGON’ errors with Reports after the change you might need to use setspn to modify the accounts used by kerberos. 
Apologies, should have mentioned. No replication and also it's a default instance.
I've never done this before, but the [Microsoft Documentation](http://msdn.microsoft.com/en-us/library/ms143799.aspx) checks out. If you wanted to take precautions, take a [Copy Only](http://msdn.microsoft.com/en-ca/library/ms191495.aspx) backup and take all user databases offline during the rename.
Most definitely. Even after making this change it may be good just to restart the SQL services, which you probably can't do during business hours. For me any production server change no matter how big or small happens after hours. The only thing I might change mid-day is a SQL code change that can be rolled back quickly if it doesn't do what it should.
this made me think of data's ode to spot
This won't affect your databases, other than them not being available while the instance restarts. It's a very benign thing that is largely just updating internal metadata. Source: done this very thing at least a few dozen times.
All went without a hitch. Thanks all!
Thanks for your comments! I pull up results just fine with the SELECT Customer.Name FROM Customer command. I copied and pasted (then formatted for legibility but that was just adding spaces to the end of lines) I could see this being the database manager as I think it's one designed by teacher for the course (SQL Interactive Demonstrator by Bill Weinman). Even rookier question, where can I download a good DBMS? Are any free? Thanks again for your help! Really appreciated it and that definitely saved me tearing out some hairs. 
Having a teacher who is also a parent shouldn't in and of itself create a problem. Tbl.person can have a PK on some sort of id field, with FK's is the other tables as you said. That shouldn't prevent them from sending notification emails to themselves if their own kid is in their class. But automatic cascading deletes are not your friend for the reason you described IMO. If you don't delete people that much, then I'd create a stored procedure that checks the other tables before deleting the person record. Or alternatively just delete the person from the teacher table, then have a routine maintenance task look for orphaned person records and remove them. Downside to both of these is that the queries need to be updated if you change the db schema by adding another FK relationship. But I'd also consider denormalizing so that there is no person table. Might be easier to implement as the only overlap should be teachers with kids... Having two manage two records for those folks might be easier then all your queries having to join the extra table for common person attributes. Just use a common naming standard for "email" etc and to send a message to both parents and teachers you would be a UNION query of the two tables. Lots of ways to skin this cat.
In short; i dont know. The point is to make a database, expressed in an er-diagram, where the demand for a teacher or parents possibility to send an email to multiple receivers(again teacher or parents) is fullfilled. In other words, the relations and tables for this function has to be in a way that someone could use it to program some sort of intranet for the school. But it doesnt really matter what way that is done, just as long the er-diagram represents an appropriate design for that function. As you surely could guess this question is related to a school assignment of mine. It is a creative task which is to draw an ER of a school database, with an appropriate design, making different possibilities of use, for the school. But i just cant figure out how the message relations would work when the different types of people is related to the same table, tbl.person
&gt; where can I download a good DBMS? Are any free? mysql.com 
Thanks again, much appreciated.
Did /u/r3pr0b8 's suggestion resolve your problem, or is your full select still 2x3?
Well, I don't have another DBMS besides the one I'm using which is running through PhP. So, I'm just working on the assumption that this is a conversion error, not a SQL issue. I'm going to look into downloading and working directly with MySQL so as to avoid these errors. Thanks for checking!
The dataset should be a list of the sub reports to run along with any parameters the would need to be supplied to them. Add a row group and group. Right click the row selector to the left of the header row and click Row visibilty. Change it to be based off of an expression. Then do a iif on dataset.fields.reportname and return false where you want to hide it. It does not need the queries of the sub reports, only the parameter valuesthat the sub reports need. What is the expression you are using? 
I don't see the conflict when you have Person_ID as a foreign key in each of the other 3 tables. When you delete a student, a parent, or a teacher, you validate they are not in either of the other two tables before deciding to delete the Person record. You just have many to many relationships between teacher and pupil and pupil and parent, so you need two join tables A teacher is teaches to 1 or mote students, and a student can have one or more teachers, so table TeacherPupil has Pupil_ID and Teacher_ID A Student is related to 1 or more parents, and a parent is related to 1 or more students so table ParentPupil has Pupil_ID and Parent_ID These relationships will let you email all students a teacher has, or all teachers a student has, or all teachers for all of a parents students 
It could run a SQL query against a database, or it could run an executable that runs a stored procedure in a database. Think of it as a Windows Scheduled Task, you can basically make it do anything. Maybe if you tell us why specifically you're asking this question it would be easier to understand what you're getting at.
The Job is stored in the msdb database and can interact with any other database. Each step has a dropdown for which DB the command will be run on. The easiest way to find which DB is used would be to right-click - Create To and then use a tool like Notepad++ to find all the DB and [Uses] references. Or just look at a step. 
 Select item from inventory where item not in (select item from holds) or select i.itemcount - isnull(h.holdcount,0) from inventory i left join holds h on i.itemID = h.itemID
You're moving to a pain in the ass DB and you want to know if it will be helpful? It will not. 
Should probably replace the isnull function with a case or coalesce statement as isnull isn't available on many sql distributions. 
Yet another reason not to use MySQL.
Make the linked server in SQL Server to query DB2 and use that until it goes away. Or use a [reporting tool](http://www.actuate.com/products/birt-designers/birt-designer/) that can query multiple sources and join the data in memory. I linked the free version that my company makes, but I think most can do it.
That sounds like future programmers' problems... those poor bastards. 
Put down that you can do SQL. If you've worked on any interesting projects put that down too. When I interviewed for my current data analyst position they got a feel for my skills by asking what the most complex query I wrote was. It sounds like I was in a similar place as you now are, had a job where I used some SQL, but it wasn't most of my job.
Too mu cheaper of what you're asking is based on the specific details of the database schema you're actually working with. So it's tough to propose a solution as it could be radically different. 
So this is the relevant data for the question I'm working on I believe. The question is specifically: 1. Weekly Report of the number of patients assigned to a specific room (Room number given by the user) http://imgur.com/a/VBLqC
This is the data I'm working with and this is the question I'm supposed to answer: 1. Weekly Report of the number of patients assigned to a specific room (Room number given by the user) http://imgur.com/a/VBLqC
It's possible. First off, your first dilemma is that the same roomnumber exists in two centers. Meaning you have to seperate the centers first, then the rooms. After that you need to make a discussion on whether you wanna simply 'fake' the "week" by giving two interchangable dates (meaning 2 parameters, StartDate and EndDate) or creating a way of looking at the current date, finding the week day, then calculating the entire week from that. Option number two is somewhat complex. 
How would you propose to 'fake' a 'week'? Do you mean add an extra column into the table where I give the week number? Or something else? I am not sure I understand the interchangeable dates thing you said.
You could mention which functions you're comfortable with. If you put SELECT, UPDATE, DELETE statements that will show that you have more than just Microsoft Access experience.
I'm unsure on any syntax changes from T-SQL to Access, but try this. DECLARE @Room INT DECLARE @WeekStart datetime DECLARE @WeekEnd datetime -- SET is testing purposes, or for parameters, if report is called directly from Access. SET @Room = 4 SET @WeekStart = '2014-11-24' SET @WeekEnd = '2014-11-30' SELECT I.RoomNumber , A.BED_ID , A.AdmissionDate , A.DispatchDate FROM Invoice I JOIN Admission A on I.BED_ID = A.BED_ID WHERE I.RoomNumber = @Room AND A.AdmissionDate BETWEEN @WeekStart AND @WeekEnd AND A.DispatchDate &gt; @WeekEnd
i think that might work, my report has about 45 columns that all have to stay in the report. I will give this a shot and see if it works on the actual dataset. Thanks
As /u/gsxr_jason points out, you are not removing duplicate rows in the example given. You are removing unique data rows that happen to share a value. It would help if you gave a clearer example, and clearer explanation of what you are trying to report.
Here's a trick to removing 'duplicates' in a more general sense. SELECT * FROM a_table where rowid in ( SELECT unique_rowid FROM ( SELECT important_rows, min(rowid) unique_rowid FROM a_table GROUP BY important_rows); This will give you only the duplicate rows with the lowest rowid. Since rowid is unique, there will only be 1 per duplicate. You can change important_rows to any number of rows you care about, I think you want to use personId, name for your example. You can also delete the rows not in the returned rowids to completely remove them. ... on phone, can't check syntax, but this should be reasonably close to proper oracle syntax and should be translatable to any other SQL flavor.
Ok. Looks like SQL has it pretty well figured out however according to the wikki this is more than just database language issues. At least from what I gathered. Again I'm a beginner so I'm sure I'm just being an annoying newb but still.... Lol. EDIT: typo
I have three SQL servers doing the same thing. None are domain controllers.
You have all the data you need, except for calendar data for a whole years worth of weeks to join on . I noticed that the rooms numbers, though the same, are in different Centres. So you should identify patients in the RoomID in the Centers. e.g there is Room 4 in Centre 1 and Centre 2. To generate the report for the whole year you need a date reference table of sorts. Other DBs have number sequence generators that can be used for this task, but MSAccess does not. I would work with Week numbers to create the report. Therefore I would create a simple reference table with the week numbers 0-52. You can then join records that occur within each Week reference to report on.This can be done by determining if the AdmissionDate week occurs prior to or in the same reference week, and if the DispatchDate week is equal to or greater than the current week, or is NULL. You can get the current week date number with this function, This gets the year of the associated record, and does a week count from the beginning of that year to the Admission Date datediff("ww",dateserial(year([AdmissionDate]),1,1),[AdmissionDate]) I have given you enough information how to accomplish the data task. You just have to figure the finer details... http://www.techonthenet.com/access/functions/index.php for MSACCESS functions. edit: You do not need Distinct as the count will distinct the records as required in the GROUP by statement. edit2: I also see there is talk of you just typing a date in and see what comes back for that date.. that would not need the calendar reference table as you could work the Weeks out from the date entered, and use the same logic I mentioned above to determine a match.
Doesn't seem to work. I think it is because it is at a networked location rather than a local one. I could change the MSSQLSERVER to run under a domain account but I'm worried about security implications. I wonder if there is a better way to backup to a networked location without just mirroring. I'm trying to reduce overhead in our backup process.
Try running the create through a isql session on the unix host, you may get a more useful error message.
I have no problem letting the service run under a domain account per se. My only worry is that this is not best practice. It seems odd that there is no native way to schedule backups on a network location. But if that's what I have to do I will do it. Just looking for some advice before pulling the trigger. Thanks.
I am more concerned on whether or not you actually want to pursue this as a career/hobby or if it would merely be a once-off exercise to help out your friend. Once off - do not complicate it. There are tones of affordable web hosting packages with free templates, carts and inventories. Gauge the interest and feedback he receives and from there upgrade it to e-commerce. Career/hobby - As mentioned, you will need to understand the principles of DB design, when you use a function and not stored-proc or triggers. Once you have grasped all that, you will need to learn a client and server side scripting language to handle events and db interrogation, then hope that it works efficiently to attract/retain customers. Almost everything I mentioned in the career section is super summarized. I am a head data architect and essentially thought I would come on to reddit for the holidays and help people who are interested in data/db's. 
Yes this is what I ended up doing. Got it working with alter statements. Thanks for the advice.
Wow, anyway, I am still trying to understand how your example is that of a duplicate? Unless you only want to see a single record with orderid '1093322'? I was going to suggest using GUID's too to do it the proper way but then read your question again and still baffled. Anyhow, we just here to help. So not going the TSQL route -here is what you do to only see one '1093322'. If you do this correctly you will get the results you asked for: 1. Select orderid, personid, [name] from e 2. Copy and paste the entire dataset into excel 3. Ctrl-A 4. On the top menu: Go Data &gt; Remove Duplicates 5. In the modal: Unselect all. 6. Tick Column A [If you have done exactly as I said your orderid is ColumnA] 7. Click OK and this will leave you with a single version of each orderid (which I still cannot comprehend :-)) Good luck
You can install a sql server locally on your computer and practice with microsoft's free database files called AdventureWorks or Northwind ( I don't have a source for northwind); http://msdn.microsoft.com/en-us/library/aa992075.aspx You have to download and install MS SQL Server 2008 R2 and the Sql Management Studio 2008 R2 to go with it. This will get you familiar with what a database design looks like, how a schema is built, data types, stored procedures; The works. Also, look for some E-books to get some basic understanding of the language to start off you can check the book SQL in a nutshell :http://gegeek.com/documents/eBooks/SQL%20Pocket%20Guide%20Third%20Edition.pdf. I have the pocket guie, the full version, and i just got a new book called "Troubleshooting SQL Server; A guide for the accidental DBA". It's a little advanced in some chapters, but i just look things up online that i don't understand. Finally, checkout the many different SQL Community forums like sqlservercentral.com ; sqlskills.com ; johnsamson.com. The forums definitely helped me build a path to learn this at my own pace, and they're surprisingly active so you can expect a reply by the next day usually. There's a bunch more you'll find on your way. I very recently went up from helpdesk to a junior dba position after about a year and a half of working here ( My current workplace is also my first IT workplace, and office workplace in general); SQL is my first programming language as well but i've dabbled with MS Access so had a decent understanding of relational databases and i also dabbled with HTML/java so i was "comfortable" reading very simple code. We don't have any one with a dba position at my office at the moment ( We did, but i was still in helpesk then, he's long gone now XD), so i'm learning a lot alone so probably much slower than if you had a mentor. MY 'mentor' is our Business Intelligence analyst; He got trained by our "dba", but he's 85% business 15% tech, so there's only so much i can learn from him. Hope that helps :) 
Oh, I forgot completely about Lynda.com. Thank you for remembering me!
Thank you for the tips!
so you are saying: If (mycolumn.2=NULL, mycolumn.4, OR mycolumn.2&gt;today, mycolumn.4) else (mycolumn.2&lt;today, NULL)
pretty much. Humour me (I'm not great with SQL but learning), would that go into the where or select statement? 
I was thinking about this, and it should not make any difference what account the SQL service is running under as to what you see when giving folder permissions... unless the account you are using for the task is not a domain account.. You should be logged in as a domain administator to set the permissions. Also, to get around the security issue you have in not wanting to use a domain account for the service, you could try creating the identical local account on the server you want to save to and then see if it allows you to connect using the local account on your SQL server. It has been many years since I was administering domains, so my recollection is a little fuzzy. The other option is to save the backup to the local server and then have an agent run under a domain account to copy the backup to a network location. Making any copy is mirroring, you cannot get away from that. Consider the performance will be better backing up locally ,then copying. The *straight to network* backup only be as fast as the network connection. It is more reliable doing a local backup, then copy.
You can use the CASE function Select mycolumn.1 , mycolumn.2 , mycolumn.3 , CASE WHEN mycolumn.2 is null then mycolumn.4 WHEN mycolumn.2 &lt; today() then mycolumn.4 ELSE NULL END From mytable
Look into case statements. It will do exactly what your saying 
yeah, but use the correct CASE syntax, mkay? SELECT mytable.mycolumn_1 , mytable.mycolumn_2 , mytable.mycolumn_3 , CASE WHEN mycolumn_2 IS NULL THEN mycolumn_4 WHEN mycolumn_2 &lt; CURRENT_DATE THEN mycolumn_4 ELSE NULL END FROM mytable
yeah, but your generic answer had a syntax error in any/every dbms
If you manually type in computername/username does it resolve? 
I can give the machine permissions on the folder, just not the NT Service\MSSQLSERVER service permissions.
The employees table has a self reference column [reportsTo] which is the employeeNumber of the person that employee reports to. You can query the same table more than once for different joins using an alias. You can see below that I join to the same table using different alias's for the same table , e1 and e2 SELECT e1.firstname, e1.lastname, o.city, e2.lastname FROM employees e1 , employees e2 , offices o WHERE e1.reportsTo = e2.employeeNumber AND e1.officeCode = o.officeCode
Other poster already hit it. Join employees to employees.
Thanks for taking the time out of your holiday to comment on my post. It's greatly appreciated given the hate I've received on the xpost version of this thread. I am definitely interested in becoming a DBA sometime in the near future. I am not ready to take on that role yet, but got excited when I heard about what he needed. 
thanks. i did not know about the aliases thing. somehow the teacher only showed us: SELECT FROM WHERE `'this'`.`'something'`= `'that'`.`'something'` also LEFT JOIN `'V'` ON `'S'`.`'Var'`= `'V'`.`'Var'` and USING (something). is there any possible way to do this without aliases? 
Yeah, too much industry secrets for that. What are you looking to do specifically, I may be able to assist
If you only need the [reportsTo] value then no join is required as that value is available to you. If you need the name of the person they report to you will need to join the table to itself. Another method is to do the join in the SELECT clause, but it is still joining to itself. SELECT e1.firstname , e1.lastname , o.city , (select e2.lastname from employees e2 where e1.reportsTo = e2.employeeNumber) as ReportsTo FROM employees e1 , offices o WHERE e1.officeCode = o.officeCode
yes thanks i did it like this SELECT `employees`.`firstName`, `employees`.`lastName`, `offices`.`city`, e2.`lastname` FROM `offices` LEFT JOIN `employees` USING (officeCode) LEFT JOIN employees e2 ON `employees`.`reportsTo` = e2.employeeNumber` this way it also shows the boss who reports to no1 
&gt; given a course ID, gets the teaching assistants that ARE NOT currently assisting that course SELECT TA.ta_key FROM TA LEFT OUTER JOIN Assists ON Assists.ta_key = TA.ta_key AND Assists.course_key = 42 WHERE Assists.ta_key IS NULL 
Cool, good job. I assume it works, I am not familiar with the new ANSI joining methods of SQL. I find it far less flexible and easy to read. FYI if you add four spaces in front of a line of text in these textboxes, it will auto format for easy reading.
LEFT OUTER JOIN attempts to find a match using all the criteria in the ON clause if no match is found, all the columns from the right table are set to NULL, but the row from the left table is still returned (which is how a LEFT OUTER JOIN works) so testing for NULL in the WHERE clause finds unmatched TA's
Hey xb10h4z4rd, man, I'm looking to sink my teeth into table.columns that were not randomly generated. Why? It's otherwise a but of a curiosity killer. For example, how cool would it be if you could actually learn per capita spending on clothing items, across regions? Or compare the sales of white tables vs blue tables. What you think?
I've told people (who think I'm crazy) that I'd gladly pay to work at Amazon or Walmart if it meant I could run queries on their data in my own free time. I could learn so much. Sadly, for privacy reasons (among other reasons) I don't think any major company would ever reveal their customer data like this.
Wish I could provide you with copies of my work database. Under about a hundred NDAs though :/
[http://chinookdatabase.codeplex.com](Chinook) might be worth checking out for a different sample dataset to test against.
Stack Exchange makes their database available for querying. They periodically have full dumps of the database you can download to restore to your own instance. http://data.stackexchange.com
[In case you haven't seen this yet](http://www.udel.edu/evelyn/SQL-Class2/joins.jpg)... 
Well we are trying to run the queries on our own. The prof is not helping at all, haha. So we're getting stopped by Access for one reason or another. Okay so I've taken your feedback and I think I've simplified what I need to do. Instead of doing joins, I'm just going to try to keep it simpler and use DatePart and Group By functions. Just not sure how that's going to go. 
So what error is Access giving you? I've never used Access, but surely it's not just saying "no." 
I've scrapped that query for now and started over but the one it gave me was "Syntax error (missing operator) in query expression '(DateDiff('d',AdmissionDate,DispatchDate)DailyCost+QtyCost)'." This is what I'm working on right now for the same question. Started over completely. Trying to keep it simple. http://imgur.com/WJ3SXgW Not very far yet. I apparently need to get a GROUP BY function in there at the end. 
And this is the data I'm pulling from currently. http://imgur.com/EewT6jL
I know you said you scrapped it, but a quick glance at the error: '(DateDiff('d',AdmissionDate,DispatchDate)DailyCost+QtyCost)' Made me wonder... is there supposed to be some sort of operator in front of DailyCost? 
Maybe you're right. To be honest I am in over my head here. I've been told to stop making it more complicated for myself and don't use unnecessary functions (like datediff). Apparently I should be able to solve it with datepart, groupby, and AS.
Yes indeed, group by patient_id probably would be easiest. 
For getting the Days in hospital you can simply use the dates on their own; no datediff("d"..) required. SELECT P.PatientName , ((A.DispatchDate - A.AdmissionDate) * I.DailyCost) as DailyCost FROM Admission A , Invoice I , Patient P WHERE A.bedId = I.BedId A.patientID = P.PatientID
What helps beat is to build individual select statements and gradually add the info you need.
No worries at all. Here's some pointers: - Look at companies you would like to work for and go on their career boards and look at the tech they use - most use commercial DB's like IBM/Oracle/SAS/MSSQL etc but narrow it down to the companies you eventually want to apply to when you are ready. Many beginners start off with open source, master it but then cannot get into the bigger companies since they have not adopted it as greatly and find that you either work for a tiny company as a guru but cannot get into the bigger companies since they still go commercial. - Once you have made the decision on which to focus on, get to understand the theory and make sure that your mindset is geared towards being logical (your spacial recognition skills need to be good when working with data) - You mentioned becoming a DBA so guess you have already made your mind up on what it is that a DBA does. You also need to understand that it essentially is the mechanic role since many of your team and systems will be dependent on 99.9% uptime so you will need to be on call when need be and not get too wasted on a Friday night (especially after applying patches) - so be ready for that :-) - You will also be the go to person (with guidance from your architect) on what the best landscape would be through security/migrations/upgrades etc etc etc - actually a lot so you need to know what you are doing and be confident about your decisions/recomendations you make because you will be accountable. Other than that it will be a breeze :-) While you learn - get on sites like expertsexchange/linkedin and try helping others and see if you are up to it. I wish you all the best...
I used to be a vendor for Kroger Corp. Had access into the data warehouse and was pretty amazed at the sheer volume of data. Not to mention once I started to see analytic tools offered by dunnhumby, they've got customer insights down to a science.
Might want to check out /r/datasets/
Found the problem. I did setup up a temporary reserve really big and the cache for the server was too low and it timed out. I did changed the reserve cache and everything worked fine. 
SOE exposes the main dataset for most of their games through a REST API at census.soe.com. I'm in the process of building an ETL to draw chunks of it into a MySQL DB for playing with. The biggest challenge has been learning to manage a dataset with millions of records in a new RDBMS (I use MS SQL at work). 
In plain english, describe the 3 tables. Describe how they are related. Those english descriptions of the 'relations' can be implemented in the database as foreign key constraints. This will enforce the business rules, or relations. You'll want to alter your tables by adding your foreign key constraints to the tables. We [Oracle] have development and design tools that can make this process easier - but YOU will have to define those relationships. Once you know those, defining the foreign key constraints is pretty easy. You can even do this visually with Oracle SQL Developer Data Modeler. It's a free design tool. 
 CREATE #dupeTable ( fieldA int, fieldB char(1), fieldC varchar(25) ) INSERT INTO #dupeTable SELECT a.fieldA , a.fieldB , a.fieldC FROM ( SELECT COUNT(*) , fieldA , fieldB , fieldC FROM Table GROUP BY fieldA, fieldB, fieldC ) a DELETE FROM Table INSERT INTO Table SELECT * FROM #dupeTable The sub-query will de-dupe everything. I'm assuming you can use a COUNT function. I don't understand why your version of SQL doesn't support analytical functions, even SQL Express allows these.
Assuming your Evidence table only contains one row per File/Suspect combination, it could be as easy as: SELECT FileHash FROM Evidence GROUP BY FileHash HAVING COUNT(*) &gt; 1 You can join to other tables as need be.
Thanks for the reply. I modified that slightly and it works. Thanks!! Im still a little lost. This is what I have working. It prints all of the file hashes and the accompanying suspect information. SELECT Evidence.Hash, Evidence.SuspectID, SuspectFirstName, Suspect.SuspectLastName FROM Evidence INNER JOIN Suspect ON Evidence.SuspectID=Suspect.SuspectID ORDER BY Evidence.Hash Your code works nicely. It prints out the file hashes that have more than 1 suspect associated with it. SELECT Evidence.Hash FROM Evidence GROUP BY Evidence.Hash HAVING COUNT(Evidence.SuspectID) &gt; 1 But I get an error when I try to combine the two. SELECT Evidence.Hash, Evidence.SuspectID, SuspectFirstName, Suspect.SuspectLastName FROM Evidence INNER JOIN Suspect ON Evidence.SuspectID=Suspect.SuspectID GROUP BY Evidence.Hash HAVING COUNT(Evidence.SuspectID) &gt; 1 Error message is: Msg 8120, Level 16, State 1, Line 37 Column 'Evidence.SuspectID' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. Any thoughts? I appreciate it. (I am also Google-ing solutions). Edit: Formatting 
Ok. I made more edits. I no longer get an error but nothing is returned. SELECT Evidence.Hash, Evidence.SuspectID, SuspectFirstName,Suspect.SuspectLastName FROM Evidence INNER JOIN Suspect ON Evidence.SuspectID=Suspect.SuspectID GROUP BY Evidence.Hash,Evidence.SuspectID, SuspectFirstName, Suspect.SuspectLastName HAVING COUNT(Evidence.SuspectID) &gt; 1 
Also, using an IN operator may be clearer [which your database may just compile to a LEFT join as /u/r3pr0b8 describes] SELECT * FROM TA WHERE ta_key NOT IN (SELECT DISTINCT ta_key FROM Assists WHERE course_key=?) [Fun learning exercise: Do your database's "EXPLAIN QUERY PLAN" equivalent on this query both with, and without, the DISTINCT. Then do it on the query from r3pr0b8]
Tableau is great for analysis through visualization. Excel has PowerBi and Powerpivot which are great for those that are familiar with excel. But these both tend to lead to visualization as final output. Your question is more involved than that. What do you want to show? To whom do you want to show it? How should it be shown? These questions start to help decide which tools you'll use to produce and display your information. 
I do epi/medical research and some consulting for healthcare non-profits and for the pharmaceutical industry. Currently use a combination of R, Python, and SAS. The program I do my analysis in really depends on the project and who I am working with (not going to use Python if no one else has a clue how to use it). SAS tends to be the industry standard when working with CRO's, but when doing my own research or working on a project without any set rules I typically use R or Python. Occasionally will use Tableau but since the majority of my work focuses on predictive modeling I don't spend a lot of my time working with or creating dashboards. 
Awesome!! Thanks. Works like a charm. Much appreciated. 
You could also write it as (find all the evidence where there exists another reference to the same evidence but with a different suspect): Its probably a little quicker but much less flexible. So its not really better or worse. just a different way of solving the same problem. SELECT Evidence.Hash , Evidence.SuspectID , Suspect.FirstName , Suspect.SuspectLastName FROM Evidence INNER JOIN Suspect ON Suspect.SuspectID = Evidence.SuspectID WHERE EXISTS ( SELECT 1 FROM Evidence e WHERE e.Hash = Evidence.Hash AND e.SuspectId &lt;&gt; Evidence.SuspectId )
I've been using toad data point
You don't have an aggregate in that query so why use group by and having? Or do other languages besides sql server allow that?
So I think it means something like this then? CREATE TABLE DLYON_S ( S_Id int NOT NULL CHECK (S_Id &gt; 0), PName varchar(8) NOT NULL, Colour varchar(8) NOT NULL, Weight int NOT NULL CHECK (Weight &gt; 0), City varchar(8) NOT NULL, PRIMARY KEY (S_Id) ); CREATE TABLE DLYON_PARTS ( P_Id int NOT NULL CHECK (P_Id &gt; 0), SName varchar(8) NOT NULL, Status int NOT NULL, City varchar(8) NOT NULL, JoinDate date NOT NULL, PRIMARY KEY (P_Id) ); CREATE TABLE DLYON_SP ( S_Id int NOT NULL, P_Id int NOT NULL, Qty int CHECK (Qty &gt; 0), PRIMARY KEY (S_Id, P_Id), FOREIGN KEY (S_Id) REFERENCES DLYON_S(S_Id), FOREIGN KEY (P_Id) REFERENCES DLYON_PARTS(P_Id) ); Basically, using the create table statement I need to declare a primary key for them, and then specify constraints on them as well? Is this not the same thing? It also allows me to alter the current tables too. alter table "DLYON_SP" add constraint "FK_DLYON_SP_PNUM" foreign key ("P#") references "DLYON_PARTS" ("P#") / alter table "DLYON_SP" add constraint "FK_DLYON_SP_SNUM" foreign key ("S#") references "DLYON_S" ("S#") / alter table "DLYON_PARTS" add constraint "UNIQ_DLYON_PARTS_SNAME" unique ("SNAME") / 
yeah, i can understand it although it does have a syntax error
Why would I want to? What a mess.
Interesting. Some SQL syntax check also said that, but it works in ms access. Can you point me where the error is? (IMO is correct) The SQL is the generated by another software and I needed to make some changes (added the 3th union all) so this is the beast.
In the first and second unions you are doubling up records that are between Oct and Nov for 2013, marking them with a value in Venda0 (first part of union) and also Venda1 (second part of union), is that intentional?
&gt; Can you point me where the error is? a subquery in the FROM clause needs an alias 
My guess is that they're comparing Oct 2014 and 2013 costs, and want the 2013 sort-of-yearly totals to shed light on whether 2013 monthly number is typical? That's a stab in the dark, seems like an odd way to go about doing this. General purpose maybe to compare vendor billing between to time periods, grouped by sub contractor and contractor. Not sure why its grouped by contractor and subcontractor. Whole thing looks very messy.
I know what you're saying but why would they be more interested in 2013's Oct-Nov totals than 2014's? If that is true, it's a strange and, imo, not very effective or enlightening set of stats.
venda 0 =&gt; '00' AND '11' venda 1 =&gt; '10' AND '11'
That's what I mean, the data in Venda 1 is already included in the range of Venda 0, and they are unioned into the same subquery.
Search on free + data + sets. There is plenty of free govt and science data. Amazon web services hosts public data sets [here](http://aws.amazon.com/public-data-sets)
Sorry to say, but that is nothing to be proud of. 3 Subqueries unioned together with 2 and 3 being pretty identical. Also, WHERE YEAR = '2013' AND MONTH BETWEEN '00' AND '11' a year and a month as a string (lets not even talk about '00') .... brrrrr Got one for you too, this is copy pasted from a data migration stored procedure (yes, including the comment) ,blnCanBeRepeated = wi.LoeschFlag ^ 1 -- if you dont get this, dont touch my proc ! Hint, its TSQL this is fun, lets do one more : declare @someNumber int select @someNumber = ((max(tbl.someInteger) / 1000000 ) +1) * 1000000 from someTable tbl 
I manage a team of two DBAs, however, I'm also still an active DBA. My time is spent about 80% as a DBA and 20% as a manager. My tasks * Managing all the DBA projects * Managing the DBA's assignments * Approving timesheets and other misc manager busy work * Attend of shit-ton of meetings because everyone thinks they need a DBA present * Backup/Restore and Cloning of databases * Database patching &amp; upgrades * Database and SQL tuning * Writing SQL &amp; PL/SQL * Database Encryption (at-rest and in-flight) * Database Auditing * Developer assistance * Data Modeling (OLTP and Warehouse) By no means is that a complete list, but as you can see, most of my time is spent "in the weeds" of the database and less as a manager. Don't know if this helps you any, but it's one perspective.
Yeah, so I'm taking an intro level Database Project course which my teacher is working us through basic SQL: DML, DDL, and DCL. With our final project only a few weeks away I've decided to move past Microsoft Access and step into the big leagues. I'm so pumped! I hope I didn't butcher the install too bad :P 
Yes, and this is what is needed. Crazy request, right? This was needed to calculate the seller/client performance of - 2013 full year; - 2013 out and nov; - 2014 out and nov;
This is what has been requested. Sales of by seller/customer: - 2013 full year; - 2013 out and nov; - 2014 out and nov; &gt;Whole thing looks very messy. How would you do that in another way? (Remember that comes from a generated ms access app. I just added one union)
Awesome, thanks for the help!
thanks, was just trying to see what role people have as it relates to reporting and data analysis. Where i'm at it kind of stops after reporting services, we don't get to do much data analysis which is kind of shame because I do know R pretty well and I have a decent understanding of statistics. Going that extra mile is not worth it where I work because nobody cares. 
Was his problem the date? I'm unclear as to the change you made. 
Just for the sake of learning, are there differences in performance between the two? 
Why do you need a tbl.person at all? 
Looks like you're using report builder and I've done all my ssrs development in Visual Studio so take that into account when reading my suggestions. There are multiple ways to tackle it. You can place your subreports in a list item with 3 separate detail groups (you can make the list item itself based off a static / dummy dataset, i. e. Group expression is =1) . You then define the page name setting for each group. Then make the visibility of your "header" row based on the page name (I think it's a global parameter). I can't remember if Page Name is a new thing or not. If you don't have it, then define a dummy value in your dataset for your list item (eg, select 1 as Val Union all select 2 Union all select 3). Then tie each detail group to a specific value (group filter) and then make your header row based on "Val". 
They are based on completely different assumptions about the data. There is no performance comparison. 
Yuck... SQL should be much nicer than Access for you. Which version of Access did you migrate from? IMO, Access is a nice frontend tool to reach users while interacting with a SQL backend. It's also nice for small database needs. Once you start getting past a few hundred thousands records though... Access is found very wanting. Today I spend two hours applying a few indexes to 3.7M rows and ~columns. That might be poor interplay between Win7 and Access 2002 though.
I'm trying to give this a shot, still really new at this but is this right? http://i.imgur.com/QFk9imX.png I don't really know where to go from here, is the top row my header and the bottom ones are where my subreport goes? **Edit:** Been playing with it a while, I've got it like this now http://i.imgur.com/KW86SYU.png?1 with each subreport inserted into the Details and the headers into the [Group5], [Group4]...etc parts. But the problem is that when I run the main report the images/group headers are not inserted into the margins of the page which is what I want. What am I missing?
The problem with them almost being identical is performance.
Close. You don't need a column group. Add 2 more row groups (add group, adjacent after). 
I'm hoping it's not too late to reply to this vexing-JOIN issue. I think I've figured it out, but I'm hoping you or someone can confirm it. This is from the section on JOINs in Kathi Kellenberger's 2014 edition of Beginning T-SQL and uses the AdventureWorks sample database from MSFT. Assume 3 tables - Customer, Sales Order Header, Sales Order Detail. A customer record exists but no orders have been places, rendering the two Sales Order fields NULL for that customer. Joining Customer to SOH using a LEFT OUTER JOIN shows the customer record and NULL. Joining to the SOD table using a LEFT OUTER JOIN shows the customer record, NULL, and NULL again. However, if you remove the "LEFT OUTER" portion for the second join (the SOH--&gt;SOD join), the entire row disappears, even though the customer field was not NULL and was displayed from the first LEFT OUTER join. I thought about it for a while and realized that it must be a matter of joining table C not to table B and then B to A, but rather joining table C to table (B+A) since B and A have already been joined. They're not distinct, discrete entities anymore, are they? So by the time the second JOIN statement is executed (which joins a NULL to a NULL, which doesn't work), the first JOIN statement is already done and can't "come to the rescue" of the null-order row anymore even though, by its own rules, the row should display because there's an entry in the Customer field. Am I thinking about this right? (And for that matter, am I not totally mangling my description of the issue?) Thanks.
Like this? http://i.imgur.com/5Vipl0R.png
 SELECT partNumber FROM tbPartComponent GROUP BY partNumber HAVING COUNT(*) = COUNT(DISTINCT CASE WHEN letter IN ('A','B') THEN letter ELSE NULL END)
nope, it wasn't the date, it was conflating the two different syntax structures -- CASE value WHEN compare_value THEN result [WHEN compare_value THEN result] ... [ELSE result] END CASE WHEN search_condition THEN result [WHEN search_condition THEN result] ... [ELSE result] END you cannot spot the error any more because he has edited his response, but it used to say -- CASE mycolumn.2 WHEN mycolumn.2 is null then mycolumn.4 WHEN mycolumn.2 &lt; today() then mycolumn.4 ELSE NULL END can you see the syntax error now? 
[https://imgur.com/a/IW91o](https://imgur.com/a/IW91o) Sorry as I changed the solution some after noticing that the header row wasn't repeating when you have adjacent groups. I create a table that uses my dummy dataset (select 1 as Val... etc). Both the parent group and detail group are based on the Val field. I set the page break setting on the Parent Group (set to between each instance). For each row in the detail group (each row that has a subreport), I defined Row Visibility tied to "Val" (ex. *iif ( Fields!Val.Value = 1, false, true)* ) I then set the visibility for my "header row". In my example, I didnt want to see the header on subreport2. So I did *iif( Fields!Val.Value = 2, true, false)*. Now when I preview, I see the header on page/subreport 1, it doesn't show on 2, and then it's seen again on 3. aaaaand here is the entire rdl code (i'm using vs 2012 -- edit: this RDL should work for vs 2010 and maybe 2008 r2 not sure on anything earlier) * &lt;?xml version="1.0" encoding="utf-8"?&gt; &lt;Report xmlns="http://schemas.microsoft.com/sqlserver/reporting/2008/01/reportdefinition" xmlns:rd="http://schemas.microsoft.com/SQLServer/reporting/reportdesigner"&gt; &lt;Body&gt; &lt;ReportItems&gt; &lt;Tablix Name="Tablix4"&gt; &lt;TablixBody&gt; &lt;TablixColumns&gt; &lt;TablixColumn&gt; &lt;Width&gt;2.44792in&lt;/Width&gt; &lt;/TablixColumn&gt; &lt;/TablixColumns&gt; &lt;TablixRows&gt; &lt;TablixRow&gt; &lt;Height&gt;0.25in&lt;/Height&gt; &lt;TablixCells&gt; &lt;TablixCell&gt; &lt;CellContents&gt; &lt;Textbox Name="Textbox22"&gt; &lt;CanGrow&gt;true&lt;/CanGrow&gt; &lt;KeepTogether&gt;true&lt;/KeepTogether&gt; &lt;Paragraphs&gt; &lt;Paragraph&gt; &lt;TextRuns&gt; &lt;TextRun&gt; &lt;Value&gt;This is my header&lt;/Value&gt; &lt;Style /&gt; &lt;/TextRun&gt; &lt;/TextRuns&gt; &lt;Style /&gt; &lt;/Paragraph&gt; &lt;/Paragraphs&gt; &lt;rd:DefaultName&gt;Textbox21&lt;/rd:DefaultName&gt; &lt;Style&gt; &lt;Border&gt; &lt;Color&gt;LightGrey&lt;/Color&gt; &lt;Style&gt;Solid&lt;/Style&gt; &lt;/Border&gt; &lt;BackgroundColor&gt;White&lt;/BackgroundColor&gt; &lt;PaddingLeft&gt;2pt&lt;/PaddingLeft&gt; &lt;PaddingRight&gt;2pt&lt;/PaddingRight&gt; &lt;PaddingTop&gt;2pt&lt;/PaddingTop&gt; &lt;PaddingBottom&gt;2pt&lt;/PaddingBottom&gt; &lt;/Style&gt; &lt;/Textbox&gt; &lt;/CellContents&gt; &lt;/TablixCell&gt; &lt;/TablixCells&gt; &lt;/TablixRow&gt; &lt;TablixRow&gt; &lt;Height&gt;0.25in&lt;/Height&gt; &lt;TablixCells&gt; &lt;TablixCell&gt; &lt;CellContents&gt; &lt;Subreport Name="Subreport4"&gt; &lt;ReportName&gt;subreport1&lt;/ReportName&gt; &lt;Style&gt; &lt;Border&gt; &lt;Style&gt;None&lt;/Style&gt; &lt;/Border&gt; &lt;/Style&gt; &lt;/Subreport&gt; &lt;/CellContents&gt; &lt;/TablixCell&gt; &lt;/TablixCells&gt; &lt;/TablixRow&gt; &lt;TablixRow&gt; &lt;Height&gt;0.25in&lt;/Height&gt; &lt;TablixCells&gt; &lt;TablixCell&gt; &lt;CellContents&gt; &lt;Subreport Name="Subreport5"&gt; &lt;ReportName&gt;subreport2&lt;/ReportName&gt; &lt;Style&gt; &lt;Border&gt; &lt;Style&gt;None&lt;/Style&gt; &lt;/Border&gt; &lt;/Style&gt; &lt;/Subreport&gt; &lt;/CellContents&gt; &lt;/TablixCell&gt; &lt;/TablixCells&gt; &lt;/TablixRow&gt; &lt;TablixRow&gt; &lt;Height&gt;0.25in&lt;/Height&gt; &lt;TablixCells&gt; &lt;TablixCell&gt; &lt;CellContents&gt; &lt;Subreport Name="Subreport6"&gt; &lt;ReportName&gt;subreport3&lt;/ReportName&gt; &lt;Style&gt; &lt;Border&gt; &lt;Style&gt;None&lt;/Style&gt; &lt;/Border&gt; &lt;/Style&gt; &lt;/Subreport&gt; &lt;/CellContents&gt; &lt;/TablixCell&gt; &lt;/TablixCells&gt; &lt;/TablixRow&gt; &lt;/TablixRows&gt; &lt;/TablixBody&gt; &lt;TablixColumnHierarchy&gt; &lt;TablixMembers&gt; &lt;TablixMember /&gt; &lt;/TablixMembers&gt; &lt;/TablixColumnHierarchy&gt; &lt;TablixRowHierarchy&gt; &lt;TablixMembers&gt; &lt;TablixMember&gt; &lt;Group Name="Group1"&gt; &lt;GroupExpressions&gt; &lt;GroupExpression&gt;=Fields!Val.Value&lt;/GroupExpression&gt; &lt;/GroupExpressions&gt; &lt;PageBreak&gt; &lt;BreakLocation&gt;Between&lt;/BreakLocation&gt; &lt;/PageBreak&gt; &lt;/Group&gt; &lt;SortExpressions&gt; &lt;SortExpression&gt; &lt;Value&gt;=1&lt;/Value&gt; &lt;/SortExpression&gt; &lt;/SortExpressions&gt; &lt;TablixMembers&gt; &lt;TablixMember&gt; &lt;Visibility&gt; &lt;Hidden&gt;=iif ( Fields!Val.Value = 2, true, false)&lt;/Hidden&gt; &lt;/Visibility&gt; &lt;KeepWithGroup&gt;After&lt;/KeepWithGroup&gt; &lt;/TablixMember&gt; &lt;TablixMember&gt; &lt;Group Name="Details"&gt; &lt;GroupExpressions&gt; &lt;GroupExpression&gt;=Fields!Val.Value&lt;/GroupExpression&gt; &lt;/GroupExpressions&gt; &lt;/Group&gt; &lt;TablixMembers&gt; &lt;TablixMember&gt; &lt;Visibility&gt; &lt;Hidden&gt;=iif (Fields!Val.Value=1, false, true)&lt;/Hidden&gt; &lt;/Visibility&gt; &lt;/TablixMember&gt; &lt;TablixMember&gt; &lt;Visibility&gt; &lt;Hidden&gt;=iif (Fields!Val.Value=2, false, true)&lt;/Hidden&gt; &lt;/Visibility&gt; &lt;/TablixMember&gt; &lt;TablixMember&gt; &lt;Visibility&gt; &lt;Hidden&gt;=iif (Fields!Val.Value=3, false, true)&lt;/Hidden&gt; &lt;/Visibility&gt; &lt;/TablixMember&gt; &lt;/TablixMembers&gt; &lt;/TablixMember&gt; &lt;/TablixMembers&gt; &lt;/TablixMember&gt; &lt;/TablixMembers&gt; &lt;/TablixRowHierarchy&gt; &lt;RepeatRowHeaders&gt;true&lt;/RepeatRowHeaders&gt; &lt;FixedRowHeaders&gt;true&lt;/FixedRowHeaders&gt; &lt;DataSetName&gt;DataSet1&lt;/DataSetName&gt; &lt;Top&gt;0.32167in&lt;/Top&gt; &lt;Height&gt;1in&lt;/Height&gt; &lt;Width&gt;2.44792in&lt;/Width&gt; &lt;Style&gt; &lt;Border&gt; &lt;Style&gt;None&lt;/Style&gt; &lt;/Border&gt; &lt;/Style&gt; &lt;/Tablix&gt; &lt;/ReportItems&gt; &lt;Height&gt;4.62375in&lt;/Height&gt; &lt;Style /&gt; &lt;/Body&gt; &lt;Width&gt;6.5in&lt;/Width&gt; &lt;Page&gt; &lt;LeftMargin&gt;1in&lt;/LeftMargin&gt; &lt;RightMargin&gt;1in&lt;/RightMargin&gt; &lt;TopMargin&gt;1in&lt;/TopMargin&gt; &lt;BottomMargin&gt;1in&lt;/BottomMargin&gt; &lt;Style /&gt; &lt;/Page&gt; &lt;AutoRefresh&gt;0&lt;/AutoRefresh&gt; &lt;DataSources&gt; &lt;DataSource Name="DataSource1"&gt; &lt;ConnectionProperties&gt; &lt;DataProvider&gt;SQL&lt;/DataProvider&gt; &lt;ConnectString&gt;Data Source=.\mssqlserver12;Initial Catalog=AdventureWorksDW2012&lt;/ConnectString&gt; &lt;IntegratedSecurity&gt;true&lt;/IntegratedSecurity&gt; &lt;/ConnectionProperties&gt; &lt;rd:SecurityType&gt;Integrated&lt;/rd:SecurityType&gt; &lt;rd:DataSourceID&gt;12999254-edd8-4054-b196-fbde9d14bda8&lt;/rd:DataSourceID&gt; &lt;/DataSource&gt; &lt;/DataSources&gt; &lt;DataSets&gt; &lt;DataSet Name="DataSet1"&gt; &lt;Query&gt; &lt;DataSourceName&gt;DataSource1&lt;/DataSourceName&gt; &lt;CommandText&gt;select 1 as Val union all select 2 union all select 3&lt;/CommandText&gt; &lt;/Query&gt; &lt;Fields&gt; &lt;Field Name="Val"&gt; &lt;DataField&gt;Val&lt;/DataField&gt; &lt;rd:TypeName&gt;System.Int32&lt;/rd:TypeName&gt; &lt;/Field&gt; &lt;/Fields&gt; &lt;/DataSet&gt; &lt;/DataSets&gt; &lt;rd:ReportUnitType&gt;Inch&lt;/rd:ReportUnitType&gt; &lt;rd:ReportID&gt;0ab63efa-cdc8-4952-8c2c-f768e922da47&lt;/rd:ReportID&gt; &lt;/Report&gt;
Ah very helpful thank you. 
I think I messed up somewhere, https://imgur.com/a/ZoAWg is the dataset query correct? The first expression is for the header row, the 2nd and 3rd ones are for row/subreport 2 and 3. Where did I go wrong?
Try setting the visibility expression for all detail rows, not just 2 and 3. Also, I'd try matching up to the example I provided with 3 detail rows first and then add in more once you get it. I have to go to bed now but I can help you some more later tomorrow if you haven't figured it out. I've done tons and tons of ssrs and I like helping out on it. 
Great, I'll keep tinkering around with it and see if I get anywhere. Thanks alot for your help!
I don't really care about performance, is one time only query. I can wait 30 seconds. Anyway, how would you improve? 
This doesn't work. It returns 1,2,3
http://sqlfiddle.com/#!4/ccc9e/1 SELECT partNumber FROM (select distinct partNumber, letter from main) m GROUP BY partNumber HAVING listagg(letter, ',') within group (order by letter) = 'A,B'
I migrated from Access 2010. What I'm hoping to do in my project is a hierarchical relational database with Mississippi census information. So, as of right now I wrote a macro to scrape specific state, county, and city information. After I've organized all the information I just have a few project requirements like building a front end application to generate a few reports. Other then that I should be golden. It could all easily be done in Access I just ran into a problem with formatting that I didn't like in access so I just wanted to try out SQL Server.
yes, you're right, i forgot the distinct count has to be equal to 2 this is tested and works -- SELECT partNumber FROM ( SELECT partNumber , COUNT(DISTINCT CASE WHEN letter IN ('A','B') THEN letter ELSE NULL END) AS count_AB , COUNT(*) AS count_all FROM tbPartComponent GROUP BY partNumber ) AS subquery WHERE count_AB &gt; 1 AND count_AB = count_all seeing as this is oracle, you may have to remove one or more AS's
Ok so i dont have phpmyadmin. Just straight up command line. When i ssh/sftp the csv file, is there a paricular folder it needs ro be in. Should i make a temp? Do u know where i would need to make it. Thanks for your help
Shoot it over bruh I'll take a look at it. If you could also show the denormalized table as well
Thanks so much for your reply and for those below, ended up replying to this one as it was at the top! Bidirectional was simply the visual side of many-to-many relationships, and we were told that in RDBMS all relationships have to be many-to-many. Surely this means then that I have to create a pivot table between any two tables where the relationship is many-to-many, and if my practical leader is to be believed, this would be all of them. My online academic portal is currently down for maintenance but will update my original question with info regarding the assignment and the few things that I have no idea how to tackle. I just want to get this database designed correctly the first time so I can get to writing the triggers and queries that I will have to use to demo my code. This code I'm not worried about, as the queries and triggers will not be too tricky to figure out once I have a minorly-populated database for testing purposes. I've definately got some questions, I just need access so I can actually formulate a question that'll help me out. Again, massive thanks guys! :'DD 
Just FYI, a pivot table is something else completely. There's no real official term for the table in the middle of a many to many relationship that I'm aware of. We generally call them "link tables" where I work. I've also heard "junction table" and just "many to many table". A pivot table is when you rotate your data so rows become columns and columns become rows. Those are used mainly for analytics and reporting.
oh my bad! I thought pivot was synonomous to junction/link, i've actually labeled some of my tables as link tables, and use linkId as their PK. 
Unless there is additional data on the link tables (beyond the keys of the two tables being linked), you generally want to use a compound primary key consisting of the two foreign keys. This guarantees that that relationship will be distinct in the table. At their simplest your link table would be something like this (MS Sql Server syntax): CREATE TABLE CaseEmpLink ( caseID int NOT NULL , empID int NOT NULL , CONSTRAINT PK_CaseEmpLink PRIMARY KEY (caseID, empID) , CONSTRAINT FK_Case FOREIGN KEY (caseID) REFERENCES Case (caseID) , CONSTRAINT FK_Emp (empID) REFERENCES Emp (empID) )
they're also called relationship, association, or intersection tables and they very rarely benefit from having their own auto_increment PK as lukeatron pointed out 
In addition to the actual, practical advice people are giving, I feel the need to be a little pedantic and point out that you're looking for help designing a DB or an RDB, not an RDBMS. The RDBMS is e.g. SQL Server or PostreSQL, the management system that's doing all the work behind the scenes.
Fogive me my stupid sounding questions but how can you have them both as a composite primary key when they're also the FK? : / These are the sorts of questions that I've sort of been too embarassed to ask in class over the years, considering I learned SQL on my own and most of my coursemates studied it prior to higher education. Another (hopefully) quick question: If A and B were two tables with a many-to-many relationship, connected by a link table for referential integrity, connected to table C that has a one-to-many or a 1-to-1 relationship would I be able to query them all at the same time? Basically, B does not connect to C, but C connects to A, so can you join them all and thus achieve a wanted query result? I'm thinking along the lines of A and B being Case and Emp, with C being a lookup for the addresses of the Employees. This would allow me to have a separate table for the addresses of the employees, rather than making the employees table larger. Is this reasonable? 
[sugarcrm](https://apidocs.sugarcrm.com/) is open source and has documented their model completely. If you download the software, you should be able to create the database yourself. 
Okay, so given that it works - which is a great start, all that remains is to rewrite the whole thing as a single SELECT (using subqueries and reference tables as needed), and then to bury that single SELECT statement in a CREATE VIEW DDL statement. Bingo!
So I've been playing with it overnight, and this is what I get http://imgur.com/a/5wtgi Just a blank page with Header and the first row. I'm not sure what I could have done wrong as I think everything is a direct copy from your example? Can you see anything that I messed up?
Ok, but how do I get the same logic to work?
As it sits you don't. You can't pass parameters into views you have to create a function or procedure.
That works beautifully! Thanks a ton.
Huh, that's a pretty interesting way to do it. I hadn't run into that function before (granted, most of my SQL experience is non-Oracle). Thanks!
Well, everything is known beforehand - I know that I want to use the min month found, I know I want it to go through the end of the year of the last value - so I don't need any information from outside the query. I bet there is some kind of CTE for this, but I can't figure it out.
Both groups are set to group on Val? Visibility settings are on the rows, not groups right? Edit: also make sure you don't have your True and False settings in the hidden expression backwards. If you have time post your RDL code and I'll have a deeper look when I get some time. I would step through it to catch anything. Create the report with your groupings first without making any visibility or page break settings. You should have the items repeated 3 times (once for each row in our dataset). Next add in the page break setting on the parent group. Now you should have 3 pages, each page should show the header and should show the 3 rows. Then change the visibility settings on each of the detail rows (IE iif Val = 1, false, true.. . Etc). You should have the same 3 pages, but now only one detail row per page instead of 3 (because 2 rows should be hidden based on the visibility change you made). Next change the visibility setting on the header row, not on the group ( iif Val = 2, true, false). This should hide the header on page 2.
I'll go through it again and post my results tomorrow, starting on a fresh report and going through the steps you just mentioned. Thanks for your patience with this, I'm still learning ssrs so your help so far has been very useful. 
I'll check it out 
It's not going to be pretty, but start with replacing @month with convert(varchar(4), dateadd(yyyy,0,[Ticket Date]), 121)+'-01-01' Note that I removed the MIN() function. Don't forget to change your GROUP BY expression accordingly. 
Thats a really odd data structure you are starting with. I assume its just a straight import from excel or something? Might be better to deal with it in excel first? If you have to do it in SQL I would unpivot then seperate out the A and B data and then join it back together. Here is an example: http://sqlfiddle.com/#!6/068cf/1
I don't really know what RDBMS you are using but assuming MSSQL: Seems like you are just grouping by the Start of the month of Closed Date and the Archetype? is there something else going on here? Can't you just do something like the below: (you don't need the function it just makes things neater) CREATE FUNCTION [dbo].[ThisMonthStart](@date AS datetime) returns datetime AS BEGIN RETURN dateadd(mm, datediff(mm, 0, @date), 0) GO CREATE VIEW MonthlyCloses AS SELECT Archetype [System] , dbo.ThisMonthStart([Closed Date]) [ClosedMonth] , COUNT(*) [ClosedCount] FROM dbo.AllTickets WHERE ISNUMERIC( [Title (LinkTitleNoMenu)]) = 1 GROUP BY Archetype , dbo.ThisMonthStart([Closed Date]) Without the function it would be: CREATE VIEW MonthlyCloses AS SELECT Archetype [System] , dateadd(mm, datediff(mm, 0, [Closed Date]), 0) [ClosedMonth] , COUNT(*) [ClosedCount] FROM dbo.AllTickets WHERE ISNUMERIC( [Title (LinkTitleNoMenu)]) = 1 GROUP BY Archetype , dateadd(mm, datediff(mm, 0, [Closed Date]), 0)
If you need the months with no closed tickets to still show up with a 0 entry in your view then you need a calendar table prepopulated with entries and then just left join the MonthlyCloses to that table by ClosedMonth.
You could either do what you wrote and use a current time command so the date always stays current OR you could pull those columns and MAX(date_append). That would pull the most recent row for each if the column values. 
edit: I solved it after trying a few things. Thanks very much for the help! Doing this still yields rows from previous month uploads: Select Col1 , Col2 , Col3 , max(date_append) from data_table group by col1 , col2 , col3 ; I think it's because data uploaded each month does not match the previous month 100%: there are records that fall off from last month, and new records being made in the current month. I'm not sure. Previously I tried using sysdate(mm)-1 to just pull everything from 1 month prior, but the date_append date is not always on the same day each month, and it can potentially pull more than just the most recent update date. 
well hidey-ho neighbor! Didn't expect to see you here :P I solved the problem a moment ago, and put it in the edit above. Thanks for the help though!
Some claim that I'm a software engineer by day.
Here are a few options to ponder. I'd suggest trying them all out so you can compare performance for your particular need and get a feel for how they all work. * Option 1: Analytic function DENSE_RANK() OVER (...). This option will be fast if the DATA_TABLE isn't huge because it'll do a single pass of the table to get the answer. WITH RANKED_DATA AS ( SELECT DENSE_RANK() OVER ( ORDER BY DATE_APPEND DESC ) RNK, dt.* FROM DATA_TABLE dt ) SELECT rd.* FROM RANKED_DATA rd WHERE rd.RNK = 1; * Option 2: Subquery to get desired date and use that to filter data in main query. This option will perform best if the # of records returned is small (1-5%) relative to the total number of records. An index on DATE_APPEND will be a huge boon. SELECT dt.* FROM DATA_TABLE dt WHERE dt.DATE_APPEND = ( SELECT MAX( dt2.DATE_APPEND ) FROM DATA_TABLE dt2 ); * Option 3: Development #1. This option works well if you need to make sure every record is processed exactly once. Add a "DATE_PROCESSED" column that you populate when the record is processed. Add a (function-based) index on "CASE WHEN DATE_PROCESSED IS NULL THEN 1 END". [This could also work by tracking DATE_APPEND_LAST and updating the index and query appropriately] SELECT * FROM DATA_TABLE dt WHERE CASE WHEN dt.DATE_PROCESSED IS NULL THEN 1 END = 1; * Option 4: Development #2. Create a table to track what you've processed and use that to probe the data table. SELECT dt.* FROM DATA_TABLE dt WHERE dt.DATE_APPEND &gt; ( SELECT MAX( pl.DATE_APPEND ) FROM PROCESS_LOG pl );
Awesome reply, thank you! I solved the problem a moment ago, but just knowing these alternate solutions is a huge help.
I should add that it looks like the code I put in the post does sort of what I want, but it creates the Total_People column and I need Total_People as a new entry for "Measure" and the actual integer value of Total_People as a new entry for "Value"
Give us a create table and insert statements and the expected results. Make sure your sample data doesn't work for the query you've made.
I've edited to show much more thoroughly what I'm attempting
 SELECT CAST(COUNT(id) AS float) AS Value, FinalDisposition AS Measure FROM [' +@dbname '].table2 WHERE FinalDisposition = 'Weekdays' OR FinalDisposition = 'Weekends' GROUP BY FinalDisposition the "id" that is being counted is id for people in the scenario that comes from other databases just like FinalDisposition is a column name from other tables in the databases. This will pull the values that I need but now how can I sum them and have the sum equal a new Measure entry?
Ok so once it's restored to a SQL Server (like SQL Server Express? would that work?), will I be able to export it to a non-SQL file-type from there?
Thanks a ton.
If you want to demonstrate the use of indexes, start by adding a lot more rows to your table. A couple millions should do the trick. Then you can process a simple request that filters on a field which is not the primary key nor indexed. Then index it. You should see the difference. Try to have a lot of different values for this field so that the index is really useful. Also, this [website](http://use-the-index-luke.com/).
Let me see if I can explain what's going on (and thanks for the help, btw!) I need to return the number of tickets that were opened and not closed within a given month. If it was opened in a prior month and not closed in this month it needs to be counted. So if a ticket was opened in May and closed in December, it needs to be counted in the September count (and all the other months too).
tuples? Yes, use what @mattdee suggested :) But I would use the INSERT stuff first. Learn the SQL basics, then go crazy with the GUI. 
I have most of the basics down I just didn't want to sit there for a while entering in data and having to check it a lot. Thanks for the tip though
Thanks
A 'trick' - you can load up your data in a spreadsheet - go crazy with copy and paste, whatever. Then in SQL Developer, right click on your table - import. It will suck the XLS/X file right in. Or generate an INSERT script if you need to 'show your work' ... I have a post on how to do this on my blog if you need help.
MS SQL 2008 R2 Server Management Studio. Restore! :-)
Thanks brah 
Never used it before. Time to learn.. /Google
Try to follow these two steps 1. Verify if the index is taken up. For that, have a look at the execution plan. If there is no difference in the execution plan after you added the index, it's just not taken up an thus cannot make any performance difference. See [here](http://use-the-index-luke.com/sql/explain-plan) how to work with execution plans. With just 15k rows you have to use a very selective column (e.g. only unique values) to make the database take the index. If you query on a bit field (such as Y/N or M/F), chances are the database decides against using it. Just try around until you can verify in the execution plan that the index is actually used. 2. Once you have verified that the index is used if present, try to measure the performance difference. That however, might be hard with just 15k rows, as mentioned by /u/Pacos. Try inserting more rows. You might also add another column to the table (e.g. `junk CHAR(200)`) just to increase the table size. (ps to /u/Pacos : thanks for mentioning my site;)
It's not that bad. Let me know if you have any questions. If I get some time later I'll draft up a script for you. 
This is the input: | Version | Scenario_Name | Measure | Value | |---------|---------------|-------------|-------| | 1.0 | Scenario1 | Weekends | 10 | | 1.0 | Scenario1 | Weekdays | 25 | | 1.0 | Scenario2 | Weekends | 12 | | 1.0 | Scenario2 | Weekdays | 30 | | 1.0 | Scenario3 | Weekends | 5 | | 1.0 | Scenario3 | Weekdays | 15 | | 2.0 | Scenario1 | Weekends | 100 | | 2.0 | Scenario1 | Weekdays | 15 | | 2.0 | Scenario2 | Weekends | 32 | | 2.0 | Scenario2 | Weekdays | 34 | | 2.0 | Scenario3 | Weekends | 52 | | 2.0 | Scenario3 | Weekdays | 13 | This is the output I need. | Version | Scenario_Name | Measure | Value | |---------|---------------|-------------|-------| | 1.0 | Scenario1 | Weekends | 10 | | 1.0 | Scenario1 | Weekdays | 25 | | 1.0 | Scenario1 | TotalPeople | 35 | | 1.0 | Scenario2 | Weekends | 12 | | 1.0 | Scenario2 | Weekdays | 30 | | 1.0 | Scenario2 | TotalPeople | 42 | | 1.0 | Scenario3 | Weekends | 5 | | 1.0 | Scenario3 | Weekdays | 15 | | 1.0 | Scenario3 | TotalPeople | 20 | | 2.0 | Scenario1 | Weekends | 100 | | 2.0 | Scenario1 | Weekdays | 15 | | 2.0 | Scenario1 | TotalPeople | 55 | | 2.0 | Scenario2 | Weekends | 32 | | 2.0 | Scenario2 | Weekdays | 34 | | 2.0 | Scenario2 | TotalPeople | 45 | | 2.0 | Scenario3 | Weekends | 52 | | 2.0 | Scenario3 | Weekdays | 13 | | 2.0 | Scenario3 | TotalPeople | 25 | The full query looks like this: --declare variables declare @dbname nvarchar (200); --database name from scenario regression reports declare @query nvarchar (max); --query to retrieve regression data declare @testid int; --declare the cursor by selecting database names with the abcdbname tag. All test scenarios must include this tag in the report base name DECLARE db_cursor CURSOR FOR select name from sys.databases where name like '%abcdbname%' --query to set the testid equal to previous test id + 1 select @testid = coalesce(MAX(TestNum), 0) from [TestLog].RegressionTable set @testid = @testid + 1; Open db_cursor fetch next from db_cursor into @dbname while @@FETCH_STATUS = 0 --keep going as long as there is more in the list of db names BEGIN --define a query that counts people by final disposition, scenario run time, and report database size. set @query = CAST('select (select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Scenario Name') as Scenario_Name, (select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Version') as Version, CAST(count(*) as float)/MAX(repnum) as value, finalDisposition as Measure, GETDATE() as DateRun, (select ' + CAST(@testid as CHAR) +') as TestNum from [' +@dbname+ '].table2 group by FinalDisposition union select (select attributedata from [' +@dbname+ '].table1 where AttributeName = ''Scenario Name'') as Scenario_Name, (select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Version') as Version, CAST(LEFT((select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Scenario Execution Elapsed Time'), CHARINDEX('m',(select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Scenario Execution Elapsed Time'),1 )-2) as float) as Value, (select 'Run Time') as Measure, GETDATE() as DateRun, (select ' + CAST(@testid as CHAR) +') as TestNum from [' +@dbname+ '].table2 group by FinalDisposition union select (select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Scenario Name') as Scenario_Name, (select attributedata from [' +@dbname+ '].table1 where AttributeName = 'Version') as Version, (SELECT cast((size*8)/1024 as float) SizeMB FROM sys.master_files where DB_NAME(database_id) = ''' +@dbname+ ''' and type = 0) as value, (select ''DBSize'') as Measure, GETDATE() as DateRun, (select ' + CAST(@testid as CHAR) +') as TestNum from [' +@dbname+ '].table2 group by FinalDisposition ' as nvarchar (max)) -- This is the part that I added specifically. -- It isn't yeilding an error message but its not doing the math that it is supposed to do. -- I'll explain further below. union select (select attributedata from table2 where AttributeName = 'Scenario Name') as Scenario_Name, (select attributedata from table1 where AttributeName = 'Version') as Version, (select CAST(COUNT(*) as float)/MAX(repnum) from table2 where FinalDisposition = 'Weekends' or FinalDisposition = 'Weekdays') as Value, 'Total People' as Measure, GETDATE() as DateRun, (select 100) as TestNum; --the two lines below insert the results of the query above into the test log database. those results are then read from an R script for generating a test report Insert INTO [TestLog].RegressionTable(Scenario_Name,Version, Value, Measure, DateRun, TestNum) Execute (@query) --iterate through the list of test scenarios FETCH NEXT FROM db_cursor INTO @dbname END Close db_cursor deallocate db_cursor It is pulling data from other databases after scenarios have been simulated and that last select above the INSERT INTO is trying to take two values that are already being pulled ("Weekends" and "Weekdays") and sum them. There's no sum function being used because the count(*) is simply counting up the total number of entries that show up and so when it counts total entries with the "Weekend" and "Weekdays" Measure entry, it gives a sum of those entries that should be a single number because no group by is being called. However, when I run this on its own: SELETC CAST(COUNT(*) AS float)/MAX(repnum) AS Value, 'Total People' AS Measure, WHERE FinalDisposition = 'Weekends' OR FinalDisposition = 'Weekdays' FROM table2 it works great. But when its put into the rest of the code, it gives back entries for "Weekends", "Weekdays", "Profits", "Costs" but each is called "Total People" in the measure column and their value is the total of all 4... **O_o** 
If the Database size is larger than 4gb, you will have to use 2012 Express. Since they increased the Max DB size from 4gb to 10 gb
Our database is 9gb, so this is extremely helpful information.
I was just talking to a colleague and I brought up Stats as a possible class I might want to take. Thanks for the input.
CCP releases the Eve Online database as a SQL2012 backup image with each release. https://developers.eveonline.com/resource/static-data-export Then a member of the Eve community provides various other formats (MySQL, Postgres, SQLite) after conversion at: https://www.fuzzwork.co.uk/dump/ 
Alright, to start, let's make sure that this isn't a Windows problem as opposed to a SQL Server problem. I'm not sure which version of Windows you're running, but generally in a batch file if you want to split multiple arguments to one command across several lines, you need to use an escape character. What happens if you create a batch file like: sqlcmd ^ -S XYZUTIL -U username -P password -d xyztest ^ -i "sProc.sql" ^ -s "," ^ -o "c:\output\output.csv" set /p delExit=Press the ENTER key to exit...: That is, place a space and a caret character at the end of each line where you're continuing some arguments to a previous command. Or, collapse all of your arguments to `sqlcmd` into a single line, so your batch file has two lines, the one that calls `sqlcmd` with lots of arguments, and the line that pauses the cmd window. If that doesn't fix things up, post back with the exact error message that you're getting.
Audit an abstract algebra class. It may be a bit obtuse, but it will help you think in sets which is invaluable for constructing queries and evaluating designs.
I'm getting a user logon error. Which is weird, because the username is a domain admin, literally the god account. EDIT. It's working now, thanks. Figured out it was the local server SQL SA.
This is exactly what I'm after. Math that will help me think in the ways that are helpful for mental database visualization.
I remember hearing that song on a Dragons starcraft II stream. Googled it on Youtube and sent it to my buddies at about 34k views. Texted all my friends who out right called me absolutely goofy and childish. Year later they are dancing gangnam style at one of theirs wedding.
&gt;I'm surprised they didn't make this a 64-bit bigint just to be safe When YouTube was just getting started and was being funded by personal credit cards held by Chad Hurley, Steve Chen and Jawed Karim, they probably didn't think a single video would get **one** billion hits, let alone maxing out a 32-bit int. They were just trying to get things up and running. When they did approach that 1B mark, however, someone should have taken notice and gotten a plan together to alter the table &amp; update corresponding code. I'd say this is less about forecasting properly from the beginning, and more about not monitoring &amp; planning capacity based upon usage trends.
&gt; the username is a domain admin, literally the god account. Try not doing that. Use a mortal user account that only has the access it requires. If it has to be the domain admin, are you certain that it's been granted the access needed everywhere? In an ideal and just world, domain admin accounts **wouldn't** be able to do everything, because it would just mean that there's an account that can ruin your whole business - and typically the people with domain admin credentials are **not** savvy with SQL Server, so you probably don't want them having the ability to (for example) access your HR database.
If you're not in it for the credit check out MIT OpenCourceWare - http://ocw.mit.edu/ . They have most if not all their statistics and math lectures online with course material.
"objects" and "traits" do not compute what are your "tables" and "columns"?
Got it, needed the local sa account. Now I just need to figure out how to append a datestamp to the output form the batch file, and I'll be all set, pretty much. Thanks.
Sorry, my bad, I'm still new to this! The code for my table looks like this: create table vals as select "a" as value, "e" as trait union select "b" as value, "e" as trait union select "c" as value, "e" as trait union select "d" as value, "e" as trait; What I want to do is create a table of all combinations of three of the values in the value column. 
Not sure entirely what you're asking, but perhaps this will get you in the right direction: SELECT CASE foo WHEN 1 THEN 'Up' WHEN 2 THEN 'Down' WHEN 3 THEN 'Left' WHEN 4 THEN 'Right' ELSE 'Six Ways From Sunday' END AS Direction FROM myTable 
Could you send me a link?
Domain Admin here, so not good at SQL server. But I'm learning, and fortunately it's only extracting info, so I'm not actually changing any data. Sooooo, how would I go about setting up the batch file to add a datestamp? Here's what I've got so far. The batch ran fine, but didn't change the filename: echo off sqlcmd -S LPCUTIL -U user -P ****** -d TRtest -i "sProc.sql" -s "," -o "c:\output\SecurityMaster.csv" ren C:\SecurityMaster.csv c:\SecurityMaster_%date:~-4,4%%date:~-7,2%%date:~-10,2%.csv set /p delExit=Press the ENTER key to exit...:
Also, check out asktom.oracle.com. It's run by Oracle VP Tom Kyte. Tom has a deep understanding of databases, is able to explain the reasoning behind his answers, and always tries to back what he says with runnable examples. It's technically Oracle specific, but since he teaches principles, you can often apply them to other databases.
Hashed PWs won't help here. What will help is running under a domain account (either logged in or with a scheduled task that runs under a limited domain account) and using integrated security so that you don't have to sling credentials around at all.
Been there, done that. Got a system that uses a sequential integer plus 2 leading alpha characters for unique IDs all over. The ID field was declared as char(8), guess what happens when that sequence gets to 1,000,000 (this is actually the abridged version, the reality was more insidious). Had to alter a bunch of tables for the wider field, then update the UI to show longer ID without scrolling/wrapping.
Bookmarked. Thanks for that info.
Also bookmarked.
Datestamp needs to be in the filename. C:\Output.csv needs to become C:\Output-20141203.csv Thanks for all the help.
Yup, sounds like what we had to go through, not only did the database need to be updated but as did all reports, applications, websites, everything to accommodate the larger value. Little did whoever know how much work it'd be when they said "Let's just add two zeros to the end of that number".
What tool/language are you using to connect to the DB? Does it need to be an automated task? If its a one-time thing from sqlplus you could do something as simple as: set colsep ";" spool test.txt select trunc(sysdate), col1, col2, 0, col3 from table; spool off
I thought that was the end of the article
That schema looks familiar, you working in TMW_LIVE?
I am writing this script in an application called PowerBuilder. When a user clicks a button, the script takes everything stored in a table, and outputs it to a text file. As far as automation goes, there is another caveat. If this text-file doesn't exist, then the system creates it. However, if this text file exists already, all the data from the table is appended onto it. 
Based on that it sounds like you need less help with Oracle and more help with PowerBuilder. I'm not familiar with that app...
Don't even use a batch file. Batch is dead. You're a domain admin, you should be using PowerShell. #require -module sqlps set-strictmode -Version latest; $MyData = Invoke-Sqlcmd -ServerInstance "YOURSERVER" -Database "YOURDATABASE" -Username "YOURUSER" -Password "YOURPASSWORD" -InputFile "sProc.sql"; $datestamp = get-date -Format "yyyyMMdd"; $OutFileName = "c:\output\SecurityMaster_$datestamp.csv" $MyData | Export-Csv -NoTypeInformation -Path $OutFileName; When you switch to integrated security, just drop the `Username` and `Password` parameters in that `Invoke-SqlCmd` line.
Thanks mate. Yes its ugly and unfortunately must be handled in sql and go straight to output. 
It sounds like you're mixing tiers. The DB can handle some file-level operations using UTL_FILE, but only if the file is on the actual DB server (or a share presented to the DB server). It is going to be a lot more complicated than you think, you'll have to write a pl/sql package that uses the different utl_file procedures to do the things you're describing. If the file is being created by PowerBuilder then that app would be responsible for the file-level stuff. 
You'd be given 1000 times more respect without those dumb images. 
The file will reside on the actual DB server. I am not using PowerBuilder to create the file. I'd like to use Oracle to create the file on the actual DB server. The user runs an application on his local machine. Certain users actions communicated to a server populates a table on a DB. When the user clicks a button on this application, the contents of this table are transferred to a text-file (which exists on the DB). If this text-file doesn't exist, the DB creates it. If it already does, all the data from the table is appended onto it.
Here is an [example from java2s](http://www.java2s.com/Tutorial/Oracle/0601__System-Packages/Savetablerecordstoafile.htm): SQL&gt; CREATE OR REPLACE PROCEDURE emps2file ( 2 loc IN VARCHAR2, 3 file IN VARCHAR2 4 ) 5 IS 6 fid UTL_FILE.FILE_TYPE := UTL_FILE.FOPEN (loc, file, 'W'); 7 line VARCHAR2(2000); 8 BEGIN 9 FOR rec IN (SELECT last_name, hire_date, salary FROM employee) 10 LOOP 11 line := 12 rec.last_name || ',' || 13 TO_CHAR (rec.hire_date, 'MM/DD/YYYY') || ',' || 14 rec.salary; 15 UTL_FILE.PUT_LINE (fid, line); 16 END LOOP; 17 UTL_FILE.FCLOSE (fid); 18 EXCEPTION 19 WHEN OTHERS THEN UTL_FILE.FCLOSE (fid); 20 END; 21 / to find out if a file exists you can use utl_file.fgetattr() There are some more examples at http://psoug.org/reference/utl_file.html
Thanks so much! Especially for communicating to me that *if* the file were to exist on a local machine (and not on the DB), the difficultly would be much more substantial.
Nice. I actually was asked about this in an interview I had recently.
But why? That sounds like something you would do with... anything but SQL.
It's more involved than this. But this is the barebones. The long version is that if a text-file does exist in this location, a SQL script appends on data from several columns from a staging table. The staging table itself has been populated with data taken from nearly a dozen other tables. If the text file doesn't exist, then SQL creates the text-file, and instead of appending onto the end, the script populates the text file with data from this staging table. The reason for the check of whether the file is there or not, is because a timed function, part of an external system periodically reads in the contents of the text-file, and deletes the file after a successful read. 
One of them is DML. One of them is DDL. Best to not confuse them. 
I don't know for certain but my guess is to avoid confusion. Delete syntax is used to remove rows of data. Edit: yeah u/LeviW is right, drop syntax removes an actual database object while delete syntax removes data within an object.
Not going to lie this sounds like the worst possible way of executing what you want done. Just because you could do it through the db doesn't mean you should.
What would you do? The link between the system I'm currently working in now, and the external system is the text-file. I have no control of the timed function or the external system. Upon a user-click in an application, the following needs to happen: 1. Check for text-file presence, as the external timed function dictates whether that file exists or not. If not, then it needs to be created. If exists, then nothing. 2. The data from specific columns of staging table needs to be saved in a text-file with semicolon delineation. Some columns are not from this staging table, and has default values. If file exists in "1.", data is appended onto end. 
If you want to get up to speed on SQL stuff, Google "accidental DBA" and get crackin'. That material is written for people like yourself who find themselves suddenly responsible for a database server.
Considering the other server is an application why not have it read from the database? If it isn't just put a client on it and handle the file operations on the OS level. I'm thinking Python or perl would handle that well.
Well can you store code? Is place/SQL an option?
The ISO_WEEK function will give you the week number for a datetime value.
Shit autocorrect thats supposed to be PL/SQL so how are you going to execute the code if you don't store it just run it as a script? It sure seems like you have a lot of cants in this project what can you do?
Delete is used by DML, so to perform a similar function in DML they have to find something else. DROP command isn't limited to Table, you can drop other things "Database, table, index, column, etc etc". I believe we took the whole database as a tree/collection of things up in the air (imaginary) then this DROP command make a lot of sense ( especially in a classroom teaching DML ).
walmart huh? :)
From a programming perspective I think it makes a lot more sense to flip that. Delete is paramount. You delete a table. You delete a database. You delete an index. You drop a row. Just my .02, delete is engrained in me. Every time I type drop it makes me nervous.
Haha yeah I can see how it might be counter intuitive when compared to a programming syntax. It makes sense in my mind b/c when you drop something, you're dropping not just the data in the table but the DB object altogether (including table metadata, indexes, constraints etc). With delete, you're simply erasing rows. It's considered good practice to always use delete...where (some SQL flavored make you use where when deleting). Keeping that I mind might help as a reminder since where is not a condition you can use with drop, you either drop the table or you don't
1. dynamic sql: in your example, you found 'entry 30' and you need to average a column in that table, so you build select avg(a_column) from entry_30_table" statement and execute. 2. Build a view over the 'corresponding tables' that combines all that data together 3. ETL all the 'corresponding tables' into one table and have a proper FK. 
so .. uhm ... where is your sum? 
What sql platform are you coding in? 
This is a really good article, very useful and informative. As soon as I get in to work I'll be replacing all those 'select count' blocks of code with the EXISTS predicate instead! 
&gt; Now how do you link these "entry" tables back to the original Table 1 you cannot
 INITCAP(lastname) AS First, INITCAP(firstname) AS Last, that's fucked, daddy
What happened to ROLLUP?
wow, what a useful article ^that, ^by ^the ^way, ^was ^sarcasm
Look carefully. It's an easter egg.
Thank you so much. I'll give it a whirl. 
When you truncate don't you give that slice of the disk back to the OS for management? I read that this has an adverse effect on performance as it fragments the .mdf.
Sure http://www.thatjeffsmith.com/archive/2012/04/how-to-import-from-excel-to-oracle-with-sql-developer/ 
Start with [this calendar script](http://www.experts-exchange.com/Database/MS-SQL-Server/A_12267-Date-Fun-Part-One-Build-your-own-SQL-calendar-table-to-perform-complex-date-expressions.html). You will need to do some custom logic to get your quarter's week number, but it can be easily done in the UPDATE statements.
No worse than just doing a DELETE FROM [table]... with no WHERE. Very similar effects. The difference is in how the change is logged. For that, read your SQL db engine of choice's manual. For [SQL Server](http://msdn.microsoft.com/en-us/library/ms177570.aspx) &gt;Compared to the DELETE statement, TRUNCATE TABLE has the following advantages: &gt;• Less transaction log space is used. &gt; &gt;The DELETE statement removes rows one at a time and records an entry in the transaction log for each deleted row. TRUNCATE TABLE removes the data by deallocating the data pages used to store the table data and records only the page deallocations in the transaction log. &gt;• Fewer locks are typically used. When the DELETE statement is executed using a row lock, each row in the table is locked for deletion. TRUNCATE TABLE always locks the table (including a schema (SCH-M) lock) and page but not each row. &gt;• Without exception, zero pages are left in the table. After a DELETE statement is executed, the table can still contain empty pages. For example, empty pages in a heap cannot be deallocated without at least an exclusive (LCK_M_X) table lock. If the delete operation does not use a table lock, the table (heap) will contain many empty pages. For indexes, the delete operation can leave empty pages behind, although these pages will be deallocated quickly by a background cleanup process. &gt; &gt;TRUNCATE TABLE removes all rows from a table, but the table structure and its columns, constraints, indexes, and so on remain. To remove the table definition in addition to its data, use the DROP TABLE statement. If the table contains an identity column, the counter for that column is reset to the seed value defined for the column. If no seed was defined, the default value 1 is used. To retain the identity counter, use DELETE instead. &gt; &gt;Restrictions &gt; &gt;You cannot use TRUNCATE TABLE on tables that: &gt;• Are referenced by a FOREIGN KEY constraint. (You can truncate a table that has a foreign key that references itself.) &gt;• Participate in an indexed view. &gt;• Are published by using transactional replication or merge replication. &gt; &gt;For tables with one or more of these characteristics, use the DELETE statement instead. &gt; &gt;TRUNCATE TABLE cannot activate a trigger because the operation does not log individual row deletions. For more information, see CREATE TRIGGER (Transact-SQL). &gt; &gt;Truncating Large Tables &gt; &gt;Microsoft SQL Server has the ability to drop or truncate tables that have more than 128 extents without holding simultaneous locks on all the extents required for the drop. 
I assume since you're using INITCAP and JOIN/USING you're in DB2 or Oracle given these are not available in MS SQL but let us know so we're not guessing. 
Didn't look at your query, but why not make a table with holiday dates and query according to dates on the table? Since thanks giving is essentially the 4th week of november not necessarily a fixed dated so neither is black friday. There could be a better way i'll look into further since being exposed to these types of situations are great for experience
Thanks! I got it working yesterday. Did it exactly how you recommended. 
I saw that when I was googling around this morning, but are we sure the second part of the where clause is ignored if the first part is true?
&gt;[Fisher Price](http://fisher-price.com) Yes. That's pedantic. I'm downvoting myself. Sorry.
Can you union the 1000 individual tables to one table, adding a column keyed to the entry?
Was everyone here born in 76? March 76 here! We do something similar, if I have a lookup I'd do something like this when populating my Lookup: Select Type, Description from MyLookupTable Union Select '%','Select All'; Then use something like this in the query: Select Type, blah1, blah2, blah3 From TableBlah Where Type like @Param1 Like can cause some issues with indexing, but this is the best way I've found to do this in SSRS. Also if you're using Numeric values you need to case it to a string data type (varchar, nvarchar, whatever). 
This needs to be farther up. 
This worked for me. Thanks.
I stand corrected and vetoed your 'self downvote' as you Sir are correct.
In SSRS go in to the data set and go to parameters. For the column(s) that are multi-valued add the expression =Join(Parameters!XYZ.Value, ",") Go to the Parameter menu and go into settings for the parameter and set it to allow multiple. In the procedure when using the variable run it through [charlist_to_table](http://stackoverflow.com/questions/6543834/stored-procedure-parameters) as an in. Dependent on what you'll be passing through and how much of it, consider increasing the size of the variable accordingly. Regarding handling new values automatically in subscriptions, it will if you use select all. If you're pulling a dataset to give available values it will run that query on each execution and will select all from it.
This is correct. 
This is correct. 
This is correct. 
Already discussed to death [here](http://redd.it/2mhpwp). **Summary**: Article of rehashed Postgres features written by person who has barely ever used MS SQL.
Can you assume that each employee or volunteer only works at one site? If so, here's one approach. SELECT DISTINCT(City), COUNT(1) AS Sites FROM ( SELECT EmployeeID AS id, City FROM Employee UNION SELECT VolunteerID AS id, City FROM Volunteer ) z GROUP BY City
thought this tone was familiar in the article..
first, DISTINCT is ~not~ a function, so don't snuggle the first column in parentheses right behind it second, GROUP BY produces distinct results, there is no need for DISTINCT finally, your assumption that all cities have at least one site is problematic (i.e. could be wrong)
You should describe the issue if you want some help. 
There are tons of errors that I am getting such as: ORA-02291: integrity constraint (CM320G16.SYS_C00193294) violated - parent key not found ERROR at line 1: ORA-00917: missing comma ERROR at line 1: ORA-01861: literal does not match format string It might be easier to see my code and spool file
First error is related to a foreign key constraint. Sometimes people are inserting data in the wrong order and they get this error. Third error is most likely a pattern error in a to_char or to_date-function call. I can take a look but I don't have access to a DBMS until Monday.
Makea new post with links to all your code and full errors.
If you need help, try an IDE. Oracle also gives you a graphical user interface, Oracle SQL Developer. When you run commands, it will show you graphically where your syntax errors are. We can also parse and show 'grammar' issues as you type them. Additionally, there are wizards for creating objects, so you concentrate more on the logic than the syntax.
Don't think you can do it with 'straight' SQL. You'll need to use PL/SQL and the UTL_FILE package and even then create some database objects, like DIRECTORY. You might have more luck with Stored Java Procedures, which lets you run, guess...JAVA, via the database. And then you can call that via a PL/SQL object. But JUST SQL, don't think it will work. I'm with the others here, I think there are better ways to build this.
Try using a WHERE clause with a BETWEEN on the orderDate. Just leave out your GROUP BY. You don't need to GROUP BY, and you don't have any orderDate values equal to 2003.
Select ordernumber from orders where orderdate = 2003
I think the delete is where you need the sub query. Delete person From person Where SSN IN (SELECT SSN FROM Person LEFT JOIN Lease ON Person.SSN = Lease.SSN WHERE Lease.SSN IS NULL) 
&gt; Delete person &gt; &gt; &gt; &gt; From person &gt; &gt; &gt; &gt; Where SSN IN &gt; &gt; &gt; &gt; (SELECT SSN FROM Person LEFT JOIN Lease ON Person.SSN = Lease.SSN WHERE Lease.SSN IS NULL) Awesome! I had to add Person.SSN in the SELECT statement in the subquery, but it worked. Thank you. Now I just need help with the other two! lol
This wont work, maybe you mean Select ordernumber from orders where YEAR(orderdate) = 2003 But, better use a date range query else mysql cant use an index
Good points, all somewhat overshadowed by the snarky tone. Brushing off parallelism was kind of funny. With SSD storage the bottlenecks are shifting to CPU. Seems to also kind of ignore the fact that most people don't want to shift data out of their OLTP system to perform business processes (i.e., number crunching) when it's necessary. Anyways, I like and use both, for different reasons, and scenarios. I think there may also be an issue with his sample csv file, I was going to give it a try with SSIS, but it looks like binary.
U/r3br is correct. Fuck the subquery. Just use a left join. I suppose you could use a count and group by clause on top of just selecting an inner join where ssns don't exist. But this in general is shitty query writing. Go on stack overflow. Their resources are great. Also: your instructor sounds like they don't know what the fuck they're doing. I'd suggest finding better resources if you actually want to learn this stuff.
I figured out how to do that one using a subquery to join the tables and find where the SSN is null. 
Do all of the "entry" tables have the same number/format of columns?
in SQL Server i'd write something like: select OrderDate, OrderNumber FROM orders WHERE OrderDate LIKE '2003%' Order by OrderDate asc this should give you all orders in the year 2003 in the right order. Edit: i don't know MySQL syntax either but I'd imagine something along these lines would work :) 
Converse to what the article says, you need either instance name (assuming SQL Browser is working) or port number, not both. 
Much appreciated. I have a bead on PIVOT and UNPIVOT now, but since I need to query dynamically (I essentially have three different tables with each their different number of "FIELDx" columns. I solved it in VB code, but I have to admit I really wanted to build this as a query instead.
I am just a student in college still and we have 4 hours of SQL a week in class. From a college perspective people hate it because it's hard and requires allot of work outsider class to get the hang of. At least it did for me, I hated it at first but because I had to pass a SQL exam to get to my final year of college I was forced study outside class since I was REALLY bad at it. After spending time getting to grips with the basics and teaching myself at home I started to really enjoy SQL and am considering a career as a DBA, though I still have a long way to go before I would consider myself good at it. 
Exactly. Start &gt; Run &gt; cliconfg
The person who created this problem made a really bad example by using SSN as a key. Please do not ever do this in a production database. SSN should be in as few tables as necessary, and those tables should be readable only by people who absolutely positively must have access to SSN information. Where I work, the borrower name and address requires special access (which I have). Everything else is keyed to the Loan Number. A more real-world example would key this to a TenantNumber, CustomerNumber or similar field. Just sayin.
I'm assuming the question is how do you get the ID you just inserted for the FK... MySQL - [LAST_INSERT_ID()](http://dev.mysql.com/doc/refman/5.0/en/information-functions.html#function_last-insert-id) MSSQL - [SCOPE_IDENTITY()](http://msdn.microsoft.com/en-us/library/ms190315.aspx) PostgresSQL - SERIAL and [curval()](http://www.postgresql.org/docs/9.3/static/functions-sequence.html)
SSMS will tell you what indexes you need. Just make a query then right click and display the Execution Plan.
OK, this has helped me greatly! I had to use mysqli_insert_id()
You can also set your fk variable to = @@identity and it will set it to the last inserted value
&gt; PL/PGSQL: this is PostgreSQL's native procedural language. It's like Oracle's PL/SQL, but more modern and feature-complete. Complete and utter bollocks! This is one of MANY incorrect comments in this article.
Yes it is. You can use xp_cmdshell to run a command i in a Shell. http://stackoverflow.com/questions/10501274/sql-server-2008-copy-a-file-and-rename-it
Id start In PowerShell and use the Invoke-SqlCommand cmdlet. Pretty sure there is a way to return the query results as an array and then copy the files using a for-each loop over the array. This also sounds like a task for SSIS, but I'm not great with SSIS so hopefully somebody else can help you with the implementation there
Yes, my first thought for this would be SSIS.
Cursor might not be the best way of doing it, but as a one off script it shouldn't be harmful. Make sure the command string is valid before you remove the "Top 1" and uncomment the xp_cmdshell line. declare @filename varchar(255) declare @cmd varhcar(1024) declare c cursor for select filename from documents where item in (1,2,3) open c fetch next from c into @filename while @@fetchstatus = 0 begin set @cmd = "copy ... ..." select @cmd -- exec xp_cmdshell @cmd fetch next from c into @fn end close c deallocate c
I have never used PostgreSQL, but from this article I'm definitely going to check it out. I can tell though that some of this is totally slanted towards PostreSQL and not completely accurate, but still gives me a few reasons to at least look at PostgreSQL.
I haven't used Oracle in years, but one thing I've heard from friends who have made such a transition is to see what the standard operating system is at the Oracle shop. It's given an MS SQL shop will run Windows, but I've seen many Oracle shops run Linux, Mac, or even some Unix variant instead. From a workstation standpoint that might be good to know so you can start acclimating yourself on the new environment if it isn't Windows.
That's a perfect question! I've played around on linux at home and can (slowly) find my way around Bash. By no means would I say I'm a regular user of it. That's a really good question to look for. Thankyou!
I can get behind that. In fact, I've never looked at it from that security perspective. My DBA isn't too big into security, so it doesn't surprise me that it's never come up.
I'm an MS SQL DBA by day, but at home it's all Mac and Linux. They're all different for sure, but given you have a decent computer background (which I assume you do since you're on Reddit) none of them are hard to learn. If it is Linux I bet you won't need to go into shell much if at all. It's been years since I used Oracle, but last time I did they had a Java-based tool and a web-based interface to manage the database server. That was 10+ years ago, so I'm unsure if that's changed. 
*My DBA isn't too big into security, so it doesn't surprise me that it's never come up.* This is scary... Security should be paramount when it comes to maintaining any database server, especially if it contains sensitive information about customers (birth dates, SSN, bank info, etc). Just saying. 
I know and agree. There are some precautions in place. But I wouldn't feel super secure with it. I'm working on making my situation better.
Use PowerShell. 'ls' is an alias of 'dir'.
Lol, it is; Don't hack me bro. How do you know of it?
I work with tmw data all day long, plus your query ran fine with our data ha
Well we're here to bounce such questions to if you ever have any :-D
you killed it. It's a linux environment. Which they're still okay with Me playing around on the command line was enough to keep them happy. All interview rounds are complete! I've gotta wait it out by the phone... Hopefully later this week.
I made the transition but as a developer not a DBA. Code was all similar, just minor syntax differences, temp tables are totally different (though CTEs can get you mostly the same thing), and some window functions in Oracle not in SQL Server. 
Good. It seems like its not a huge change. But will have some growing pains to get used to
I recently did. I hate Oracle but it has its perks. Granted MSSQL has many awesome features that I'll miss. One big issue is there is no Profiler that I've found for Oracle. SQL Profiler was the most amazing thing ever.
Good luck, and please post an update if you get it. I'll send tons of rainbows and unicorns our way for luck X-D Oh, and speaking as a Linux nerd what distro are they using? I'd assume Red Hat or possible CentOS. I believe both are certified to run with Oracle.
I switched back and forth between the two. I started with Oracle, then switched to MSSQL (but never left Oracle completely) and then back to Oracle. If I have to choose I'll always go the Oracle way. It just feels more complete. MSSQL implementations sometimes felt like "Oracle has it so we should have it too, but in a completely different fucked up way" (e.g. trunc() of date-columns was a pain in MSSQL imho). I'm not a DBA but an engineer/developer for data warehouse.
Red hat 5. That number might be wrong. I'm still in bed. My notebook is downstairs.
Might not be understanding your question? I did create a table with holiday dates that I could join against.
Can you show us what describe table says about orderDate as well as showing us some sample data: describe orders; I'm only interested in the one line of output that is specific for orderDate. select orderDate from orders where orderDate IS NOT NULL limit 10;
Simple way is to give your item and user table an id field, then you want a lookup table, essentially. a table like CREATE TABLE user_to_item ( user_id int, item_id int, ); That way you can join on all three to get a summary of what a certain user may have: SELECT * FROM user_to_item INNER JOIN item ON user_to_item.item_id = item.id INNER JOIN user ON user_to_item.user_id = user.id WHERE user.name like 'Guy who should have fish' Does that make sense? //Edit, query example, "Who has a goat?" SELECT user.id, user.name FROM user_to_item INNER JOIN item ON user_to_item.item_id = item.id INNER JOIN user ON user_to_item.user_id = user.id WHERE item.name = 'Goat' /* or get it by its id */
&gt; Is there a more efficient way to only join when a previous Left Join had no data? two separate queries
WHERE (Inspection_Date&lt;=Date()-365 OR Next_Inspection&gt;=[Start Date] And *Next_Inspection*&lt;=[End Date]);
Either side of the "AND" has to have a boolean expression. A boolean expression is something that evaluates to true or false. Next_Inspection&gt;=[Start Date] Is a boolean expression, if the next_Inspection value is &gt;= the StartDate that evaluates to true, otherwise to false. &lt;=[End Date] Is not a boolean expression as it evaluates to nothing since you're not comparing anything before the &lt;=. So in order to make this a valid statement you would need to change it to this: Next_Inspection &gt;= [Start Date] AND Next_Inspection &lt;= [End Date]
Brilliant! It works!
Awesome - that's solved it.
You could also use "Next_Inspection BETWEEN [Start Date] AND [End Date]"
Google 'many to many relationship'. You'll need a linking table as /u/DUBYATOO said.
Do you know a good resource to research how to do SSIS?
Don't worry about efficiency, and program for ease of reading the final program.
Thinking about efficiency is awesome, but you really have two choices, keep working on an elegant solution that gets you the right answer most efficiently, or start with a brute force approach that at least gets you an answer and make it more efficient as you go. For your particular case, you still don't mention *what* you're trying to gather with this query. Are you trying to present a list of the resources the player still needs to acquire before they can buy this building? Are you trying to answer yes/no whether this player can buy this building? 
For example: Building 1: House Cost 1: 10 logs Cost 2: 10 stone Building 2: Woodcutter's Lodge Cost 1: 25 logs Cost 2: 20 stone etc etc So ideally, I'd like to have: Tent: {[logs,10][stone,10]} etc etc I don't know if MySQL has the capability to produce a result like this. It'd be great if it did, but worst case, I can run a single query for each building.
This is what kills devs. No offense, but in reality you are probably the only person that is going to play it. Get an ugly solution that works and if it does take off then you can go back and clean it up.
There is already interest in the game, and the single player version saw several thousand players, so I know there will be others on the game. I'd like to do it the most efficient way not only in case it takes off, but for my own knowledge.
I think a left join is likely going to be my solution here. My biggest concern was in the extra processing that PHP would require, and I was hoping to try and find a native solution within SQL that could cut out the middle man. Thanks for such a detailed write-up. Hopefully I can work my way through this part of the dev process and put it behind me nice and quickly!
 function getBuildingCosts(Cache $cache, Db $db) { $building_costs = $cache-&gt;get('building_costs'); if (false === $building_costs) { $building_costs = array(); $sql = "SELECT b.id AS building_id, b.name AS building_name, r.id AS resource_id, r.name AS resource_name, bc.quantity FROM building b JOIN building_cost bc ON bc.building_id = b.id JOIN resource r ON bc.rescource_id = r.id ORDER BY b.id, r.name;"; $results = $db-&gt;fetchRows($sql); foreach ($results as $result) { $building_id = (int)$result['building_id']; $resource_id = (int)$result['resource_id']; if (!isset($building_costs[$building_id])) { $building_costs[$building_id] = array( 'id' =&gt; $building_id, 'name' =&gt; $result['building_name'], 'costs' =&gt; array() ); } $building_costs[$building_id]['costs'][$resource_id] = array( 'id' =&gt; $resource_id, 'name' =&gt; $result['resource_name'], 'quantity' =&gt; (int)$result['quantity'] ); } $cache-&gt;save('building_costs', $building_costs); } return $building_costs; }
If you're truly interested in efficiency then using multiple queries, combining the data in php, caching the result and triggering updates on modification would be the absolute best option; and a key-value store like Redis would be perfect to that end. There's really no point in wasting resources to compute/retrieve a dataset that rarely changes (presumably).
You're right, it won't change further into development. I guess I could always incorporate an "Update Cache" function accessible only by admins which could alleviate the issue...
What I'd recommend you want to do is have a table for resources (logs, stone, iron, etc), a table for buildings, then have a third table which is an intersection table. This table would have 3 (possibly four depending on how you feel about compound keys) columns: building_id (a foreign key to the buildings table's id column), resource_id (a foreign key to the resources table) and a resource_count column.
I don't even see it on the page. Where is it?
Two unrelated questions: in your first query you have "order by 2 desc", how does the 2 affect the query? Also, why do you use apostrophes for your thousands separator?
Precisely how I've structured it! What I've ended up doing is to pull the data with a query joining the three tables, then used a loop to reconstruct it into an array, creating a single entry for each unique building. The costs have been reassembled into an array for ease of use later in the code. This looks to be the best option at this point in time, but I can probably improve it later on.
Its most likely going to be negligible... I wouldnt worry about that processing time for php.
 what is the purpose of the pl/sql procedure 
&gt;"Inspection_Date&lt;=[End_Date]-365" I don't know about access, but in SQL it would be either: [Inspection_Date] &lt;= dateadd(dd,-365,[End_Date]) [Inspection_Date] &lt;= dateadd(yyyy,-1,[End_Date]) [Inspection_Date] &lt;= dateadd(mm,-12,[End_Date]) The [dateadd](http://msdn.microsoft.com/en-us/library/ms186819.aspx) function on MSDN
Chances are I'll cache the results later anyway, so that should solve that problem. Thanks to you and everyone else for the help, it's great to finally have a method in place!
&gt; in your first query you have "order by 2 desc", how does the 2 affect the query? 2 is a (1-based) column index referencing a column from the `SELECT` clause by index. This has always been part of the SQL standard and is supported in all databases. It can be very useful for quick ad-hoc queries where you don't want to type the full column name, or where you don't have a column name (like in the example where 2 references the `avg(govt_debt)` column expression), or where you're not sure what the column name might be (e.g. in the case of ill-defined `UNION`s) &gt; Also, why do you use apostrophes for your thousands separator? Because I'm Swiss... :-) [Just realised that we're the only ones on the planet doing that](http://en.wikipedia.org/wiki/Decimal_mark#Examples_of_use). Will try to avoid this in the future
It was there in an older version of the page.
If you're not selecting anything from plink, use a "where exists".
aha, i think that makes sense. thank you very much. that and the other guy mentioning "many to many relationship" has sealed the deal for me. much appreciated.
brilliant. i think knowing the actual terminology has helped a LOT in getting some working examples etc. thank you very much :)
This procedure updates pay rates for Salaried and Hourly paid records where the effective date of a record is 180 days old. It flags any records whre the pay_type is not recognised. The procedure is fed the new rates calculation value via paramter1 and parameter2. 
You appear to be missing the Packages table. • *Packing tasks result in the creation of packages* missing Packages table • *The packing list describes the ideal contents of each package, but it is not always possible to include the ideal number of each item. Therefore, the actual items included in each package should be tracked.* should be in Packages table Its been a while since I did ERD, but shouldn't there be indicators for link type between the other tables... 
This I didn't know about Oracle. I love my Profiler, so bummer if Oracle doesn't have an equivalent tool.
I read that it does but it's not free or cheap
Yea, I think he is missing the relationship symbols between item and packing. Also, its not clear how packing and task relate.
The fastest way to do this would be to index C and use a subquery in the WHERE &gt; Step 1: Create the index, **only required once**. CREATE NONCLUSTERED INDEX IX_YourTable_C ON dbo.YourTable (C DESC) &gt; Step 2: SELECT * FROM dbo.YourTable WHERE C = (SELECT MAX(C) FROM dbo.YourTable WHERE c&lt;1000000) The reason this will be the most efficient is that outside finding the MAX, which is going to require a scan no matter what, will be the most expensive operation. NOTE: The index may slow insertions and you may want to remove the DESC
Is this MS SQL or Access? The two are not interchangeable. You didn't ask about this, but I have to point it out. You're using `TOP 1` without an `ORDER BY`. **Ordering is never guaranteed** without `ORDER BY` (at least in MS SQL Server) so your `TOP 1` can theoretically produce varying results, which you may not expect.
Use pastebin to upload
Yeah, that's why I love MS SQL... out of the box you get reporting, ETL, data analysis, trace/profiling tools, performance analysis, scheduling, replication, and much more. Oracle and DB2 have these but they're all at an extra cost on top of just the data engine.
You should be using a stored procedure for this. Send us the query not your code. Your code is ripe for sql injection.
I don't use Oracle, but I did find this on Stackoverflow where someone asked the same question with lots of results: http://stackoverflow.com/questions/148648/oracle-is-there-a-tool-to-trace-queries-like-profiler-for-sql-server Also this was on the Oracle site: https://docs.oracle.com/cd/B28359_01/appdev.111/b28424/adfns_profiler.htm We were just talking about this in another post, so I'd be curious to know as well since I find Profiler to be an invaluable tool in MS SQL. 
Index rebuilds, especially for a single table, should not take very long unless they are truly massive or go across several partitions. If you want more verbose output rebuild it via a query instead of the SSMS menu. Example: *** USE YourDatabase; GO ALTER INDEX Name_Of_Index_Here ON SchemaNameHere.TableNameHere REBUILD; GO *** If you are lazy like me, here is a method to generate the SQL for all the indexes/tables in a specified database *** USE YourDatabaseHere GO -- Create a NVARCHAR to hold all the commands DECLARE @indexRebuildString NVARCHAR(MAX) = N''; -- SELECT the value into the string and concat them all together SELECT @indexRebuildString += N'ALTER INDEX all ON ' + name + ' REBUILD; ' FROM sys.tables; -- SELECT the result, copy and paste this into a new query window SELECT @indexRebuildString -- Even more lazy uncomment below to have the query run automatically -- EXEC sp_executesql @indexRebuildString; ***
Security is not an issue as it's an internal system
I have this Job running nightly on our DB with frequently updated data (it takes about 15 minutes): DECLARE @Database VARCHAR(255) DECLARE @Table VARCHAR(255) DECLARE @cmd NVARCHAR(500) DECLARE @fillfactor INT SET @fillfactor = 90 DECLARE DatabaseCursor CURSOR FOR SELECT name FROM MASTER.dbo.sysdatabases --WHERE name NOT IN ('master','msdb','tempdb','model','distribution') WHERE name IN ('sfdc') ORDER BY 1 OPEN DatabaseCursor FETCH NEXT FROM DatabaseCursor INTO @Database WHILE @@FETCH_STATUS = 0 BEGIN SET @cmd = 'DECLARE TableCursor CURSOR FOR SELECT ''['' + table_catalog + ''].['' + table_schema + ''].['' + table_name + '']'' as tableName FROM ' + @Database + '.INFORMATION_SCHEMA.TABLES WHERE table_type = ''BASE TABLE''' -- create table cursor EXEC (@cmd) OPEN TableCursor FETCH NEXT FROM TableCursor INTO @Table WHILE @@FETCH_STATUS = 0 BEGIN IF (@@MICROSOFTVERSION / POWER(2, 24) &gt;= 9) BEGIN -- SQL 2005 or higher command SET @cmd = 'ALTER INDEX ALL ON ' + @Table + ' REBUILD WITH (FILLFACTOR = ' + CONVERT(VARCHAR(3),@fillfactor) + ')' EXEC (@cmd) END ELSE BEGIN -- SQL 2000 command DBCC DBREINDEX(@Table,' ',@fillfactor) END FETCH NEXT FROM TableCursor INTO @Table END CLOSE TableCursor DEALLOCATE TableCursor FETCH NEXT FROM DatabaseCursor INTO @Database END CLOSE DatabaseCursor DEALLOCATE DatabaseCursor
I got it this is the final result: $sql = " UPDATE cssTable SET asset_location='{$_POST['asset_location']}', asset_status='{$_POST['asset_status']}' WHERE asset_id= '{$_POST['AID']}' ";
For REBUILD be very careful. If you do an offline REBUILD, which is the default, it will take the index offline so the table may not be available while it's running. You can kill it at any time, but just know if you have an OLAP system using the table it may have issues. You can do an ONLINE rebuild, but this works by copying the index, rebuilding the copy, then overwriting the current index with the rebuilt index. The caveat is for large indexes you need to be sure you have ample space in your LOG file and that it can grow if needed. My rule of thumb is I reorganize when the index is between 5% and 39% and rebuild when 40% or higher. For indexes that are fragmented less than 5% you don't need to bother with those. Also you don't need to worry about indexes with say 50 pages or less, these will almost always be fragmented which isn't an issue on smaller indexes. Think about it, a table that has 3 pages will generally be 33% or 66% fragmented at all times. Here's my script to do all this, if anyone's interested. Running nightly the fragmentation should stay fairly low so only a reorganization is needed, but on the first run some will do rebuilds. I commented out the EXECUTE portions, so running this on a SQL Server will only give you the script that'll run. You can review it and see what work it would do and manually decide if you want to rebuild or reorg. DECLARE @IndexTable TABLE (RebuildScript VARCHAR(255), Index_PageCount INTEGER, FragPct NUMERIC(18,6)); DECLARE @Query NVARCHAR(500); INSERT INTO @IndexTable SELECT 'ALTER INDEX [' + b.name + '] ON [' + s.name + '].[' + o.name + '] ' + CASE WHEN avg_fragmentation_in_percent &gt;= 40 THEN ' REBUILD WITH (ONLINE = ON)' ELSE ' REORGANIZE' END + '; --' + CONVERT(VARCHAR(10),page_count) + ' / ' + CONVERT(VARCHAR(10),avg_fragmentation_in_percent), a.page_count, avg_fragmentation_in_percent FROM sys.dm_db_index_physical_stats (db_id(), NULL, NULL, NULL, NULL) AS a JOIN sys.indexes AS b ON a.OBJECT_ID = b.object_id AND a.index_id = b.index_id JOIN sys.objects AS o ON a.OBJECT_ID = o.object_ID JOIN sys.schemas AS s ON o.schema_id = s.schema_id WHERE b.index_id &lt;&gt; 0 AND avg_fragmentation_in_percent &gt; 5 AND a.page_count &gt; 50 ORDER BY a.page_count; SELECT * FROM @IndexTable WHILE (SELECT COUNT(1) FROM @IndexTable) &gt; 0 BEGIN SELECT TOP 1 @Query = RebuildScript FROM @IndexTable; -- EXECUTE sp_executesql @Query; PRINT @Query DELETE FROM @IndexTable WHERE RebuildScript = @Query; END I like this method because when running manually you can see everything it'll do before it runs. 
I can't say that I've seen any other company's database tools rival what I have at my disposal with MS SQL. Like I said in the prior thread with MS SQL Server Standard Edition, which is pennies compared to competitors, you can build and distribute reports, run ETL processes, do performance analysis with SQL Profile, do DW with Analysis Services, schedule jobs with SQL Server Agent, Service Broker is wonderful, data Replication, and the list goes on. These are all completely separately licensed applications on the DB2 and Oracle fronts. I think the only exception to this is IBM Change Data Capture which is IBM's replication tool. They purchased it from DataMirror about seven years ago, and it's by far the best replication tool I've used even between MS SQL servers. 
Is this an OLTP or OLAP database? If OLTP I'd definitely split the data into its own table instead of duplicating it. Duplicated data in an OLTP database is just asking for trouble because if something gets out of sync it could be a nightmare to track down. Also this would satisfy 3rd form normalization. Now if it's an OLAP database for reporting or DW then duplicating data is more then okay, it's almost suggested. 
Well, that led to a couple of google searches. It's transactional, but I did think about that and I guess should have been more explicit. No information would actually be duplicated, just the column names would be duplicated. So, a simple example would be something like voting on books and movies and cacheing the totals for kids vs adults so: p_key book_name total_score kids_score adults_score p_key movie_name total_score kids_score adults_score vs p_key book_name p_key movie_name poly_id poly_type total_score kids_score adults_score Actually, in this case I guess it's best to do poly_id poly_type score category So in no case can I see how info can get duplicated. And perhaps importantly there's no cross comparisons. So books are only compared to other books and movies are compared only to movies. And the other ways the two would (might) interact wouldn't need to be as immediate. I guess though I do have to stick with the last format, since added star ratings or another category would cause the number of columns to grow factorially and that's not reasonable.
Use OLA Hallengren's Scripts: https://ola.hallengren.com/
Is there a reason you are using this design?
I followed your first example and got the output: Command(s) completed successfully. - After 15 seconds. However when I check the fragmentation on this index it's hasn't changed at all? EDIT: Okay - It appears to be working on some indexes but not others, any ideas?
&gt; If you do an offline REBUILD, which is the default, IIRC, offline rebuilds are also the *only* option unless you're running Expensive Edition, at least through 2012 (haven't checked 2014).
The individual table who's index is being rebuild will be taken offline 
Which database? Distinct isn't parenthesised. Check string concatenation operator isn't || rather than + on your platform. You must give constant values a column name e.g. 10 as MYCOL
Yes I believe this is true. I though there was some restriction on running offline rebuilds, but I couldn't remember what it was when I typed my post.
Look for things that are duplicated. That's all your homework that I'm doing for you.
Packing is simply one type of Task. When a Task is a Packing task, there is an associated Package. • *For all tasks of type “packing,” there is a packing list that specifies the contents of the packages.* Though it also looks like they want another table slipped in there somewhere that I cannot quite get my head around without spending more time than I have.
So, it looks like you're coming from a different programming language... The following is for SQLServer create table #testA ( testAid int identity(1,1), a1 varchar(100), a2 varchar(100), a3 int, a4 varchar(100)) create table #testB ( testBid int identity(1,1), b1 varchar(100), b2 varchar(100), b3 int) insert into #testB (b1,b2,b3) values ('apple', 'is not', 5) , ('box', 'flight', 1) , ('sue', 'breathing heavily', 10) insert into #testA (a1,a2,a3,a4) select 'someText', b1, b3, b2 + ' some other text' from #testB select * from #testA 
(drID , drName , Act) would be the candidate key right?
using the as MYCOL allowed me to execute the statement, but all ints i tried to enter statically showed up as 0. So close!
I'm using MicrosoftSQL which has similar syntax to MySQL. I am not familiar with SQLServer, but that syntax does not work in this instance. Thanks for replying, though!
Would you store Acct info on a table of Drs? I don't want to give the answer away. 
SQL Server, as in Microsoft SQL Server... I assure you the syntax is good because I tested it before posting it.
What does your query look like now? What are the datatypes you're inserting from and into (post your table definitions)? Provide some actual test data (refer to my post below or use [sqlfiddle](http://sqlfiddle.com/)). Edit: When inserting into another table, you don't have to alias column names that are being selected (though it is, likely, best practice) for a query to execute. If I were to rewrite your example so that it works it would look like this... insert into Table1 (c1, c2, c3, c4) select distinct 'someText', d1, 5, d2 + 'someOtherText' from Table2; that's assuming a couple things (data types between tables match, d2 is actually a varchar so that appending a varchar doesn't throw an implicit cast error, etc)
Trust me, your main hackers are going to be disgruntled employees, current or otherwise.
You kind of did. :)
Check to see what the [pages](http://stackoverflow.com/questions/11946957/sql-server-number-of-8k-pages-used-by-a-table-and-or-database) are on some tables/index, or let alone how many pages there are. Fragmentation on a couple hundred pages is unavoidable and nothing to worry about. 
If you want to learn quickly on the cheap I'd suggest finding some videos on YouTube. Lots of videos showing basics to advanced stuff. Also I don't mind giving you some pointers, I use both daily. Just let me know some specifics on what type of projects you plan on tackling.
I took an online class from cvktech.com for informatica and one for Ab Initio. It was 400 dollars for a month. That's 20 sessions one on one with a trainer who's a professional ETL developer. Only hitch is they work in India and sometimes they don't have a reliable connection, so the sessions can be drawn out due to somewhat frequent technical difficulties. Classes are late in the evening so they can give them early in the moring since there's a 13 hour time difference. 400 is definitely a fair price. Also, you can opt for a group session for less, but then you have to wait a while until they get enough people to start one. 
A fully licensed enterprise manager implementation rivals if not outshines SQL profiler in my opinion( I work on both oracle and SQL server). The downside is it's mega-uber bucks to license all the packs for it to be useful. There is a limited bit of functionality in oracle SQL developer but it's very bare bones. I'm sorry I don't have a better answer but since I get to use the super expensive tool I've never needed anything else. 
That's more helpful than you realize. Thanks!
Learn SSIS but instead of SSRS, I would recommend learning about PowerBI instead (Power View, Power Query, Power Pivot and Power Map). SSRS was great about 5 years ago and still is well suited for specific reporting solutions but the Power BI suite is what will become the new standard.
Little more data. This is for an iPhone. I would like to be able to edit deleted texts. 
Sidebar: &gt; Learning SQL &gt; A common question is how to learn SQL. Please view the Wiki for online resources. &gt; Note /r/SQL does not allow links to basic tutorials to be posted here. Please see this discussion. You should post these to /r/learnsql instead. link to discussion mentioned: http://www.reddit.com/r/SQL/comments/2jcw2y/what_do_we_think_about_tutorials_especially_basic/
I already took the class, I am looking for advice for the test it self. From people hopefully have taken it before.
I just took 1Z0-061, which is for 12c but is, in terms of questioning, a good reference for the 11g one you are about to take. The most important advice I can give you is: Sleep well the night before. You will get 75 questions, to be answered in 2 hours. The amount of text you need to process is huge and the difference in possible answers can be quiet hard to find. Although I am pretty good with SQL, this is the hardest exam I have taken so far.
unless you have an ORDER BY clause, the order is "indeterminate" you ~might~ get the results in the order the union queries were written, and you might not and if you do go for the ORDER BY clause, you don't need to do any splitting
 SELECT * FROM Products WHERE strCompanyName LIKE '%something%' OR strProductName LIKE '%something%' ORDER BY CASE WHEN strProductName LIKE '%something%' THEN 1 ELSE 0 END DESC , CASE WHEN strCompanyName LIKE '%something%' THEN 1 ELSE 0 END DESC , CASE WHEN strDescrition LIKE '%something%' THEN 1 ELSE 0 END DESC 
 For example: SELECT p.* FROM Products p WHERE strCompanyName LIKE '%something%' OR strProductName LIKE '%something%' ORDER BY case when strCompanyName LIKE '%something%' then 1 When strProductName LIKE '%something%' then 2 Else 10 end asc, strProductName ASC
Why have you used the alias for products?
although UNION removes duplicates, this union query ~will~ duplicate any product that satisfies more than one of these conditions the dupes are a direct result of your [OrderBy] column so that's a non-starter, unless of course OP agrees that a product showing up more than once in the results is a good idea by the way, this is a platform neutral subreddit, so your use of microsoft non-standard syntax is, um, troubling
There's nothing wrong with aliasing your table names in your queries, even queries as small as this. It's a good habit to get into for larger, more complex queries - it makes them much easier to read. What *should* be avoided, however, is `select *` in any kind of production code.
Wrong. Security is **ALWAYS AN ISSUE**. One piece of malware gets on someone's desktop and your whole system is p0wn3d because it found your insecure web app.
ask OP, not me ... this is how i interpreted his requirement 
This app sounds like a cornucopia of antipatterns. Don't use `select *`. It may prevent your database from creating a good query plan and you will get unexpected data when someone changes the table you're selecting from. If you need 3 fields from that table, select those 3 fields (even if they're the only 3 fields on it today) only.
I think what I mean is something along the lines of SELECT * FROM Products WHERE strCompanyName LIKE '%something%' OR strProductName LIKE '%something%' ORDER BY strProductName, strCompanyName, strDescrition DESC rather than doing the whole LIKE operator. I'm just trying to figure out the difference.
this ORDER BY does not sort the matched products first, it sorts all products based on their name only, regardless of whether the name matched also, in this case, the 3rd ORDER BY column is useless/redundant (unless the same product from the same company is in the table more than once, with different descriptions) 
I knew of SSIS when I started my current job (about 2.5 years ago), had not used it before. Personally, I learn better by trying things and seeing how they work, ultimately messing something up and then fixing it. Below is merely an outline of how I learned how to utilize SSIS. It won't be very specific but, in my opinion, it's a good place to start as far as understanding what you can/can't do. Once you understand the basics of SSIS, it isn't that complicated. Typically you'll want to transfer data from one source to another, so you use a data flow task. Inside of that task, you'll have, at the minimum a source and a destination (I would start with simply transferring data from one table to another to get the feel of how it does things). This would, at the very basic level, be your E and L in ETL. Once you get the hang of that, then there are many different transformation tools you can use to transform data between the source and destination components. Derived Columns for instance allow you to add columns to the data, these new columns can be based on other columns or even variables present in your package. Besides Data Flow Tasks (I call them DFTs), there are many other components you can use, Execute SQL Task and Script Task are the two non DFT components I use every day. Execute SQL Task is just what it sounds like, it executes a block of SQL. Script Tasks are probably the more complicated component as it allows you to use VB or C# to perform various tasks that you may not otherwise be able to accomplish with the other components. That all said, there are certain.... nuances... with SSIS that you'll likely run across and have to rely on stackoverflow or google for helping you find an answer. Null values from SQL to SSIS variables is one of those instances that I can remember off the top of my head.
Packing should be the data for the packing list. Since there are many types of tasks, you wouldn't want to duplicate the task just because of its type. Likely the insert into this table would be via application logic or trigger on Tasks. Something like item -&gt; PackingListItems -&gt; PackingList -&gt; Task -&gt; Volunteers * Item is your catalogue of items. * PackingListItems contains ItemIDs and associated PackingListIDs for the items that have been filled to that specific packinglist. * PackingList is the detailed information (might need a PackingListRequirements table that stores what ItemIDs the PackingList actually Requires) * Tasks are all of your tasks and their codes * Volunteers are the volunteers.
OK thanks
 CREATE TABLE #TEMP ( [COLUMN_NAME] VARCHAR(10) NOT NULL) ON [PRIMARY] DECLARE @YES CHAR(8) SET @YES = 'YES' IF @YES = 'YES' GOTO A ELSE GOTO B A: INSERT INTO #TEMP ([COLUMN_NAME]) SELECT 'SAYS YES' GOTO C B: INSERT INTO #TEMP ([COLUMN_NAME]) SELECT 'NOT YES' C: That's how I would code it. The problem with what you've written is that you should turn the **INTO #Temp** statements into an **INSERT INTO #Temp** statement like the one below: INSERT INTO #Temp SELECT 'SAYS YES' However, in order to do this, you will need to create the table #Temp prior to this step. 
That (a habit) more than anything else. I also deal often with a dozen or more tables joined and I'm not really interested in columns from all of those, so it is of marginal utility too.
you need a LEFT OUTER JOIN with an IS NULL check
 select a.AllParts from ( select mp1.PartID as AllParts, mp2.PartID as ModelParts from (select distinct PartID from ModelParts) mp1 left outer join (selet distinct PartID from ModelParts where ModelID = 3) mp2 on mp1.PartID = mp2.PartID ) a where a.ModelParts is null
* You're joining to all ModelParts records, and then filtering (`WHERE` clause) down to only the records you care about (`ModelID=3`.) Instead, you should only join to the records you care about. Change the ON to `(P.PartID = MP.PartID and MP.ModelID =3)`. Normally, either way would work, except for the next part... * You want records where that join does *not* return results. Use a `LEFT OUTER JOIN` instead, which causes the row on the left (`Parts`) to be returned even if there's no match to the right side. So in addition to what would be returned from an INNER join, you'll get a row where `P.PartID is not null and MP.PartID is null`. Use that in a WHERE clause: `WHERE MP.PartID IS NULL`. Putting those two parts together, you get: SELECT P.PartName FROM Parts P WITH (NOLOCK) LEFT OUTER JOIN ModelParts MP WITH (NOLOCK) ON (P.PartID = MP.PartID AND MP.ModelID = 3) WHERE MP.PartID IS NULL
That is some seriously fugly SQL. I'm embarrassed to read it. Also, check the sidebar to understand how to format your code on Reddit, it can make it easier for you to get help. Convert(date,GETDATE()) -- seriously? ISNULL(pa.Status,'U') = 'C' -- Really? (A null value will *never* = 'C', so don't bother with ISNULL) Perhaps the error is related to the single quotes you put around "Determination" (identifiers should never be single-quoted in SQL. Most systems use double-quotes, SQL Server uses [Square Brackets]). But I also think there's a chance that long messy string is not being parsed correctly by VBA. Try to "echo out" (Debug.Print) your query to make sure the SQL command is actually correct. Ninja edit: I see now, you're getting a compile error and not a SQL error. Definitely a problem with your query formatting -- VBA can't even interpret your string expression. Think: Are you wrapping that SQL command in single quotes or double quotes? If it's single, then you've definitely got a problem with those single quotes wrapped around 'Determination'
This is not Access SQL. What type of database are you querying? Did you pluck this query from another query application where it is known to work on the target DB?
I've only been dabbling in SQL for about 3 months now and I'm self taught so im not surprised it's fuggly. The query executed in sql server management study just fine. I'm mainly confused on how to write the VBA code so that it executed in a macro in excel. Would it be easier if I turned the query into a stored proc?
I love pretentious code writers. Offering constructive advice without being condescending is a valuable skill. 
It would have sufficient validation in the from end to ensure the correct data type reaches the back end. But in terms of security I have some research to do as I only know basic sql. Thanks again for the input!
&gt; It would have sufficient validation in the from end to ensure the correct data type reaches the back end You can never have too much validation. What if something goes wonky between the front end and the query because you have some buggy addition that turned into string concatenation? It passed validation on the "front end" but would still puke when you hit the database. Always assume that someone is trying to do evil things to your web app &amp; data. Including other people working on the code itself.
You've piqued my interest. Where can I pick up some PowerBI skills?
The challenge is that you need the calculated age to be inside the case statement, instead of patient_dob. You have the age calculation worked out, but it's not possible to "self reference" that column later in the query. Here's one approach to try. It stuffs the age calculation into a subquery, which we join back to the main table and drop into the case statement. This assumes you have a primary key column named `id` in the `pat` table. SELECT pat.patient_dob, CASE WHEN pat_age.age BETWEEN 10 AND 20 THEN '10-20' WHEN pat_age.age BETWEEN 21 AND 30 THEN '21-20' WHEN pat_age.age BETWEEN 31 AND 40 THEN '31-40' ELSE 'Deceased' END AS Age FROM pat JOIN ( SELECT id, TIMESTAMPDIFF(YEAR, pat.patient_dob, CURDATE()) AS age FROM pat ) pat_age ON pat_age.id = pat.id
 SELECT TIMESTAMPDIFF(YEAR,pat.patient_dob,CURDATE()) AS "Age", IFNULL(dept.department_name, 'Age Total') AS Department, ward.ward_name AS Ward, sum(bed_allocated_days) AS "Days Treatment", CASE WHEN TIMESTAMPDIFF(YEAR,pat.patient_dob,CURDATE()) &gt; 50 THEN 'Old' ELSE 'Not Old' END FROM... 
Your CASE is evaluating pat.patient_dob, not TIMESTAMPDIFF(YEAR, pat.patient_dob,CURDATE()), so they're all "greater than" 50, which is why everyone is 'Old', with the exception of your NULLS. If you correct that to CASE WHEN TIMESTAMPDIFF(YEAR,pat.patient_dob,CURDATE()) &gt; 50 THEN 'Old', you'll be closer. However, if you don't add the ELSE 'Not Old' (or whatever) to your CASE statement, you're going to end up with NULLS for anything &lt;50. The following query produced the desired results for me. ---------------- SELECT TIMESTAMPDIFF(YEAR,pat.patient_dob,CURDATE()) AS "Age" , IFNULL(dept.department_name, 'Age Total') AS Department , ward.ward_name AS Ward , sum(bed_allocated_days) AS "Days Treatment" , CASE WHEN TIMESTAMPDIFF(YEAR,pat.patient_dob,CURDATE()) &gt; 50 THEN 'Old' ELSE 'Not Old' END
hey, i want to exclude any entries in Column 1 linked to 999 so i don't want the output to show 1234 as it is linked to 999 the way i'm doing it now it still shows because it has other entries linked to it. hope that helps clarify things
Select * from table1 t1 where not exists (select null from table1 t2 where t1.column1 = t2.column1 and t2.column2 = 999)
that's right :)
I don't know what else to do? How else could you accumulate the grades of hundreds of students, that are taking several classes, and show all their averages in a bigger table?
 select case when Country = 'USA' then 1 else 2 end MyGroups, count(*) Records from Addresses group by case when Country = 'USA' then 1 else 2 end
Try using the floor function to group your ages I'm on my phone at the moment, so will provide an example when I get to my PC
&gt;Convert(date,GETDATE()) -- seriously? So? perfectly resonable way to strip out the time of a datetime value. pa.UpdateDate &gt;= DATEADD(day,-3,convert(date,GETDATE())) Its a sargable expression, so he is good on that front, and do I need to explain to you the difference of the convert(date, getdate()) and a getdate()? 
To group in ages bands of 5 years SELECT FLOOR([age]/5) * 5
Alternatively to the left outer join plus is null check, you could use Not Exists: select * from parts p where not exists (select * from modelparts mp where mp.partid = p.partid and mp.modelid = 3) Though I much prefer the left outer join option, myself.
I'm missing what you want to do. Are you trying to find what your new record number should be when you do an insert? What happens to each record when you do the next Ammend/Renew? Why can't you just select Max instead of doing recursion?
 SELECT column1, column2 FROM table WHERE column1 NOT IN ( SELECT column1 FROM table WHERE column2 = 999 ) You can use '999' instead of 999, I do not now if it is num or varchar. Also be careful with the NULL values when using NOT IN.
Holy shit I had no idea you could group by case.
I personally would use temp tables for this.
Searching all the tables? Don't you mean only the tables that store USER ID? Can you narrow down the search depth?
It depends on how you want to track it. If someone books say a weekly reservation starting on 2015-01-01 for 12 weeks do you want to keep this in one row with these details or do you want it to generate 12 unique entries for the reservation? I'd suggest having a Reservation table that initiates the process, then when the reservation is set a second process updates a schedule table with the individual reservations. So in the prior example this would have one entry in the Reservation table but 12 entries in the Schedule table. If the person called saying he wants to change from 9am to 10am you'd key on the Reservation able to identify all entries in the Scheduled table and change the dates on those 12. This would also allow you to satisfy someone who calls and says they just want the 3rd and 5th weeks changed to 10am but the rest lat 9am. They're still on the same Reservation, but you can have independent times per date if needed. 
&gt;Every time you reference a table in a FROM or JOIN statement you SHOULD give it a nickname. FTFY
I'd like to think this could be done in a single Select joining back to a Date Lookup Table, but I'd have to think about that for a bit. In the meantime this does it in a Loop. You'd just have to add some logic to see if the interval is Day or Week and change the DATEADD value based on this. DECLARE @Loop INTEGER = 0, @Interval INTEGER = 0; DECLARE @Schedule TABLE ( ID INTEGER NOT NULL IDENTITY(1,1), Name NVARCHAR(50), ScheduleDate DATETIME); DECLARE @Reservation TABLE ( ID INTEGER NOT NULL IDENTITY(1,1), Name NVARCHAR(50), Period NVARCHAR(50), Interval INTEGER, StartDate DATETIME); INSERT INTO @Reservation (Name, Period, Interval, StartDate) VALUES ('J Picard','Week',12,'2014-02-03 14:00:00'); SELECT @Interval = Interval, @Loop = 0 FROM @Reservation WHERE ID = 1; WHILE @Loop &lt;= @Interval - 1 BEGIN INSERT INTO @Schedule (Name, ScheduleDate) SELECT Name, DATEADD(WEEK,@loop,StartDate) FROM @Reservation; SET @Loop = @Loop + 1; END; SELECT * FROM @Schedule; ID Name ScheduleDate 1 J Picard 2014-02-03 14:00:00.000 2 J Picard 2014-02-10 14:00:00.000 3 J Picard 2014-02-17 14:00:00.000 4 J Picard 2014-02-24 14:00:00.000 5 J Picard 2014-03-03 14:00:00.000 6 J Picard 2014-03-10 14:00:00.000 7 J Picard 2014-03-17 14:00:00.000 8 J Picard 2014-03-24 14:00:00.000 9 J Picard 2014-03-31 14:00:00.000 10 J Picard 2014-04-07 14:00:00.000 11 J Picard 2014-04-14 14:00:00.000 12 J Picard 2014-04-21 14:00:00.000 Edit: I just noticed you said MySQL... I did this on MS SQL, so I'm unsure if Table Variables would work the same way in MySQL. Hopefully though you can see how to transpose this to MySQL. Please remember to put the database type in your subject when posting. 
Is there a charge to just view the queries? If not then why require logging in? There are lots of free sites online that allow people to freely share productive queries, so why wall yours off or put behind a paywall if there is a charge to view them?
You could do a case when statement that sets the number you want first as 1 then the second as 2 etc
Are you trying to have result sets display that aren't related, but in one table?
I suppose I could, but I'm unclear on how to do that even. 
Personally I'd say its a good argument for an extra field to be added just for sorting purposes, and then maybe a trigger which updates it based on GID. At least that way MSSQL can use an index when sorting (assuming you add one). &gt; Would this best be done after the query in PHP You cant do this without first selecting the full table into PHP, which is never a good idea
Do not hard code the email address.. Have the email address/es in a table, you can then add and delete as required for testing purposes, and it will be easier for future changes with config in the database and not changing code. Testing will determine if the code is working, if the code works in Test Dev environment, it should work in Prod. You could also have an audit table that lists all emails sent. This can be compared with actual emails received. Each email would surely be sent to the Database admin so that should not be too hard to determine. 
Add an OrderBy column.
Obviously the best way to get a great feeling for SQL is to use it. Write a trigger, create a function ...if you don't have time I'd google interview questions for SQL or any product using SQL. 
Thanks. Do you mean do this: http://msdn.microsoft.com/en-us/library/ms345578(v=sql.105).aspx I followed that and set it to an AD account that we've set up - svcsqlservice Still have the same issue. Any other ideas?
Use SQLFiddle (www.sqlfiddle.com) to set up real situations and play with them. When our team interviews we look for a quick study, ease of use within our environment, quality of code, and personality.
As a SQL Data Analyst I can tell you the normal SQL questions are usually pretty basic to determine your skill level. For example: What is the difference between a Union and a Join? If someone fails this question, it is a pretty good indicator of their level of knowledge. Focus more on queries that fetch data "how do you determine uniqueness?" and don't worry too much about inserts/updates or DB management - that stuff wouldn't really come up in the day to day. You should know the basics on aggregations, when to use them and why. If you want to really impress them, take some time to study Common Table Expressions. They are pretty simple to implement, but can solve some pretty gnarly problems. And be rock solid on syntax. Syntax in SQL is the easiest of all languages - pretty much by definition! So, learn where the clauses go, what order they go in. Finally, understand that as an analyst you are going to get asked the wrong question. Trust me, it happens all the time. Your job is to get more details and drill into the real reason the question is being asked - and get at a better question. EG: "Why aren't patients being billed correctly?" A: Has the business defined parameters around correct billing? Is this a timeliness issue or a contact information issue? How do you define a failure in billing? If I gave someone a question like this in an interview and they immediately tried to solve it, I'd basically pass on them. They need to be an analyst - so they better stop, think about the problem, and start planning on what information they really need. I've had interviewers get into interesting debates with me about fictional scenarios - and those are generally the ones we are interested in (because even if their SQL is rusty, we know the lights are on). Anyway, good luck!
Ooh, syntax highlighting's a good idea. Thanks for pointing this site out, I'll take a look!
Are you sure the DNS for the new server is set up properly? Can you ping it by name from remote machine? 
I didn't personally change dns, but I can ping and rdp into the box by name.
Maybe this will help... http://support.microsoft.com/kb/811889 Ignore the bits about kerberos if you're not using it. Ive seen this error before and I wasn't using Kerberos, but I stopped using MSSQL quite a while ago and cant remember what was causing it
So you want "account1" to be able to grant privileges on "table1" to all database principals except "account2" and "account3"? (Standard SQL uses "revoke", not "deny". If you happen to be using SQL Server, "deny" is deprecated.)
&gt; I want account1 to lose the ability to grant table1 access for account2 and account3 Well, something along the lines of revoke grant option for all privileges on table1 from account1 cascade; would do *that*. By design, this doesn't affect existing permissions granted to account2 and account3. You'd have to handle that separately. If you want to handle privileges for "account2" and "account3" independently of other users, I think you'll need to - create a database or application role, - assign "account2" and "account3" to that role, - update privileges for that role, and - revoke the grant option on that role from "account1".
You should set the db read only then do your backup unless you are sure no data changes will come in. Also set the ttl low on the dns record. I always cname to the original record so in the future I could just update the cname to the new server
You need a data model. Basically, a logical description of your data. How it's stored, how it's related. It SHOULD tell you exactly where you could find USER IDs. If you don't have that, you can start querying the data dictionary views, like dba_tab_cols - this is a view that stores the information for every Column in the database. If you're very lucky, the user iDs are only stored in one place, or if in multiple places, are always stored under the same column name.
 I just threw this together, let me know if this works select A.Fruit, A.eatDate, x.pickdate from tableA A cross apply ( select top 1 pickdate from tableB B where B.pickdate &lt;= A.eatdate and B.fruit = A.fruit order by b.pickdate desc )x
This is in MS SQL, and could probably be done cleaner, but this will get you the results you're looking for I think... SELECT TableA.Fruit, EatDate, MAX(PickDate) AS 'PickDate' FROM [TableA] left outer join [TableB] ON [tableA].Fruit = [TableB].Fruit WHERE TableB.PickDate &lt;= TableA.EatDAte GROUP BY TableA.Fruit, TableA.EatDate
I don't really follow what you're trying to do, but I don't see a fundamental problem with generating multiple branches at the same time, you just need to make sure there's some column value that allows you to distinguish one branch from another. In the recursive (i.e. lower) part of the query, you might need to produce the union of two different queries, to produce your two branches. A useful thing to know is that any time postgres says "you can't do this in a recursive CTE", you can instead do it in a subquery *within* the CTE. (For example: aggregate/group by, window functions, and limit can all be done this way). So your recursive CTE might end up with something like: SELECT ... FROM ... -- produce your source data here UNION ALL SELECT * FROM ( SELECT ... FROM ... -- recurse on the first branch here UNION ALL SELECT ... FROM ... -- recurse on the second branch here ) s edit: I just played around with this, and it turns out you can't refer to the recursive CTE twice within the recursive part of the query, which the above code would require you to do, but you can work around this by using a lateral join. (I tend to call my recursive CTEs "prev", because it helps me remember that I'm referring to the value from the previous recursion.) WITH RECURSIVE prev AS ( SELECT ... FROM ... -- produce your source data here UNION ALL SELECT (s).* FROM prev, LATERAL ( SELECT ... -- recurse on the first branch here UNION ALL SELECT ... -- recurse on the second branch here ) s ) Here, you can refer to prev within both of the nested queries that have comments beginning "-- recurse on", and notice that they don't require a FROM clause to do this. (In practice, you'll may still want a FROM clause here, but only to bring in whatever *other* data you need, not for prev itself.)
If PDO and your database can handle returning multiple resultsets from a single stored procedure (you can do it with SQL Server &amp; ADO.NET, I don't know about other environments), then I would wrap it all into a single stored procedure and return each resultset that you need from that SP. That way you're only making one call to the database, creating one connection, etc. Assuming that your tables are properly indexed for optimal performance with these queries and your I/O system is fast, it'll be the repeated round-trips to fetch data from 4-5 queries that will slow, not the queries themselves. On the flip side, if you've got the right indexes, doing everything in one `JOIN` may be much better performance-wise for the database - to the point where the benefit there outweighs the slight hit you take for getting all the extraneous data in each row. TL;DR: Test it both ways in your environment. And not just with a stopwatch, look at the I/O stats and execution plans. It's the only way to be sure. I would also suggest that you experiment with `INNER JOIN` vs. `NATURAL JOIN`.
It's been a while since I worked with mysql but doesn't natural join look only at foreign keys? 
perhaps this may help -- [Minimize Bandwith in One-to-Many Joins](http://sqllessons.com/sqlhack75.html) you can use UNION to replace the two queries with one, provided you can "hack" a common row layout for both queries
This may make me look foolish, but I had no idea what `NATURAL JOIN` did until I looked it up for this thread. Had never bothered to learn it on my own when I saw it on reddit, was never taught it formally, and never saw it in my professional travels. It seems...very dangerous. I'm kind of glad I never knew about it until long after I'd learned the other joins I use commonly.
It is dangerous. You're never really sure what the query engine will do. It's an interesting idea, but it ends up just not working that well. Sybase has `KEY JOIN`, which joins on foreign keys, but then you might run into a table that has two foreign keys to the same table.
The ON DELETE CASCADE is actually a clause of the FOREIGN KEY, not the TABLE definition So just set the FK up to cascade delete, and omit it from the other FK's
When I build my databases I usually build them with NATURAL JOIN in mind, often prefixing column names that could be ambiguous to avoid issues.
Cool, I'll look into that. Thanks!
&gt; I usually build them with NATURAL JOIN in mind, often prefixing column names that could be ambiguous to avoid issues. So you're doing extra work to avoid doing different work? Just because you *can* use `NATURAL JOIN` doesn't mean it's a good idea. Just do regular `INNER JOIN`s with proper foreign key relationships. Don't screw around with hoping that the database guesses the right way to do the join.
Then you're already behind the 8-ball, as you can't use stored procedures. If you're already concerned about the scalability of your app, move away from SQLite and onto a more robust RDBMS like PostgreSQL.
I'm not too concerned about scalability. Even if it's super popular, I wouldn't expect more than a few hundred hits to this particular call. I could just I've just casually self-taught myself SQL the past few years and I like to know and follow best practices when possible, even if it's not really needed.
I don't know… I guess I just don't see it as extra work to name a column user_id instead of id. 
If you want to use best practices, quit using natural join. I've never seen it used in production code, and it reads like a dangerous shortcut. Best practices would also mean having stored procedures available, but that's impossible as long as you use sqlite. 
Thanks. I'll continue as before and just pretend it doesn't exist then.
Yes, we do not have the address hard-coded. We have a config table that we use to read the values in and act on them. I was just hoping there was some system table that would keep track of calls to utl_mail or trigger fires or something I could query to confirm that the trigger did fire. I like the idea of an Audit Table. My DBA suggested a table that all email triggers load and then we act on those records to deliver the email. I like that idea too but I am not sure I will get approval to make those changes. Anyway, thanks very much for your time and input. I appreciate it.
SQL Compact is Microsoft's [embedded database engine](http://en.wikipedia.org/wiki/Embedded_database). This means that there is no "server" application that reads and manages the database. Instead you have a database file(s) and your application has the tools built in to read and manipulate the database. The install of SQL Compact is the install of those tools. If your app requires it and can't use a full sql connection then you may need to install it. Use your own discretion.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Embedded database**](https://en.wikipedia.org/wiki/Embedded%20database): [](#sfw) --- &gt; &gt;An __embedded database__ system is a [database management system](https://en.wikipedia.org/wiki/Database_management_system) (DBMS) which is tightly integrated with an [application software](https://en.wikipedia.org/wiki/Application_software) that requires access to [stored data](https://en.wikipedia.org/wiki/Computer_data_storage), such that the database system is "hidden" from the application’s end-user and requires little or no ongoing maintenance. It is actually a broad technology category that includes &gt; &gt;* database systems with differing [application programming interfaces](https://en.wikipedia.org/wiki/Application_programming_interface) ([SQL](https://en.wikipedia.org/wiki/SQL) as well as proprietary, native APIs), &gt;* database architectures ([client-server](https://en.wikipedia.org/wiki/Client-server) and in-process), &gt;* storage modes (on-disk, [in-memory](https://en.wikipedia.org/wiki/In-memory_database), and combined), &gt;* [database models](https://en.wikipedia.org/wiki/Database_model) ([relational](https://en.wikipedia.org/wiki/Relational_database), [object-oriented](https://en.wikipedia.org/wiki/Object_database), [entity–attribute–value model](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model), network/[CODASYL](https://en.wikipedia.org/wiki/CODASYL)), and &gt; &gt;* target markets. &gt;The term *embedded database* can be confusing because only a small subset of embedded database products is used in [real-time](https://en.wikipedia.org/wiki/Real-time_computing) [embedded systems](https://en.wikipedia.org/wiki/Embedded_system) such as [telecommunications switches](https://en.wikipedia.org/wiki/Telephone_switch#Digital_switches) and [consumer electronics](https://en.wikipedia.org/wiki/Consumer_electronics) devices. (See [mobile database](https://en.wikipedia.org/wiki/Mobile_database) for small-footprint databases that could be used on embedded devices.) &gt; --- ^Interesting: [^Empress ^Embedded ^Database](https://en.wikipedia.org/wiki/Empress_Embedded_Database) ^| [^Apache ^Derby](https://en.wikipedia.org/wiki/Apache_Derby) ^| [^SQL ^Server ^Compact](https://en.wikipedia.org/wiki/SQL_Server_Compact) ^| [^Comparison ^of ^relational ^database ^management ^systems](https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cmvtowi) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cmvtowi)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It may just be an optional thing for those that don't have a full connection then. You would have to ask the publisher of that app. If possible you could try not letting it install it. See if it can work without it. In my opinion, it is bad app design to use an embedded database if you also have a full blown database engine you are also using.
You can use a Ranking function to filter out the duplicates: DECLARE @EmployeeTable TABLE (EmpID INTEGER, LastName NVARCHAR(100)) INSERT INTO @EmployeeTable (EmpID, LastName) VALUES (1,'jones'),(1,'jones'),(2,'Adams'),(2,'Adams'); WITH tbl(EmpID, LastName, RowID) AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY EmpID,LastName ORDER BY EmpID,LastName DESC) AS RowID FROM @EmployeeTable) SELECT * FROM tbl WHERE RowID = 1; This should give you a dataset minus the duplicates. I'd suggest building a temp table with the normalized dataset, truncate the source table, then reload it from temp after adding the appropriate key constraints. 
I'd suggest Ralph Kimball's book "The Microsoft Data Warehouse Toolkit, 2nd Edition". It's more about data warehousing from a MS SQL standpoint, but it's VERY good. Also the Microsoft Press books on SSAS are good too, but they don't have as many real life examples from what I've seen. Great for reference though. 
without kowíng the table structure, there is no way of telling. I hope you got an identity or a changeDate column in that table, if not, you have no way of telling which was the latest duplicate insert into that table. What I would do would be this : SELECT * FROM ( SELECT cnt = count(*) over(partition by EmpID, LastName, FirstName) , * FROM EmployeeTable )sq WERE sq.cnt &gt; 1 This will give you the duplicated rows, based on that partition by clause. I would do this, so you can actually see the data. With that, you can have a look at the data, and figure out if you have an indicator of what row to delete. /u/samalex01 posted a query that is similar, but since his partition clause matches the order by clause in that rownumber, you would delete a random row. I hate get_random on a select. On a select to be used as a delete, brrrrrr. Truncating tables in production also is about the last thing I'd ever do. Just delete the stuff you don't need, after create constraints (primary key, if you can't unique index), and your good. Do not dump a production table into a temp table and truncate the production table to later repopulate it. You do not do shit like that in production. Seriously, do not do shit like that. You only do that, if you have nothing to uniquely identify a row of your table, which is very bad on its own accord. That is the only god damn moment, where you deduplicate a table with that brute force "dump it into a staging table, truncate it, and insert a select distinct". Like I said, the very last thing I would do, very very out of options situations. 
 --Build some example data IF Object_id('tempdb..#Employee') IS NOT NULL DROP TABLE #employee; CREATE TABLE #employee ( empid INTEGER, lastname NVARCHAR(100), firstname NVARCHAR(100) ) IF Object_id('tempdb..#OtherEmployeeRelatedData') IS NOT NULL DROP TABLE #otheremployeerelateddata; CREATE TABLE #otheremployeerelateddata ( id INTEGER, empid INTEGER, streetaddress NVARCHAR(100) ) INSERT INTO #employee (empid, lastname, firstname) VALUES (1, 'Jones', 'John'), (2, 'Jones', 'John'), (3, 'Adams', 'Bill'), (4, 'Adams', 'Fred'); INSERT INTO #otheremployeerelateddata (id, empid, streetaddress) VALUES (1, 1, '1 Jones St'), (2, 2, '1 Jones St'), (3, 3, '1 Adams St'), (4, 4, '2 Adams St'), (5, 2, '1 Holiday St'); SELECT * FROM #employee; SELECT * FROM #otheremployeerelateddata; --Update Existing linked data with the original EmpId WITH orderedemployee(empid, lastname, firstname, rowid) AS (SELECT *, Row_number() OVER ( partition BY lastname, firstname ORDER BY lastname, firstname, empid) AS RowId FROM #employee) UPDATE o SET empid = original.empid FROM #otheremployeerelateddata o INNER JOIN orderedemployee duplicate ON duplicate.empid = o.empid INNER JOIN orderedemployee original ON duplicate.firstname = original.firstname AND duplicate.lastname = original.lastname WHERE duplicate.rowid &gt; 1 AND original.rowid = 1; --Remove the duplicate records added after the originals WITH orderedemployee(empid, lastname, firstname, rowid) AS (SELECT *, Row_number() OVER ( partition BY lastname, firstname ORDER BY lastname, firstname, empid) AS RowId FROM #employee) DELETE #employee FROM #employee INNER JOIN orderedemployee ON #employee.empid = orderedemployee.empid WHERE orderedemployee.rowid &gt; 1 SELECT * FROM #employee SELECT * FROM #otheremployeerelateddata 
I'm gonna guess that you're either not joining properly (remember to find a common column and set them equal in the where clause), or your dates aren't being formatted properly. What's your best attempt look like?
I am not really going to set up your tables or look at it in that level of detail but what you basically need to be looking at here is the difference between a left join and an inner join. Also, the SQL version could impact the results here. I generally use MSSQL so that's the way I think. I also don't see where your actual query is in all of your table creation code so I am not sure what to advise from. High level, I would work it something like this: SELECT Name ,Address ,coupon ,dateUsed FROM customers c LEFT JOIN address a ON a.customerKey = c.customerKey INNER JOIN coupon cp ON cp.customerKey = c.customerKey AND cp.dateUsed BETWEEN '9/1/2014' AND '12/24/2014' Just keep in mind that between is inclusive so if you want it to be exclusive you will need to create a &gt; and &lt; setup of your own. 
First of all "Why do you think it is a BAD idea ?" SQL CE is tiny engine which can run smoothly on very little resource. Some of the apps are design to work with older hardware/OSes which have no driver/direct connection to newer SQL servers. Vendor won't rewrite the code for one customer who don't want to install SQL Compact. Does your company use old hardware ? Old OSes ? Probably the app need sqlcesa35.dll to sync with your SQL Server 2012. Talk to your vendor and ask them why. I have developed those similar apps long time ago and still selling to new customer. Pricing is simple, you want to use this industry strength hardware with old OS(CE5.0) which work perfectly fit your requirement for $4000 or do you want me to write a new app which work with your fancy mobile phone/tablet for $6000. Most of the vendor choose the old way, those devices work as intended and last more than a decade. 
If you write a program to always have full access to a full database engine than it is somewhat pointless to include and use a secondary embeded database engine. That was the point I was naking. I don't know what his app does or what it is supposed to support. I was making a generalized opinion on basic app design. That doesn't mean it applies to every situation. Thank you for clarifying.
If you are writing a new program, that will be different story. But if you are maintaining multiple customer for single application (8~10 years).. there is no point changing the code every now and then. As long as the app is working, I won't touch the codes. Of course, latest customer might complaint the 8 years old screen design or connectivity. That was written before smart phones are mainstream. Those device don't have such connection like wifi/3G, it work as independent application with it's own local db engine. User usually bring the device around multiple warehouses, do whatever they are meant to do, do necessary data processing .. then come back to workstation, link it up with it and sync back to main server.
Sidebar says it all!
 SELECT c.CUSTOMER_NAME, c.CUSTOMER_STREET FROM INVOICE i JOIN CUSTOMER c ON i.CUSTOMER_ID=c.CUSTOMER_ID JOIN CUSTOMER_INVOICE ci ON i.INVOICE_ID=ci.CINVOICE_NUM JOIN DELIVERY d ON i.TICKET_NUMBER=d.TICKET_NUMBER WHERE ci.COUPON_YESNO='YES' AND d.DELIVERY_RECIEVED BETWEEN '01-SEP-2014' AND '24-DEC-2014'; 
Why are you installing "an app" on your SQL Server box in the first place? I'm all for consolidating hardware where possible, but SQL Server is one of those services that needs to have its own dedicated server.
its a new program. it's just sloppy that's all. I don't have anything to back up my theory that installing SQL CE directly on the SQL server is BAD - but it just doesnt seem right. Was hoping someone here would give me some concrete reasoning or research points so I could blast the dev.
It definitely sounds sloppy. I'd keep pushing back - or maybe just try installing on another server and see what happens.
As condescending as this reply was probably supposed to be, I actually found it rather helpful. Thank you!
Having CE component doesn't mean sloppy, are they having some redundancy for different types of devices ? Since you are sysadmin, you have no say in system design, you should talk to the person responsible to the system design (from your team.) If this requirement is by design, you have to go with it.
further digging show that it's just a requirement in the installer for their module and does not actually even need to be installed. So their installer is requiring me to install things I don't need. This is a matter of doing things sloppy, or doing things right the first time. 
I definitly get that. I did not assume that you would do it without knowing what you are doing either. I would not presume OP having to much experiance however, otherwise he wouldnt have asked the question after all. So I wanted to make it very clear, that he should not deal with live data (an assumtion of mine) that way, without first exploring better options. If there is no criteria to uniquely identify a row, of course, there is no other course of action than to rebuild the table with what boils down to a select distinct. It should, as we agree, be the last option however, and that I wanted to get across in a very clear way ;)
You should never use it period
I think about joins in terms of Ven diagrams. Inner joins are the overlapping section. Directional joins are the whole circle on the side I'm joining towards and the overlapping part of the other circle. This would actually be easier for me to explain using MS Paint. I also always use explicit joins. It's just easier for me to keep track of what I'm doing the trying to match up lines of code from different clauses in the statement.
[Cough.](http://www.codeproject.com/KB/database/Visual_SQL_Joins/Visual_SQL_JOINS_orig.jpg)
Perfect! Have an upvote fine sir.
If that were true, it wouldn't be part of SQL.
As a database developer I've never seen it used in a script, for good reason. it would be irresponsible. I'd get out of the habit now.
ok, I had to read again what you are trying to do. The good news is this is nothing to do with self-joins, but it is to do with joins. To solve this, do the select in stages. Select the ratings info: SELECT r.* FROM Rating AS r Now join the other tables, so you can access the nicer data - note here we will get all the data: SELECT r.*, rv.*, m.* FROM Rating AS r JOIN Reviewer AS rv ON (rv.rID = r.rID) JOIN Movie AS m ON (m.mID = r.mID) Now instead of selecting all the columns, just select the ones you want and put in an ORDER BY clause: SELECT rv.name, m.title, r.stars, r.ratingDate FROM Rating AS r JOIN Reviewer AS rv ON (rv.rID = r.rID) JOIN Movie AS m ON (m.mID = r.mID) ORDER BY rv.name Lastly, change the ORDER BY clause to sort by different columns. So this is just to do with joins, not self-joins. I hope this helps a little. 
No worries, you know sometimes we just have to trust that it will work... :)
 ON (n1.mid = n2.mid) why do people use these parentheses? i see it all the time if it's in some book or course, please let me know, so i can seek out the author and punch him or her right in the nose
I finally figured out joins as a connect the dots type situation. What I think is your bigger problem, and probably why you're getting 40 rows of data, is perhaps you are asking for too many things in your initial SELECT statement perhaps? 
1) Most Code Generation Software does this by Default 2) Code Readability 3) I always do it, since I need to do it when I am grouping or nesting clauses together anyways, so it just makes sense to do it for all for consistency sake. 4) I cannot for the life of me understand even for a moment why you would take issue with it, especially to the point of anger. You seem like a jerk.
clarity on multiple clauses e.g. ON n1.mid = n2.mid AND n1.othercol = n3.othercol OR n1.lastcol = p1.id Obviously this needs parentheses somewhere otherwise it is not obvious how it should be parsed :- is it --&gt; ((a and b) OR c) or --&gt; (a and (b or c)) . If you put parentheses everywhere consistently it [arguably] makes the code more readable and maintainable. 
i agree parentheses are required when mixing ANDs and ORs but even in your example, you have an outer set of parens that aren't needed and your argument about consistency? see ralph waldo emerson's quote 
anger? i was trying for humour and your name-calling makes you look childish
I must admit, I thought he was just using a humorous phrase. But then I am English and we have a quirky sense of humor!! But lets stop the argument.... its Christmas. Can we all just get along :-)
&gt; ralph waldo emerson's Oh dear, you drove me to here: http://www.brainyquote.com/quotes/authors/r/ralph_waldo_emerson.html and now many minutes of my life have been spent reading so many of his great quotes!! The only one I saw pertaining to this is (and I say this with respect and a little smile): Little minds have little worries, big minds have no time for worries. :-) 
&gt; I said you **seemed** like a jerk. nice backpedal, wonderfully kind person 
WHERE NOT EXISTS you mean to say where note exists? or the created at? Sorry for the confusion. 
English version.. Give me all notes where there is not a second note within li_limit seconds. Maybe it's easier if I format it: SELECT * FROM Notes N WHERE NOT EXISTS ( SELECT 1 FROM Notes WHERE n.Employee_ID = Notes.Employee_ID AND SecondsAfter( n.Created_At, Notes.Created_At) &lt; li_Limit ) WHERE NOT EXISTS is indeed the SQL I intended to use and I did not type NOT where I ment Note.
It seems the return type of the ISNULL function is being cast as the type of the first argument, which is inferred as `varchar(2)` from your `CASE` statement. Observe the difference between these two statements: DECLARE @Test varchar(2) = NULL SELECT ISNULL(@Test, 'This Is NULL') --returns 'Th' and DECLARE @Test varchar(3) = NULL SELECT ISNULL(@Test, 'This Is NULL') --returns 'Thi'
It's not a backpedal at all. It is exactly what I wrote. But really, perhaps you should consider not talking about punching people in the face because you don't like something.
i pity your inability to understand hyperbole if we ever meet in real life, rest assured i would not feel the slightest inclination towards violence, even though you appear to be a person with a huge stick up your ass
You aren't helping your case in the slightest. But thats how it is, right? You really should ask yourself, if you wouldn't act like this in person, why is it OK to do so here? http://i.imgur.com/Dt8Q6bn.jpg
great, that helps a lot. I didn't realize that multiple tables could be joined.
I never in a million years would have guessed things could get so heated of some parens.
 not sure if this how you want the output, just threw this together, let me know if it works select sku, type, sum(case when date &gt; '01/01/2014' then quantity end) s from x group by sku, type having sum(case when date &gt; '01/01/2014' then quantity end) &gt; 0
which dbms? "upsert" is handled differently in mysql, for example as for uniqueness of 2 columns, that's routine -- ALTER TABLE foo ADD UNIQUE ( userid, contentid )
I'm not a mod so feel free to ignore, but it would help if your code was formatted.
MySQL. The issue with the uniques is that the hid can be entered 100 times, but must never be paired with a duplicate cid.
This is interesting. Have you considered using 'With' to create a sub query that your select statements are based on? I'm no expert so I'm not sure how to go about this, but maybe that could be an approach that works?
If they're going to be dicks about permissions I'd send the dbas a query every five minutes and ask them to run it and send me back the execution plan. Then again, don't listen to me, I work in a boutique consulting shop where these kind of political fights are irrelevant.
Put a max() or min() around x.sfrstcr_term_code in the outer select (select max(x.sfrstcr_term_code) from sfrstcr x where x.sfrstcr_pidm = c.sfrstcr_pidm and x.sfrstcr_term_code = (select max(y.sfrstcr_term_code) from sfrstcr y where x.sfrstcr_pidm = y.sfrstcr_pidm and y.sfrstcr_term_code IS NOT NULL )) LastTerm Or figure out why the query is returning more than one row and add a filter to eliminate it.
It's a good question. I've been here a few years and I've never gotten any explanation on why we can't do it, just we can't. Also I should say my company is actually hired to work as consultants for this company on their data. Pretty sure it's political. Big pain in the ass especially since we're getting beat on for having poor performance on SPs and reports. Well what do you expect without giving us access to the tools?
Sounds very political to me. Developers **should** have an environment where they can do whatever they want. Just lay it out - by not having access to the tools, you can't do your job properly and won't be able to write quality code and reports. Probably won't get very far, but at least you have it documented.
&gt; sqlplus Does sqlplus work with MS SQL Server?
does oracle not have MONTH and YEAR functions? SELECT MONTH(transaction_date) || YEAR(transaction_date) month_year , COUNT(*) FROM orders.sales_order_listing GROUP BY MONTH(transaction_date) , YEAR(transaction_date) ORDER BY YEAR(transaction_date) , MONTH(transaction_date)
What data type is the field? Is it a string or an actual date/timestamp?
Banner. Hell of a thing.
Hello fellow Banner user! Ellucian still runs the old Boracle listserv; it's a great resource for technical questions about Oracle and Banner. I would highly recommend joining and asking there. http://lists.sungardsct.com/cgi-bin/wa?A0=BORACLE&amp;X=17417C666508463D45&amp;Y=rethepa%40earlham.edu If you ask there, you'll get people(including me) who have access to the database schema and the ability to run your query and debug it. That being said, sfrstcr holds registration 1 or more records per term_code per pidm. any pidm who has more than 1 record for a term could make that throw that exception. As mazerrackham said, min or max will work. You already have the predicate "x.sfrstcr_term_code = ..." which will make that subquery only return a single term_code (although there may be multiple rows of that single term_code) Rather than min or max, you could also use the DISTINCT operator. It would be preferable for readability. Side notes: one of your term codes isn't quoted, 201510 is not the same as '201510' term_codes are varchar2(6) as defined in stvterm.stvterm_code. If down the road, someone wanted to redo how terms are stored to 2016SM, 2016SP, 2015FL for summer spring and fall, your query would blow up. sfrstcr_rsts_code IN ('RE','RW','RL') shouldn't be hard coded, there is a table that defined registration code records (stvrsts maybe) that will let you grab the codes that represent only registrations. If a year from now, A&amp;R wants to add a new registration type, RT (register by SMS) this code will still return correct looking results, but they won't be correct.
Evidently it does have that function; however, it doesn't work the way you used it. it might work with some other databases, though. But I did combine that with extract and now I have: select extract(month from TRANSACTION_DATE)||'/'||extract(year from TRANSACTION_DATE as TIME PERIOD, count(*) from ORDERS.SALES_ORDER_LISTING group by TRANSACTION_DATE; That works for the actual extraction of the dates but now it isn't grouping by the way that I actually want it to. It is pretty much giving me the month/year for every single record on the original table. I just need to figure out how to correctly group it!
hmm that actually might work. I tried using to_date but it kept making me give a min or max date parameter. But this might work. Thanks. I will try it real quick!
It's an actual date/timestamp.
&gt; now it isn't grouping by the way that I actually want it to because you have to group it by month and year, not date so use the same non-string-function bits you used in the SELECT 
select count(*), to_char(trunc(field, 'MONTH'),'MM/YYYY') from table group by trunc(field, 'MONTH');
you're exactly right. I figured that when I looked at your other SQL you provided. It works. But the way you did it (above) works more efficiently. Thanks!
That worked like a charm. Thank you so much. That really really saved me so much time. It's a lot better than exporting to excel and using a pivot table just to get counts! Thanks again!
wow that's a cool way to do it too. I can't wait to try that one. That would be a sub-query, right?
Ok now I understand that. I'm thinking of so many times in the past that I could've used that! Oh well, this is the best way to learn sometimes! THanks.
Make it a managerial decision. Tell them that you're going to need to run execution plans on several queries many times each in order to improve performance. Then ask them if they'd prefer that you ask the DBA to run each execution plan or to give you the permission and you'll do it yourself. Tell them that you think the latter option would be better business value and that you will comply with whatever direction they give you. I know that doesn't answer your alternative question, but execution plans are your best option and if you have to run them through a DBA that's going to be better than any alternative you might find. It might even work to throw out the temporary DBA option - e.g. "Make me a DBA on Dev for 2 days so that I can complete this work and then revoke my DBA access. I can get this task done without bothering the DBAs while following company policy."
Yes. It's a good trick for grouping where the column(s) you want to group on are generated via logic rather than taken straight from the table.
Do this^ Execution plans are very necessary in the design process. DBA's can build indexes and evaluate queries for performance issues after the fact but this needs to be done in development as well.
&gt; Hello fellow Banner user! Ellucian still runs the old Boracle listserv; it's a great resource for technical questions about Oracle and Banner. I would highly recommend joining and asking there. http://lists.sungardsct.com/cgi-bin/wa?A0=BORACLE&amp;X=17417C666508463D45[1] If you ask there, you'll get people(including me) who have access to the database schema and the ability to run your query and debug it. Thanks! Always good to find other users in other places. How do I join the listserv? When I click Join it says some login from earlham.edu??? &gt; That being said, sfrstcr holds registration 1 or more records per term_code per pidm. any pidm who has more than 1 record for a term could make that throw that exception. As mazerrackham said, min or max will work. You already have the predicate "x.sfrstcr_term_code = ..." which will make that subquery only return a single term_code (although there may be multiple rows of that single term_code) Rather than min or max, you could also use the DISTINCT operator. It would be preferable for readability. Thanks for the breakdown. The max seemed to work in this instance but I will remember that for future reference and it seemed that I needed to use that because I wanted the latest term or am I reading it wrong? If you can't tell, I am a novice when it comes to SQL so far. &gt; Side notes: one of your term codes isn't quoted, 201510 is not the same as '201510' term_codes are varchar2(6) as defined in stvterm.stvterm_code. If down the road, someone wanted to redo how terms are stored to 2016SM, 2016SP, 2015FL for summer spring and fall, your query would blow up. Understood. We normally quote anything, this whole scripted was pieced together from when we first started using SQL. &gt; sfrstcr_rsts_code IN ('RE','RW','RL') shouldn't be hard coded, there is a table that defined registration code records (stvrsts maybe) that will let you grab the codes that represent only registrations. If a year from now, A&amp;R wants to add a new registration type, RT (register by SMS) this code will still return correct looking results, but they won't be correct. Understood as well. Most of the time we use IS NOT NULL but in this instance we only need them if they are registered. All of our registration codes begin with R and we often use LIKE but again it is coding from sometime ago but we just add registration values as they are created. 
Yeah, I didn't suggest distinct because I didn't know if it was returning the same value multiple times, or multiple distinct values. 
I'm surprised this hasn't been answered correctly. First and foremost - YES! Oracle matters a plenty. They sell one of the most robust relational database management systems in the world; they also happen to sell a whole damn lot of licenses. They also offer the most extreme (and complete, perhaps) suite of products that integrate, utilize, and/or manipulate their database platform in a way that manages and grows your business. They even own Sun/Java. You not seeing jobs with Oracle skills requirement is no surprise - software versions and vendors often have this strange coincidence where entire metropolitan areas focus on a particular technology. Some places have high demand for java, others have high demand for .net. Same thing happens for databases. Reasons for this can be education, a large organization with a very specific demand base, and even the strength of local community involvement. It's great that you're looking and it's even better that you're asking questions. Learning Oracle ANSI SQL will not hurt you in the least - you'll only gain from this exercise. If you want or need to transition to MS T-SQL, you're not going to be miles away from that goal. The languages are largely the same, the management is where things are unique. But that uniqueness is beneficial - and it'll help you build a strong mind around why those differences matter. .. and they do matter. Keep doin' what you're doin'. The fact that you're asking means you're bound to do great. And as others have said, be ready willing and able to know more than one ecosystem. Very few places only operate one product.
Is the data ever accessed outside of the website?
pivot table?
Why don't you do a LEFT OUTER JOIN between your scans and importlist tables and add a WHERE condition for importlist.objectid IS NULL. This should be faster.
Select s.objectid, s.location from scans s left join import list i on s.objectid = i.objectid where s.inventid='id' and i.objectid is null A join will perform way better than checking against a subquery. Make sure you have an index on objectid on both tables and on scans.inventid 
without data, roughly select objectid, location from scans left join importlist on inventid where inventid like 'id' and objectid is null;
Are you using any indexes? Have you tried a Left Join rather than a NOT IN?
Oh yea it's an amazing [Excel function](https://www.youtube.com/watch?v=peNTp5fuKFg)that can do so many things (including the SQL COUNT function). If I wasn't able to figure out how to get my results using SQL, I would've had to export to Excel and use a Pivot Table to get the same info. Since my table is less than 1 million records, it would've fit in one go.
That's awesome. I'll have to remember this. This is a good one. Thank you.
But ObjectID is never null. In fact, ObjectID is always there in scans and importlist tables. What happens is that the ObjectID in scans cannot find a match in importlist.
which will leave it null in the outer join, which is not the original dataset, but rather the dataset as it matches
No case is needed here. Use [DATEPART](http://msdn.microsoft.com/en-us/library/ms174420.aspx) and the quarter (Q,QQ) date part. DECLARE @Date DATETIME = GETDATE(); SELECT ISNULL('Q' + CONVERT(VARCHAR(2), DATEPART(QQ, @Date)) + ' ' + CONVERT(VARCHAR(4), DATEPART(YYYY, @Date)), 'Not Awarded') AS Award_Quarter You are getting an error because you have too many opening parentheses.
Well I'll be damned. I had no idea that it had a quarter function. TIL. One thing though, it is cutting off the words 'Not Awarded' to 'Not Awar' Any thoughts there? EDIT: Guessing it has to do with how many characters it is being given in the varchar statements + string Q - 1 datepart(QQ) 2 ' ' 1 DATEPART YYYY 4 EDIT 2: Switched the convert for quarter to be varchar 5 and it worked. Thanks dude!
For future reference, it's because of how isnull attempts to convert the second value to the data type of the first. Coalesce uses the [Data Type Precedence](http://msdn.microsoft.com/en-us/library/ms190309.aspx) table to decide what it should do. Below is an example: declare @shortvar varchar(5) = 'Five!' , @longvar varchar(10) = 'ThereAre10' select isnull(@shortvar,@longvar) select coalesce(@shortvar,@longvar) set @shortvar = null select isnull(@shortvar,@longvar) select coalesce(@shortvar,@longvar) 
It is possible. Windowing functions will take care of this easily. http://msdn.microsoft.com/en-us/library/ms186734.aspx I don't know what your table structre looks like, but something like this should work. select OrderID, OrderAmount, MonthOfOrder, RowNumber from (SELECT OrderID, OrderAmount, DatePart(Month(OrderDate)) as MonthOfOrder, ROW_NUMBER() OVER(PARTITION BY DatePart(Month(OrderDate)) ORDER BY OrderAmount DESC) AS RowNumber FROM Orders ) o where RowNumber&lt;=5 order by MonthOfOrder, RowNumber 
Alright cheers mate I'll give it a go! 
Does it just add the values together to come up with a number?
Its not clear to me what you want to do but you can use Sql Server Management Studio to insert rows. 
You can answer both questions with a single scan of both tables by using set operations rather than joins. This is a really useful method anytime you expect two tables to have the same data but want to get a report of where they don't match. SELECT COUNT( FROM_SCAN ) SCAN_COUNT, COUNT( FROM_IMPORT ) IMPORT_COUNT, INVENTID, OBJECTID, LOCATION FROM ( SELECT 1 FROM_SCAN, NULL FROM_IMPORT, INVENTID, OBJECTID, LOCATION FROM scans UNION ALL SELECT NULL FROM_SCAN, 1 FROM_IMPORT, INVENTID, OBJECTID, LOCATION FROM importlist ) AS INV_CHK GROUP BY INVENTID, OBJECTID, LOCATION HAVING COUNT(FROM_SCAN) != COUNT(FROM_IMPORT) ORDER BY INVENTID, OBJECTID, LOCATION, SCAN_COUNT;
What did your final query look like?
Try to post this on /r/dotnet , you will probably find more help there. My first guess is you have `ItemUpdated="DV_Update"` when it should be `OnItemUpdated="DV_Update"` 
/u/IAmAJerkAME has a solution for you for the short term, but looking further out [you need a date table](http://www.brentozar.com/archive/2014/12/simply-must-date-table-video/)
That error message is telling you that there is a trigger on the table that invalid. try the following: sqlplus / as sysdba SQL&gt; alter trigger XDB.XDB_PI_TRIG compile;
"Front end" to me means an end-user application. I would expect it to be a screen in your web application or a tool within your fat app. I've seen applications specify CSV formats or have Excel templates for users to bulk load data, but... they're not really using anything special to upload the data. Either the file is uploaded to the server or selected from the app, then the application parses the file and handles the insertions.
In the isnull example provided it Q(1)+varchar(2)+' '(1)+varchar(4) so thr ending data type was varchar(8). So the second value was forced into that varchar(8). Coalesce, is a little more complicated though. If you take my example above and replace @longvar with an int variable, you'll actually get a cast error saying it can't convert Five! To an int due to that precedence table I linked. I'm on mobile else I would paste the example but you get the idea hopefully.
I think I made a query like this before, when I get to work I'll see if I can find it and change the code to your situation.
https://community.oracle.com/thread/1073972?start=0&amp;tstart=0
&gt; XDB.XDB_PI_TRIG This is trigger in SYS schema, not XDB
The name for this type of problem is "gaps and islands." There's a [category for that on SO](http://stackoverflow.com/questions/tagged/gaps-and-islands?sort=votes&amp;pageSize=15), but I'm way too tired to figure out the logic. 
For the desired date format you can use the following (at least I think it should be correct): select to_char(trunc(TRANSACTION_DATE, 'MM'), 'MM/YYYY'), count(*) from ORDERS.SALES_ORDER_LISTING group by trunc(TRANSACTION_DATE, 'MM') order by trunc(TRANSACTION_DATE, 'MM')
I'll check it out, thanks!
Thanks, any code examples would be great
Wow.... this here is a learning experience.... 
OK, since I am a little lost on this SQL, I just tried running it as-is. And I get this error: #1248 - Every derived table must have its own alias 
&gt;I have to write a query which grabs the top 5 largest orders (I assume by largest it means highest cost) Assume nothing! Get this clarified. It could be dollars, it could be total item count (purchased 2 each of 5 items, result is 10), unique item count (purchased 2 each of 5 items, result is 5), weight or volume.
I edited the original query to add "AS INV_CHK" to the "derived query" which is the subquery between the first from and the group by.
Assuming you have an index on the pk field, just split the job into separate updates, one for each column. That way you are only touching the data you need to.
That's a valid concern, but there is no other way to avoid touching those records, and since you need to wait for the process to finish anyways I don't see why that should stop you. Alternatively, you could just identify which records have changed, delete them and replace them entirely. That would speed things up for you as well.
If they don't already exist, try adding indexes on Scans.inventid and Importlist.inventid. If either one of these tables is rarely updated and don't already have clustered indexes, then maybe even go that far.
Partition the table of it's not. 
I can't delete, those AccountIDs are in use all over the application. I am going to try this individual update thing and see if it is any better, thanks!
How would that help if the table is updating based on the clustered primary key already?
Smaller tree to navigate. Depending on what your columns are, it can supplement the clustered index, kind of like an index for indexes. We have our biggest tables partitioned on a datekey along with a clustered index on a separate identity column. So when we pass the datekey, it only needs to navigate a smaller index tree
I think you might have a conceptual problem: "list of Accounts" doesn't sound like a 'fact' table to me so it shouldn't have 'dimensions'. On the other hand, if you have a SCD of Accounts you should have the effective period fields for the records and simply insert changed records and close the effective periods of prior 'most current' records. 
I was looking to create or use some sort of form window to do this. I remember doing something similar in VB a long time ago but I forgot how to do it anymore. I would really just prefer to have a program that I can download and just point it to my database. This way, end users can enter the data in themselves. 
Do you know why you joined movie on itself? Just think about it logically. You need a list of the maximum ratings and then you need to pull the title for that movie. Just use a subquery to get the list of max ratings. You don't need to filter out the null ratings because by virtue of an equi-join you won't return any of those movies that don't have any ratings. SELECT m.title, maxRating.stars FROM movie AS m JOIN (SELECT mid, max(r.stars) FROM rating AS r GROUP BY m1.mid) AS maxRating ON m.mid = maxRating.mid ORDER BY title
MS Access?
I have a different table with account history (values for a certain start/end date). The purpose of this table is to have the most up-to-date information possible, and easy/fast to query. With the table you are referencing, we would need to either determine the most recent date by account everytime we query the table - or have a flag in the table database to show which record is the most recent. Either way, we are adding an extra WHERE clause to every single instance that we join to this table (which is probably thousands of times across thousands of sprocs).
 select m.title, max(r.stars) from movie m, rating r where movie.mid = rating.mid group by m.title ?
I'll post it back home tomorrow morning mate. Haven't had access to my laptop today!
That worked, thanks
NoDeheral answer lead me to something like that SELECT m.title, r.stars FROM movie AS m JOIN rating AS r ON (m.mid = r.mid) GROUP BY m.title HAVING MAX(r.stars) &gt; 1 thanks for the help guys
&gt; So many unclear sentencing 
http://www.w3schools.com/sql/default.asp http://sqlfiddle.com/ You now know SQL and it was free. There's always more to learn, but the basics are incredibly basic. 
Oh OK, cool, thanks!
You caught me copy pasting from my document of stolen fixes from the Internet :) Same should apply though, just fix the referenced schema. 
Maintaining a denormalized (current state) rrelatively wide reference/lookup table with billions of records with a possibility of batch updates seems like a recipe for a maintenance nightmare. Nevertheless, you could probably try to optimize fillfactor to avoid row migration and/or disable all constraints prior to your batch and 're-enable them after to enable delete old/insert new approach.
^ best idea so far. Drop indexes, update, create indexes, update statistics
Interesting. To follow this train of thought, I guess the table that is updated may have a number of dependencies (foreign keys) which are checked for every update. How many records are actually updated per batch, as a percentage of the table? If the purpose of this table is only to act as a fast lookup for a historicised table, you may want to simply create it anew at every batch. (I understand this may be completely impossible due to other design constraints, but you may be better off reconsidering the whole current setup.) 
The answer to which SQL is probably directly correlated with what the company uses. The bigger 4 are MS SQL, Oracle, Postgres &amp; MySQL from what I've seen. Some might throw DB2 in there, but I have a distaste for it and tend to ignore it. You can always learn ANSI standard SQL on any of the 4, so I'd just install one on your home machine and start playing. If you've got a Windows machine, here's the resources I'd point you towards since MS SQL is my wheelhouse at the moment: * [SQL Server Express 2014](http://msdn.microsoft.com/en-us/evalcenter/dn434042.aspx) * [Adventureworks Example DB](https://msftdbprodsamples.codeplex.com/releases/view/125550)
How limiting is the Express version?
Check out the [ROW_NUMBER](http://msdn.microsoft.com/en-us/library/ms186734.aspx) function and see if that will work for you. This assumes that there are other columns you can group on. After using the ROW_NUMBER function you can filter to only return the first instance like this example from the link: WITH OrderedOrders AS ( SELECT SalesOrderID, OrderDate, ROW_NUMBER() OVER (ORDER BY OrderDate) AS RowNumber FROM Sales.SalesOrderHeader ) SELECT SalesOrderID, OrderDate, RowNumber FROM OrderedOrders WHERE RowNumber =1;
IMO the big thing to learn is how to JOIN and the difference between a INNER JOIN and a LEFT JOIN. There are other types, but they rarely get used. The SELECT and WHERE clauses are pretty straight forward. 
2014 has the same limitations as 2012. See this link: http://stackoverflow.com/questions/1169634/limitations-of-sql-server-express
Or you can get SQL Server Developer Edition (which is equivalent to Enterprise) for $60.
So, essentially your checking for duplicates? If so, the best way in SQL Server is really with a CTE. This stack overflow link is focused on deleting records, but you can simply change the delete statement to the select statement of your choice (just treat the CTE like a table in the select): http://stackoverflow.com/questions/18390574/how-to-delete-duplicate-rows-in-sql-server
Why not do something even simpler select whatever, count(distinct jobid) group by whatever
Let's talk about your users table, specifically the part where you are storing credit card numbers. Depending on the country in which you operate, you will need to meet certain regulatory standards. Given that you are asking about which database to use, I am going to guess that you are somewhat new to database engineering and therefore inexperienced with database security. (Not trying to be mean, just making an assumption.) Either don't store credit card info or outsource this task to a trusted third-party payment provider.
Alright cool, and I do dev work on Linux boxes. Do those tools work on any Linux distros? 
Yea I will be doing dev work, I will be talking to the customers either internal or external and figuring out what they need then implementing it.
I think it really depends on what the company is using. Its likely one of the four that /u/guttermonkey mentioned. I would recommend getting to know how to implement queries in _that_ database. Then I would recommend learning how to determine if the query is performant (like using query analyzer in PostgreSQL). I would also recommend becoming familiar with the nature of transactions: begin, rollback, and commit.
So developers will argue about where the cusp is. Some say a few thousand, some say a million. It really depends on the server your db is running on, how many active users, number and type of fields, etc. I usually go with about a million. Anyway. Any output that you need to reference that is below that cusp, should be a CTE or table variable. You should just run it in memory. Any output over that cusp should go into a #temp table so its written to disk, with indexing on the table after its made. Cursors should be avoided. I've never seen a case where they're better than other options. Set based work is safer in terms of data integrity and usually much faster.
Haha, I figured. Thanks man I appreciate it. 
INSERT INTO db2.table(`col1`,`col2`,`col3`,`col4`,`col5`) VALUES (SELECT db1.table(`col1`,`col2`,`col3`,`col4`,`col5`) WHERE db1.table `id` BETWEEN 1x AND 100x); --db1 is the old database --db2 is the fresh database --col# represent the names of the columns you want to move -1x AND 100x are the lowest and highest numbers in the db1.table `id` column you wish to move I'm on an iPAD so I can't test this and I don't really like the look of the parenthesis around the select statement (plus I think it can be easier stated by simply using SELECT INTO). I trust that any errors will be corrected by other commenters - but that's the direction to take assuming you don't have the table in the fresh database populated in all the columns with dummy data. 
alright cheers will try this
CTE's make people who use Cursors confused. 5 days to run a script is perfectly acceptable to an SQL developer who doesn't understand CTE's or not using cursors. Making that same script take 25 seconds confuses them, and management. "It must not work properly."
I believe a more accurate description of the problem is "cursors are generally bad". (Of course, I hear there are problems that require cursors for any reasonable implementation, but I haven't run into one yet.)
Prior to SQL 2014, cursors are better at running sums
Then you're just doing a bad job of explaining it.
5 days to run a script is not acceptable when a business requirement states that the script must run nightly, or on the fly. If this script didn't run in a timely fashion, we could end up telling a customer a product was RoHS compliant, when one of the materials was not. That's a big flippin' deal. 
"VALUES ( SELECT" as well as "db1.table(col1..." are both egregiously invalid syntax
Looking at job openings at Boeing it seems to depend on where you're at and what department you're in. I see some ETL jobs in Virginia that need MySQL and Oracle and some analyst positions in the UK that need MS SQL.
The first response in that thread has the limitations listed in it.
Postgres and MySQL are very easy to install and use in Linux. You probably already have MySQL installed if you look for it.