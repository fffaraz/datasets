There are a few SQL questions on leetcode.com that you can try as well.
Just started a new job as a DBA a few months ago. I should have asked better questions at *my* interview. The only person on the (6 person) team who knows what keys are is the director of BI... it's a good thing I like talking about SQL Server internals.
Thanks for this. I will focus on these. Once I've done these and got comfortable with them, what should I move on to? Thanks
SQL is capable of far more than just pulling data. It can create and modify databases, tables, views, stored procedures, indexes.... just about anything you need to do to manage a database. Being able to script all of these activities is incredibly important when it comes to reliable, consistent, and error free development and maintenance. It can also be used when inserting, modifying, and deleting data as needed. Reading data, aggregating and transforming it as needed is probably the most common usage, but it doesn't even come close to requiring or taking advantage of SQL's full power.
Maybe window functions and learning about 3NF and database design theory... It'll be a lot easier to write reports when you understand the fundamentals of how the database is designed
Found some details here: [https://www.qualified.io/kb/hire/tips/interpreting-results](https://www.qualified.io/kb/hire/tips/interpreting-results) My impression is the assessment will require writing SQL, all events are being logged and can be played back, so they are trying to gain insight into your internal process while you develop the solution. It looks like everyone should be able to score 100%, but HOW they arrived at the solution will distinguish applications. * Code documentation * Customize test suite * Questionable copy and paste? * Refactored? * Consistent style e.g. beautified? I'd approach this with a pad of paper and have the best plan possible before typing. &amp;#x200B; &amp;#x200B; &amp;#x200B;
This does help, thank you!
I noticed that too; that they can see your entire process. Do you think, in the case of using paper, it would look suspicious if I don't type for a long time then suddenly type the right answer? Or are you simply saying don't just start willy-nilly typing away without at least a decent plan?
Not sure about that but was basically warning against typing willy-nilly. Its not really an issue with aliasing in SQL, but I was mainly thinking about my tendency to type, backspace, retype 10 different names for a variable before deciding on one when coding. 
Hackerrank has some as well. They're less edge-casey than Leetcode's, but a lot more repetitive as well. 
Yeah you want a cross join. You want every date to have every salesperson? That's exactly what that will accomplish.
there may be better approaches, but my experience says you need to identify the employee-date combos that are missing first and to do that you need to generate the desired range of dates with which you can do a LEFT OUTER JOIN there are many ways to generate a range of dates but for simplicity i'm going to assume they reside in a dates table i'm also going to assume you have an employees table along with your schedule table SELECT e.emp_ID , d.date FROM employees AS e CROSS JOIN dates AS d LEFT OUTER JOIN schedule AS s ON s.emp_ID = e.emp_ID AND s.date = d.date WHERE s.emp_ID IS NULL save these combinations in a temp table, and then, after an optional step in which you inspect and confirm you got the right data, insert these into your schedule table INSERT INTO schedule ( emp_ID , date , event ) SELECT emp_ID , date , 'default entry generated' FROM temptable 
Postgre has an awesome generate series function that will return a block of dates. I dont know much about it, but my friend was showing me. You can send it a date range and interval and will will return a table for each date (because your query is expecting the employees table to have all available dates). Cross join that with a distinct employeeid, then join your employees to it.
It looks like you would need to sum the date columns along with the grouping. SELECT EmpID ,JOB ,ITEM ,PITEM, PROJ ,SUM([02/24/2019]) ,SUM([02/25/2019]) ,SUM([02/26/2019]) ,SUM([02/27/2019]) ,SUM([02/28/2019]) ,SUM([03/01/2019]) ,SUM([03/02/2019]) ,NOTE FROM ProcessTable GROUP BY JOB, ITEM, PITEM
Top 3 Google results for "tsql pivot": https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot https://blogs.msdn.microsoft.com/spike/2009/03/03/pivot-tables-in-sql-server-a-simple-sample/ https://stackoverflow.com/questions/10428993/understanding-pivot-function-in-t-sql
1. inner join, left join 2. Obtaining an intuitive understanding of null 3. Coalesce, case 4. Subqueries 5. Avoiding the use of 'distinct' (it's not inherently evil but often leads the new to think they're finished when they're not. If you aren't 100% sure why you need to use distinct to get rid of duplicate rows then figure out why theyre they're before using it)
You gotta be kidding me lol
Why?
I’ve done this type of work before. I use python and other scripting languages to send requests to the API, SQL DB, and do comparisons. I get all active user IDs from the API, get all active user IDs from SQL DB, compare, update metadata, and push the necessary updates to their respective destinations. This script can be scheduled in a server via CRON (Linux) or Task Scheduler (windows). It’s best to do any updates to records on production app during off-peak hours, so be wary of time zones when creating scheduled job. It’s also a good idea to keep a table that logs all script executions and changes to users, for auditing, analytic, and record keeping purposes, also easy way to roll back records if needed.
Would you be willing to share your code ? ( anonymized of course ) - I totally get the idea in principle and can see how it could work, but I don’t even know where to begin. I understand if you don’t / can’t. Thank you. 
Are you familiar with any programming language other than SQL? Here's a few examples of consuming a REST API using C# https://code-maze.com/different-ways-consume-restful-api-csharp/ After consuming the API, you can use ADO.NET to update your database (be wary of SQL Injection vulnerabilities at this stage) https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/ado-net-code-examples You can wrap all of this up in a Windows service application to handle running the process in the background and handle automatically starting/restarting the service in the case of the computer turning off and back on. https://docs.microsoft.com/en-us/dotnet/framework/windows-services/walkthrough-creating-a-windows-service-application-in-the-component-designer
Swagger is just a tool that maps the routes on the Rest API. SwaggerUI gives it a page that lists them. REST API and HTTP calls are what you'll be working with. The way I would do it? Powershell and automate it. Use Invoke-SQL to get a list of users from your users table. Loop over this array of users from your SQL result set. Use *Invoke-WebRequest -Uri http://example.com/User/Update -Method POST* to send updates. Maybe get fancy and do a Get user first and compare if anything's change and then send an update. If there are errors you want to log. Log them to the machine or to a text file. Done. 
I asked a similar question a few weeks back and didn't get a single comment. I ended up using a cross join, the structure of your table and outputs is important. https://www.reddit.com/r/SQL/comments/axgia8/postgres_conundrum_populating_dates_not_used_for/ You can see my solution there. 
So, you need two CURSORs and an ACCEPT. Doesn't seem too bad. 
PL/SQL runs on the server, so it can never "prompt" the user. If at all you can make your SQL client (SQL\*Plus, SQL Developer) "prompt" the user for input. 
Where can we learn Sql sqllite and other Sal based languages 
Use a data integration tool that has readymade connectors, has a drag and drop interface to let you design your integration and mapping of source and destination fields, and has a built in scheduler. Check out Boomi for example.
You need [a date table](https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/). You'll associate each date with a period and when you join your accounting table to that date table on the transaction date, you can filter by period from the date table.
Why can't you just concatenate the year + period and cast to an integer and then compare. So Cast(Cast(y as varchar(6)) + Cast(p as varchar(2)) as int). This should get you Between 201806 and 201903. This would exclude 201910 and 201802. I'm assuming your year and periods are integers if not then you don't need to do the inner casts first.
this is the easiest solution. spent the last three years developing hcm integrations through boomi and hana, and using eclipse if the hcm didn't already have a pre-built package available or needed some changes.
Yeah that's what I was (badly) describing and trying to avoid but I suppose a scripted generation wouldn't be so bad from a maintenance aspect. I'm actually now suspecting this table must preexist in most databases in one form or another; I just presume my ideas are always ridiculous and there must be slicker way to do things. I suppose I should be pleased with myself haha. Thanks for the video dude :)
Thanks, I did mull that one. The slight annoyance is that the single digit periods will need a preceding zero inserting for it's "ordered index value" but not the end of the world. I wonder about performance vs the date table lookup given the digit-bulking-check step. Guess I'll find out. Thanks for the advice dude, much appreciated.
I think you could divide the two columns by getting the data from the previous record. Just check out LAG, it might be what you need. 
Hi there! You could use the free version of [ScaiPlatform](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119), which can be found on [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_r_sql_promo_q119), [Google Cloud Platform](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119), and [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_pntr_red_ps_r_sql_promo_q119). It is **a free real-time sql gui** that runs on top of multiple sql databases. Users can create through the UI a pivot table without coding which will be automatically refreshed with the latest data. At the same time, users can create their own reports and dashboards, as well as import/export data without coding. 
Thanks but I'm on Mysql 5.5 and LAG doesn't support in my server.
This is the best way because then no matter what table you have with a date field in it, you can join it to your date table containing your accounting periods and you don't need to separately store year and period columns in every table with dates.
&gt; I'm actually now suspecting this table must preexist in most databases in one form or another I wouldn't put it in every database, I'd create a central "utility" database that everyone using the server can use.
While that will work, it's not SARGable and performance will scale poorly.
&gt; I wonder about performance vs the date table lookup Spoiler alert: it'll suck. You can't take advantage of any indexing on that date field.
Whoops, just saw the last comment on the post, completely missed it. 
Performing any function on the columns throws out the ability to perform an index seek and will result in that function needing to be performed on the column for the entire table.
 Not a problem, i edit the post and add the last line `After` your comment. &amp;#x200B;
Join them together. If you don't have gaps in your date field, it should be as easy as: SELECT ... FROM price this_day INNER JOIN price prev_day ON prev_day.date = DATE_ADD(this_day.date, INTERVAL -1 DAY) If you do have gaps, it gets a little trickier. MySQL 5.5 is old as fuck and does not even support CTEs, so you'd have to resort to something like SELECT ... FROM (SELECT pr.* , (SELECT date FROM price WHERE date &lt; pr.date ORDER BY date DESC LIMIT 1) as prev FROM price pr) this_day INNER JOIN price prev_day ON prev_day.date = this_date.prev Where `...` are calculations like `(this_day.BTC - prev.day.BTC) / prev.day.BTC AS BTC_perc_change`. You know the drill.
Believe you can use math to 'concatenate' and still obtain advantage of indexing. E.g. 2016*100=201600 + period
thank you,this solution is good to calculate single column percentage at static table, that table maybe changes(add or remove column) in future. 
Sure you can do that with dynamic SQL, but I don't feel you're going in the right direction here. Maybe you're modelling your data incorrectly? Instead of having however many columns, you should have something like: Date | Type | Value | ---|---|--- 2019-03-18 | BTC | 5 | 2019-03-17 | BTC | 4.5 | 2019-03-18 | ETH | 0.01 | 2019-03-17 | ETH | 0.008 | You can have as many values as you want then, without the need to change your schema. What I feel you're trying to do now is like designing a report in SQL, it's not really made for that.
The name of the DBMS is [PostgreSQL](https://wiki.postgresql.org/wiki/FAQ#What_is_PostgreSQL.3F_How_is_it_pronounced.3F_What_is_Postgres.3F), the short form is Postgres - but not "Postgre" - but apart from that, I agree. This kind of problem is typically solved using [generate\_series()](https://www.postgresql.org/docs/current/functions-srf.html)
Use the LAG or LEAD function to pull a value from a row above or below to the current row. You can then perform your calculation. 
Assuming you are talking about dates in a year, you can use using generate\_series() to generate all dates for a given range (e.g. this year) and then use that as the source for the INSERT. You didn't show us your table definitions and actual data, but something along the lines: insert into schedule (emp_id, "date") select et.emp_id, g.dt::date from employeetable et cross join generate_series(date '2019-01-01', date '2019-12-31', interval '1' day) as g(dt) where not exists (select * from schedule s where s.emp_id = et.emp_id and s."date"::date = g.dt::date); The cast to a date (`g.dt:date`) is necessary because generate\_series() always returns timestamps, not dates
Hey by the way, how did you paste the data into tables like that?
 I'm on Mysql 5.5 and `Window Function` doesn't support in my server.
Hi. i don't know what is meaning of " paste the data into tables ".
Sorry I mean - how did you format your data in your reddit post so that it appears as tabular format? | Date | ETH | XRP | LTC | USDT | BTC | etc... | |----------------|------|-----|-----|------|------|--------| | Today | 3111 | 0.3 | 100 | 1 | 3500 | | | Tomorrow | 3211 | 0.6 | 90 | 1.14 | 3540 | | | After Tomorrow | 3215 | 0.8 | 122 | 1 | | |
update SnowLicenseManager.dbo.tbluser set LastLogon = '2019-03-17' where lastlogon = '2019-03-18' &amp;#x200B; &amp;#x200B; Just learning sql myself, so may want someone to confirm before trying this..
your welcome , use : https://www.tablesgenerator.com/markdown_tables
you'r right , your schema is correct and more flexible, however i run your "SQL query" and the output is "(N/A)" on all return fields.
Hey, just found this - paste straight from Excel / Google Sheets [https://donatstudios.com/CsvToMarkdownTable](https://donatstudios.com/CsvToMarkdownTable)
Hmm, How about doing a self join on the table? In your subquery you can use criteria to specify exactly which row(s) to bring in.
Thanks for the reply! I really like this idea. I'm going to try it out today and see if I can get it up and running.
in self-join you need to calculate each field diff separately and it's good for static table as you see in this Query : SELECT c1.dt as Init, c2.dt as End, ((c2.ETH - c1.ETH) * 0.1) AS ETH, ((c2.XRP - c1.XRP) * 0.1) AS XRP, ((c2.LTC - c1.LTC) * 0.1) AS LTC, ((c2.USDT - c1.USDT) * 0.1) AS USDT, ((c2.BTC - c1.BTC) * 0.1) AS BTC FROM currency c1 LEFT JOIN currency c2 on c2.dt = c1.dt + 1 ; but my table is dynamic(maybe changes add or remove column).
So you can use Min/Max functions in your subquery? That would take care of the ‘dynamic’ problem.
An elegant solution, good stuff.
Would it be better to filter out the rows that are in the future rather than updating them? select * from SnowLicenseManager.dbo.tbluser where LastLogon &gt; '2019-03-18' and lastlogon &lt;= dateadd(dd,1,getdate()) This should filter out all records that have a lastlogon date greater than or equal to tomorrow.
Thanks guys, you pointed me in the right direction. I ended up using: &amp;#x200B; UPDATE dbo.tbluser SET lastlogon = '2019-03-17 00:00:00.000' WHERE lastlogon &gt; '2019-03-18'; 
Ya that’s what I have, I just don’t understand the nested cursor part
1 Install a developer edition of SQL Server at home 2 Create / restore a sample database such as AdventureWorks 3 Start writing queries, attempt different things based on requirements that you make up such as: a. what was the highest selling product last year b. what are the current years sales v previous years sales c. who are the top performing salespeople d. where are the top selling regions etc etc. &amp;#x200B; I would recommend the book T-SQL Fundamentals by Itzik Ben-Gan (you can get a free e-book if you search for it)
there's a numbers table in your utility database too, isn't there i know numbers are easy to generate from thin air, but a numbers table is a lot easier to use, especially if you support rookie developers
You'd have to post what exactly are you running, it seems to work perfectly fine on my end. [Here's a working example](https://www.db-fiddle.com/f/jUBHdD7Q2XBno9gCBKjgsn/0).
Sorry , it's run correctly. i set `date` field as `varchar` instead `datetime`. thank you for the solution. but i still search to find a dynamic query not based on column name. Thank you buddy. 
1NF means to break up data into groups however, not as "complete" as my professor would like. She defines it as "being a control freak but not having enough sleep to break it all the way down"! &amp;#x200B;
Are you fine with creating and running stored procedures? That's the only way to go about dynamic sql, unless you're fine with executing a query, copying its results, pasting it as another query and executing that.
[sqlzoo.net](http://sqlzoo.net)
Ok starting a new root thread for a dynamic solution, [here you go](https://www.db-fiddle.com/f/jUBHdD7Q2XBno9gCBKjgsn/1). The first query will produce a query that you need to copy, paste and run. The second query is exactly that. Now if you change the definition of the price column on the left, add a new field to it and fill it in, [the first query will produce a different result](https://www.db-fiddle.com/f/jUBHdD7Q2XBno9gCBKjgsn/2). So what you can do with this, is instead of copy-pasting, you can save the results of the first query into a variable, prepare a statement with that variable and, run that statement to get the results. [Like this](https://www.db-fiddle.com/f/jUBHdD7Q2XBno9gCBKjgsn/3). Or, and this is something db-fiddle won't let me do, you can create a stored procedure that does it for you, and then only call the stored procedure when needed. Here's the code: DROP PROCEDURE IF EXISTS get_price_change$$ CREATE PROCEDURE get_price_change() BEGIN select @qry := concat('select this_day.date, ', (select group_concat(concat('this_day.', column_name, ' - prev_day.', column_name, ' as change_', column_name)) from information_schema.columns where table_name = 'price' and column_name not in ('date')) , ' from price this_day inner join price prev_day on prev_day.date = date_add(this_day.date, interval -1 day)') into @qry; PREPARE stmt FROM @qry; EXECUTE stmt; DEALLOCATE PREPARE stmt; END$$ I don't know what client are you using to connect to MySQL, but generally you'll need to set the delimiter to $$ first (`DELIMITER $$`) and then, once these statements are executed, reset it back to ; (`DELIMITER ;`). phpMyAdmin has a setting in its query tool for this, [at the bottom](https://i.imgur.com/AAyEdpF.png).
You could also use a free ETL program like Visual Studio SSIS and download SQL Server Express 2017 Express for free, which let’s you store up to 10GB of Data. Move your data into SQL Server Express 2017 and do your analysis/calculations in a much better and more robust environment that supports the functions you need.
Thank you very much. Thats really helpful.
You're getting duplicates because the B.vSeries column has a duplicate value (ending in 49687) that matches the A.ISeries column, so for every value ending in 49687 (there are 9 of them, hence why you're getting 9 extra "results") on table A, you get 2 rows, one with the vDateTime for the first entry, and one for the vDateTime for the second entry. Are there more columns we're not seeing in your example, a unique key of any kind?
First, define exactly what a single 'matching' record is (based on your results, it would seem that there's some ambiguity in that).
Is.... that a row for each possible day of the month? With exceptions defining which months do/don't have a 29th, 30th and 31st day? What in the actual hell is this thing?
This doesn't seem to be the whole table.. Can't get mad at partial tables sorry. 
Now i see the problem thanks, B Table has to have unique B.vSeries in order to take the B.vDateTime and match it only once when i JOIN ON A.lSeries = B.vSeries
Maybe it would be better if there was a column that tracked the month date, like a sequential number. There's not enough sequential numbers. Call it idMonthValueDateId_valueID, for example. Although we have a column for excluding exclusions on leap years, we should also have a column to exclude excluded exclusions for leap years. But let that one be a varchar with values like yes, Y, correctamundo, ay, si, comprende, YASS, yeet, n, N, nigh, noperinos, negatory, etc. ExcludeFor_IdMonth should be adjusted to data type "long" in case anyone wants to store a few quoted stephen king full length novels separated by commas. 
Rows 1 through 28 are exactly the same. Only posted part of it for brevity.
Exactly. This DB also has tables for Year, Month, Date, and WeekDay. Did you notice the IsActive, DateAdded, and DateModified columns? Why would you deactivate a row or want to track when a new date is added to the calendar?? Maddening.
please, dude, post text, not screen shots
What's wrong with this table? One of the best books I ever read was stored as a 30 row table (one row per letter and 4 punctuation marks.) It then had one column with a comma separated list of all the indices where that character appeared.
Leap year is cancelled! Time to deactivate :)
In Sql Server you can just do ... Set x = 1, y= 1, z= 1 Where ...
Like this? Update FruitList SET [Deliveries]=1 SET [Trucks]=1 SET [Buckets]=1 WHERE FRUIT IN ('apple','pear','banana') and [Deliveries]&gt;0 and [Trucks]&gt;0 and [Buckets]=0 This doesn't work because this would change all of them if it met the criteria when I only want to change the ones that are not zero.
On the bright side, you have a short, ready to use number table already populated...
It would be commas in between instead of the word set again, but either way I missed the difference in the where clause...back to the drawing board.
How do relation-headings like this exist, yet I can't find a job doing SQL? &amp;#x200B; Are people specifically hiring for morons?
2,9,4,6,11 .. This explains the entire thing to me. The person that created it could barely count, how were they supposed to know days?
 UPDATE FruitList SET Deliveries = CASE WHEN Deliveries &gt; 0 THEN 1 ELSE Deliveries END , Trucks = CASE WHEN Trucks &gt; 0 THEN 1 ELSE Trucks END , Buckets = CASE WHEN Buckets &gt; 0 THEN 1 ELSE Buckets END WHERE fruit IN ('apple','pear','banana')
This has been my job for years! Scrub old tables for nonsense.
I sure am glad there's an `IsActive` row. They'll be ready the next time we disable a month.
* IdMonthDate and MonthDateValue are exactly the same. Normally I'm all for a separate primary key but here I don't really see the point. * IsActive column is unnecessary unless we plan to deactivate dates in the future * DateModified/Added columns are useless My guess is this was created through some sort of tool. Maybe it was a pre-defined table template that included some of these columns? If it was made by hand that's even worse. Either way, /r/thanksihateit 
Check out Donal’s answer at the following SO link. This should get you started. I’m gonna try it out too, so please let me know how it goes! [StackOverflow link](https://stackoverflow.com/questions/25798702/sql-query-doing-a-loop-but-using-an-array-of-variables) 
&gt; My issue is that SRC_ID IN (A, B, C, D) correlates to TRGT_ID IN (001, 002). What? I'm not being sarcastic; I'd actually like to understand, but this makes no sense to me. &gt; ... there is currently absolutely no existing relationship between these two keys in the database. No relationship between 2 keys? Do you mean no functional-dependency between a key and a foreign-key? If I had 2 relation-headings: ``` Rel1: {SRC_ID} Rel2: {TRGT_ID} ``` ... and a possible relation-body like: ``` SRC_ID: A, SRC_ID: B, etc. TRGT_ID: 1, TRGT_ID: 2, etc. ``` I could-not perform a "standard" JOIN of the form: `Rel1 INNER JOIN Rel2 ON SRC_ID = TRGT_ID` If there was some *function* in your database that connected `A =&gt; 1`, `B =&gt; 2`, etc., that might be possible. Is there application-code (code outside your database) that knows, or establishes, the connection between `SRC_ID` and `TRGT_ID`?
Oh nice! I knew this worked in JS but not sql.
I think the answer to this will depend on which DB software you are running. For example, Oracle has SAMPLE, MSSQL has TABLESAMPLE. Which rdbms are you running?
Would something like: select top x percent * from table order by newid() Work? Assuming mssql. If it's a large table it will be expensive.
Redshift which has sample. I could also go for a temp table with counts and then use a subselect to get the number for one individual. But it's not dynamic. If number of individuals change I need to change my code.
MasterData related tables all have fields like that, and most people in masterdata management will stand by those. It might not make sense for this data, but it might make sense from a standardization point of view (all tables have these fields, no matter what they hold, so you can always see when, who, etc. modified the data last time). There could also be a log in there like &lt;tableName&gt;_log that would hold the entire history of modifications for each given table, and even though it would obviously be empty for this table, apart from the initial inserts, its existence would be part of the standard process.
SQL dev here. Can confirm im a moron
I have an additional constraint of having a column and needing to select 10 percent of rows for each of them
&gt;I'm not being sarcastic; I'd actually like to understand, but this makes no sense to me. You read that correctly, it is absolutely a mess. I ended up taking up a similar approach to what you said. How that relationship works is like the following: SRC_ID | TRGT_ID | A | 001 | B | 001 | C | 001 | D | 001 | A | 002 | B | 002 | C | 002 | D | 002 | So with that auxiliary table I made, instead of making that *UNIQUE_ID* column similar to an identity column (serving no purpose), I re-purposed it to tag it as follows: UNIQUE_ID | SRC_ID | TRGT_ID | 1 | A | 001 | 1 | B | 001 | 1 | C | 001 | 1 | D | 001 | 1 | A | 002 | 1 | B | 002 | 1 | C | 002 | 1 | D | 002 | I can control everything with 1 where clause and obtain absolutely everything I could possibly need, simply `WHERE UNIQUE_ID = 1` But, I don't have an automated way of grouping them like that...so I'll have to think of something.
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). You can do exercise on this resource. Well designed for beginners. For free.
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). You can do exercise on this resource. Well designed for beginners. For free.
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An **advanced** course is also available.
I never realized October and December don't have 31 days while November does
Thank you!!!
[https://www.hackerrank.com/](https://www.hackerrank.com/) may by useful. It's an online service to practice your coding skills. You can choose any difficulty level you prefer. 
February + "30 days has September, April, June, and November." It's so bad it's kind of impressive.
 SELECT a.columns FROM tableA AS a LEFT OUTER JOIN tableB AS b ON b.key = a.key WHERE b.key IS NULL 
You sure about that?
Calendar tables so seem weirdly wasteful.
Nope
Is this part of a warehouse for reporting? 
Accounting and finance fields have a lot of instances where you would want to do something like this. Additionally other qualifiers where the definition of a 'work day' can come / go or not conform to standard calendars. Not ideal in that it doesn't reflect "reality" but tells a better story when you have the right business context.
You joke, but where I used the work they had a global settings table and the setting values were never consistent. Was it 0 or 1, yes or no, Y or N, true or false, it all depended on who wrote the code that day and needed a global setting. Oh, and there was never a guide on what value to use. Fun times!
I have decided we no longer have a 14th of the month. Thank you for your consideration.
Not exactly. There are various approaches but following your lead here: Select * from (Select * from tableA EXCEPT Select * from tableB) q1 Union Select * from (Select * from tableB EXCEPT Select * from tableA) q2
If I'm reading him correctly this is only half. With a join approach you'd want to do a full outer where either side is null. To mirror the type of output he's suggesting you'd need to coalesce rows. Better approach though if you care about being able to tell what the source table was
&gt; Meme Netbeans extracts the results as pdf which i had to convert to an image for illustration purposes, a real text is not needed i found the obvious problem. **GROUP BY** NOT **GROUP\_CONCAT** i keep only the latest dates
Strangely enough, certain chapters of Twilight are 1:1 with pi in this table structure (counting double digit index as two numbers).
Pending your table you might need to generate new ids so it is random
&gt; With a join approach you'd want to do a full outer where either side is null that's equivalent to `A EXCEPT B UNION ALL B EXCEPT A` which isn't what OP asked
Sometimes you have to see bad design in action to appreciate good design.
I can't wait to see the table that defines which years are leap years.
Organizations are funny as are reporting tools and often "calendar tables" are necessary to correctly capture the business logic of different business units within an organization. Where I once worked, March 25th might be considered "March" for prop orders, but "April" for futures orders. Part of sales used a 4-5-4 calendar, so the every day first 4 weeks of Summer were "June", even if some of the days were in May, and all the days of the next 5 weeks were "July", even if some of the days were really in June or August. Point of sale data, however was based on a week starting on Friday. If Friday happened to be March 31st, then the entire following week (through Thursday) was considered in "March". It gets especially messy when trying to compare year-over-year results. That "IsActive" field is probably used in some reporting system - should that day be used/shown in the reporting systems. And "DateAdded" and "DateModified" are often required in enterprise data systems. And it looks like this portion of the calendar was filled out several years in advance so that applications that need the calendar don't run into an error. There's probably an annual process to extend the calendar another year. I see database people (and IT people in general) struggle with things like this often and most of the time things they think are dumb are simply a result of not understanding the business processes and requirements or failing to see how a large organization has complex needs that are difficult to express in a simple way. 
You may want to mark a set of dates as "Inactive" for transactions or reporting. This is pretty common.
Ahhh. I'd be interested in seeing how you end up solving this.
Oh... it's in the order of the rhyme... wow. 
just the months? not whole dates? in a table, not in the query? i'm sorry, i don't believe that i will. thanks, though
That was my assumption as well. I always have a date dimensions table with a billion rows for everything you could want to know about a particular dimension. Yes, I know you can calculate all this stuff on the fly, but I find it more convenient to just do a table join.
I raise you a RAISERROR().
There happens to be a complete reproduction of all of the digits of Pi (spelled phonetically) in the [Library of Babel](https://sites.evergreen.edu/politicalshakespeares/wp-content/uploads/sites/226/2015/12/Borges-The-Library-of-Babel.pdf). Also, happily, there is a much better written version of the Twilight saga among those shelves.
Yeah this awesome, lol, nice find.
What RDBMS?
Yeah it kinda reminded me of a time table our reporting db has, although I find ours very useful (first/last dates of quarter, month, federal quarters, etc). I'm a little surprised at the out-of-hand denouncement of the example though. How many times have we all thought something was dumb until we understood the context, and purpose? Or... it could just be dumb. 
MSSQL
What's the problem with LAG()? Just trying to find something potentially faster?
Nothing obvious to me yet. As I said it worked fine on the smaller set of data set I have now. I'm just fairly new to all this, so I thought someone here may have a better suggestion.
The school district I work for does this every calendar year. If only I could screenshot a few of the tables we have, oh boy. 
Looks fine to me. 
U hireing? 
How do you set a table on fire?
what do you expect to happen if your hrs value drops for X straight days?
Good point, exactly why I posted this here. I guess I have more thinking to do.
I don't understand the entire context here but it sounds like this is an entry form problem and not a data problem.
There are many flavors of SQL you can run on a Mac. If you want one of the easiest to just run without much work for your part use one of the Docker images with SQL installed such as SQL Server. https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker?view=sql-server-2017&amp;pivots=cs1-bash
Macs run SQL very well. You're looking at MSSQL, though. It's only available for Windows and I believe Linux. Most of the open source guys use MySQL. It's available on Mac. If you know your command line, I highly recommend [installing it via Homebrew.](https://gist.github.com/nrollr/3f57fc15ded7dddddcc4e82fe137b58e) There are a lot of other variants, including PostgresSQL. Here's a helpful guide. [https://db-engines.com/en/system/Microsoft+SQL+Server%3BMySQL%3BPostgreSQL](https://db-engines.com/en/system/Microsoft+SQL+Server%3BMySQL%3BPostgreSQL) 
We use IsRowcurrent
Ok, in this market I'm not sure how you can't. I have 5-10 recruiters a week reaching out to me with multiple openings. DBA anyway, not dev.
Generate a uniformly random number between 0-1 for each record ID. Take all the values between 0 and 0.1. That’s a random 10% sample. This works in any database for any dataset.
nvm, got it. i just change the primary key to something else. 
Nope. This table is in an OLTP database. It's almost like the previous developers went to a data warehousing class and then started to implement fuckery all over the place. Pardon my language. This entire database is a POS.
I prefer my Tally table created by Jeff Moden's awesome CTE: [http://www.sqlservercentral.com/articles/T-SQL/62867/](http://www.sqlservercentral.com/articles/T-SQL/62867/)
I've been scrubbin' for days, yo.
If this was in a data warehouse I would be more forgiving. But the previous developers used INTs for dates AND times instead of just using dates and times! Maddening.
True. True.
My favorite comment!
Will share tomorrow. It's just as bad/useless as this table.
Thank you so much, Genius , readable and explained answer to this question. I read your solution more and more and teach from that. I hope anyone who have the same issue in future, find this Topic and solve his own problem. One of the best "Answer/Question" ever i have. I hope everyone have any Knowledge, share it's to other People like you. Thank you for your efforts.
SQL is a language (structured query language). There are various SQL servers for Mac. One of the easier stacks to get up and running that include a MySQL server is MAMP (Mac Apache MySQL PHP/Perl/Python). 
Christ. It's always a bad sign when a joke turns out to be real. At this point, my money is on it being wrong to boot (i.e. not accounting for the 100 year and 400 year exceptions)
Glad I could help. You'll obviously need to adjust the query generator query to your liking in the concat function, but it should get you on the right track.
You should cancel Christmas for the devs. Looks like you should be able to with that `isActive` column. Although it's still better than the abortion of how fucked up the geonames data dumps are. They can't even stick to a single type or purpose per column.
How to set up and learn SQL on Mac [https://www.macworld.co.uk/how-to/mac-software/how-set-up-learn-sql-in-mac-os-x-3638150/](https://www.macworld.co.uk/how-to/mac-software/how-set-up-learn-sql-in-mac-os-x-3638150/) &amp;#x200B; They suggest starting with SQLite (and mention how to install), which I don't disagree and it would be a gentler introduction, or alternatively, MySQL or PostgreSQL, all good options.
Yeah I read that link earlier. It shows how to DL SQLite but kinda ignores the rest. I might just go that route. I’m just a little confused because I’m trying to learn SQL and have never actually used it. 
Poe's law?
One point to get you started: SQL is a standard language used in many products that manage relational data. Non-standard languages that are similar to SQL are also used in products that don't deal with relational data. What is relational data you can learn separately, but the number of products that implement SQL is quite huge. And they are definitely not limited by operating system. Postgresql, mysql, sqlite, firebirdsql, db2, oracle, mssql, and a myriad of derivatives sitting in cloud solutions.
Am I completely off or should you just use GROUP BY on your columns and MAX on ProdTime column ? 
Ah yes... the good old designed by an Excel user tables. A truly amazing creation to behold.
W3schools.com. You could easily breeze through their SQL course in a day. It might be more of a refresher but you may learn some things.
What variation of RDBMS do they use in-house? MariaDB, MySql, PostgreSQL, Oracle, etc...? That might influence the the answers people will recommend. &amp;#x200B; Also, maybe a good idea to remove the name of the company from your post? &amp;#x200B;
codecademy.com have great exercises for beginners
Thank u
I have got no clue what Database they are using, what I am familiar with is PostgreSQL And I don’t how to edit the heading ..
Might be worth asking an admin to change the post title if they can. Generally it's not a great idea to name-drop companies, \*especially\* if you're actively interviewing with them. I've heard good things about Khan Academy. Might be worth checking their sql course? [https://www.khanacademy.org/computing/computer-programming/sql](https://www.khanacademy.org/computing/computer-programming/sql)
How are you joining to the outer join table? What's the on statement? My best guess is whatever you're using to join on doesn't exist in the outer join table, so you just get null back. 
What loop? Also, both your lead and lag values could be null. Also, if you are taking an "arbitrary" value, how are you ensuring that your 'hrs' values are increasing over time?
The query works fine before I add the AND t.column IS NOT null, and there are a lot of joins involved, both full outer joins and left outer joins, mostly of the form xx as A ON A.id = OR.id (where OR is a table on which the first select is ran, but as I said, it returns expected data before I add this little bit)
You've double-checked to make sure the IDs exist in both the a table and or table? And made sure that all of the fields in the outer join table have data, instead of just null attribute fields?
The IDs have to exist, otherwise I wouldn't be seeing the results that I am seeing before I add the problematic bit. (The query is an "inherited" query at work and has been called many many times by users, obviously) The column I am trying to filter on is FALSE by default, so a null in that column is unlikely (and I am not interested in any other column of the table t, just this single one)
If you manually find a record that you anticipate to come back in your query, can you filter by that ID instead of on where the attribute field is NULL? What are the results then?
pls show sql
Lag and lead won't both be nulls for every "bad" entry if it is ordered chronologically. The loop starts at the frist bad entry and adds 5 minutes (the "arbitrary" amount" I may change this to an average or) to the last good HRS value.
I woke up and looked at the table again and I'm furious all over again. And for some reason now it's because there are these completely unnecessary DateAdded/DateModified rows and those seem to be the only actual date fields. There should be some kind of support group for you.
For the date. Convert it to a date (kill the time). Cast(order_time as date) Looks like you're going to have an odd query. Select Service_type , cast(order_time as date) , stuff() , count(1) From x Group by Service_type , cast(order_time as date) , stuff() 
Obfuscated table names and column names because work stuff and work secrets, but basically this: SELECT SUM(coalesce(t3.time, '0:0:0'))) FROM t1\_something\_xx AS t1 INNER JOIN t2\_something\_xx AS t2 ON [t2.id](https://t2.id) = [t1.id](https://t1.id) FULL OUTER JOIN t3\_xx AS t3 ON [t3.id](https://t3.id) = [t1.id](https://t1.id) LEFT OUTER JOIN t4\_xx AS t4 ON [t4.id](https://t4.id) = [t1.id](https://t1.id) LEFT OUTER JOIN t5\_xx AS t5 ON [t5.id](https://t5.id) = [t4.id](https://t4.id) FULL OUTER JOIN t6\_xx AS t6 ON [t6.id](https://t6.id) = [t1.id](https://t1.id) WHERE t2.z\_id = number AND t1.vac = 0 AND (t1.kind = 6) as total\_time ... so far so good, it works. (number and xx are variables passed from Python side using psycopg, and they are dynamic, so it makes testing queries especially as long as that one difficult in terminal) If I add AND t5.column IS NOT NULL before total\_time, it breaks down. 
you could always try a CTE for something like this. Other answers are helpful, too; but, I like CTEs myself (just personal preference I suppose). With a CTE you can use the Row_Number() OVER (ORDER BY your_column ASC/DESC). Then you can grab row number and not a value within the row to calculate. I found this very helpful when I had to calculate recursive averages, and daily usages between values from day to day (subtract yesterday from today and display the value recursively). Hope it helps
We could speculate, but an execution plan would be very telling. 
Okay - I think I need to have one of the senior admins here grant me more access, I'm getting a SHOWPLAN permission denied in database 'ourdbname'. Thanks for the tip!
If you can't get access they should be able to pull/send to you. 
Inners are not faster than outers. It doesnt work like that. You could do a simple left with a where filter like a.value = b.value and sql might convert it to an inner join behind the scenes. Now you have a left that executes as an inner. You could also have a covering index for a left and not for an inner, in which case the left would perform better. Let's break down your query. With dates as (select date 
You can try [array_agg](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#array_agg) to group your individual service_types by date, customer_id, and order_payment. If you use the DISTINCT and ORDER BY clauses within array_agg to deal with combinations and sets, that should handle your equivalent cases. After that, you can group by everything else and aggregate unique customer_ids. 
The first example will pull people with the current position of Sales Rep while the second solution is only pulling those that have had the title before their current title. 
Completely untested but I'm pretty sure this is it: SELECT * FROM employees e LEFT JOIN jobs j on j.JOB_ID = e.JOB_ID WHERE j.JOB_TITLE &lt;&gt; SALES REPRESENTATIVE AND EXISTS ( SELECT 1 FROM job_history WHERE JOB_ID = (SELECT JOB_ID FROM jobs WHERE JOB_TITLE = SALES REPRESENTATIVE) AND EMPLOYEE_ID = e.EMPLOYEE_ID) &amp;#x200B;
Premature save an all that. Where's ssms for the phone =l
Thats where i learned. 
Ha! Sounds fun. I've said far worse dealing with fuckery. 
well, it seems that you're implying that there's other code changing your values beyond what's you have written. Just pointing out that if for a given emp_ID you will have HRS sequence of 4,3,2,1 (for corresponding dates) you will end up with 4, null, null, null after your first statement.
How do you know that records where t5.column is NOT null are there in your base result set (prior to summing them up)? I would suggest testing your hypothesis (that t5.column could be not null) first by doing "select distinct t5.column from" instead of "select sum(...".
Isn't what we want is who previously worked as Sales Rep but currently worked on different jobs?
Could the left join be giving the query optimizer more flexibility in how it plans and affects the order in which it performs the joins? That is something I see at work when looking at slow queries. 
...yes-ish? For your regular app, the fact that your cluster uses AG would be transparent (or ignored, if you connect to an instance directly). For an AG-aware app, you need to configure a replica for read-intent and the app needs to specify read-intent in the connection string. I've never worked with this config so I don't really know all the details. I would consult your DBA for any concerns/considerations with AG. Depending on your business ask and the size of the DBs, you might be able to set up a process to restore backup to a second instance. I feel that this is a much less involved route.
Yea, I didn't show the code it here because I'm on mobile. But the loop will for example will change that sequence from 4, null, null, null to 4.1 4.2 4.3 4.4.
there are tons of places to get data sets online. Kaggle.com, r/datasets, or searching github will provide plenty. Download some interesting ones and see what you can do with them. I prefer to mess around with stock and finance data.
I am making a case to our vendor now to get us SHOWPLAN access to our Dev/Test environments. Thank you! 
&gt; You could do a simple left with a where filter like a.value = b.value and sql might convert it to an inner join behind the scenes. Now you have a left that executes as an inner. What? Give an example, please. &gt; You could also have a covering index for a left and not for an inner, in which case the left would perform better. What again? So, let me get this straight, you have a covering index for select &lt;columns&gt; from A left join B on &lt;conditions&gt; and you change to select &lt;columns&gt; from A inner join B on &lt;conditions&gt; and your index stops being covering somehow?
So I've been doing some reading up on indexes since you posted this and I appreciate the example you've provided on the query, I'm going to try applying some of what you've shown in my environment and see what the impact is. Now a note on our environment, I am accessing an application DB on a server that is vendor owned and we're generally understood to have "read only" access. The available indexes are defined and able to be referenced in an html DB Info document, so I don't believe that I'll be able to create indexes if/until we manage to get our own hardware (I get a permission error when I tried to create an index in our dev environment). We're also unable to create stored procedures which has been fun considering the amount of daily reports I pull out of this thing.
What problems are you having? An ever-increasing number of records is expected for many kinds of tables and shouldn't automatically be a problem.
Generally, if a table starts out small you may not have/need any secondary indexes for queries (as a full table scan may be fast enough). However, eventually, given enough growth, a proper index strategy is needed. In theory, the order of how to handle things for increasing rows might be: 1) Proper index strategy 2) Partitioning of large tables 3) Sharding A good archival/cleanup process is also important if old data can be removed/stored elsewhere once it isn't read actively
https://youtu.be/Hgbhdgf6Y3M 7:45 and you'll see a left run as an inner. And yes, an outer and inner are two different execution plans, but can be the 'same'. You're dealing with loops, merges and hashes. A covering for an outer would result in an index and a nested loop but in the case of an inner vs left (especially in a Y or self) you run the risk of spilling over into hash matches and or key lookups. I'm not sure I have a video example for you on this. 
Well, there's a few things you need to consider: 1. Is it actually a problem? Don't forget that a properly indexed table can remain useful even when it grows HUGE (e.g. I have a 5TB non-partitioned table which is indexed and accessible by indexed columns fine, but a full table scan currently takes several hours). 2. Have you considered physical storage requirements - things like partitioning data by month/year can help in certain circumstances. 3. How big is the table going to grow? Are you bound to keeping all that data for a fixed period (e.g. 7 years). Can you delete it at any point?
Ok I will be more precise, I have to invoke a query that selects top 50 sorted by ascending order. Now that the number of records are increasing, the execution time for query is getting slower. This query is invoked via a REST service that is written in Node.js Since the query execution time is increasing the response time of the rest api is also increasing which is not desirable for me. 
Optimization is an important thing. You could make a crazy query run acceptably fast with some physical modeling and code optimization. How many files does the db use to store data? Are the clusters separated from the indexes? Simple stuff with such a huge impact.
Do you have an index on whatever column is being used for sorting?
Yes, there a primary key and we are sorting according to this primary key only. 
&gt; If I add AND t5.column IS NOT NULL before total_time, it breaks down two comments -- first, i am unfamiliar with the "breaks down" error message if you mean it doesn't return what you think it should return, then i suggest you investigate the data, because, asl the kids say, "sql don't lie" second, there's no way, **no way** you can seriously understand how a query works that has two FULL OUTER JOINs in there...
I think our team is hiring not sure though im basically still a junior dev
By "it breaks down", I mean it doesn't return any data (the frontend shows None) And about the other thing, yes I don't understand this, but I have to somehow add the feature that was requested. The query is something that already exists.
According to the information you listed, yes.
That's pretty weird. How many rows does it have? Have you rebuilt indexes? What's the actual query being run? What's the query plan?
Around 300 million rows. No I haven't rebuild indexes. I even don't know what rebuilding indexes is. **select top 50 from table_name order by primary_key_column** 
| What | the | heck | | :--- | :--- | :---| | is | wrong | here?|
Why are you selecting top 50 out of 300 million? What is the point of this exercise?
Can't say that, confidential, sorry But it is something related to lottery. 
You're going to need to d a little bit better than that. Statistically there is nothing about those 50 that "do" anything and there would be a variety of ways to get what you're looking for without selecting from the 300 million. How is new data being pushed into your source table? Why not just select top 50 from the import process and call it a day? If you're doing something like 'randomly' picking top 50 every day to see how long it takes to match 2 numbers, or whatever... I'm not sure what to tell you. You could partition your table possibly to have new data on one partition and old data on another, or break your table up into 2 and write a sproc... So you take the new data you receive and put it into dbo.newTable. You write a view that union's dbo.newTable with dbo.oldTable, and then before you ever import new data you run a sproc that takes the newTable and inserts it into the oldTable and then truncates the newTable. Sans something like that you, you'll have to live with it. 
I'm going to assume you're doing something like this: https://www.brentozar.com/archive/2018/03/get-random-row-large-table/ If that is the case, you can't do anything. You want to generate a random sample of your table (not sure why 50 out of 300m, what w/e) and you need to hit the entire table in order to do so. I imagine you're doing something like looking at distributions of "new" vs "old" numbers to see what the probability would be of new vs. old numbers appearing in your sample. Even if you're doing some marginally like this you'd need to hit the entire table, and you need to evaluate this mechanism in your "confidential process" if it is not desirable, because it is only going to get worse. 
That's a lot of rows, but it doesn't seem like it should lead to bad perf on an indexed search. Check out the query plan. Rebuilding indexes: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/reorganize-and-rebuild-indexes?view=sql-server-2017
If your date keys are integers that look like dates (20190319) and there are not foreign keys to claim, I have an explanation. If not, the explanation could still be useful. Suppose there are dates in the date table from 20000101 to 20191231. Statistics will notice the range between max and min values of 20191230 - 20000101 = 191129. It will also notice that there are 7304 unique values in the table. This works out to a density of 7304 / 191129 = 3.8%. This can affect the query plan. When you inner join claim to datetable, the optimizer sees you are joining integers with a density of 3.8% with unrelated integers with a density of 3.8%. The optimize might expect statistically that 3.8% * 3.8% = 0.14% of the records will actually match. So optimizer might estimate that instead of returning 10 million records from claim, only 14,000 records will be returned. Now join to the date table 3 times. 3.8% * 3.8% * 3.8% * 3.8% * 10 million = 21 records. The end result is that the optimizer could make some poor choices for handling the query, expecting 21 records, but actually getting 10 million. Changing the joins to outer joins fixes this, because the optimizer no longer expects the joins to reduce the record set, resulting in better choices estimating a 10 million record set. The optimizer is a bit smarter and more nuanced that what I described, but that is the concept that might be affecting performance.
Are you creating new rows, or do you only want the rows to appear in a SELECT statement?
SQL does not recycle Identities, nor is it a good idea to try and force it as it has the potential to mess up links between tables (for instance if only part of the delete fails you might still have a record #12 for one table and then someone tries to add client #12 and suddenly information from the previous client #12 shows up or you get a cannot insert because it violates primary key) Honestly the simplest solution would be to have one office reseed their identities to the end of the other offices Identity when the merge of the two databases happens and to keep identities rolling forward from there. 
I suspect your example still applies - my Date Table has 54000+ Rows, and the DATE_KEY is pretty much Milliseconds before or after Epoch time. (before 1/1/1970 the key is a negative numeric, and every date key is incremented by 86400000)
Yep. That's it.
Thanks!!! https://imgur.com/gallery/WrKPhfd
Usually, but some software relies on advanced database functions that don't work the same in AG. If the vendor themselves won't provide support for it then you are in uncharted waters where it may or may not work and out may or may not break features they use now or might add in the future. In most cases being under vendor support is more useful to long term "availability" than almost any other consideration. When it crashes and you call in a ticket they are going to hammer you on the unsupported db configuration and end the call there until you get back to a supported config.
I believe I'm attempting to create new rows.
AG comes with a potential performance impact on all write operations, I find this is often 20-30% increases in wait time compared to a single db running simple recovery, so maybe that could be an issue. Also read only replicas are only in sql enterprise licensing last time I checked, enterprise costs BIG $$$ so if you weren't using it before that's another consideration to factor in. Since the app doesn't actually support any of these tricks then I would assume your heavy read operations are all due to some custom reporting you do against the data. Could be easier to just build an ETL process to move the necessary stuff to a separate database that you control fully for reporting and archiving. That doesn't address the desire to have failed node redundancy but you can only do so much on your own. Sql has had dozens of approaches to this problem over the years and your DBA is the person who is best suited to suggesting work sounds within your environment. If your DBA doesn't have any ideas on this then that is a red flag in itself as far as how risky it might be to ask them to cook up a scheme that is unsupported since they may not personally have the experience to dig themselves out of the hole you are considering digging. 
 SELECT GroupKey, CONCAT(a.GroupDesc, ' ', b.TypeDesc), b.TypeKey FROM GroupTable a OUTER APPLY (SELECT * FROM TypeTable WHERE a.GroupKey = 2) b You'll have to format that CONCAT to only include a dash if b.TypeDesc is not NULL, but otherwise, that's your code.
You, my friend, are a genius! Well, relative to me anyway. Thanks very much. That was perfect. &amp;#x200B;
No problem.
&gt; 7:45 and you'll see a left run as an inner. Ok, that's probably on me not connecting "simple left with a where filter" to a query with join/on condition as well as the where part. Yes, the optimizer can re-write joins (even subqueries) as needed IF a shortcut is detected (full to left join, for example). &gt; A covering for an outer would result in an index and a nested loop but in the case of an inner vs left (especially in a Y or self) you run the risk of spilling over into hash matches and or key lookups. Why would there be a "key lookup" if there's a covering index? Key lookup (by definition) is key search in a non-covering indes. 
you do not need a subquery use COALESCE to prevent nulls move the WHERE conditions to the WHERE clause SELECT @StartDate , ... , COALESCE(SUM(Table3.Col2),0) - COALESCE(SUM(Table4.Col1),0) AS result FROM Table3 JOIN Table5 on Table5.Id = Table3.FinanceAgreementId JOIN Table1 on Table1.Id = Table5.CompanyId JOIN Table2 on Table2.Col1 = Table3.TradingCompanyId JOIN Table4 on Table4.CompanyId = Table5.CompanyId WHERE Table3.Col4 = @StartDate AND Table3.Col1 &lt;= @StartDate AND Table4.CreatedOn &lt;= @StartDate GROUP BY ...
Option 1 was the ticket. I had a feeling I was missing something really obvious. Thank you so much for you help.
How to deal with ever increasi--it's confidential.
Virtualbox is free and is pretty good for creating VMs. &amp;#x200B; I would however avoid installing anything until doing an introductory course. Try this one for example : [http://www.sqlcourse.com/intro.html](http://www.sqlcourse.com/intro.html) It has an embedded SQL interpreter. &amp;#x200B; Then once you're comfortable, maybe download Virtualbox, a Linux distro, create a VM, install (my personal favourite) PostgreSQL, fool around, create another VM, get replication setup etc etc. &amp;#x200B;
If the deleted record was the last you can execute DBCC CHECKIDENT ('yourtablename', RESEED, thelastvaluefromyourtable). I didn't understand very well what you are trying to do, by what I understand there's a column with repeating IDs, from different databases that need to be merged, if this is the situation, I would recommend that you delete the primary key from the table and create a new surrogate, using UNIQUEIDENTIFIER type, to maintain identity even between databases, then restablish the dropped foreign keys, using this new column. I did this once and it worked fine.
WHERE MONTH(myDate) = 3 (for March)
I'm trialling it purely for the gitlab integration which seems to work nicely. In terms of functionality for the DBA/Dev it seems light years behind SSMS. 
Looks like a big Postgres release today hopefully addressing some of your pains 😊
I honestly can't think of any off of the top of my head. I even googled it and didn't see anything I recognized. I've been working pretty heavily with SQL over the past decade. I am not a developer, but even the true SQL developers I have worked with I do not believe had any official certification. They were more programmers who also knew SQL. There is definitely a lot of SQL work to be done, but I wouldn't put off job hunting until you have a certification. I am sure you are fully able to pick up some SQL dev work right now.
One route is to look at the MS SQL server admin certs (or Oracle database SQL certified associate or mySQL database admin). That way, you get the knowledge of triggers, SP, access and permissions...ect. I'm working that route myself but I'd like to spread out to BI/reporting too. In talking with several recruiters, if you don't have a strong background on something, pick up some of the surrounding skillsets to bolster your resume. 
I thought about this a bit recently... I decided that anything too time intensive or expensive probably wasn't worth it. I decided to use [DataCamp](https://www.datacamp.com/courses/tech:sql) to get the baseline certifications for SQL. Their certs can be added to LinkedIn and (in my opinion) show a basic understanding of SQL. I believe all you need is a [month subscription or the free option](https://www.datacamp.com/pricing) to knock out the certificates and add them to you LinkedIn profile. Best of luck to you!
It says month is an invalid identifier. I remember it being something like to_char.... 
What's the error message?
This. At best, certs will get you to the technical interview, but they won't get you the job. What you really need to do is be able to show you know what you're talking about. So sure, get some certs if you think they'll help get you to the technical interview, but any technical interviewer worth their salt won't care what certs you have. They'll put far more stock in the answers you give.
 WHERE MONTH(date_field)=4
For entry level I don't think you need any certs. I'm my experience there is a shortage of sql people for the number of data centric roles. Getting your basic skills to the point where you can confidently talk about them and how you have utilised them and being genuinely interested in developing them further and being keen to learn more will get you further in the data development industry then any cert
I feel like there's an order table missing. But I don't know your data as well as you do.
I would think you want something like this, though I don't know what DBMS you're using select a.CONFIRMATIONNUMBER, a.TIMEOFORDER from ADDRESS_TABLE a join CLIENT_TABLE b using(CONFIRMATIONNUMBER, TIMEOFORDER) where a.TIMEOFORDER &lt; CURRENT_DATE() and a.TIMEOFORDER &gt; CURRENT_DATE()-30 and b.DOB &lt; .... (dont know what date this would be for 70 yrs old) and a.ZIP = '(zipcode you need)';
It has to be proportional to the number of row from that specific source. Let's say I have 1000 rows that have value A in column source. I need to have 100 from that. If I have 50 rows from source B I need 5. That is the basic logic. I know how I would do it programmatically but in SQL it's a bit different. 1. I was thinking of firstly generating a table with distinct sources and counts/10 2. Generate a row number partitioned by source order by random. 3. Join the two tables generated before and join by source but not sure how to deal with the second condition in a join It would go something like where row number field &lt; count column Writing this comment I think I just answered my own question :))
MTA are fairly easy. 
I learnt SQL when working in application support. If you’re looking for entry level certifications MTA and MCSA is a good start but as mentioned you need to be able to apply it. They might get your foot in the door but don’t discredit the opportunity to look at database administration roles or support roles to get the experience you need. There are variants of SQL, a lot of discussion over which to start on but my recommendation would be TSQL then maybe look at learning some PostGres or MySQL if you can too. UPDATE COMMENTS SET SENTENCE = ‘GOOD LUCK!’ WHERE SENTENCE IS ‘’
I’ve worked in technical SQL role for over 7 years and can say, when hiring, I couldn’t care less about certifications. Almost all test answers can be found online so having the certs doesn’t really mean anything to a lot of hiring managers. If you want them, go for it, just realize it may not do anything to get you ahead of a candidate that doesn’t have one. I think the caveat would be if you’re looking to be a traditional DBA, it would be helpful. I’d focus on getting a good foundation of the basics/intermediate SQL skills. We always give one or multiple exercises to get an idea of what a candidate knows. We’ve had people interview who had 2 or 3 advanced certs that couldn’t create a mildly complex stored procedures. It’s all about how you think things out and how you go about solving problems. Good luck!
I appreciate the response! After taking a look I will definitely try to knock those out for my Linkedin as it seems like a good spot to get started.
Hmm, I haven't been programming much in the past couple months so I feel like trying to jump in to a SQL job interview right now I'd probably completely bomb it lol. So I'll definitely probably take those courses the one dude suggested and get my self in gear. Also when job hunting for Database Positions I noticed they typically "require" 1-2 years experience which kinda may me shy away and made me think a certification could make up for that which led to me to asking this question.Once I've retoned my SQL skills I'll be more confident in applying to those positions and answering interview questions. I appreciate the advice, Thank you!
I appreciate the answer, definitely helps to get a response from someone who does hiring haha! In my current state, I wouldn't even say I'm ready for a job in SQL atm but I feel like setting goals/deadlines to complete is the best way for me to learn which led me to think a certification would be a good goal. Its sort of relieving though that there are no "must haves" cert though as I won't have to pay for them haha. Thanks for the advice!
Id consider someone with a 461 or 761 mscert equal to a BS with no experience, if I could only interview a few people. A lot of the 'advanced' SQL techniques are not taught in your average uni. 761 and 762 and you are at the front. I'm not sure about the "all Answers are online" ... even if they are, you took that extra step where others have not. I've met far too many in a sql development role with a bscs that left me scratching my head. I've met self taught individuals with 2 years xp and with no degree that were beyond someone 15 years in. But it all comes down to the interview. I look for hunger and passion. It's easier to teach someone who wants to learn vs untrain someone who settled with what they currently know.... but they need a solid footing SQL.
good for you! I graduating in IS , around 2008, it took me 6 months to get a shit helpdesk job paying $28k a year. I unfortunately stuck myself there for 4 years before getting a real raise to $60k then they let me go for making too much. After that I pretty much fell into SQL which I was always interested in, and now work on ERP systems.
Anytime. If you're interested in an analyst/bi role more than a pure developer, I'd say get a little experience in some BI visualization/reporting utility after you get a bit of a foundation in SQL. I'm not sure where you're out of, but in our area, business/data analysts are a lot more sought after. If I had an entry level guy interviewing with some sort of experience/understanding in that area, they'd likely be near the top of the list. Good luck
I'm a Database Developer have been for almost 6 years. I currently do data warehousing. I don't have any certifications, as far as I know no one I've ever worked with (developers) have had any certifications. I've worked at 2 different Fortune 500 companies so it's not just small companies who don't care. Assuming you took some database classes in college and know how to JOIN some tables together and understand how to use aggregate functions (SUM, MIN, MAX) start applying for positions. This is a very in demand skill. If you want to accelerate your job search, call a staffing firm. They get paid to find you work. Look for a shop where you'll be surrounded by other developers. You'll want to have peers to learn from otherwise you'll be stuck in an entry level position.
Hey everybody! &amp;#x200B; Thanks for all the help! &amp;#x200B; I ended up having to use a bastardized generate\_series code. &amp;#x200B; Code included below. including the final join code. &amp;#x200B; &amp;#x200B; with tbldates as ( SELECT sdate::date FROM (SELECT '2018-01-01'::TIMESTAMP as tm UNION SELECT '2020-12-31'::TIMESTAMP as tm) as t TIMESERIES sdate as '1 day' OVER (ORDER BY tm) ), tblempid as ( select distinct employeeid from employeetable) select sdate, employeeid from tbldates cross join tblempid as tblempdate &amp;#x200B;
Your explanation makes no sense when compared to the desired output. You said "if the item in the UNMBR column contains the same value, group all rows", yet there are two rows in the desired output for 0000 (W7ZYAA, W94E02). Also the desired output does not correspond at all to the contents of the first screenshot, which I assume is because there are a lot more records in the source table. But it's impossible to figure out what you need without seeing source and the corresponding desired output. What you're telling us now is "I have rows X and Y, here's X. And this is the desired output produced from row Y". You'd need to provide desired output for the 40 rows shown in the input. &gt; I want the UIC value replaced so that they are displayed with the first complete alphabetical/numerical order value first, with the remaining values being added only as a "\" and the last two values of that UIC. This will not be possible in Access's SQL alone. It requires some sort of `group_concat` or an equivalent, and it's not possible without implementing VBA. It would be tricky even in a sane SQL variant, and certainly does not meet the "This has to be simple query" precondition. 
Information Systems, is that synonymous with Information Technology? &amp;#x200B; I'm learning SQL myself, it's quite good.
1. It's hard to read, especially with lots of tables. 2. It smushes together the structure of the query (joins) with the logic of the query (filters). 3. It won't throw an error if you miss a join condition. 4. It's easier to leave out a join condition, especially with lots of tables. 5. Leaving out a join condition has a realistic chance of crashing a database. 6. It's less flexible. How do you write a full outer join? 7. It's incompatible with correlated sub queries, aka CROSS APPLY. 8. It's incompatible with some language specific syntax, join hints. 9. Your teacher doesn't like it. 10. I don't like it.
COALESCE is just shorthand for convenience. You can recreate the same logic in your join with AND/OR
Thank you for the feedback, very insightful
Yes, but you will kill all performance.
If the tables were realistically large, this will perform dramatically worse than a join. Magnitudes worse.
Not usually. In major applications, this syntax is equivalent to "join...on..." syntax and/or is optimized the same way. That said, it still sucks for all the reasons /u/jc4hokies mentioned.
Change the client table so that you use both the clientId and officeId are both the primary key. It's called a composite primary key. This way you can have duplicate clientId fields but the combined officeId and clientId together must be unique. &amp;#x200B; |officeId|clientId| |:-|:-| |1|1| |2|1| |1|2| &amp;#x200B;
So, you just need a table that lists all the roles, a table that lists all the users, and a table that lists all the roles that a user has associated with them. Correct?
because you are doing a cartiesian join where you are joining every record in table a to every record in table b and then filtering to only show where section in a matches section in b. If you put the inner join predicates where they go you only get the records you want the first time.
I read everywhere that it is hard to read. But I started working for a company where all of the people who write queries join like this. I have used inner joins in the past, but I like this better. Its faster to write, and its not hard to read at all. So I started to do my joins like this. I even started using (+) for left and right joins. &amp;#x200B; You do what works best for you. You follow the practice that everyone in the company follows. Just be able to understand either or.
Readability and maintainability. &amp;#x200B; You write code today to solve a problem you have right now that you understand because you've spent time wrapping your head around it. &amp;#x200B; Later, someone else comes along and is tasked with modifying this code. If they find a jumbled, disorganized mess, written by someone who likely understood the problem, but was too lazy, disorganized, untrained, or undisciplined to write their code in a way that made the intent clear, now they have double the work to do to maintain this code. &amp;#x200B; They first have to make sure that they understand what it was trying to do and determine if that's what it was actually doing. The JOIN type is not explicitly stated so which type of JOIN was intended and which one is actually happening? The person editing this query later might not know and need to go look that up. There is no ON clause, but there is a filter applied with the WHERE clause instead. In this example, it's easy to understand the intent, but in my environment many tables are joined many different ways, and you would go crazy trying to map back every condition in the WHERE clause to which JOIN it was supposed to belong to. Using the ON clause makes its purpose far more clear. &amp;#x200B; After all of the mental gymnastics required to determine what the code is intended to do and actually doing, they can finally get to the business of modifying it to do the new thing. But first, they will likely rewrite it so they can be sure that their edits will do what is intended.
This is incorrect. Functionally "WHERE x.a = y.a" Is the same as an inner join.
\&gt; Your teacher doesn't like it. And neither would your employer! &amp;#x200B; \&gt; I don't like it. And neither would your replacement!
Yes usually. Google performance if joins vs criteria.
 SELECT T.*, D.* FROM TABLE1 T LEFT JOIN TABLE2 D ON D.VALUE1 = T.VALUE1 AND ( D.VALUE2 = T.VALUE2 OR (D.VALUE2 IS NULL AND D.VALUE2A = T.VALUE2) ) &amp;#x200B;
Awesome reply. Helped more than just OP. I'm curious though, are there good reasons for it in some scenarios? For Context: Within our oracle database we have a View that has 10+ joins all written like this. Mostly joining on keys and between Dates(goal is "active users" with some basic data for grouping). All the data is pre-ingested by DBAs (I'm a BA) - but finding the need to occasionally do my own aggregations to capture the correct business logic. We have access to some data-viz tools that connect directly to the database, but I've found SQL is sometimes easier to get what I want.
Those are non-ANSI compliant joins. American National Standards Institute (ANSI) has been issuing standards since 1986 for SQL. While many of the SQL Engines will support non-ansi compliant joins but I've seen them occasionally act really funky. Look up ANSI compliance if your interested in the changes made over the years.
Out of curiosity, in your new role are you working on Oracle? In my experience, the WHERE instead of ON usage tends to be an Oracle thing, mostly due to old habits...
&gt; I'm curious though, are there good reasons for it in some scenarios? No. I'm wracking my brain trying to imagine some obscure dynamic SQL edge case, but coming up with nothing.
Is TIMEOFORDER a date/time or just a date? I would be a little concerned that it time is included, joins may not work properly since they wouldn't match. CONFIRMATION NUMBER may be enough to use for a join. 
I'd argue it's worse with more joins, as it makes it harder to follow, and adds more opportunity for error. Say there are 12 joins like this, but one day you accidentally delete the 7th join condition from the WHERE clause. Now you run the query and it doesn't throw an error, but is taking forever. You get lucky and it eventually finishes and you see that you have way too many rows, because you've effectively cross-joined a table. Now you have to hunt for the missing join criteria amongst all the other ones (and any actual filtering criteria that you also have). 
Is your myDate input an actual date variable? If it's stored as a varchar, you will run into issues. You are able to cast the varchar to a date though, using the str_to_date() function, see link below. https://stackoverflow.com/questions/24160484/convert-varchar-to-date-in-mysql
What about for something like google BigQuery where “or” isn’t allowed in a join condition?
I work with an Oracle based ERP. The amount of ANSI 89 joins is mind boggling, with right joins, no less. We're talking 5 to 8 tables with huge swaths of the join criteria. It's a mess. If I am copying code I don't want overwritten in a patch, I always rewrite it, even if I'm only going to use it once.
&gt; just the months? not whole dates? You'd have to ask the business owners/users. If this is the complete table structure, then clearly if it's being used then it met the needs of the business. &gt; in a table, not in the query? Sure. You set up configuration stuff like this in a table so that you can make a process where non-DBAs can make the required updates. This also gives you the ability to have an audit-trail of who make what changes to the config table. If you wire this into a query, then changes require a DBA to alter the query, maybe on a regular basis. Plus sometimes what the business needs is pretty complex and difficult to express in mathematical logic that can be converted into a query. Sometimes the business logic is, "this change needs to happen when we decide it should happen". That's hard to code into a query without some kind of table to store the decisions of the business users.
For me it was interesting! I practiced using [stratascratch.com](https://www.stratascratch.com). They have questions from technical interviews from other companies so I found it helpful to use for interview practice. I think this platform may be interesting for you too.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have questions from technical interviews from other companies so I found it helpful to use for interview practice. You can do beginner exercises on this platform. I like [sqlzoo.net](https://sqlzoo.net), datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches.
Actually those **are** ANSI compliant joins. The JOIN operator was introduced in SQL:1992. Before that using the implicit joins in the WHERE clause was the only way to write a join. 
Even Oracle recommends to stop using the proprietary `(+)` operator. The reason so many people still use the old-style, implicit joins in Oracle is, that when Oracle introduced the explicit JOIN operator in Oracle 9i there were a lot of bugs related to that (especially when used in materialized views). So in the Oracle world this "explicit JOINs don't work" kind of stuck and even though 25 years later all those bugs are resolved, many die-hard Oracle DBAs and users still think it's unsafe to use. &amp;#x200B;
I like your flair. 
You can always put the same logic in the where clause. 
I don't know how things are organized at your company, but in my experience there's often a HR screening interview before the technical interview. Do you think certs are useful to get past the screening interview?
&gt;where c.course_no = s.section_no Too ambiguous. You should have primary key and foreign key fields having the same name and type.
I'm on DataCamp, they ran a $12/month deal in early January and I signed up. Most of the courses take 4 hours to complete. After a course you won't be proficient with anything, but you'll know enough to get started. &amp;#x200B; So my $0.02 on the value of certifications: 1. They might help you get past an HR screening interview 2. Self-study in general indicates a certain amount of drive, passion and self-discipline that might reflect well on you. 3. From a practical standpoint, certifications (and formal training) put structure around a subject. Without structure, it's easy to have holes in your understanding and knowledge. At least with certs you'll be studying a body of knowledge and you'll know what you don't know
JOIN keyword instead of comma, &amp;#x200B; Your current query is small but if it's large then it would become difficult to read. Suppose you placed the code in a report and 2 days later you want to change it. It would be irritating even for you to read that, been there.
Are you using SQL Server or MySQL?
I finally doubled down and typed the whole looong query into psql, only to discover t5.column is ALWAYS null. I have no idea why all those outer joins are there at all... why join those tables if that query isn't using t5 at all? Something to ask my work colleagues when I get a chance.
Wouldn't this return duplicates? I don't see it running WHEN a join returns specifically null cols. &amp;#x200B; (will try later and comeback with an answer)
My teacher literally taught my class to write SQL the way OP has in my capstone class for my MIS Bachelor's. I learned the right way on my own at my first job...
I could not pass my IT exam by the first attempt because I was in lack of a proper study material. So I could not prepare well which got expected results and I failed. But I am thankful to Realexamdumps.com for being there this time and offering help in the form of [**70-764 Dumps**](https://www.realexamdumps.com/microsoft/70-764-practice-test.html). This study material encompasses each syllabus topics. It packs each concept in a very concise manner. It was almost impossible for me to go for my IT exam without [**70-764 Dumps PDF**](https://sites.google.com/site/microsoft70764dumps/). My good wishes are with all IT students and everyone who was part of this stuff compilation. 
I work in a shop with 14 web developers, and more than half do not have a college degree. More than anything, proven skills go much further in that field than education. This is very different from data sciences. I am personally friends with 3 Data Scientists, all of whom have a masters degree in Data Science (a lot of math and stats). &amp;#x200B; Data Science is a field where proper education will get you much further, because you can't get a masters degree without proving you have a deep and well founded understanding of the math and stats you apply when working in that job. Sure, you can learn those skills on your own, but, good education has a lot to offer in that field. For instance, I've thought about becoming a Data Scientist (currently, I am a Project Manager, but my background is in DB architecture) but there is a mountain to climb between what I'd call a Data Analyst and a Data Scientist. You could become an analyst with minimal education, and that could eventually get you into a Data Scientist role overtime. But an analyst is given simple requirements with an expected outcome from business. Data Scientists have the additional role of knowing what questions to ask, and deciding the approach to answer them. &amp;#x200B; Simply being good at math, or solving math problems, does not intrinsically mean you have the ability to apply high level concepts in your analyses. Data Science is like being an actual researcher. You have a thesis, a hypothesis, and methodologies, and a study to answer the problem and confirm or deny the hypothesis. That requires an academic discipline rarely acquired outside of college, but not impossible. &amp;#x200B; That being said, web development is a much easier field to enter without that specific skill set. And over time, you could develop those skills to become a Data Scientist if you're afforded the opportunity to do data analysis. But I find many companies who don't already have an Analysis team or dedicated Data Scientists don't even know they need one, and therefore, would not offer those opportunities. &amp;#x200B; This is not meant to discourage you. Always try. This is just been my experience in the field.
How would you rewrite it?
&gt;the 7th join condition from the WHERE clause \+1, to illustrate how simple it is, if done correctly, as I saw no one showing the "good syntax": JOIN table ON join_condition JOIN table2 ON join_condition2 ... WHERE other conditions &amp;#x200B;
1. Please specify the database platform you're using 2. Your code is wide open to SQL injection attack. This has been one of the top 5 security vulnerabilities on the web **for two decades** so please, fix that yesterday. There's no excuse for it. 3. `select *` will make your DBA cry. Specify the fields you need from the table.
Didn't work, it used the first key by default.
Your prof gave you the best possible feedback. It is ANSI-89 style, and perfectly valid as an inner join. Many people still use 89 style. As far as being "not recommended", that's a matter of opinion, with 92 style being the more popular opinion and 89 style being the less popular opinion. The optimizer doesn't care, so do what is best for you and those who read your code. That's probably ANSI-92 style, but your 89 style is perfectly valid. I write both depending on what system I'm working on. We have one system with over 4000 tables and you won't find a JOIN ON in any query. But in another system we don't have joins in the where clause. Be flexible. A lot of folks on here are adamant that there is only one correct way to write code. 
in addition to what /u/FrankExplains said - a cartesian join is most commonly done in this style because of the possibility for syntax errors.
Something like this? http://www.databaseanswers.org/data_models/libraries_and_books/index.htm
Whichever route you do decide you want to pursue, experience is king. I'd recommend whichever route you decide to take you start up a pet project and build yourself something. I know it won't be easy - especially if you're already working low paying jobs to cover yourself - but it will make all the difference if you can go to interviews and show something you've done yourself. As an added bonus, shows a lot about character and work ethic as well as the experience you learned to make it. Could even pretty easily come up with a project that uses both skill sets. Degrees are just a way of showing experience. You could cut out the middle man and design and build a project of your own to describe/show off at interviews. You seem to have a good attitude about it. Good luck!
I took a career searching job once upon a time. The dude said if you meet 75% if the requirement, you should apply. Applying is free. It just takes some time.
[removed]
I don't think it's old habits, I think Oracle employees are forced to conform to a house style that uses it. New code from Oracle still has joins denoted by the equal sign in the where clause. Their ERP software has been written by thousands of people over the years, and I have yet to see one JOIN ON syntax. They simply do not use them. Since there is not even one case of it being used, I'd say that they are purposely avoiding them. Check out this small analysis on some of their code where I'm fishing for a join: https://i.imgur.com/mNjherV.jpg 212 SQL files in this directory, thousands of lines of code, but no "join" word found anywhere (except commented code). If I grep for "(+)", the proprietary Oracle left/right join syntax, I have 276 of those in this directory. But to contradict that point, [here is a well known Oracle employee](https://asktom.oracle.com/pls/apex/asktom.search?tag=recommended-join-style) claiming that JOIN ON should be used over = and (+) syntax, yet no one at Oracle is doing it.
This example doesn't necessarily need a join. SELECT s.section_no AS course_no , s.start_date_time FROM section s
yes thank you, i think the basic one is also available on the site and would be more suitable to me but how I wanted the database i could use in mysql, not just the model. i don’t know if i can download it from the given link or if the email given would be active or not.
My SQL teacher took off a lot of points if we used JOIN at all in our assignments. Even the book suggested using JOIN, but boy did he not like it.
&gt; Are "conditional conditions" possible in a join statement? the "on" clause accepts any logical expression allowable in the per-record/base result set context. E.g. you can join records if today is Friday.
This question is a little vague. Are you joining A, B, and C to create D? Are you unioning them to create D? Does D contain only records that are unique to itself, or also unique with respect to A, B, and C (i.e. no record in D would appear anywhere in A, B, or C)?
D only contains unique records from A, B and C. If you picture a venn diagram with three circles representing A, B, C then D = total - (anb + anc + bnc + anbnc) *n is the intersect 
If records can be considered unique on a single field SELECT COALESCE(a.ID,b.ID,c.ID) AS ID , COALESCE(a.Column1,b.Column1,c.Column1) AS Column1 , COALESCE(a.Column2,b.Column2,c.Column2) AS Column2 FROM TableA a FULL OUTER JOIN TableB b ON b.ID = a.ID FULL OUTER JOIN TableC c ON c.ID = ISNULL(a.ID,b.ID) WHERE (a.ID IS NOT NULL AND b.ID IS NULL AND c.ID IS NULL) OR (b.ID IS NOT NULL AND a.ID IS NULL AND c.ID IS NULL) OR (c.ID IS NOT NULL AND a.ID IS NULL AND b.ID IS NULL); should be faster than (SELECT * FROM TableA UNION ALL SELECT * FROM TableB UNION ALL SELECT * FROM TableC) EXCEPT ((SELECT * FROM TableA INTERSECT SELECT * FROM TableB) UNION (SELECT * FROM TableA INTERSECT SELECT * FROM TableC) UNION (SELECT * FROM TableB INTERSECT SELECT * FROM TableC)) 
Sounds as if you are looking for a UNION: create table new_table as select the_column from table_a union select the_column from table_b union select the_column from table_c; UNION automatically removes duplicate rows from the result, so the resulting table will only have unique rows. &amp;#x200B;
 `NOT IN ["Category 1", "Category 3"]` is not even valid (standard) SQL
Is it really this simple? 
Golden rule of SQL: THE ON CLAUSE OF A JOIN IS ACTUALLY A WHERE CONDITION!!!!
If all three columns have the same column, then yes. If "have the same column" means they have many more columns, and only share one common column, then this gets much more complicated - especially because it would be totally unclear what the list of columns for that new table would be. &amp;#x200B;
It's the full time and hour of the order in question. In both tables, it's formulated in this manner: '2019-03-13 00:34:14', for example. Usually this is supposed to match in both tables since they concern the same inquiry made at the same exact time in our system.
no it's not and yes it will 
&gt; In a single statement, would it be possible to create tableD that only has unique records? sure CREATE TABLE d AS SELECT * FROM a UNION SELECT * FROM b UNION SELECT * FROM c the UNIONs will ensure no dupes survive 
The maximum number of rows in a worksheet in excel is about 1,000,000 rows. When we work with large amount of data, excel will not be able to handle the volume anymore. Usually, primary data is stored in a database somewhere, and step 1 will be to "clean" this data. The only way to manipulate this data (when stored in a database) is via SQL &amp;#x200B; &amp;#x200B;
Excel is fine if your data fits within an Excel spreadsheet. But if you're doing true BI, you're going to hit the Excel 1M record limit very quickly. SQL is the language used to query large sets of data within a database. All data everywhere is stored in databases, so you'll need to know SQL in order to pull it out.
I completely understand, this is very insightful. One of the reasons why I’m asking is obviously coz I don’t want to try something that hasn’t been proven to work. With that said i was thinking web development would be the way to go, its been done many many times and i find motivation in knowing subconsciously that something is possible and can be done. Which would automatically make me fail at Data science coz I searched the whole web and couldn’t find any story of someone that had no college degree.
1:M and M:M join types are both valid in your database design and will cause rows from one or both tables to be duplicated.
Yikes, that just proves out the horrible cliche of "those who can do, those who can't teach"
I just tried, but I'm getting an error message: DB SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=using;a join CLIENT\_TABLE b;&lt;space&gt;, DRIVER=3.57.82 \[SQL State=42601, DB Errorcode=-104\] Next: DB SQL Error: SQLCODE=-727, SQLSTATE=56098, SQLERRMC=2;-104;42601;using|a join CLIENT\_TABLE b|&lt;space&gt;, DRIVER=3.57.82 \[SQL State=56098, DB Errorcode=-727\]
Is there a `created_at` field? That or an integer autoincrement key would help.
I just tried, but I'm getting an error message: DB SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=using;a join CLIENT\_TABLE b;&lt;space&gt;, DRIVER=3.57.82 \[SQL State=42601, DB Errorcode=-104\] Next: DB SQL Error: SQLCODE=-727, SQLSTATE=56098, SQLERRMC=2;-104;42601;using|a join CLIENT\_TABLE b|&lt;space&gt;, DRIVER=3.57.82 \[SQL State=56098, DB Errorcode=-727\] &amp;#x200B; I've written it like this: select a.CONFIRMATIONNUMBER, a.TIMEOFORDER from ADDRESS\_TABLE a join CLIENT\_TABLE b using(CONFIRMATIONNUMBER, TIMEOFORDER) where a.TIMEOFORDER &gt;= '2019-01-01 10:17:10' and a.TIMEOFORDER &lt;= '2019-03-20-23.59.01' and b.DOB like '2000%' and [a.ZIP](https://a.ZIP) = '588%' with ur; Anything seems out of order?
1. The differential files still exist, but they can't be reapplied to your database (or if they can, you'll hose data) 2. I don't believe so. Diff backups store pages that have changed since the last full backup, so if you had data on the same page as what you just "fixed" that also changed, your data fixes will be lost. There may also be an issue with the log chain in general if you attempt this. 3. The horse is out of the barn now, but for future issues I'd: * Take more frequent log backups (based on the RPO you want) instead of hourly diffs (diff backups do not truncate the transaction log, log backups do) * Restore the data to a _new_ database up to your recovery point, and use that as a reference to fix the affected data instead of rolling back the entire database
I’m actually taking this course right now. Did you feel like you had a good base of knowledge after you completed? 
Did he give a rationale for that?
Thanks for the reply. We've just met with each department on RPO/RTO times so that's helped us in our IT Dept to better understand department requirements for their data. With your last bullet point, that's a great idea to help with the recovery process. It's a good approach and something we'll factor in to our Business Continuity/DR plan. Are you aware of any backup software that could get back to that 3pm point even after restoring to the 9am recovery point? Or a way to simply erase the 10am bad transaction? We use Veeam for VM backups, but we don't have it reach-in that granular for our DB servers and let the SQL team handle backups through SQL itself. &amp;#x200B;
SQL is much more powerful than Excel, because it is a human-readable language, it leads to reproducible results and it can be extended easily. Not to mention that it is your only option for talking to databases. As you get into complex analysis you will find that pivot tables upon pivot tables don't cut it anymore and that's why learning SQL is imperative for anyone in BI. Writing SQL is a necessary step to having the data in a spreadsheet to analyze. Unless you have a Wordpress website and are storing everything in a google sheet. haha
Since I'm pretty much blindly trying a bunch of different things, I'm getting a bunch of different error messages. I've answered with one of them on another comment, but so far, the only thing that works to actually get results is to do: select \* from ADDRESS\_TABLE join CLIENT\_TABLE on(ADDRESS\_TABLE.CONFIRMATIONNUMBER = CLIENT\_TABLE.CONFIRMATIONNUMBER) with ur; But everytime I tried to do another kind of JOIN, try to SELECT anything else than "\*" (I want to exclude some unnecessary columns, and possibly exclude columns that are there twice) , I get an error message. Same thing if I try to include a condition that touches the TIMEOFORDER column. All of these error messages seem to concern my TIMEOFORDER, which is the only way I think I can sort results properly. And I cannot decide to SELECT either TIMEOFORDER or CONFIRMATIONNUMBER in the list of columns I want to select, because I'm getting an error message for that. If I want CONFIRMATIONNUMBER to be present, I need to select "\*" (and get way too many columns that are unncessary). With this code for example: select \* from ADDRESS\_TABLE join CLIENT\_TABLE on(ADDRESS\_TABLE.CONFIRMATIONNUMBER = CLIENT\_TABLE.CONFIRMATIONNUMBER) where EMAIL like '%[MAIL.COM](https://YOPMAIL.COM)' and TIMEOFORDER &gt;= '2019-01-01 10:17:10' and TIMEOFORDER &lt;= '2019-03-20-23.59.01' with ur; I'm getting this error message: An error occurred when executing the SQL command: select \* from ADDRESS\_TABLE join CLIENT\_TABLE on( ADDRESS\_TABLE .CONFIRMATIONNUMBER = CLIENT\_TABLE .CONFIRMATIONNUMBER) where EMAIL like '%MAIL[.COM](https://YOPMAIL.COM)' and TIME... DB SQL Error: SQLCODE=-203, SQLSTATE=42702, SQLERRMC=TIMEOFORDER, DRIVER=3.57.82 \[SQL State=42702, DB Errorcode=-203\] Next: DB SQL Error: SQLCODE=-727, SQLSTATE=56098, SQLERRMC=2;-203;42702;TIMEOFORDER, DRIVER=3.57.82 \[SQL State=56098, DB Errorcode=-727\] 1 statement failed. Execution time: 0s
That would be nice, don't you think? But he didn't, and getting him to comment further was like getting blood from a stone. It was an online class, and he was doing work overseas while also teaching adjunct. An earlier assignment prompt said "don't use JOIN", but the assignment that I lost points on didn't explicitly state to not use JOIN.
You missed the hanging parentheses `["Category 1", "Category 3"])`
the trick to a good solution is to recognize that it's possible a team might have a season where they never lost, or alternatively never won tharefore, any solution which involves a join has a good chance of being wrong a UNION, on the other hand, covers those scenarios SELECT Season , Team , SUM(Wins) AS Wins , SUM(Losses) AS Losses FROM ( SELECT Season , WTeam AS Team , COUNT(*) AS Wins , 0 AS Losses FROM Results GROUP BY Season , WTeam UNION ALL SELECT Season , LTeam , 0 , COUNT(*) FROM Results GROUP BY Season , LTeam ) AS d GROUP BY Season , Team 
but if you also want to consider that in some season a particular team as not played at all (i.e. wins=0 and losses=0) then you have to factor in a join to the Teams table in each subselect
Thanks for the answer, that works well. It is more complex than I was thinking but a great help!
Having an auto-incremented surrogate primary key like brandit_like123 mentions is a pretty common practice, I find it useful, and more or less always have one on every table with very few exceptions. However, I think the more important question is what you actually want to sort on. I mean, you could use the order rows were inserted, but that syntax is intended to generate a rank based on some business-specific criteria (like give me the rankings of all the marathon runners, that are male and over 55, and rank them by their completion times). It'd be rare that you're looking to retain the order in the table and just replace one number (the PK) with another one (the rank in the result set). 
There's no reason to think `created_at` is unique.
The question is a little unclear but if I am understanding you correctly, then this should work. Create 2 CTEs (or temp tables), one for wins, and one for losses, for each team-season. Then join the two With Wins AS ( SELECT Season, [Week], COUNT(WTeam) AS WinCount,TeamID FROM Results LEFT JOIN TeamID ON WTeam = TeamID GROUP BY Season, [week], TeamID ) ,Losses AS ( SELECT Season, [Week], COUNT(LTeam) AS LossCount,TeamID FROM Results LEFT JOIN TeamID ON LTeam = TeamID GROUP BY Season, [week], TeamID ) SELECT TeamID.TeamID, Wins.Season, Wins.[Week], WinCount, LossCount FROM TeamID LEFT JOIN Wins ON TeamID.TeamID = Wins.TeamID LEFT JOIN Losses ON TeamID.TeamID= Losses.TeamID 
google "top N rows per group" depending on your DBMS, you'll want the ROW_NUMBER solution
This course gave me a great base and was really how I learned SQL. It really taught me how to write simple select statements to retrieve data from a database.
I personally write joins this way because I understand it better this way and it *will* work, just not recommended for beginners who need to learn and understand the logic behinds joins. 
Just speaking about the company I work for - they don’t make any difference. HR will review to determine if the candidate has met the minimum requirements that we specify for the position and review the other typical resume items before forwarding the list of people. For data / business analysts - we put no weight in having a certification just because it’s so easy to find the answers for the exam online. If I had two candidates that were equal in talent, certs wouldn’t play into the decision either. It’s much more a decision about which one you think you can work better with. 
Excel doesn't enforce structure. Databases do. In Excel you can't be sure that some intern won't leave blank cell U6435 blank. And you're really limited with flexibility of what you can do. Most importantly, did U6435 cause a cringe? 7000 records in a database is nothing. It really is a tiny table. SQL handles millions without breaking a sweat. 
 select .... from team t join results r on r.wteam = t.teamid or r.lteam=t.teamid group by... 
This could get you started. I did it for SQL Server, but it should work in some other DBMS. Suppose you want the first three per group, also, you need to check for a way to handle ties. &amp;#x200B; WITH Customers AS ( SELECT store_id, accumulated_spending, ROW_NUMBER() OVER(PARTITION BY store_id ORDER BY accumulated_spending desc) rownum FROM table ) SELECT store_id, accumulated_spending FROM Customers WHERE rownum &lt;= 3; &amp;#x200B;
please, flesh that out a bit, i want to see what you count
For this one you could just use an IN (x,y,z)
 [Division Code] = '1' OR [Division Code] = '60' OR [Division Code] = '1' OR [Division Code] = '921' OR... [Division Code] IN ('1', '60', '1', '921')
MySQL has cute?
You could try your from statement like this and Coalesce on D1.Column,D2.Column. From Table1 T LEFT JOIN (Select D.* from Table2 where D.VALUE2 is not null ) D1 on D1.Value1 = T.Value1 and D1.Value2 = T.Value2 LEFT JOIN (Select D.* from Table2 where D.VALUE2 is null ) D2 on D1.Value1 = T.Value1 and D1.Value2A = T.Value2 
So this is a little tangental to the subject here but I wanted to give you credit for leading me down this path - there was another stored procedure that I found was using non-indexed columns in the where clauses in 11 different locations between the main query and all the subqueries. Knowing I cannot add indexes to the application vendor (read only) DB I built a temp table with a temp index, doing the slow non-indexed scan once, then replaced all other references to the original table with the #temptable name. Now that's finishing in about 10% of the time it took before. This probably isn't going to be super handy in many cases but this was a big win for me. Enjoy the gold stranger :)
 select r.season, t.teamid, sum( case when r.wteam = t.teamid then 1 end) as wins, um( case when r.lteam = t.teamid then 1 end) as losses from team t join results r on r.wteam = t.teamid or r.lteam=t.teamid group by r.season, t.teamid 
Were introduced on MySQL 8.0.
In addition to doing `IN ('1', '60')` you may also want to use `CASE` rather than `IIF`. Especially if you have more than one condition or case. ADD [Division Override] AS (CASE WHEN [Division Code] IN ('1','60') THEN '1' ELSE '921' END) Also consider using integers (and not strings) in your literals, if your [Division Code] column is int.
This works as well, but took about 30x longer than /u/r3pr0b8 
I think what you're looking for is a full outer join, which will keep each entry and populate null fields when there are no matches between tables. If you need to do this with three tables, it should work by joing the first two, then joining the result with the thirs table. 
MySQL Bootcamp by Colt Steele on Udemy.com
thanks for helping out. I attempted to just join tb1 and tb2 with full outer join earlier and it only displays tb2 values. No null values at all. I tried left and right join, too, just for the hell of it. I can't attempt the results - Join because the results are still not what I need.
What platform are you using: if Access may need to use a switch statement.
Something like this might work Select tb1.names, tb3.dates From tbl1 Join tb3 ON tb1.NAMEID = tb3.NAMEID Left Join tb2 ON tb1.nameid = tb2.nameid Where tb2.nameid is null AND tb3.dates BETWEEN 2/1/2018 AND 2/28/2018 AND tb1.name = 'John'
T-SQL Querying
You need to allow for null values when filtering an outer join. There are many ways to accomplish that, but it's a bit complicated if you want to use a full outer join. Try something like this: SELECT tb1.names, tb3.dates FROM tb1 FULL OUTER JOIN tb2 ON tb1.nameid = tb2.nameid FULL OUTER JOIN tb3 ON tb2.dateid = tb3.dateid WHERE (tb3.dates BETWEEN 2/1/2018 AND 2/28/2018 OR tb3.dates IS NULL) AND (tb1.name = 'John' OR tb1.name IS NULL) AND (tb3.dates IS NOT NULL OR tb1.name IS NOT NULL) 
Thanks for the help. tb3 doesn't have nameID; it has dateID
This. Highly rated book that works as a great desk reference. 
T-SQL Fundamentals by Itzik Ben Gan , followed by T-SQL Querying by Itzik-Ben Gan. The former is extremely thorough and will definitely level up your SQL skills!
Google with a pretty big shout out to stack overflow since Google usually leads me there
Hi trying to learn SQL myself. Does anyone know what would be considered good syntax for this query then? Thank you!
Yup. I loved T-SQL Fundamentals. It's one of the only programming books I've read cover-to-cover.
Thanks! Still only gives me tb2 data. Is it because the nameID is joined?
Database *can* enforce structure. It is still up to the developer to implement the proper constraints and relationships.
I just replaced the nameid with dateid. Joining the two complete tables first then left join tbl2. When you run the data tbl1 names will all show and tbl2 names will show when they match and will be null when they don't.
&gt; Still only gives me tb2 data. I don't understand what you mean by this. Rather, I don't understand what output you're trying to achieve without seeing an example of data you expect to receive output for but are not. I would need to see all relevant fields in all three tables.
It is a date datatype. I figured it out, it was: to_char(date_In, ‘MM’) = 04 Thank you!
correct, as i anticipated pay no attention to the man behind the curtain mumbling about slow execution times but of course, the ~real~ answer is, don't try to do with sql what really should be done in the presentation layer anyway
&gt; Will both codes result in the same output? what happened when you tested it? ™
They did... but I'm worried that maybe this was the case for a very specific instance 
Let's just say that it's much harder to make a mess in a database than in a spreadsheet.
When I started, I used the Oracle OCA SQL Fundamentals exam prep book.
I think I understand what you're looking for. If I'm right then easiest solution is to change right Join to left join and put everything in the where statement into the joins.
Thank you for this insight. I've been working with Excel this year and noticed that it becomes buggy when working with about 200'000 rows.
"SQL and Relational Theory" by Chris Date.
I'm using SQL Server Management Studio, I'm moving from Access to SSMS and am noticing that I can't always copy over the Access SQL code to SSMS👎
I think you're looking for a cross join of nameids and dates except the ones that you have already.
Thank you so much, this worked!!
Thanks so much for your time, this worked:)
in a general sense rank() is not the same as row_number() - if 2 records have the same "order by" position, rank() will give exactly same value. Row_number() wont do t hat.
The Itzik-Ben Gan books are great, but IIRC they are a pretty MS SQL specific. To learn tricks and tips that work on practically every SQL platform out there I'd recommend "SQL for Smarties" by Joe Celko. 
I don't see why you were downvoted. This is the only answer so far that seems to understand the question (new table is comprised only of records that do not exist in the other tables). I'll add on to your answer though. Let's set up some tables and data CREATE TABLE TableA (ID INT PRIMARY KEY, Column1 VARCHAR(50), Column2 VARCHAR(50)) CREATE TABLE TableB (ID INT PRIMARY KEY, Column1 VARCHAR(50), Column2 VARCHAR(50)) CREATE TABLE TableC (ID INT PRIMARY KEY, Column1 VARCHAR(50), Column2 VARCHAR(50)) DECLARE @Iterator INT = 1, @Value1 VARCHAR(50), @Value2 VARCHAR(50); WHILE @Iterator &lt;= 100000 BEGIN SET @Value1 = NEWID(); SET @Value2 = NEWID(); IF @Iterator &lt;= 40000 INSERT INTO TableA VALUES (@Iterator, @Value1, @Value2); IF @Iterator BETWEEN 30000 AND 70000 INSERT INTO TableB VALUES (@Iterator, @Value1, @Value2); IF @Iterator BETWEEN 20000 AND 40000 OR @Iterator BETWEEN 65000 AND 100000 INSERT INTO TableC VALUES (@Iterator, @Value1, @Value2); SET @Iterator = @Iterator + 1; END Based on this, we want the records from these domains: [1, 19999]U[40001, 64999]U[70001, 100000]. Running your first query, it executes in about 850 ms, and returns the correct results. Running your second query, it executes in about 800, also getting the correct results. I'd also argue that because there's nothing guaranteeing Column1 and Column2 having the same values, I'd argue that your second query is more correct. However, there is also a better way SELECT Id, Column1, Column2 FROM ( SELECT * FROM TableA UNION ALL SELECT * FROM TableB UNION ALL SELECT * FROM TableC ) Vals GROUP BY Id, Column1, Column2 HAVING COUNT(1) = 1 This query gets the correct results while running in less than 650 ms. Here's my full debug query with time measurements DECLARE @DebugTime DATETIME2(7) = SYSDATETIME() SELECT COALESCE(a.ID,b.ID,c.ID) AS ID , COALESCE(a.Column1,b.Column1,c.Column1) AS Column1 , COALESCE(a.Column2,b.Column2,c.Column2) AS Column2 FROM TableA a FULL OUTER JOIN TableB b ON b.ID = a.ID FULL OUTER JOIN TableC c ON c.ID = ISNULL(a.ID,b.ID) WHERE (a.ID IS NOT NULL AND b.ID IS NULL AND c.ID IS NULL) OR (b.ID IS NOT NULL AND a.ID IS NULL AND c.ID IS NULL) OR (c.ID IS NOT NULL AND a.ID IS NULL AND b.ID IS NULL) ORDER BY 1 PRINT DATEDIFF(ms, @DebugTime, SYSDATETIME()) SET @DebugTime = SYSDATETIME() (SELECT * FROM TableA UNION ALL SELECT * FROM TableB UNION ALL SELECT * FROM TableC) EXCEPT ((SELECT * FROM TableA INTERSECT SELECT * FROM TableB) UNION (SELECT * FROM TableA INTERSECT SELECT * FROM TableC) UNION (SELECT * FROM TableB INTERSECT SELECT * FROM TableC)) PRINT DATEDIFF(ms, @DebugTime, SYSDATETIME()) SET @DebugTime = SYSDATETIME() SELECT Id, Column1, Column2 FROM ( SELECT * FROM TableA UNION ALL SELECT * FROM TableB UNION ALL SELECT * FROM TableC ) Vals GROUP BY Id, Column1, Column2 HAVING COUNT(1) = 1 PRINT DATEDIFF(ms, @DebugTime, SYSDATETIME()) 
https://i.imgur.com/081v5AC.jpg I tried to make an example. I need the output to give me the name and the dates that don't show up in tb2, however every time I try this, I just get tb2 as an output
Generally whatever makes most sense to you is best, particularly for anything adhoc. If you are writing production code sometimes you'll need to sacrifice clarity for efficiency. (Or you're doing something that doesn't come easy in a particular sql implemention then sometimes you'll need to sacrifice clarity for insanity just to get something done.) But most of the time whatever reads well is best. And imho that's #1 here.
The only way that makes sense is if the column was subsequently 'altered' to be an int (I say 'altered' because I think it would always require a transition column)
Joes-2-Pros for Microsoft SQL Server. It’s a total of 5 books and has some good examples and trainings 
I'll throw in my vote for these two. He can be a little dry at times but his books + real life experience are all anyone needs to be successful at SQL imo. 
I'd go with somewhat different approach: tell them that you have customers, product price list, and optional discount programs (say by flat amount or % of purchase). Ask them to think about this and draft you _within a specific timeframe_ (15 min, for example) a generic ERD that would support an ordering system . Have a list of general/common sense use cases/use stories (4-5) that cover the gamut of what you're expecting from the solution. Ask a candidate to explain if their ERD is a good fit for the use case and if not, explain how would they refactor ERD to fit it. This way you're getting an idea if they can estimate their time for the solution, how well they can present their solution and ~10 data points on whether they can apply and adapt their data knowledge to business scenarios. Keeping this consistent could also help you to remove bias and have an objective and comparable record of your candidate results. It helps if someone from your org goes through this first and gives you an idea what to expect and a baseline performance of your existing resource(s).
That's a good solution, thanks for that! 
What RDBMS?
I figured it out! I was writing out the queries in a Word document for this assignment and pasting them into PuTTY. Apparently Word's "smart quotes" (fancy special characters for quotation marks and apostrophes) were tripping MySQL up, causing the error!
IBM Db2
Data camp 💯💯💯
Are these still up to date for learning in 2019?
Username checks out. 
Absolutely. They teach fundamental TSQL skills and strategies that are relevant and useful for every version of SQL Server past 2000. 
Thank you so much for your input! I don’t know anyone in this field so I really appreciate it! 
Exam Ref 70-761 Querying Data with Transact-SQL is also great. It doesn't matter whether you are planning to take the test or not.
Yes as long as you're on the most recent editions. Check them out on books.google.com. 
Which code is more suitable for the problem?
I love the first book. Haven't gotten a chance to get into the second one yet. Does it talk about creating meaningful indexes or table design topics? Also do you have any recommendations for table relationships and design or how to handle parameter sniffing?
What is "the problem"? For the data sample you provided both methods would work.
I create fake databases and tables on the fly at home all the time for practice and screwing around. I have on with tables of junk data just for queries I respond with on here. It really isn't that hard if you're just wanting to try doing random things and the dataset itself doesn't matter. If the dataset matters then try to get the schema of the db you wish to target in a dump format, take that home and rebuild the db and put in your own data for you to mess with. (Be sure you have permission to take the schema, sometimes those are touchy business.)
&gt;We are expecting a model should a client can have many orders I'm expecting my employers to be able to be fluent in the English language, but we all make mistakes we wish we could take back in hindsight. Do you want someone that can past your test on the first pass, or do you want someone that goes home and dreams about the problems your company is facing, and comes to work with a solution that works? Does your test test for that? You're getting too caught up in concepts like normalization, or PK/FK relationships. This is important, but can often be enhanced after the creative and difficult work has been done/started. The importance of these concepts can differ greatly from company to company based on need, and you can very well be passing on great candidates who would easily be able to adapt their methodology to your needs, but who aren't able to do so on the spot in an interview. If you want them to rethink how they work, then you should rethink how you interview.
The dataset that I'm working with really doesn't matter, so that's good news! How do you go about creating a fake database? I could probably get a dump format of this data with the right approval, but since this is more for practice any data will work. 
I disagree 
May be it differs from people to people. But why?
I don't think i'm out of line to expect a senior database developer to be able to draw a simple ERD.
"Fake database"? I think I understand what you mean; I cannot think of a worse name for it. "Sandbox database", "learning database", "test database", "temporary database". Any of these are better terms. The data does matter. Useful data always exists in some domain, usually a business, and that business has rules, and therefore generally-agreed-upon-interpretations regarding that data. I don't believe one can effectively learn SQL by querying "meaningless data"; for how will you know if the results are correct/meaningful?
This is an important answer. I support reporting add-ins for excel like Jet, and reverse engineering the reports is a nightmare because any cell can add to the query. With SQL I can write a solid underlying query and reuse it in SSRS, excel, a batch job with sqlcmd, R, powerBI, metabase, you name it. There is definitely a sort of symbiosis for BI, SQL to return just a bit more data than you need in a granular form, then a programmatic language like powerquery or R to summarize the data along the appropriate dimensions.
Inside SQL server 2000 by Karen Delaney. A fantastic book about the inner workings of SQL Server and still largely relevant today.
Nice. Case When is such a good simple formula, I love it 
What are you trying to accomplish? There isn't enough info here. Merge is typically used to mirror a table, so you may be better off using inserts or deletes, depending on what your goal is
Thanks for pointing out, I'll correct. I missed this while drafting a post. 
Provide CREATE TABLE / INSERT INTO statements to prepare data and describe what output you want to accomplish. 
if you're on MS SQL, the "when not matched" clause is optional
In a merge statement, a common scenario is to specify the clause WHEN MATCHED THEN &lt;action&gt;, but less common to specify WHEN NOT MATCHED THEN &lt;action&gt;. You can do both, or choose only one. Therefore, you do not need a WHEN NOT MATCHED THEN clause if you wish to do nothing. Simply omit the clause. Or alternatively if you absolutely have to have that code for some reason, then maybe you can do something like WHEN NOT MATCHED THEN SELECT NULL FROM SOME_TABLE, or some other static value that fits into your query result. Also, do you have to use merge? Is that the best solution for what you're trying to accomplish? Merge is great, but simple updates to tables are best left to the INSERT/UPDATE/DELETE statements. In some scenarios, merge will perform better than update.
Horia Negrilla has put together a script to walk your backup folders created with Ola Hallengren’s backup plan and creat a script you can tweak to restore the database. By default it will find the latest full backup, then the latest dif, then all the log backups since the last dif. Run the script &amp; then database is restored to the point of the last differential. Start here: https://sqldotnet.com/category/dba
This should be 'bread and butter' stuff for a database developer, not only would I expect them to pick up on the M:M relationship, I would also expect to to talk about ways we can avoid this design in the database by using bridge tables etc.
Kinda hard to give an answer without seeing the data, but you can try something like `abs(t2.value - t1.value) &lt; 0.0001` to check for a difference being smaller than something between the two values. 
This looks promising but his download link is broken (for me at least). I suppose I could piece it together with his code snippets though. Thanks for the info!
A recursive CTE joins to itself. You don't have a recursive join there. The base case is correct, but the subsequent cases should join to the CTE, not the base table. ;with cte as ( select id, title, parentId, '' as filepath from folders where parentId is null union all select a.id, a.title, a.parentId, (b.title + '\' + a.title) as filepath from folders a inner join cte b -- recursive join here on a.parentId = b.id ) select * from cte 
Thanks for the gold! At least it was something resolvable. Given your situation that #temp work around was pretty legit. Hopefully you got some kudos from your colleagues!
This is a complete stab. But is your value something like '123 5 6789' ? Including the spaces this has a length of 10. During the insert this might be interpreted as a number and the spaces will be taken out, leaving you with the number 12356789 and a length of 8
Business Systems Analyst my stack used to work not be what I needed for reporting but is slowly moving towards a developer position. SQL/T-SQL, Power BI, SSRS, SSIS, Python (scripting and data visuals, work doesn’t support this language but I love it) they are currently having me build an API which I have no idea how to do, or know any c#. 
You had a few errors in there, here is the correct solution: &amp;#x200B; with cte as ( select id, title, parentId, cast(title as varchar(8000)) as filepath from folders where parentId is null union all select a.id, a.title, a.parentId, cast(b.filepath + '\' + a.title as varchar(8000)) as filepath from folders a inner join cte b on a.parentId = b.id ) select * from cte &amp;#x200B;
I have an instructional video (with downloadable code samples) that covers ancestor, descendant, and cyclical (tree) ctes. Just replace the columns/table with yours. https://youtu.be/YVMgJlAiyNs
Fake it till you make it! I started off with Business Analyst / Excel / Access, made some connections to move towards a developer position, then learned Tableau /SQL / SSIS on the job. Then built entire databases from scratch / migrated databases. You can do it! Doubled my salary in 2 years, now only using Tableau and SQL. Wish I could pick up a use-case to use Python and learn it, but I am moving away from technical into business and strategy. 
Hah! I got it, the raw data has this as text field prefixed by 0 but the destination table has this as an int column and it removes the 0 during the insert with an implicit conversion. Thanks for pointing me in the right direction!
🎉 glad you got it! Thought it would be something about string &gt; int conversion
Dude seriously! That is literally the same events that happened to me! I started off as a help desk technician, was asked if I could build a report for the department to show performance. Ended up making an excel report that connected to share point, and an access DB to hold all the data and do ETL from our service desk software. Soon I was a full analyst working for the company completely redesigning our databases and setting up automated ETL processes. I don’t know where down the line I showed capability in programming web apps but that them now pushing me towards a developer role, hate to say it I was happy with just working with data and building automated databases and processes. I completely hear you though, I too want to move away from technical and into more business and strategy. It’s great knowing how to do it but isn’t the ultimate goal to tell others to build your vision. 
Yuck!
I think this absolutely the case but how can i force it to x decimals so 10.45000 and 10.45 will return appropriately? 
Thanks. I guess I had too much faith in the ISNUMERIC function.
If you're using MS SQL, be careful when choosing merge. There are a lot of issues with it that will probably never be fixed. Do the research on it, and if you decide that the problems with merge won't affect you, go for it. Otherwise, you're better off writing a query that will insert or update as needed, even though it's more work.
Fake is my preferred term since it's not the real schema for the targets and just "look-alike" with garbage data. I've improved quite a bit by creating junk data for querying against. If you just want to practice aggregates and dynamic grouping, or play with mimicking window functions, or just trying to figure out how to get the right data out when the data in is garbage... My junk data is similar to that of live data in principle but not practicality. *shrug*
Data Analyst, dealing 99% in Excel and Power BI. Learning SQL now to expand my knowledge and make some of my work easier. A good amount of my work is through an OLAP database, which is too slow and restricted for what I need to access; learning SQL will allow quicker and more robust access to the databases.
Note: table_a is larger than table_b. Will this still work if the "key" in table_a has duplicates? 
When I try running this code, table_c (output table) has more records than table_b (table_a has more records than table_b)
Join on Name.
 If you want to trim your 0s, cast (value as float) or cast(value as decimal(20,#ofdecimals) You could also do.. round(value,0,2). For round, the first number is where you want to round. A 0 is truncate. For the second number the 2 represents the number of spaces to the right of the decimal. Round(10.45000,0,2) should return 10.45 Round (11.34822,0,2) should return 11.34 (no rounding up).
Maybe `cdbl` or `cdec`? https://support.office.com/en-us/article/type-conversion-functions-8ebb0e94-2d43-4975-bb13-87ac8d1a2202
Are you sure that's what causing your problems? I don't think any sane SQL variant, and even insane ones like Access, treats them as unequal. SQL Server, also a Microsoft product, [does not for sure](https://i.imgur.com/OCQlyc1.png). I don't have Access, but I would really be surprised. Can you post your query, or a minimal representation of it, that's causing you trouble? 
This is actually wrong and won't run.
Software Developer that does some mild data engineering from time to time. Our software stack includes postgreSQL for the data layer where we do all sorts of experiments in python/java and then productionize into new features for the web app.
 SELECT ... FROM customers c INNER JOIN orders o ON o.customer# = c.customer# INNER JOIN orderitems oi ON oi.order# = o.order# INNER JOIN books b ON b.isbn = oi.isbn INNER JOIN bookauthor ba ON ba.isbn = b.isbn INNER JOIN author a ON a.authorid = ba.authorid INNER JOIN promotion p ON ?????? you're missing the join condition to `promotion`
This works. Thank you very much! You're a star
You're welcome.
NHS (UK Health Service) - Software Developer. For me, SQL primarily exists as a backend to the web interfaces and services I develop day to day, and as a reference for various compliance and performance reports based on the usage of those interfaces/services. C#/ASP.NET/JS do the frontend and API work, SQL Server provides the database, and SSRS/Qlik handles reporting.
Look at WebAPI if you're using .NET for the API, it's not particularly complicated, honestly.
Data Engineer/Data Scientist. I use R, Python, T-SQL, U-SQL, a little bit of C#, Power BI. I've done a lot of traditional software development earlier but I prefer to work with data. 
Programmer Stack: Oracle SQL, PL/SQL, Bash, Redhat, UC4, Python I write extract files and transfer things around.
Director of Business Intelligence and Systems Integrations - it's all a facade. I make the reports that the CEO and CFO ask for then move data from one system to the next as they jump from platform to platform.
Bi engineer. From junior database admin to technical reporting specialist. Now I maintain the databases, and work in ssrs,SSAs and powerbi to maintain our reports. 
I’ll have to take a look, I know the worth that an API can bring to a business, perhaps I can practice with this on my person project. 
first thing first - once you're done with this exercise, please forget about "USING" syntax (largely for the same reasons you don't want to do "natural" joins in a real world) secondly - once you've used 'using(column)' in the join, the column source is no longer any of the tables that you've joined, so o.order# is NO LONGER VALID, and you have to refer to the column without any alias. So, simply do this: SELECT DISTINCT c.lastname, c.firstname, order#, c.state
Sounds expensive and time consuming
IT Manager. I got sick of all the complaining about Excel being unreliable because everyone was trying to use linked 100+ MB Excel documents so I subscribed to Azure SQL and started loading data from various systems into it and now it's probably 1/2 my job. I'd love to get more automated but we're buying new companies at a rate greater than one a month so until we get them migrated to our systems we're stuck with giant CSV files to be parsed and processed and spit out into my elaborate Excel spreadsheets full of pivot tables and slicers.
Integration and BI Developer. I work with Microsoft Dynamics NAV, .NET, and MS SQL. Job includes: maintaining the Database, writing reports for Finance and Marketing, implementing/maintaining integrations to Many outside systems, day-to-day support of existing systems, writing enhancements and corrections to existing systems.
Based on idiots down-voting you, I get the impression most people would rather give a man a fish, than teach him to fish. Or, be given a fish, rather than be taught. Hey, less competition for you in the long run.
Function title: Winston Wolf Stack: Whatever my client is runnig Role of SQL in work: Whatever my client wants me to do.
If you've ever used anything with an MVC framework, WebAPI is basically just MVC but without the View and returning XML/JSON instead. It's built into .NET and is a core part of how .NET works nowadays, so you'll find loads of documentation on it
Associate Database Administrator. Oracle all day. SQL for v$ tables and other DBA functions, never touch the data as I don’t own it, the app teams do. 
Silver because that exact same thought crossed my mind. The never ending quest to see the light bulb go off in someone else's head when it finally clicks. So much better than providing answers.
look into "case" expression
 SELECT Name , SUM(CASE WHEN Payment_type = 'Food' THEN Payment_Amount ELSE NULL END) AS Food_payments , SUM(CASE WHEN Payment_type = 'Rent' THEN Payment_Amount ELSE NULL END) AS Rent_payments FROM Payments GROUP BY Name
Electronic Health Record software developer. I write the SQL scripts for reporting, and stored procedures and views for the application. I design the table structure for new solutions and their respective indexes, triggering, etc. I also run performance tests to audit which of our procedures / scripts need to be looked at for excessive run time or reads. The EHR suite I am working with currently uses SQL for a lot of its backend processing, but some of that is supposed to get shifted to a proper C# middle-ware, along with an overall application interface overhaul.
Data architect 95% MsSQL, 5% spss / power bi, 0% ssis. Converting c# code, unnecessary sql code and business rules into a hyper normalized (master data managment) data driven solution. Thus ensuring maximum scalability, expandability, and quality of life for any application. The model remains the same no matter the business. 
I don't disagree with you, per se, but I think my point still stands. It isn't out of line for me to expect you can write a simple sentence, properly, and yet you failed to do that. Whether someone can or cannot write a simple ERD is not really being measured by your test. You're testing whether your candidates are good at taking tests. Just as your single mistake here isn't a good test for whether you are or are not fluent in English.
Database administrator, mostly MariaDB lately. &amp;#x200B; My job is mostly saying no to developers that try to make expensive SQL queries, which leads to the service going down, which leads to many high school students around the world not being able to do their homework or check the plot summary of the latest Netflix show. Lately I am also doing backups which may or may not end up on the Moon.
Manager of Program Evaluation This is the Frankenstein stack that I've inherited: Access on the front that is supported by MsSql. Reports were generated in excel and access and were basic summery and count reports. I've since hooked Power Bi up directly to our SQL server and am generating reports through Power Bi. I dream of the day where I can get rid of the Access front end, but I'm still searching for a better solution. 
Software Developer MSSQL, C#/ASP.NET, Angular JS
Full outer join with no conditions in the where clause should accomplish this
If you need total records but ultimately need a 1:1 join, aggregate the data in the table that has duplicates, then join that table to the other which has unique records
System and Database Admininstrator MSSQL Stack, SSRS, SSIS, trying to get PowerBI accepted by the business, light webapp development with .net, Sys Admin on a 2008-16 MS Server farm with a few Linux boxes running also. SQL &amp; database management make up probably 60-70% of my time with report writing &amp; automation. Very interested in learning Python/Powershell for automating tasks outside of the databases in manage.
Any update on this?
I've written two APIs in C# to pull and proceed data with SSIS, still don't have a clue about C#. Googled it all the way. It's working and they're happy. 
Have you ever heard of autocorrect or a swypo? "Should" should have been "showing". Most people are able to understand that these things happen and don't harp on about it. Just like most senior db Devs should be able to draw a simple ERD. 
Sure but that isn't the point. Before I got into this field I used to be a teacher. If I gave a class of students a test, and 100% of them failed a specific question... I understood that it was my fault for either not teaching the material properly, which doesn't apply to your situation at all, or that it was my fault for designing a bad question. That's the point I'm making. If 60/40 were passing your question, we wouldn't be having this conversation. But 100% are failing it, and your observation about "expecting something" goes both ways relative to your innocent auto correct error. People perform poorly on interview related tests, and are prone to make simple errors like that. If you hire someone who fails that question, but ultimately shows they are competent and understand the subject you were questioning them on, and ultimately demonstrates they are capable of doing the job... then it's a bad question and you should re-evaluate your testing methodology if you want to evaluate candidates.
What do you think this post is for? I'm asking for help in phrasing the question so I can test db design skills. I'm sure that I'm not asking the question correctly. How would you test a candidate's database design skills in an interview situation?
comment out begin/end try and the whole catch block, then re-run to see where the error occurs (for example, if it happens before you set @counter to something)
Welcome to corporate business
Conscious I'm using those for front end web stuff in the NHS, but only to deliver our BI reports from PowerBI and ad-hoc JS charts. What projects have you done? Never come across a software developer in the NHS before so quite interested what you're doing!
BI Developer within the NHS. SSIS for ETL, T-,SQL for converting OLTP to OLAP dimensional model. Power BI to visualise data. .NET, JavaScript, C#, VB, HTML/CSS to handle internal web portal for BI solution. SQL fundamental to my daily work, SSMS is open pretty much 100% of the day.
https://stackoverflow.com/questions/23515347/how-can-i-fix-mysql-error-1064
left join... returns all rows from the "LEFT" table (table_a) even if no matches are found in the "RIGHT" table.
Operations Analyst, Excel , Access, MS SQL and am learning Python trying to bring the small family run business I work for up to speed. Or get paid somewhere else. It is a trucking company that had everything on paper two years ago and I am working to get a data base system in place.
Contract BI Developer - Microsoft full stack TSQL, SSIS, SSAS, DAX and POWERBI. In my current role I'm using all those skills equally. 
Analytics director, Redshift/Tableau for the most part, some Excel thrown in for fun. 95% of my work is off the redshift environment, every now and then I have to bang on Oracle directly. 
We do a lot of our own internal software and “sell” some of it to other trusts (“sell” in quotes because we don’t aim to make a profit, just subsidise our own development costs) I’m not really sure how much detail I can go into, but most of my work has been around integration between systems, along with some compliance systems (tracking staff training, equipment compliance etc) - stuff that doesn’t really exist off the shelf, although over time private companies have slowly made similar products so we’ve sometimes replaced our projects with commercial software since it’s usually cheaper to buy than develop 
Amazing answer. I had no idea that you could refer to an object within itself! I'm currently working on a project using ctes and this is very helpful! I still don't fully understand it but I tested it and it works for me.
What is an “expensive SQL query” if you don’t mind me asking? I’m not too SQL savvy 
 SELECT "Employee Name" Name , "Person Number" PN , SUBSTRING(MAX( CONCAT(CASE WHEN "Phone Type" = 'Home Mobile Phone' THEN '1' WHEN "Phone Type" = 'Home Phone' THEN '2' WHEN "Phone Type" = 'Work Phone' THEN '3' ELSE '4' END) , "Search Phone Number") FROM 2) Phone_Number FROM "Workforce Management" GROUP BY "Employee Name" , "Person Number" 
Reporting Analyst Oracle PL/SQL and Excel. I write queries to retrieve data from database and import data into excel to create final reports. 
How big is the company you work for? From your comment I feel like I take our reporting and analytics team for granted.
I would give them a question without expecting a certain answer. Ask them to give an example of what an ERD might look like given a few business factors, and then have them explain, in writing, why they are choosing that framework. Stress that it isn't supposed to be "production worthy" and that you're just asking for a napkin diagram. The diagram isn't the important thing, the explanation is. If their explanation hits on topics that are relevant to you, then they understand, if their explanation doesn't, then they don't. A lot of times I design something which is ass backward, but I have a very specific reason for doing it that way based on the overall execution/usability. There is no right way to design a database, there are some ways that are better than others for certain needs, and there are some ways that are worse than others for other needs. Any notion that there is only one way to do something in this world is simply inaccurate, and if you are expecting a very specific answer then your problem is going to be communicating the precise business needs to a candidate, giving them time to absorb those requirements, and then having them give you an idea. Simple example here is an OLTP vs an OLAP. If you are doing lots of transactions throughout the day, you need a very specific design to accommodate that. On the other hand if you are designing a database for consumption by analytic tools like SAS, SPSS, or Tableau, you might break a lot of "normalization rules" that are "sacred" to the OLTP world, otherwise your usability is going to suck. A good candidate (to me) would be someone who can give you a 5 second drawing, and then a paragraph explaining their reasoning. Even if you 100% disagree with them, and 100% think their idea is bad/wrong... they still might be a perfect candidate that is fully capable of doing what you want. In the end I wouldn't base my observation off the answer, I would base it off having a conversation with the candidate about their answer and having a conversation. Can they intelligently speak about the concept of design? Do they understand why you might not like an idea / have a counter proposal ready on the spot that satisfies the gap? Do they handle criticism of their ideas well? Are they thinking about something you had not considered, and are they thinking about the future in a way you haven't (because it isn't relevant to your actual business in ways the candidate cannot understand?) To me the only usefulness of a question like this is the conversation it lets you have with the candidate. The actual sketch &amp; answer is mostly irrelevant. If you really are hung up on having a "question" with an "answer" then let them sketch a design, let them write a paragraph, then having a conversation with them, and then them redo their sketch and answer based on that conversation. I remember losing a job once because I couldn't, using a pen and paper, sketch / write a framework for a fairly simple analytics query that calculated a basic probability. Why? Because I don't work that way. I need to run a few tests, look at data, modify my code, use Google, etc. -- so I went home from the interview defeated, Googled the question, and then spent the next few weeks kicking myself in the ass for not answering the question in a way that reflected my knowledge of the subject matter. Now years later, in a much better position, I look at that experience and consider myself lucky that they turned me down, and I hope they found someone as good as me to do what they were looking for. I might not be that talented, or that smart, but I never stop working. I live, breathe, and even dream about statistical problems until I have an answer with an obsessive tenacity, and I regularly accomplish tasks much faster than my colleagues, many of whom are smarter &amp; more talented than me. I'm not trying to convince you to hire me, or that I'm super awesome, but if your test can't identify someone like that versus someone who just happens to know the answer your looking for... are you really getting a useful assessment, and are you going to be happy with your choice in 6-24 months when your new hire has learned your database well enough to become a critical asset? To me that is the biggest problem with our field. If you hire me, it might take me a year or two before I can really start making large improvements. I can produce some deliverable, go to meetings, talk about design, etc., but it really takes at least a year or two to properly immerse yourself in a company's data before you can start making some intelligent observations. If I were you I would be focused more on abstract questions, and one that I posed [here](https://www.reddit.com/r/SQL/comments/a9itx1/merry_christmas_have_a_sql_challenge_i_thought_of/) not long ago is a good example. I really like it because there are two ways to approach solving the problem, one which is very inefficient and unwieldy from a database perspective, and one that is very clever. I would consider any candidate who can answer it either way to be qualified, but the discussion afterwards about which one is superior should be very intuitive to anyone who is capable of answering it the wrong way. I would expect the candidate to then immediately have an "epiphany" and suddenly say, "oh shit, I'm wrong, this is a much better way to do it." So you end up with four groups of people answering the questions: Those who cannot answer it, those who answer is the bad way and don't understand the good way, those who answer it the bad way but do understand the good way, and then those who immediately seek the good solution. You would then want to pick a candidate who is in either group 3, or 4, and your total selection would be based on other factors, personality, etc. Don't test what you think someone *should* know, test what they are capable of knowing, and how well they handle conversation/confrontation of superior ideas. 
.
 create trigger dbo.myTrigger after insert as begin if exists ( YOUR_STATEMENT_HERE ) begin rollback transaction end end Basically YOUR_STATEMENT_HERE will be something like: SELECT * FROM INSERTED i INNER JOIN courses c on c.courseID = i.courseID where c.PASSED = 1 Basically a SQL statement to look and see if the course inserted was passed... if so, roll back the transaction ("block the insert")
Ucase makes all the characters uppercase so you don't have to worry about their case. So you would wrap the around the value you want to compare and then around the input. Like will be used in the way it always is. Not going to give you the coffee typed out since it's home work though. Writer are you getting stuck?
Thanks but still stuck...can't figure out how to write those ucase and like. I mean where and what to wrap in those. I tried a lot of ways to write them but no one worked...And I asked here because I've been trying to figure this out for almost 2 hours and I hoped sometimes would help me with the code. :/ It's not that I am lazy or anything. I just can't figure out how should I write them.
Let's see what you've tried. That way I can see where you're going wrong.
I think people took it negatively as him promoting himself, but the video really explains recursive CTE's very well. 
OK, that makes sense. Thank you!
Pim application specialist. I use SQL to write and maintain the synchronizations from and too our ERP. Also use it to build some completeness reports. 
This is what I got so far. I mean, this is the best I've been able to do, but I still get a syntax error. It uppercases my input but can't connect it to the table entry. I hope you understand what I'm trying to say. I also have to make it take "INPUT" even though I enter "INPUT " (with multiple spaces). &amp;#x200B; SELECT COUNT(*) AS CountryNeighborNo FROM NEIGHBORS WHERE UCASE(Country) =DLookup("x", "COUNTRIES", "Country = 'LIKE " &amp; UCASE([Please enter desired country name:]) &amp; "'") OR UCASE(Neighbor) =DLookup("x", "COUNTRIES", "Country = 'LIKE " &amp; UCASE([Please enter desired country name:]) &amp; "'"); &amp;#x200B;
`COALESCE()`
Log backups should be generating files IF your database is in Full recovery mode. You can take Full and Diff backups of Simple Mode databases, so I'd check recovery mode first. The 3 main backup types - * Full - a full backup of the database, whoo. * Diff - Tied to a specific Full backup and contains the changes since the full. * Log - Changes since the last Log backup. If you don't backup the Log regularly the file with grow and grow (possibly filling the disk). Scenario 1 - Diff and Full Full at 2am, Diff at 8am, Diff at 10am The 10am file contains all the changes from 2am (last Full) so it will be larger than the 8am. If the Full file is corrupted, the diffs are worthless, you can't use today's diff against yesterday full. Note, if you take a full backup to copy to Dev, make sure you pick 'Copy Only', else today's remaining Diffs are tied to that backup, not your normal maintenance backups. 'Copy Only' is in MS SQL, not sure about other platforms. Scenario 2 - Full - Diff - Log Full 2am, Diff at 8am, 10am, Log every 30 minutes Database pukes at 9:50am. You can restore Full, 8am Diff, 8:30, 9, 9:30 logs and only lose 20 minutes of data. This is quicker than restoring each log from 2am - 8am. Diffs are also nice because if a log file were corrupted (say 4:30am) the 8am diff would allow you to start after that file providing another safety net in the recovery process. Logs are not tied to a specific Full backup so if today's backup is corrupted but yesterdays is OK you can restore yesterday, last diff from yesterday, then all log files until 9:30 today. Logs also allow you to restore to a specific time say right before a bunch of data was deleted. So thats how the different backup types can interact. As to your restore on dev server - Little trickier to automate. The reason SSMS can do all the work for you is because there is backup history is MSDB (?) which it queries. Dev Server has no knowledge of Prod's backups. But if you had a network share and you always name the backup the same thing, you could create a job to restore the database on a static name + path. 
You dont use SSIS for anything? Not even loading data from CSV's to DB?
Did you have prior exp in the field before hand? Getting myself situated with the language fresh, and would like to know more about what I can do to build on a resume!
&gt;What is an “expensive SQL query” In an OLAP (analytics query) context, where you may be the only user (or one of the few) of a machine, a query taking 1 hour or 2 is not a big deal. You may prefer the fastest way, even if it makes your machine work harder, but you will have to wait anyway. In an OLTP context (the typical case would be computation needed to serve a web page or an API call), you may have to serve 300 000 queries every second, 24 hours a day. Not using an index or creating too many locks may multiply its latency 100x, 1000x or even more. The incoming queries, however, won't stop. Contention on a record or a table may mean thousands of request per second may fail, maybe the very moment you deploy the latest version of your application. Those queries will be expensive, not in money (although any downtime can be quantified in dollars), but in time/resources such as cpu or disk operations which were not necessary and were preventing others from execution. 
Director of Consulting Services. SQL databases (of the Microsoft, flavor) support all the businesses we work with. We're a Microsoft shop primarily, focused mostly on ERP platforms. We don't do any web stuff, aside from some web apps that tie back to our back-end systems. So MS SQL Server for primary data storage. PowerBI for web-based presentations outside of the various ERPs or custom apps we support. I spent half of my day buried in SQL code for one thing or another.
Are you hard coding the variable in? What do you mean how can you set it without a prompt? How do you intend on setting it?
OK. So you don't use the = AND like you use one or the other and you need to add wildcards. It's been a while since I used access so I think the wildcards are * but if that doesn't work try % instead. Also your ' should be after the like not before it. Does the below work? SELECT COUNT(*) AS CountryNeighborNo FROM NEIGHBORS WHERE UCASE(Country) =DLookup("x", "COUNTRIES", "Country LIKE '*" &amp; UCASE([Please enter desired country name:]) &amp; "*'") OR UCASE(Neighbor) =DLookup("x", "COUNTRIES", "Country LIKE '*" &amp; UCASE([Please enter desired country name:]) &amp; "*'"); 
Unfortunately, it doesn't work but I figured it out with TRIM() function. &amp;#x200B; Thank you so much for help :D
I want to hard code the value of the variable. Currently, upon query execution, you get a popup to enter the values. This won't work when running a query outside of the PLSQL application so I need to have the value set within the query.
Is it exactly like in that statement above?
Are you me?
If you mean my example in my comment, then yes - more or less. The values are different though. &amp;#x200B; There are just about 15 combinations of variables that have to be run (so 15 times of executing the query and changing the values) and I want to put them all in Power Query so they're all run in succession without having to change the values with the prompt. For the query to work though it needs to have the values defined though.
Oh, so you still want them to be variables, you just want the fast/bulk method of inputting them over and over again. 
Maybe because you declaring @return_values as nvarchar without a length. I think default nvarchar length is 1. Try nvarchar(11)
Yeah each variable is referenced in the query multiple times so I don't want to have to change the value at each reference point. I just need to have them defined without a popup prompt like how you would use variables in any other language. &amp;#x200B; Pseudocode... &amp;#x200B; Var1 = value1 Var2 = value2 SELECT * FROM Table1 WHERE col2 = Var1 AND col3 = Var2 &amp;#x200B;
I had prior experience with databases, I migrated my old company to a new ERP system, but didn’t understand half of what was in front of me. I started doing a lot of tutorials, reading articles and practicing on my own time, eventually I understood what was being requested and why. Even further back I worked strictly with excel and access to build a few things that were quality of life. I decided to pick up python on my own because I knew I wanted to do data analysis, even though my office doesn’t support it I use it for my personal projects. I also just lately started Odin project for full stack web dev since that’s what my company is asking if me know. But it’s not something I want to do as a career. My word of advice is to experiment and to keep learning. 
I don’t know if it’ll work with the application you’re using or not, but I know it’ll work with SQL*Plus: Get rid of the first two lines, change var1 to &amp;1, change var2 to &amp;2, execute the script by saving it somewhere on your local machine, then executing the file like: @“C:\filename.sql” ‘inputvar1’ ‘inputvar2’;
I don’t know if it’ll work with the application you’re using or not, but I know it’ll work with SQL*Plus: Get rid of the first two lines, change var1 to &amp;1, change var2 to &amp;2, execute the script by saving it somewhere on your local machine, then executing the file like: @“C:\filename.sql” ‘inputvar1’ ‘inputvar2’;
The real question is... are you... me? 
It needs to work within Power Query as that is what I'm using to automate this whole reporting process. &amp;#x200B; Not sure if I'm following though. What would that query look like (with that super simple example above)? &amp;#x200B; Like if instead of those sample variables it was City = 'Philadelphia' State = 'PA' &amp;#x200B; Also, what is SQL\*Plus? I don't know much about the various applications to be honest. 95% of the time I use SSMS.
You could give it a shot and see if it works. Are you using Oracle or MSSQL? When you said PLSQL Developer, I automatically defaulted to Oracle, but SSMS and PLSQL Developer are definitely not the same database backends. SQL*Plus is Oracle via command line, a lot like MSSQL has SQLCMD for command line operations. If it was like the example above, you’d say:
I was just saying majority of my time is spent in SQL Server Management Studio rather than PL/SQL Developer. Different databases. We replicate a lot of the Oracle data in SQL Server environments which is where I do most querying rather than production system dbs. &amp;#x200B; I only am familiar with querying and not really any DBA SQL stuff and definitely nothing Oracle related other than SQL queries from Oracle dbs, so excuse my ignorance.
For this I'm using Oracle. [https://www.allroundautomations.com/plsqldev120.html](https://www.allroundautomations.com/plsqldev120.html) PLSQL Developer and the only part I use is querying via a SQL Window. &amp;#x200B; Can your method be done through this?
I also get [CTE](https://concussionfoundation.org/CTE-resources/what-is-CTE) from SQL Server...from banging my head on the desk. 
Honestly, no clue as I’ve never interacted with PL/SQL Developer. I’m only going on the basis of how substitution variables work in SQL*Plus/SQL Developer for Oracle.
Supervisory Investigative Analyst / Chief Data Scientist SQL daily into SQL server, DB2, or Oracle. Use toad data point to access them all. SAS, SAS-VA, Power-BI, R, Python, lots of scripting. Use Excel mainly to display extracts to clients, and visualize small sets of data. Use VA or BI to visualize larger sets. Building SAS-VA system as a report viewer and datamart. 
Ah gotcha. Ok I'll play around with it. Thanks for all the responses!
Put the case when in a subquery then use it in the where clause
You can query csvs using open data source. So, if you have a table of "datasets" like tables, files, etc. And you have a taxonomy setup... you can declare a dataset ad a specific type (xls,xlsx,csv) and query it / create a view... doesnt really matter what you do after that. The file is a record and is typed to use a specific FROM statement.
Are you using access as a data entry point? 
Anywhere you want to put Philadelphia, put &amp;1 in the code. Anywhere you want to put PA, put &amp;2 in the code.
What do the tables look like? What are the joining on?
This is what i have: SELECT T1.customer_id, T1.customer_name, T1.customer_type, T2.order_id FROM customers T1 INNER JOIN sales_orders T2 ON T1.customer_id = T2.customer_id WHERE T1.customer_type = 'R' ORDER BY T1.customer_id, T1.customer_name, T2.order_id And yes, I'm trying to see how many customers where their type is "R" have the highest amount of orders placed. I tried changing ORDER BY to GROUP BY but the same result appears. Just streams of Customer number A x100 customer number B x20 etc. etc.
What database platform?
I personally haven’t worked at all with MVC framework, we have a fresh college student that does and was building out a front end for data entry that I designed, we are now migrating everything to Angular. I will definitely be looking into anything I can to expand my knowledge. 
SELECT TOP 10 c.CUSTOMER_NAME FROM customers c INNER JOIN ( SELECT DISTINCT soCnts.CUSTOMER_ID, COUNT(soCnts.ORDER_ID) OVER(PARTITION BY soCnts.CUSTOMER_ID) AS “CNT” FROM sales_orders soCnts INNER JOIN ( SELECT CUSTOMER_ID FROM customers WHERE customer_type = ‘R’ ) typeR ON soCnts.CUSTOMER_ID = typeR.CUSTOMER_ID ) cntCustomers ON c.CUSTOMER_ID = cntCustomers.CUSTOMER_ID ORDER BY cntCustomers.CNT DESC;
Access is essentially a form that users enter data into. My database is terribly organized and doesn’t follow normalization standards which makes it difficult to pull data out in a reliable manner. 
Retreading Business Analyst - SQL, Excel, and Qlik for ETL and reporting.
I tried that early on by switching between varchar25 and nvarchar. No luck. I believe the 0 is the response for a successful execution and not the output of the SP
I’m not at my computer to test, but try taking out the GO at the end of the query. 
Programmer/Database administrator. 25yo. T-SQL, C#, ASP, SSRS, IIS are my main duties. I use Sql daily (actually taking 5 from a big stores procedure I’m writing to make this comment) as it does many things for my work: billing, inventory, data holding for production, laboratory data. SSRS and report builder are for the obvious. I had to write an ASP C# website for people to use in the field. SQL holds all the submitted data and does its wizardry on the backend. In college I fell in love with JAVA. Through work I found love for the .NET framework. Gotta say I hated SQL in college but when I got to real world application it’s been my bread and butter. 
Nothing is complicated once you understand it. 
Doubling in 2 years? Congrats, that’s really fast! Mind if I ask how has your job scope/ title changed since your original Business Analyst role? You mentioned only using Tableau/SQL now, which sounds like the typical scope for a business analyst.
ELI5 - an expensive query is inefficient or takes a lot of CPU. You make it less expensive by tuning it. 
Similar situation for me. Financial Analyst. Work for a smallish management company and am the only analyst in the company. I started using SQL to pull date into PowerBI, which I integrated the company into this past summer. Also have been now tasked with using SQL Server to pull in two different databases and use the read/write capabilities (ODBC) in both to make sure they are up to date and such. Been playing around with APIs recently, hoping to build my own custom data connector or just be able to pull data using APIs but it's been a struggle. 
Data Engineer SSMS, PBI, TeraData, some TD and TOAD to access our hadoop lake, and a bunch of proprietary metadata and QC applications. 
Yes, but even before I understood it I found it wasn’t particularly complicated when I first looked into it 
There's not enough details in this post to be able to accurately give an answer. Also, because this is homework, what have you got so far?
string aggregate product ids as a tag per order, find orders with the same tag
Look up customer ID and filter or sort by the datetime they were purchased maybe? There should be an OrderId index as well that would work. There aren't enough details here.
lol epic.
We're big and little at the same time. 1,500 employees overall but most are at acquisitions that run independently with only accounting and payroll being largely centralized.
Senior applications developer Stack- environments: ms SQL, teradata, Oracle, aster, vertica. Etl: ssis, python with pyodbc, mload, bteq. Tools: sublimetext, dbeaver, ssms, visual studio, SQL assistant. And whatever else they can throw at me and my team. Sql plays a huge role in my day to day. I primarily find and catalog datasets from the enterprise data warehouses. Take aggregated sets to make available for reporting in ssas/ssrs. I also supply data to a vendor application for text analysis, and am the primary technical support for that tool. I have worked with IBM Watson as well as sas contextual analytics and sas viya. We are building a data dictionary to support our analysts so they can better source their data needs when questions arise. I work mostly with survey data, contact management data, sales data. Call recording metadata, and call switch data. And an internal chat bot. Our data warehouses have thousands of schemas each with their own tables and designs. Each line of business that was once it's own company has its own data warehouse so moving disparate source data and making it all work together for reporting and analysis is the big draw for keeping us employed.
Data integration architect - daily sql
I'm just learning SQL in an ad-hoc reporting role, and this sub has been amazing for learning thanks to posts like yours. (That - and getting into the weeds trying to get the right numbers out of a database). I'm kind of guilty of this too - I wrote a solution for someone too on this sub. I think I just got eager that I COULD solve it - and did not stop to think if I should. Recently though - about 3 weeks into learning, I'm starting to notice I think about the data I'm trying to select fundamentally differently. It feels like my minds eye IS the cursor. Is this kinda normal for learning? Other factors to consider: I'm pretty high right now writing this post - so could just be woke af stoner thoughts. PS: Thank you to you and others for making this a great place to learn.
What you seek is called Relational Division 
My title is "Developer" but I have a bit of a developer/analyst role for data analytics for our production systems in our plants. About 7-8 months out of college. I do basic SQL, query optimization, some stored procs for reporting, SpotFire reporting and HTML/CSS/Python within it. Also doing ETL for data warehousing and also taking on a pseudo-DBA role for SQL Server
I'm a BI Dev with the same skill set. How do you like contact work and what are some good places to look for other contact opportunities? 
&gt;Other factors to consider: I'm pretty high right now writing this post - so could just be woke af stoner thoughts. A physchoactive substance that let's your mind wander deeper into abyss that is your current focus; surfacing a different and sometimes greater understanding of things you thought you understood? Is it learning or is it some sort of discovery algorithm taking your current knowledge and parallel processing it through your wisdom and critical thinking banks? I have no idea what you are talking about... As far as the solving part. That's why I cruise this subreddit, looking for that brain exercise. You should do more of it (answer posts). If someone posted the answer you thought of, see if you can solve it another way (just so you can get into the habit of attacking the same thing in many ways). Sometimes, like recursive ctes, there are only a couple of solutions. Sometimes I dont feel like typing out what I've already explained in a video... sometimes I get downvoted for that. Such is lyfe.
Similar situation for me. Financial Analyst. Work for a smallish management company and am the only analyst in the company. I started using SQL to pull date into PowerBI, which I integrated the company into this past summer. Also have been now tasked with using SQL Server to pull in two different databases and use the read/write capabilities (ODBC) in both to make sure they are up to date and such. Been playing around with APIs recently, hoping to build my own custom data connector or just be able to pull data using APIs but it's been a struggle.
Dang.
Not homework, just work. I want the customer's email address for the orders that have the same products in their orders. So far i've grouped by customers email and concatenated the productIds into a comma delimited string. That gives me close to what i need, but then i need to join the new string back to the products. I was just hoping there was a cleaner way without having to split strings later ect... Any ideas would be great.
OP said he needs to reduce the number of rows. COALESCE won't do it.
Contract work is good if you like to get a project completed and are able to get up and running in a new environment quickly. As contract resource more is expected of you to deliver in a shorter period of time. I love this as it challenges me and also helps me grow my skills exponentially by working in different environments and meeting a variety of developers. It is worth noting that with contracting there is additional admin work in maintaining your own business, tax, insurance etc. All of my work comes from LinkedIn and networking with other contractors or previous clients. 
You aren’t declaring the nvarchar length in your exec statement at the top. Try changing that to 11 too
Join productid to product and then concatenate product instead of product id. You're really not giving enough details to properly answer your question. We've only got context clues. Which relevant columns do you have and in which tables?
You need a left join but you need to understand that this can still return more than 100 rows. If b.key is not unique and has duplicate values that match a.key then this will return more rows by duplicating the data in Table\_A.
FROM A \`LEFT OUTER JOIN\` B ON A.KEY=B.KEY
Change the select to return, like this: set @coderoot = (select client_prodcoderoot from clients where client_id = @client_id) set @result = @coderoot + RIGHT('00000' + CAST(@id as varchar(6)),6) RETURN @result And i think you dont have to add the execute to a variable, just run it to receive the returned result. i may be mistaken tho, cant test it right now. &amp;#x200B;
You need to return the result, also, stored procedures can only return an int. When you execute a select statement in a stored procedure, that produces a result set, which it will send to the client, but that's different then returning. When you return, the stored procedure ends, and gives the return code, an integer.
Assuming an orders and an order\_line table, then you could do the following in Postgres: with ordered as ( select order_id, array_agg(product_id order by product_id) as products from order_line group by order_id ) select o1.order_id from ordered o1 where exists (select * from ordered o2 where o1.order_id &lt;&gt; o2.order_id and o1.products = o2.products); Here is an online example: [https://rextester.com/HMWHW55867](https://rextester.com/HMWHW55867)
Wow.. i think it did exactly that.. I never would have been able to put all that together. if you don't mind me asking - where did or what does "soCnts" come from? 
I also find raw T-SQL objects complemented with sqlcmd to be more efficient than SSIS. If I need to perform any "complex" operations, I'll use C#/PowerShell/bash. SSIS is clunky to me.
You'd have to do a subquery on the "Person Phones" table to return 1 record per employee with each phone as a column. Then you'd join the Worker table to the subquery and use coalesce on the phone columns to get the single best phone number for that worker. Finally.... WTF SQL Server syntax?!? Spaces in the table names?? Selecting FROM the schema name rather than a table? SQL Server is the devil.
Title: Business Analyst - Health Insurance sector Stack: SSMS, Visual Studio (SSIS), data dumped into Excel for reports to send up to mgmt or manipulating into csv/flat files for vendor/application processing via FTP. There are some in house developed MS Access dbs used to manipulate the data into the correct format but all I do there is plug in the file name/path I want to run through it. I don't know what's under the hood there. There are a staggering amount reasons why a claim or payment fails to process correctly and this most times involves diving into the data and see what caused the issue so it can be corrected, or drive configuration changes that would handle common failures to increase the overall % of automatically processed claims. How I got into SQL - a medical issue with a team member at my last job lead to me being brought in as backup/secondary on a few projects while they were out of the office, and they taught me a lot of the basics, I realized I really loved it, for a year and a half I would maybe write a query or create an SSIS/SSRS package once a week along with the rest of my duties, and recently took a job where I'm pretty much in SQL 90% of my day. This has been the first time in my career where I can see myself doing this kind of work long term and actually enjoying what I do.
Better formatting: SELECT TOP 10 c.CUSTOMER_NAME FROM customers c INNER JOIN (SELECT DISTINCT soCnts.CUSTOMER_ID, COUNT(soCnts.ORDER_ID) OVER(PARTITION BY soCnts.CUSTOMER_ID) AS “CNT” FROM sales_orders soCnts INNER JOIN (SELECT CUSTOMER_ID FROM customers WHERE customer_type = ‘R’ ) typeR ON soCnts.CUSTOMER_ID = typeR.CUSTOMER_ID ) cntCustomers ON c.CUSTOMER_ID = cntCustomers.CUSTOMER_ID ORDER BY cntCustomers.CNT DESC; The `customers` nested query uses the alias "typeR". It is joined to the `sales_orders` subquery "soCnts" on the CUSTOMER_ID column. Then the result of that `sales_orders` and `customers` joined set is joined to the outermost `customers` table, which is where the top 10 customers is selected from.
I’m late, but by the way, definitely look into writing the API in .Net Core instead of .Net Framework. I’m not sure how often new developers still fall for the trap of defaulting to .Net Framework, but .Net Core is about to hit major version 3.0, is very stable, and it’s more than clear that at some point in the future, .Net Framework development will stop as .Net Core keeps moving forward. Some of the shiniest new C# 8.0 features, for example (as of now in prerelease), will only be available to .Net Core applications.
I may be misunderstanding but if T1 holds all "in days" and T2 holds all Holiday, why not just join on Employee ID?
I'm unsure what you want from this query. Do you just want a list of dates by employee where the employee is either present or absent? If so, wouldn't that be solvable with `SELECT EmployeeId, Date, 1 AS Present FROM T1 UNION SELECT EmployeeId, Date, 0 AS Present FROM T2`?
How do you do the formatting? I still haven't figured it out on Reddit lol. 
It's just an alias I made up. 
So, dumb question - like typeR, "soCnts" is just used as another alias? Also, from the nested query, you can just assign it an alias just by typing it after all that? I thought you had to do that "AS [insert here]"
So, this might give you a start. I assume you need to join to some other tables for employee names, managers, manager emails, etc. select t3.date, empid, case when t2.date is not null then 'Holiday' when t1.date is not null then 'Worked' else 'Unscheduled Time Off' status from t3 left join t1 on t3.date=t1.date left join t2 on t3.date=t2.date Of course, this is also assuming I understand what you're trying to do. It's not entirely clear. 
`SELECT` [`T1.ID`](https://T1.ID)`,` [`T1.DATE`](https://T1.DATE)`, 'WORK_IN_OFFICE' FROM T1` `UNION` [`T2.ID`](https://T2.ID)`,` [`T2.DATE`](https://T2.DATE)`, 'HOLIDAY_OR_LEAVE' FROM T2` 
then join to the employee/manager table.
seems like no one understands what you're really asking... lol. i'm not sure if you care about weekends, and if you can exclude weekends from your dates table (t3). why not try: `select * -- whatever you need` `from dates t3 -- filter out the weekends if you don't need it inline, or with cte` `left outer join t1` `on` [`t3.date`](https://t3.date) `=` [`t1.date`](https://t1.date) `left outer join t2` `on` [`t3.date`](https://t2.date) `= t2.date` &amp;#x200B; use the above in a cte or a temp table, and if t1's field is null, select t2's field, will that work?
In a union all, if you alias the columns from both subqueries to be the same names, you can force those values to fall into the same output columns (the unique date fields would both have the same alias "date"for example). You can also use a cross join. 
Yeah, it's just an alias too. My formatting sucks for my query, so it's hard to tell, but yeah, you just type it after the sub-query and it becomes the reference to the sub-query. 
&gt; Business Systems Analyst any clue where i can start to get better at all this? I am interested in SQL stuff
Sorry for the bad explanation. &amp;#x200B; T1 only holds the dates in which employees have physically logged in i.e. f we're looking at attendance through January-March and I was on holiday for all of February, then none of Febuary's dates will occur under my ID in T1. T2 only has the dates when employees went on holiday i.e. February's dates (following the same example as above) and none of January or March. T3 holds all dates &amp;#x200B; Because T1 and T2 don't share any common dates, I'm unable to join on this field which is why I believe I have to drive the query using T3 as the main table but unsure how to do this.
Apologies for the bad explanation. **I want the dates as well as the rest of the employee information** (the blackened out part above the red line in the example I provided). &amp;#x200B; &amp;#x200B;
If i wanted to take this a step further.. say include the total dollar amount of each customer there. Would I further keep nesting/joining? SELECT order_id, unit_price FROM sales_orders INNER JOIN sales_orders.order_id = sales_order_lines.order_id and then in the ORDER BY at the bottom just include unit_price?
What you might be looking for is IN ALL. https://www.w3schools.com/sql/sql_any_all.asp You would use a correlated subquery that returned a column of ProductIDs where the OrderID was less than the one you are testing against in your outer query. This method wouldn't limit the result to only matching the one previous match, but every previous match. That's a different problem to solve entirely. 
For total dollar amount you would have to change some more stuff, try this: 
Wrap it all in a subquery then link into your employee info something like Select * from employeeInfo ei inner join ( SELECT EmployeeId, Date, 1 AS Present FROM T1 UNION SELECT EmployeeId, Date, 0 AS Present FROM T2 ) e on e.id = ei.ID 
Yeah there's an order_qty column in the sales_order_lines However, i'm getting back an error here: The multi-part identifier "coLines.ORDER_ID" could not be bound. The multi-part identifier "coCnts.UNIT_PRICE" could not be bound. Invalid column name 'CUSTOMER_TOTAL' I'm not sure what the bound errors are. i know the customer_total error is just because that doesn't exist within customers. The only other info in the customers table would be customer_id and the customer_type which we're already using. The only info with a unit price is the sales_order_lines table which hold the order_id - order_qty - unit_price Then sales_orders also holds an order_id and customer_id
I fixed it. SELECT TOP 10 c.CUSTOMER_NAME, c.CUSTOMER_TOTAL FROM customers c INNER JOIN ( SELECT DISTINCT soCnts.CUSTOMER_ID, SUM((soLines.UNIT_PRICE*soLines.ORDER_QTY)) OVER(PARTITION BY soCnts.CUSTOMER_ID) AS “CUSTOMER_TOTAL”, COUNT(soCnts.ORDER_ID) OVER(PARTITION BY soCnts.CUSTOMER_ID) AS “CNT” FROM sales_orders soCnts INNER JOIN ( SELECT CUSTOMER_ID FROM customers WHERE customer_type = ‘R’ ) typeR ON soCnts.CUSTOMER_ID = typeR.CUSTOMER_ID INNER JOIN sales_orders_lines soLines ON soCnts.ORDER_ID = soLines.ORDER_ID ) cntCustomers ON c.CUSTOMER_ID = cntCustomers.CUSTOMER_ID ORDER BY cntCustomers.CNT DESC; Fixed it.
The union itself works fine, however, the trouble is coming when I try to "fill in the blanks". I can't join it to employeeinfo (ei) on date because ei doesn't have T2's dates (which are in the union) so it's going to continue to exclude the non-matching dates. I'm so sorry for explaining it poorly but this is as best as I can explain it: T1: ID (a), Date (x but NO y), + rest of fields T2: ID (a), Date(y but NO x) T3: Date(has both x and y) &amp;#x200B; I need to union T1 to T2 to get all the dates but the only shared field is ID and then somehow populate all of the fields (firstname, surname etc.) that exist for x dates in T1 but not in y dates in T2 as T2 dates are not recorded in T1 and vice versa. 
Sorry gotcha. Is the employee info static or does it change over time?
Always start with the focus of your question / challenge. In this case its dates. -- Select date From dates dat -- Next, what do you want to know about these dates? If an employee was here or on vacation. This needs to be one dataset. So this is where the union comes into play. -- Outer apply ( Select employeeid From attendance Where date = date Union all -- remove all for distinct list Select employeeid From vacation Where date = date ) employeeIds -- Lastly you want more information from the employees.. -- Outer apply ( Select name, and, Stuff From employees Where employeeids.id = employeeid ) employeeinfo -- Then your date filters -- Where date between start and end -- 
You're getting hung up on joining on dates, and you don't need to. Your final report is going to contain two conceptual sets of things: the first is the unique set of EmployeeID, Date, and Status. The second is descriptive information about the employees identified by EmployeeID. **The Date field is not part of the employee description**, so you don't need to join on it when tacking the the descriptive info onto your main result set. This is what u/tasslehof was hinting at when he said "Select * from employeeInfo ei" above. There should be a separate table, apart from T1 and T2, that just gives info about Employees. If there isn't, or you just don't have access to it, you can roll your own using the approach below. WITH EmployeeDescription as ( -- Use DISTINCT to create an ad hoc table of just Employee data. SELECT DISTINCT ID, Plant, FirstName, Surname, Role, Home FROM T1 ), StatusByDate as ( -- Use UNION to combine presence/absence info aligned on employee id and date. -- I chose text descriptions for Status instead of true/false for Presence -- because you're reporting absences due to vacation, vs sick or not scheduled. SELECT ID, Date, 'In office' AS Status FROM T1 UNION SELECT ID, Date, 'On holiday' FROM T2 ) -- On the off chance that some employee appears in T2 but not T1, -- using LEFT JOIN makes sure that their ID still shows up. SELECT s.*, e.Plant, e.FirstName, e.Surname, e.Role, e.Home FROM StatusByDate s LEFT JOIN EmployeeDescription e ON s.ID=e.ID ORDER BY s.Date 
 WTTH CTE AS ( SELECT "Workforce Management"."Person Phones"."Phone Type" Phone_Type, "Workforce Management"."Person Phones"."Search Phone Number" Phone_Number, "Workforce Management"."Worker"."Employee Name" Name, "Workforce Management"."Worker"."Person Number" PN, ROW_NUMBER() OVER(PARTITION BY "Workforce Management"."Worker"."Employee Name" ORDER BY CASE WHEN "Workforce Management"."Person Phones"."Phone Type" = 'Home Mobile Phone' THEN 1 WHEN "Workforce Management"."Person Phones"."Phone Type" = 'Home Phone' THEN 2 WHEN "Workforce Management"."Person Phones"."Phone Type" = 'Work Phone' THEN 3 ELSE 4 END) Row FROM "Workforce Management" WHERE ("Workforce Management"."Person Phones"."Phone Type")IN ('Home Mobile Phone', 'Home Phone', 'Work Phone') ) SELECT Phone_Type, Phone_Number, Name, PN FROM CTE WHERE Row = 1
Oh man, sorry to make you more nervous, but you may be in some trouble depending on the size of the data set and the complexity of the queries. Those other tools you are familiar with pull in a lot of data and then act on it. SQL is about letting the RDBMS do the work. You need to hope the dataset is small. If I had to guess, I’d say spend time on the following: 1) JOIN syntax. LEFT and INNER mostly. 2) GROUP BY syntax. Make sure you know how your aggregates like MIN, MAX, AVG, and COUNT work. 3) Temp table syntax. The most common mistake SQL devs make is trying to do too much in one statement. Go small and make temp tables along the way. If you think it may get more advanced than basics, some more advanced topics would be: 1) Make sure you understand windowing functions. Windowing functions let you rank your data to help add missing keys and tying records together. 2) Subqueries. Subqueries aren’t as performant ad JOINs typically, but they can simplify the syntax. Know the difference between correlated subqueries and non-correlated subqueries and how/when to use each. Remember the test driven development mantra of “red light, green light, refactor” - get something that goes from not-working to working first before you try to optimize (refactor). Best of luck.
It all depends how the question is framed - and the mindset of the interviewee in that moment. Interview tests only evaluate their test-taking ability. They have a place, but shouldn't be an automatic failure of an interview. &amp;#x200B; The important part is if they can explain their plan - who knows what requirements their previous job had that mandated a less intuitive/common structure. Honestly though, I'm not even sure I understand how the interviewees are failing the question, relative to the expectation. But if you're having an interviewee design a model - you really shouldn't have an expectation that is the only right answer. They need to show you they understand a data model/structure - not that they can give you the exact scenario you need in 15 minutes with minimal explanation. &amp;#x200B; I'd say your interview question is probably fine, and I'd guess your expectations are fine - just make sure your interviewee can explain their model, and if you give them a reason it won't fit your requirements, they can adjust it accordingly. You don't want the employee that can guess your interview question (or has simply worked with your exact requirements before) - you want the employee that can adapt to fit whatever problem you have, and that requires thinking unique solutions to problems.
First error is you can't drop a table that doesn't exist. Usually you preface that by some type of statement to check if it exists. second error, is the DEPTNO column on the DEPT table the same data type? CHAR(2)?
I finally got it thanks to you - it was the CTE (or lack of) that was fucking my brain up. Thank you so much!
You're welcome! Glad it helped.
Are Apples.FirstDate and Oranges.Date the same date type?
If you have to convert the dates to make it look like MM/DD/YYYY, then I'm assuming they aren't natively stored like that, so doing a where on the unconverted field (o.Date between '01/01/2018'... etc may not work as intended. You can use the convert/min right in the where statement without aliasing it like where convert(varchar(10), a.FirstDate, 101) &lt;&gt; min(convert(varchar(10), o.Date, 101)) To make it cleaner throughout you could also define variables for the date output you want to include in your where/having statements, then use the variables in your where statement instead of o.Date and a.FirstDate. I don't think SQL likes just using Alias names like -- " and 'ApplesFirstDT &lt;&gt; 'OrangesFirstDT', " declare @ApplesFirstDT varchar(10) , @OrangesFirstDT varchar(10) set @ApplesFirstDT = convert(varchar(10), a.FirstDate, 101) set @OrangesFirstDT = min(convert(varchar(10), o.Date, 101)) then you can do where @ApplesFirstDT &lt;&gt; @OrangesFirstDT and it should work. 
if you have `SUM(movieTotalGross)` in your SELECT clause, then you do not want `movieTotalGross` in your GROUP BY 
Thanks for your quick response! For some reason, when I take 'movieTotalGross' out of the GROUP BY clause, I get an error saying "Column MOVIE_PLAYER.movieTotalGross is invalid in the HAVING clause because it's not contained in either an aggregate function or the GROUP BY clause, which I'm not sure why I'm getting that error.
Thanks, but I thought you can put min or any aggregate in the where clause?
No, one is Date and the other is DateTime. I guess I could convert to Date for the DateTime, but that still wouldn't solve this I don't think.
That's right, I forgot, aggregates can only be used in HAVING, not WHERE. Try it by declaring it as a variable and see if it does what you want it to do.
wait, what? the query you posted has only `gamePlayerHoursPlayed` in the HAVING clause
4 spaces at the start of each line
They will only equal when the date time is exactly midnight and 0 seconds. A date field has a time, it just isnt rendered (kind of). That time is always set to 00:00:0000. So you are not actually getting equal results, because the date field is 1/1/2010 at midnight While the date time field is 1/1/2010 at 3 pm. So the computer says "nope, not equal", and returns them. 
I actually didn't know myself if this was going to work, so I wrote a quick script: DECLARE @t TABLE ( [description] VARCHAR(50), [date] DATE, [datetime] DATETIME ) INSERT INTO @t VALUES ('Same date', CAST('1/1/2019' AS DATE), CAST('1/1/2019' AS DATETIME)), ('DateTime has time', CAST('1/1/2019' AS DATE), CAST('1/1/19 1:00 PM' AS DATETIME)); SELECT [description], [date], [datetime], IIF([date] = [datetime], 'True','False') AS comparison FROM @t
Really? Wow. Would’ve never guessed.
My apologies u/r3pr0b8, I was thinking of the wrong project entirely. Alright, so for the CORRECT project, it's about video games and we have four different tables, DEVELOPER, GAME, PLAYER, and GAME\_PLAYER. The table relationships are: DEVELOPER is the parent table of GAME, GAME is the parent table of GAME\_PLAYER, and PLAYER is also the parent table of GAME\_PLAYER. GAME\_PLAYER has both the gameID and playerID inside it, if that's any help at all. The question asks to show "the total number of hours played in total by all players, but only show the games that have a total number of hours that are over 400". Those games are Fallout 4, Final Fantasy 7, Goat Simulator, and Ocarina Of Time but for some reason, I can't get Fallout 4 in the results output, even though the total hours played is over 400.
The bottom right corner below the reply box has a link for "formatting help" which explains all the formatting options.
This is the basic entry level oracle exam blueprint, [https://education.oracle.com/products/pexam\_1Z0-071](https://education.oracle.com/products/pexam_1Z0-071) Looking it over I would say you will obviously need to learn the oracle syntax, which is like 90% the same as what you probably already know. The places where the syntax is most likely to differ is the commands to actually create and manipulate the db objects, plus some of the functions you might/might not already be familiar with often have slight tweaks to their syntax. When it comes to the actual queries to read data you should be able to sit down in front of any SQL flavor and be able to follow them no problem, most of the differences are pretty light from the end user perspective and are more often significant on the actual admin side.
How about that.
design of tables is fine could you show your query?
Postgres is excellent but kinda slow Mysql is sloppy but fast Oracle is feature full but ten million dollars (not kidding) Ms Sql is like posters but a hundred thousand dollars 
"Postgres is kinda slow" has not been true any more for at least a decade. It was true with the 7.x line and might have been true for some areas for the 8.x releases. But definitely not any longer. 
Sometimes it's relatively slow but for a good reason, like correctness. MySQL doesn't check that certain dates exist, but pg does, and that takes time. Also harder to scale writes
You can't use aggregate functions in a where statement, look up 'having'. Plane is taking off so I can't explain more right now, also you need to group by continent
Put some planets on the board, put some properties like rotation speed, satellite count, distance from sun. Put some non planets on the board. Ask them to model it... Slowly make it more complex. "Now I've discovered a new solar system, it has a binary star, what to do?" "Now I've discovered that there is a planet locked in orbit around another plant, what to do?" "MORE GALAXIES" For us architects, this is a blast.
So like, British English to American English? I will understand the other with slight issues?
This article, though written for SQL Server, describes the problem pretty well by describing what the standard serializable isolation level has to do: https://sqlperformance.com/2014/04/t-sql-queries/the-serializable-isolation-level
That's what I thought. I'm only just staring my database class now, but I think that was the first rule we learned. 
Lots of companies still use db2. That's why cobol programmers are all filthy be rich. But I can't imagine anyone would use db2 now for an entirely new project. 
Ya I mean I'm sure most IBM projects use it. I've just never met anyone that does. Must be an insular group 
3 times the countries on the same continent. They want you to use some rarely (but powerful) where filters Let me teach you Select name, continent From world as world_top That's your top. That's what you need. Your where is going to filter it down. Where world_top.population &gt; ALL ( Select 3 * population From world as world_bottom Where world_top.continent = world_bottom.continent And world_top.name &lt;&gt; world_bottom.name ) What that where does... is it compares your main population (top) with the same table (bottom), but we are tripling the population and telling sql to not compare the country we are passing down. 
Sounds like you need to write dynamic SQL. 
Check out how Oracle handles dates. At the time I did not know of the to_date() function in Oracle. 
I'm new with queries and how to write code. I'm going to search for dynamic sql. Thanks for the reply. 
So... let's say I'm interviewing you. You come in with a passion to learn, but have limited sql knowledge, for a sql position... However... during this interview project (if monitored) you are vocal about it, or make jokes about it like... man in SAS the function X would RIP apart this data in Z way... I'm not really sure how to accomplish this in pure sql... I'd have to look it up. *ask an interviewer* would you be my google? I'd give you a shot. Because in my mind I'd understand that you know the harder part behind the science, you just need a little to catch up to the easier sql part.
He’s right. Make sure you know joins well. And make sure you understand how primary and foreign key constraints work. Throw in wildcards % for WHERE NAME LIKE and it always makes you look like you know wassup. The other basics are covered above. 
Genuine Db2 dba here. Not the mainframe kind. Linux, UNIX, and Windows Hundreds of companies still use it, for projects new and old. Enterprise-class database cheaper than the other commercial ones. You'd be shocked how many banks run on Db2, but I work with clients in retail, healthcare, manufacturing, and the entertainment industries. Db2 is a dang stable rdbms.
I used MS SQL for four years, at current company using MySQL. Most syntax is similar, easily discernable and adaptable when switching. My understanding is, cost and open source vs proprietary in the case of MS vs MySQL. 
Your point #3 about Temp tables. Spot on bro! I really need to remember that. Awesome advice. Thanks 
Neither is postgre slow nor is mysql sloppy these days. I used all of them as my daily job since 2003. My first project used DB2, back then in 2004. The reason we used DB2 was because that project is a sub contract from IBM. If your clients are insurance companies. So my experience is like this: DB2 - when you got a sub contract from IBM Oracle - when you got a sub contract from insurance company/government internal projects. Those customers are not afraid of spending. They don’t like free things, because all they need is to have someone to call and if any thing happens. Back then the days everyone project is using Apache open source libraries, a client told me, make sure you don’t download binary from Apache, get them from Redhat, which they paid for. Redhat recompiled Apache’s code and make it a commercial license and sell them to insurance companies. SQLServer - most medium/large companies partnered with Microsoft. Postgre/mysql - most startups and personal projects. They don’t need someone to blame if things go wrong. They are afraid of spending money. 
I have an SSIS job that loads data from DB2. Does that count?
The screens are not providing all the info, but I think I see an error message during import. Check out a few videos on how to import data into the database. Shouldn't be too complex. I think the issue here is either the datatype or data limits. Might be the scema too. Or selecting the right delimiting character. Trying creating a table during import. Once you get a hang of it, you could try creating and then importing. 
I seen a few but some don't use the new sql workbench. Also, some might use an actual query instead of using the import option 
that's because DB2 pricing for a minimal production environment is in the Oracle ballpark
Yes, that is a good way to think of it. At my job, on a near daily basis, I use Teradata, Oracle, and MySQL (and rarely MS SQL). All have their own quirks and different things you can do as far as the syntax goes, but in general, SQL is SQL. Sometimes you have to quickly google the correct syntax for weird things you might be doing, and it might slow you down a bit, but not much. &amp;#x200B; Most databases support "ANSI SQL", which is just a standardized way of writing SQL. Learning the ANSI SQL syntax for things is usual a good approach, because then it will usually work.....usually :) &amp;#x200B; &amp;#x200B;
Lots of things. You would need to narrow your inquiry down a bit. &gt; one of the admissions requirements is having an Oracle SQL certificate. Then I would run away from anything related to that as fast as possible. Oracle is overpriced garbage.
Jon?
They are all RDBMs.. Different companies. Different functionalities. Some offer tech support... Some are free... Some have various pricing models. Some have datawarehousing addons.. So basically they are all the same technology with different commerical products. We use Oracle, MySQL, Netezza, and Snowflake in our company. Orcale has no data limits, we use it for intitial staging. MySql is free, we use them for logs. Netezza is used as the final product, but has a very rigid pricing system, and has to be installed in every system, so licensing is an issue, but has a supporting DataWarehouse addon. Snowflace is cloud based, and pricing is pay as you use, with datawarehouse addons. So basically, you get oracle certified, everything is the same after that. Only the commercial product changes. 
How is this a SQL problem and not a java problem? 
Or you can make filters optional by eg where (a.field = @variablefilter or @ variablefilter = ‘’) Then you can have all your filters in 1 query but only use the ones you want when you run it
It most likely is an issue with your CSV formatting. Open the CSV file in a text editor, not Excel, and have a good look at it. Important things are: * what's the column separator? * how do these CarExpo, BodInjN columns look like? they seem to be regular ints formatted with a thousands separator but we don't know whether they're formatted like that by Excel or directly in the file? if you open it up in a text editor, is the CarExpo value in row 1 5161 or something else? * same for these ACBodInj fields, smaller than 1, what is the decimal separator? I don't know how MySQL workbench does it, but most apps will let you choose options for importing CSV and yours are probably wrong, it says right there that data would be truncated for CarExpo and while it may look like 5161 fitting into an int(11) column, if it's something like "5,161", SQL may interpret that as 5.161 (ie. 5 + 0.161) and will fail because it doesn't want to truncate your decimal value into an int.
MySQL and MSSQL are both not open source. MariaDB would be OSS.
Huh. I knew db2 could run on hardware other than an 'as400' but I've never seen it in production anyplace. Usually Oracle for new production systems unless they wanted to go cheap (and then it's mssql). For older production systems it's always been greenscreens, cobol and db2. I'll have to look into db2 on other platforms, you've piqued my interest
TBH i would just use regular expressions (or Excel formulas) to generate an insert script unless it has millions of rows or you're developing a process that you need to repeat over and over. Import programs are cool but there's often a lot more dicking around with them than I prefer.
Sybase? Informix? Teradata? This list is lacking.
How would I do that?
One of my older YouTube videos, so the Production quality is definitely not as great as my newer videos : ) However, this contains the specific steps that I take when optimizing TempDB on SQL Server versions older than 2014 (I believe?). I am pretty certain that 2014 was the first version that would automatically create additional TempDB data files per Logical Processor core in the SQL Server Installation Configuration. Anyway, hope everyone enjoys the content and let me know if you have any questions as well!
If the query doesn't change extensively, you can include the filter conditions all in the same query and not have to write separate blocks for each set of conditions. You can use left joins on conditions like "@filter = 'x' " to essentially only join when certain filter type conditions are met. There is a drawback to this approach in that if you have too many conditional joins, or too many filters strung together with "OR" in your where clause, you may get unpredictable query plans after the first execution that do not work well with certain combinations of your filters.
Oracle is certainly costly, but i'd stop short of calling it garbage. It has it's uses.
&gt; It has it's uses. All of which have superior alternatives and often without license costs or hassles.
Db2: When you want a BULLET-PROOF, High performing database. Many of the REALLY critical systems, behind the scenes, run on Db2. (Both on Z/os and LUW). You don't often hear about them because they are back end systems that. just. work. Also, because big customers like that don't want the world to know what technologies they run on for security reasons. Db2 on LUW can do many things that other RDMS can only dream of. You can run PureScale, and stay up 24x7x365 and lose a node or two, no problem. You can run a HUGE Data Warehouse on a DPF (partitioned) cluster and do parallel processing on HUGE datasets. You can put your data warehouse in a columnar Db2 database. Or just the tables that make sense to be columnar and have the rest of the tables stay in row format. Mostly your application doesn't need to know or care about any of this. SQL is sql. 
DB2 can easily run on Windows, or Linux/Unix. All the stuff I'm involved with is DB2 10+ running on Red Hat sync'd and replicated with HADR. We have tables into the billions of record. You can even set the compatibility level of DB2 to match Oracle and write procs in PL/SQL syntax if you want. DB2 has also ported a lot of recursive functionality from Oracle and others (CONNECT BY, etc.). The notion that DB2 is only for old COBOL banking systems is silly.
Glad this community could help. They've helped me tons. 
Thanks for the reply I will try to make sense of it all over this next week and I am watching the video in small chunks to make sure I understand
Not any more. Free version (Developer-C) for up to 100 GB, and after that, IBM often offers 80% off what Oracle would be for the same server.
Our pharmacy system runs on db2 still. 
A sequence # 
I dont follow.
Thanks for the reply, I'll take this in mind. 
Use both FlatpickID and componentNo as a composite key? Or add new PK FlatpickComponentID and keep the other two columns ?
Don't. He's telling you to use a so-called "synthetic" key, which isn't a "real" key.
ER diagrams are a sham. What matters are functional-dependencies. If you understand FD, you're fine. If you can write your schema using actual SQL, and successfully load that schema in your database-product, there are probably programs you can use that will auto-generate whatever diagram you want.
Not to be a jerk but, “I get to create an ER diagram” helps you fell more positive about the experience and will likely lead to better, faster results. Good luck, I hope you learn a lot. 
ER Diagrams are usually not created in advance, but after a project has been built. If you were to create first, then it is almost certainly going to change as you go. Basically, everything you've done is worth shit until the project is finished. Are things like this prepared in advance before projects are approved, etc.? Absolutely. But they are approved with the understanding that things will change as you *actually* begin building the project. If you aren't going to ever build the project, then this is a exercise that you are supposed to fail, but show certain knowledge that implies you improve your design as you realize short comings in your approach. Don't stress about this. If you are going to build the application. Then just build it and after its built put together something. 
I think i understand this now thanks to [AbstractSqlEngineer](https://www.reddit.com/user/AbstractSqlEngineer) so I made a fake table with three rows and three columns to visualize this code and hopefully I am visualizing this correctly. Sorry if this a little long but i hope my notes are helpful to other people in the future &amp;#x200B; Select name, continent # this selects all names and contin without a filter but there is one later to narrow down From world as world\_top #names this table world top \#That's your top. That's what you need. \# Your where is going to filter it down. Where world\_top.population &gt; ALL #now worlds top population needs to be greater then everything returned after ( # This a sub query but it is a correlated one since it references outside variables world top Select 3 \* population # so a row is selected in world top and compared against every row in this sub query \# this select statement would triple population of every row by default From world as world\_bottom #names this row world bottom Where world\_top.continent = world\_bottom.continent #makes sure the continents of the two tables match And world\_top.name &lt;&gt; world\_bottom.name #the names cannot match without this this would always equate to false since it would triple the country with the greatest population and compare that against itself which is not what we want ) &amp;#x200B; so lets say we have a table like this: Name Continent Population Narnia Hogwarts 1000 Sanfransokyo Hogwarts 2000 Neverland Hogwarts 20000 &amp;#x200B; So in execution: The outer query picks a random row lets assume they are in order so the row with Narnia is first row selected. Then looks to see if its population is greater then everything returned in the sub query. So now the sub query runs and it will run three times comparing this row to every row in the table again naming the subquery table a different name. This sub query will only return a row if the continents match (which they do for every row here) and if the name is not the same ( this is important which i will explain later). &amp;#x200B; So if the sub query picks Narnia first as well it will not return it since it matches the row name of the outer query which is Narnia and as stated earlier the names cannot Match. Now we move to row 2 in sub query which is the same continent of Hogwarts and a different name, Sanfransokyo. Fulling both of the conditions to return it in sub query. This subquery will now return 3 \* the population of Sanfransokyo or 3000 in this case and same thing for Neverland and will return 60000. So now the population of Narnia in outer query ( 1000 ) is compared against 6000 and 60000 and since it is not greater then ALL of them it is not selected by outer query's filter. &amp;#x200B; Now the outer query moves to second row or Sanfransokyo and compares it against every row in the table again and same thing happens here since 2000 is not greater then both 3000 and 60000 respectively. &amp;#x200B; Finally outer query moves to third row and since 20000 is greater than both 6000 and 3000, it is selected by the outer query successfully. If the AND statement: world\_top.name &lt;&gt; world\_bottom.name (names do not match) is removed this would return false. The reason is when the sub query gets to the last row or Neverland it would triple Neverlands population and the values returned would be 3000, 6000 and 60000 which Neverland's population of 20000 would not be greater than in the outer query. &amp;#x200B; Anyways that is my dumbed down explanation so I could understand if you guys notice anything wrong with it let me know and i hope someone else finds this helpful.
tf dude. Remove the spellcheck highlighting. The keys are underlined for you in your coursework table design section.
Man, I couldn’t disagree more. When you start a project you need to have an idea of the requirements. From this you design what the scheme will look like. If during the projects changes are needed then you make these tweaks. You should never just blindly start building a database structure without knowing the full picture of what you are trying to accomplish. 
It depends on the project, in my opinion, but even then there will be a lot of things that pop up and change based on requirements. If something is so simple that it isn't going to change... you don't need a diagram. &gt; From this you design what the scheme will look like. You're describing flow charting from a classical perspective. It is useful, but ultimately it is just a thing that changes. &gt;If during the projects changes are needed then you make these tweaks. You should never just blindly start building a database structure without knowing the full picture of what you are trying to accomplish. Depends on the database. If you're trying to build an analytics database that is often how you start off.
Look for specific elements of this in lectures/tutorials/etc from the course and stick to that. There is no one way to do most things in any design, so just roughly stick to the concepts and ideas the course covered as the opinions of this other guy are what mattered and he’s likely looking for those specific things. Having said that... 1) ER diagrams are about relationships not data. An Enum is just another data type or data types. If you just want to show it’s an enum then do what you’ve done above. Else you could specify the name of the enum as the type. Else you could breakdown the data in the enum (int, string) and store them as separate columns. Else you could specify int to reflect the id of the enum element. It’s up to you. Don’t over think it, just pick what seems to represent it best. 2) Think about expressing the relationship out loud in the form of “A x can have many y” and work back from there. Can a login have many privileges? Can a privilege have many logins? If you’ve got a many to many and want to keep your tables in 1st normal form, then throw a table between them to reflect the many to many-ness of the relationship. You’ll see that implicit in this is the assumption that privileges are defined in a table of fixed length where each unique privilege type only has 1 record. If you don’t have this and you’ve only got a small amount of unique privileges, then just throw extra columns in the login table. There’s lots of ways to do this, it’s really up to your design. 3) That’s a bridge too far, I’m not doing your assignment for ya, LOL. Hope that helps, have a good one!
Nice question. Forget the haters, ER is step 1 for proper DB implementation. Your ER diagram is pretty neat considering the requirements. &amp;#x200B; 1. Enum has been represented pretty straight forward in your ER. But here is my question, where are you storing the elements of the enum? . The answer is that you should have 3 more columns which for those elements. 1. **LabMRI** 2. **XRAY** 3. **OfficeVisit** 2. For privileges 1. Create a user roles tables. 1. **Role** : admin, scheduler, medicalStaff , patient) 2. **JobType**\[PK\] : ADM | SCH| MDSTFF | PAT 3. **Privilages Desc** : Just describe roles and stuff. 2. Now you add the **JobType**\[FK\] to your employee table . 3. Now you add permissions to view based on **JobType**\[FK\] 4. For the sake of this project , just present the Roles table, and you should be good. 3. The theory part is mentioning the privilages based on the **JobType** 4. You also need to join the login table. **UserID** is PK, **StaffID** and **PatientID** are FKs 5. Your User Diagram is really good for now , just add the Roles Table , and you should be good. &amp;#x200B; &amp;#x200B;
On further inspection I noticed that your Enum column is a different table all together. &amp;#x200B; SO what you do is , you create a new table , and join Enum * **Enum** \[PK\] * **Lab** * **MRI** * **Xray** * **OfficeVisit** In your Diagnostic Table, the **Enum** becomes the FK &amp;#x200B;
yikes. i havent read the op yet but I'm here to see if i get downvoted too. 1. You don't. entity relationships don't really depend on the domains (in fact, you can go completely logical and not provide any metadata for your columns). Some tools will allow you to define that (data domains) and for some platforms it can generate a check constraint for you, for example. If enum is not a real enum but rather a check against a list of valid values, you'd want to implement a reference table and a foreign key back to your reference table. For example, to refactor your table Diagnostic, you might want to create a table CategoryType_Ref (with CategoryType_ID and CategoryType) and replace CategoryType by the CategoryType_ID in your Diagnostic table. 2. I've no idea what role in the real world the Login entity is supposed to play, but if I take your question literally, then you just have (insert/update) different values in the Privilege field. 3. I did look. Well, first of all, datatypes matter. I'll give you one example. Think about a bag of chips. What datatype would you use to store its cost? Secondly, the crowfeet is not just squiggly lines with random end hieroglyphics. So think about what the line "says" and what the relation should be. &amp;nbsp; Tell you the truth, the only useful relationship for your real-world diagram drafting is 1-to-many (-&lt;). I would willingly concede that for the exercise you might need to use others and be graphically correct. one-to-one are rare and you should ask wtf you really doing if you have one-to-one relations. many-to-many are purely logical and implemented as an associative/link table via multiple 1-to-many relationships.
To represent enums in a table in an ERD, use the UML \`&lt;&lt;enumeration&gt;&gt;\` header like this: &lt;&lt;enumeration&gt;&gt; | Foos | ---------| Alpha | Bravo | Charlie |
To give different privileges to different types of users, one way is to use role-based access control (RBAC) or the similar attribute-based access control (ABAC). A typical ERD for RBAC would have a table for Roles (a.k.a. what type of user) and a table for Operations (a.k.a. what type of functionality), plus a join table for Assignments (a.k.a. user-role) and a join table for Permissions (a.k.a. role-operation. The associations look like this: \`User &lt;-- Assignment --&gt; Role &lt;-- Permission --&gt; Operation\` 
No man, you just start tossing shit together with no clue what you're building. 
Sql server Express is free. Or you can mess around in access. 
Flatpackid and componentid are generated by the dbms. The flatpackComponentId should be the same: an identity. Always use an identity (nonclustered) as your primary key, unless your instructor is looking for something specifc. Uniqueness is a rule, not a key. The identity represents a conceptual relationship between a flatpack and a component. 
Postgresql is one of the most advanced database engine out there. It can be used for everything from a small web application to huge mukti-terabyte size data warehouses. And it's completely free. For everything. Look no further.
Yep yep. Sql server Express + PowerBI is how I did my POC. All free and PowerBI is $9.99 per user per month. Kept PowerBI and upgraded SQL server to standard once we went live. 
Just an internship. I was lucky enough to skip the whole call center experience!
Thanks! I started off with just Access/Excel reports for my team, and then quickly went into Business Analysis (requirements gathering). This is where I wanted to climb the corporate ladder, learn more, and was ambitious so I did a lot unpaid - PM, process improvement and automation, proactively saving money, etc. I learned a lot and was able to use it as a “sure I can learn this” as a DBA. That’s how I got the DBA job, plus I was cheaper by about $20k even with my 15% raise. That’s also where I learned SQL and Tableau (my bread and butter). Interesting perspective that all I need to know is SQL and Tableau. I always feel like I’m not skilled enough without Python, and feel lazy not doing DBA work anymore. Moving forward I’m trying to build the business side of my skills (Product Manager) so that I can continue to grow. 
 CREATE TABLE movie ( id INT, original_lang VARCHAR(255), original_title VARCHAR(255), overview VARCHAR(255), popularity DECIMAL(9,6), release_date VARCHAR(255), revenue INT, runtime INT, status VARCHAR(255), tagline VARCHAR(255), title VARCHAR(255), vote_average DECIMAL(2,1), vote_count INT, PRIMARY KEY(id)
I just figured out I was missing a closed parenthesis. LOL 
select distinct id, name, min(date), max(date) from table group by id, name 
Have you made any attempt on your own? If so, show it.
Approximately 103% of debugging SQL successfully comes from readable formatting.
Smells like homework
 select id, name, min([time logged]) as [Time In], max([Time Logged] as [Time Out] from [Time Log] tl inner join Users u on tl.user_id = u.id group by id, name 
The worst part about the whole thing was the error itself, like how the hell am I suppose to read this: pymysql.err.ProgrammingError: (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '' at line 1") and think "Oh yea I'm missing a parenthesis" I guess I will now xD. 
I also have a question regarding variable types like DECIMAL(length,decimal), if I have something like DECIMAL(6,3) does that mean I can only store numbers in the form of 123.452 or is something like 32.41 still valid
Is what you did still a useable format for sql?
Unfortunately, it's giving me the minimum dateTime / maximum dateTime of that specific user. Was hoping for it to be the minimum / maximum of that day for that user.
Once the missing parenthesis are added, yes. Even when putting SQL in a string in another language, it makes your life easier if you do it in a way that makes the SQL well-formatted and readable.
Closest I had was: select \* from timings where user\_id = $id
DECIMAL(6, 3) is an exact type. It'll store up to 6 total digits, 3 before the decimal point and 3 after. 32.41 is equivalent to 032.410, so it's valid
Literally the first result of "MySQL decimal data type" Google search: https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html The first number represents the precision, the second number is scale, i.e. digits after decimal point. So in your example decimal (6,3) can store any value with up to 6 digits, and 3 decimal places. Both 123.425 and 32.41 are valid.
so you must also group on the date. specifically on the day
Add an extra column where you cast the timestamp to a date then you can use that in the group by along with the other two. cast(timelogged as date)
your DISTINCT here is totally redundant
I understand that the \* is used to denote a foreign key. &amp;#x200B; When you say the primary key for flatpackComponent should be flatpackComponentid, I am not sure what you mean. Do you mean both (foreign keys) should be made primary (i.e. a composite?) or do you mean that there should be a new column created called flatpackComponentid
MySQL will work as well. Specifically MariaDB can be used for FREE even in a commercial environment. As an integrator, I have used [Ignition with MES module](https://inductiveautomation.com/solutions/mes) to track and monitor quality of product exactly as you are needing to do. If you are using it as a proof of concept, it's worth a shot to consider. There are other applications that do the same thing but the other well known competitors charge a cost before consideration. Ignition has a perpetual 2-hour trial. 
The database portion of this is fairly standard, pretty basic stuff. What you need to figure out is the software that's going to live on top of the database itself. "Visualizations" do not come out of your relational database. The workflow of your parts going through production, QA, reworking, re-QA, etc. does not come from the database itself. The scanning of barcodes is *not* a function of the database. &gt;Initially, they want to see what I can do without spending any money. Well, you can stand up a database for nothing in a variety of ways (as seen in various replies here). But that doesn't get you more than 25% of the way to the **solution** that you need.
I was looking at postgre. Installed it and started messing around some yesterday. Do you know of anyway I can analyze the data in a visual way?
Check out stuff like here: https://blog.capterra.com/top-8-free-and-open-source-business-intelligence-software/ I tried Metabase before, it's pretty basic but could be enough. That solution from Pentaho also seems nice, but might be harder to integrate. If you feel like it, you should maybe start learning Python and how to use Jupyter Notebook. There are plenty of visualization options that are quite powerful. You can check out this repository for examples: https://github.com/trekhleb/homemade-machine-learning
In general i make my SQL querys with a tool for the db in use, at work with mssql i use sql management studio. If whatever operation works there it will also work in programs i write in java or groovy. So you don't have to hassle trying to find sql syntax errors in your program code.
The example scenario doesn't say which transaction isolation level is being used nor does it show what transaction A is doing. Both matter. This link may answer your question (if trans A is doing not doing DDL): https://www.postgresql.org/docs/9.1/transaction-iso.html
Sounds like an interesting project and I'm curious about this as well. What do you think of using a functional index?
Don’t know that flavour of sql but you usually put AS ‘Flatpack’
MariaDB uses MySql
Ok so I suspect the above would work. Select blah as ‘Flatpack’ from ...
I’d have to look that up as I’m not familiar with it. I can only tell when something doesn’t seem like the most optimal solution to the problem. I’ll check out functional indexes. Thanks man. 
If you setup StudentID as a Primary Key on the table the SQL Engine will not allow you to insert duplicates. 
This
Thank You! But do I have to set it as a Primary Key for every single other table that contains the StudentID?
You can check for the existence of the value in a separate query beforehand or you can add in something like WHERE NOT EXISTS (SELECT StudentID FROM Student WHERE StudentID = value)
That would have been a little bit complicated, I solved this by setting the ID as primary key from the SQLite.
A programmer should never rely on constraints in the database to save them from themselves. You need to check and see if that ID exists before you do the insert. I'm actually more concerned about the grades table insert... and no it's not going to be the primary key on other tables unless you have a one-to-one relationship and if you do you need to reevaluate your database design.
Have you checked out PostGIS (https://postgis.net/docs/using_postgis_dbmanagement.html)? I believe you can create a GIST index with a WKT column, which you can create from having lat, long.
Set the studentid in the other tables as the foreign key. This establishes relationships in the rdbms.
I don't think it's complicated...you would have to have an If statement to check If Id exists....then create your SQL 
You can't assume that an uncommitted transaction was ever going to be committed, so of course it would be rolled back. I'm talking beyond my actual level of understanding here, but my understanding of how databases maintain true consistency is by 'queuing up' transactions in the log file in such a way that they can be committed in essentially a single action. The architecture actually requires uncommitted transactions to be logged as a precursor to committing them, so they have to exist somewhere.
New column. 
Please tell me why a programmer doesn't need to understand table structures on tables they're inserting to? Sometimes data is sorted before it's pushed to SQL and an understanding of the schema is absolutely necessary. You're saying that an absolute lack of communication between dev and database is fine?
I’m using PostGIS now. The problem is I’d need to create that index dynamically. Suppose I have a bunch of crimes that span a decade. Now I just want a density plot for car thefts that happened in 2017. That’s a subset of the data. Now I have to compute the variance of only that subset and count the total number of crimes (the kernel density estimator uses this information). Then I have to scale the lat and long with *that* variance for only 2017 crimes (this ensures that the integral over the density equals the number of crimes for that subset). I’d need a temporary table and an index to be created on each query. 
They absolutely need to understand the database. I never said that. I said a programmer should not rely on database constraints. Constraints are things like primary keys foreign keys nullability, data types. A programmer shouldn't use database errors as a means of validation. A program needs to do its own validation in accordance with the database constraints. 
In Oracle’s case, it writes to disk optimistically. This means that it sometimes writes data to disk even if it hasn’t been committed. This improves the throughout that the system is able to write to disk. This isn’t a problem because, as you mentioned, during recovery all uncommitted transactions are rolled back. 
I'm sorry, but you contradicted yourself there. If a programmer is doing their own checks before an insert than your first comment is null and void. 
My first comment said that a programmer should not rely on database constraints. All I meant by that is don't write a sloppy program and rely on the database to catch the errors. pretty sure it's just a school project so not really a big deal.
You shouldn't manage constraints on two (or more)different levels, to then only rely on errors back based on the constraints you made in YOUR system. Let the database do what it's meant to do. 
Sure thing. I was reading your comment about PostGIS, materialized views may be helpful with the subsets and calculations. Other people may disagree, but it may not be fundamentally wrong to create new tables from scaled data. Storage requirements, cpu, disk i/o are predictable and if there is still a lot of work to do before proving feasibility, it may be a quick and sure way to move forward. You are in the best position to make that decision. 
There are others that will explain far better than I can, but for Oracle, look into what UNDO and REDO are and do. &amp;#x200B; &amp;#x200B;
How do you define which of the ingredients in the relationship is the most important? Why is it lemonade and not lemon that you want?
Maybe think about it more like having a DISH which contains multiple parts, so you might have a recipe for a cake and a separate recipe for the frosting. You'd have a table that had two foreign keys: DishID and RecipeID. One row for the cake recipe, another row for the frosting recipe. 
I'd just make every recipe also an ingredient, and add a column to ingredient table linking to recipe. 
i think the best design for many to many table is to have pivot table at the middle so your solution is almost fine but i think you don't need to add id in it. |Ingredients|recipe\_ingredient|recipe| |:-|:-|:-| |id,name|recipe\_id, ingredient\_id, ingredienttype|id,name| and add **check constraint** to the recipe\_id and ingredienttype of recipe\_ingredient recipe\_id CHECK(recipe\_id = ingredient\_id &amp;&amp; ingredienttype = 'recipe') ingredientype CHECK(ingredienttype in ('ingredient','recipe') ) // if true it will throw error on insert/update [https://www.tutorialspoint.com/sqlite/sqlite\_constraints.htm](https://www.tutorialspoint.com/sqlite/sqlite_constraints.htm) this solution i not yet tested so i can't guarantee it is the best way.
ANSI SQL - a standard Transact SQL (TSQL) - MS SQL Server Procedure Language SQL (PL-SQL) - Oracle https://stackoverflow.com/questions/1043265/what-is-the-difference-between-sql-pl-sql-and-t-sql&amp;ved=2ahUKEwi_05W2kpzhAhVMs1kKHQFZDgEQFjABegQIDRAE&amp;usg=AOvVaw0n9pP1vrLK11JMIBSKC3vd
You should let the database manage the IDs. 
Does it work if you assign a default value of 0 to the column? 
looking at the subquery by itself, it produces a table with these columns -- SUBSTR(job_id,1,2) avg_min_sal avg_max_sal so outside the subquery, you cannot reference `SUBSTR(j.job_id,1,2)` simply because the subquery does not have a column called `job_id` that you can take a substring of give the substring inside the subquery a column alias like you did with the averages, and then use that column alias in the ON clause
i don't know python but you should be able to test those `item[nn]1 values for empty string and substitute the keyword NULL instead
Thanks! This actually helped me run through and figure out a few extra steps I was looking for too. I think I spent too much time caught up on the alias issue.
You need to decide what to do with the empty numbers Either don't store those rows, store the numbers as missing (null) or as 0s If you decide to store as missing, you probably need to allow nulls on those columns when setting up the table
Thanks!
I would have two separate entries for 003\[strawberry\_lemonade\], you could have 003\[strawberry\_lemonade\_a\] and 004\[strawberry\_lemonade\_b\]. One of them would map to 001\[lemonade\] and the other would map to 007\[lemon\]. It doesn't make sense to store multiple values in the same attribute.
This... A programmer should always isolate code, should never ever depend on other technologies to do the validation. Check with an if loop, and select query. Throw in an exception handler. Never expect the database to validate your values. Validate them at the controller only.
DDLs are auto commit. DMLs are should be commited. Always. So make sure you commit the DMLs or the data or changes are lost.
What RDBMS? 
&gt; RDBMS Sorry! SAP BO
There are no "sheets" in a relational database
I still run into the mixmatched key issue. Dishes would contain BOTH recipes, and single ingredients. For example a strawberry cake dish would have cake and frosting and also the single ingredient strawberries. If I make it to where dish ONLY contains formulas, then I run into the same issue with creating an ingredient for any recipes used as an ingredient; I'd have to make a recipe for every individual ingredient used in a dish. Unless I misunderstood.
The person is me, so that's easy. :D I see what you're getting at. I should clarify a bit more. I used examples that were a bit easier to understand in context. However the truth of the tables is that they contain mixtures of certain chemical ingredients. Individual components may be hard to work with but they still have their place in the workflow. Take for instance: **INGREDIENTS:** |**id**|**ingredient**| |:-|:-| |001|red| |002|blue| |003|yellow| |004|black| **RECIPE:** |**id**|**recipe**| |:-|:-| |002|purple| |002|dark purple| Ideally here, if I parse/print out a recipe for say 'purple' it should give me, red and blue. It does no problem. However if I print dark purple, under my current set up I would get red, blue, and black. BUT I have many premade mixtures already. It is likely I already have purple and a plethora of other colors premixed and ready to go, either used individually or as building blocks in larger formulas. With this simple mockup it isn't a big deal but when ingredients can get to the 100's then having smaller building blocks is easier. I can't ignore the single ingredients either. Sometimes I'll need to add a little bit of something all on it's own; to keep with the analogy, maybe I want a little more 'red' in my dark purple. &amp;#x200B; So, ideally for dark purple I would LIKE to somehow specify that I just need purple (already a preexisting recipe) and black or red (single ingredient) while keeping in neat, in the current table. This is where I hit the wall. My SQL senses tell me there's another table to add and I can't figure out how it would relate because if not I'll always have TWO keys that need to be corresponding to different tables at the same time. My feelings also tell me that having two different keys from two different tables is wrong. My cheap ass fake it 'till you make it gut says make an ingredient entry for any recipe that can be used as an ingredient itself (but that lacks elegance, and I'm TRYING to be better about "do it the right way first", since that's what SQL has taught me thus far)
Agree. I don't want strawberry\_lemonade\_a though. I don't ever need it to correlate to the individual components. For example if I query the ingredients for strawberry lemonade: Should always be lemonade + strawberry. NOT: lemon, water, sugar, strawberry. In a real world scenario I will always have lemonade ready to go, and I only keep the recipe for lemonade so I can make more if needed. THE SAME ingredients in a major recipe, may also be used in a MINOR recipe though. SO I can't just replace sugar/water/lemon with lemonade because another drink might need lemon and ONLY lemon as an ingredient. The same way lemonade might be used as an ingredient for another drink.
I'm trying to refrain from doing that as I FEEL there's definitely a harder more 'correct' way to doing it, and I already learned my lesson with taking shortcuts with SQL when this database was born; and I had to rebuild it because the relationships were all wrong. ...but sometimes you gotta do what you gotta do. Happy cake day my guy.
Some tables only need a primary key, as long as the data types on your columns are correct you know that this will enforce the correct data type which is a good start. One thing that won't be enforced are any business rules, for example take a DOB column. If you have a DATE datatype you are guaranteed that any data in that column will be a date. But if a DOB is entered at 1900-01-01 would that be valid? Probably not. Data types alone sometimes aren't enough. You need additional constraints such as CHECK and FOREIGN KEY constraints to enforce your business rules.
examples -- you can add a new order for a customer that doesn't exist, you can add items that don't exist to an order...
So I read up on pivot tables a little more, as my rudimentary understanding was that they summarize data effectively and that was the most useful thing they did, but I didn't see how it could help me (because I don't understand not because I don't agree). I did kind of get, from your example to make a sort of in-between table. Which I do have. Ingredients has it's own table. Recipe has it's own table. THen I have the reference table in between which list each ingredient in a corresponding table. If I stick to single ingredients this will last forever. I could probalby be content with it, but I'd LIKE to be able to reference other formula as formula ingredients. SO a formula may be made of single ingredients, other formulas, or both. I can't figure out how to reference both in the same table. I see in your example you kind of have both in the middle, but I was under the assumption that a field should avoid using multiple values. Again, I'm only 2 months in. I think I just didn't understand. 
Well the problem is that you are sending a single string of code. The SQL engine doesn't know if you missed a function here or a column or just the right parenthesis to end your table create. So it just says "hey you fucked up the syntax on... line 1". Because you only have 1 line of code, the error doesn't help much right? Imagine if you sent a single string of python as code, but there was a syntax error. How would you ever debug it? When you are missing an indentation it's relatively obvious, yes? The same applies to every programming language, including SQL. Just because SQL *lets* you ignore formatting doesn't mean it's a *good thing*. Well in this case, when you format the SQL, it's quite obvious that you're missing a right parenthesis. CREATE TABLE movie ( id INT, original_lang VARCHAR(255), original_title VARCHAR(255), overview VARCHAR(255), popularity DECIMAL(9,6), release_date VARCHAR(255), revenue INT, runtime INT, status VARCHAR(255), tagline VARCHAR(255), title VARCHAR(255), vote_average DECIMAL(2,1), vote_count INT, PRIMARY KEY(id) And *even if you didn't notice the missing paren at first* but you had tested this code with this formatting, MySQL would tell you that the syntax problem is on line 17. Which would be quite useful! 
something like this? Select top 1 claim_ref, doc_ref, count(*) as [total] from tables group by claim_ref, doc_ref order by COUNT(*) desc
If you have a new table for each month's activity, with the same design for each table, your database design is probably broken in the first place. One table, with a field to indicate which month the information corresponds to, is much better. Use `UNION` to combine tables "vertically" and `JOIN` to combine them "horizontally" in a query.
Maybe but it might not take the whole query, just the expression to match. I'll try an IN expression, with a list of the top claim_ref values 
I am VERY new to Microsoft SQL can you elaborate?
On which part? The documentation on how to use both `UNION` and `JOIN` is easily Google-able, and without knowing anything about your tables no one here can write your query beyond: select fields from table1 UNION select fields from table2 or select table1.field1,table2.field1 from table1 join table2 on table1.field2 = table2.field2 You have to figure out which is the one you need based on how the data is set up and what you want it to look when done
I'm not usually a dick about this stuff, but usually you'd try Google something before asking people. There are plenty of resources online on union and joins and how to perform them. 
perfecto, thank you!
I learn better when people explain concepts even after I have already googled them. but thanks for the input i guess?
So your only option is adding to the WHERE clause? Can you add a subquery? Like: WHERE column IN (SELECT othercolumn FROM sometable WHERE somefiltercolumn = 'value')
The answer here is a *huge* "it depends on the table data". If you gave us some context to your homework question it would help... Is your teacher expecting a specific answer that you were supposed to study for? Lol
The most frequently occurring value in a set of data is called a Mode. Assuming the data you are looking at is numeric, there's a chance that the software you are using has a mode() function. (Similar to Avg(), median(), etc)
The table you called "relationships" is a **junction table**. Look up the concept - plenty of material online, e.g. [https://en.wikipedia.org/wiki/Associative\_entity](https://en.wikipedia.org/wiki/Associative_entity) It's a very basic concept in relational theory and should be covered in most database 101 courses / classes / tutorials. &amp;#x200B; Conceptually, your case is the same as linking authors and books, actors and movies, etc. &amp;#x200B; I am not sure I am entirely following the part where you say one recipe can contain another recipe. I am not an expert, but I think **it is mostly a database design question**, so you might want to consider dedicated resources like stack overflow or dba dot stackexchange. Anyway, the key question I have for you is: **how do you want to store the data?** &amp;#x200B; Let's say you have one recipe for making custard cream, which you then use in multiple recipes. &amp;#x200B; Do you want to be able to drill down and aggregate? E.g. do you want to: * retrieve that the recipe for Grandma's pie contains the recipes for a) custard cream and b) shortcrust pastry , or * do you just want to retrieve all the ingredients for Grandma's pie, and are not really interested in the fact that that recipe contains two recipes within it? &amp;#x200B; How you answer these questions will determine how you want the data to be organised.
Friends don't let friends use Access. Especially when sql server express or PostGRE SQL are free [https://www.postgresql.org/](https://www.postgresql.org/) &amp;#x200B; How big is your data? How complex its structure? Do you have any experience in database design? &amp;#x200B; What tools are available? What's the budget? Can you consider tools like Alteryx or Tableau or would they be too expensive?
Maybe a HAVING filter on the MIN/MAX from_date/to_date?
Yes, junction table. Thanks for giving me the correct terminology. I've seen it called different things and so probably didn't pick up on the correct ones. I completely understand the junction table part. I believe the first example is conceptually accurate on how I set it up and I've used it for a month as intended with no particular issues. If I messed something up or it isn't obvious that I understand the concept I may have missed something but I'm open to criticism. I considered stackoverflow as I have much more of a presence there, however I refrain from asking open ended questions such as "the best way to..." and "how would you..." although I'm probably skirting the line of acceptable here and being a scaredy cat. To answer your question, I do not need any aggregation whatsoever; I could care less what makes up "custard cream". I just need it to print out custard cream, and I'll know go to the fridge and pull out premade cream. If I happen to be out, then I simply refer back to the db and print out the ingredients for custard cream and I can make another 1000 grams of it and put it in the fridge again for everyone's use.
The fromdate and todate are different on each order, nonaggregated or whatever its called, so i dont think i can do that.
Probably not the best way but you could have a not exists statement against records that it is removing. Probably the easiest solution
 SELECT B.model, COUNT(distinct,B.chassis_id) from Bike B left outer join Bike_Order BO on B.chassis_id=BO.chassis_id left outer join Orders O on BO.order_nr = O.order_nr GROUP BY B.model HAVING 20190328 &lt; MIN(ISNULL(O.from_date,99999999)) or 20190402 &gt; MAX(O.to_date)
&amp;#x200B; One way of doing it might be to have one junction table for recipes and ingredients, and another for recipes and recipes: &amp;#x200B; &amp;#x200B; Step 1 is to query J2 and retrieve that Grandma's pie requires the custard recipe Step 2 is to query J1 for all the ingredients in Grandma's pie and in custard &amp;#x200B; It would get more complex in case of multiple levels of nesting, eg if the custard recipes were itself based on another recipe. &amp;#x200B; This is just what I came up with thinking a little bit about it, and I no expert in database design. Please do post back if, as I trust will be the case, you manage to find a better way - I am interested.
Possibly yes. I'll try tomorrow as am out of time today. Thanks.
Your best bet would be to use the system catalogs to search for `int` columns and then generate SQL statements to select values with the appropriate `where` clause. You could then execute these generated select statements using [sp_executesql](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-2017). This is the query that I use to generate table definitions. You should be able to adapt it to your need. select name, typdef + ' ' + nullind + ' ' + identityind Definition from ( select col.column_id, col.name, case when typ.name = 'char' then typ.name + ' (' + rtrim(cast(col.max_length as VARCHAR(5))) + ')' when typ.name = 'varchar' then typ.name + ' (' + rtrim(cast(col.max_length as VARCHAR(5))) + ')' when typ.name = 'decimal' then typ.name + ' (' + rtrim(cast(col.precision as VARCHAR(5))) + ',' + rtrim(cast(col.scale as varchar(5))) + ')' else typ.name end as typdef, case when col.is_nullable = 1 then ' NULL' else ' NOT NULL' end as nullind, case when col.is_identity = 1 then ' IDENTITY(1,1)' else '' end as identityind from sys.columns col inner join sys.tables tab on col.object_id = tab.object_id left outer join sys.types typ on col.system_type_id = typ.system_type_id and col.user_type_id = typ.user_type_id where tab.name = 'TableName') a order by column_id Don't forget to prefix it with the appropriate `use database` command.
I skimmed the responses and I don't think you have your answer yet. Please ignore me if you do. I think I understand, and I think your root issue is you want a RecipeID to be used as an IngredientID. Just to get the ball rolling, have you considered adding a column to your RELATIONSHIPS table to allow a recipe as an ingredient, essentially ingredient.recipe.id? &amp;#x200B; |id|recipe.id|ingredient.recipe.id|ingredients.id| |:-|:-|:-|:-| |001|001\[lemonade\]|*null*|001\[lemon\]| |002|001\[lemonade\]|*null*|002\[sugar\]| |003|001\[lemonade\]|*null*|003\[ice\]| |004|001\[lemonade\]|*null*|004\[water\]| |005|003\[strawberry\_lemonade\]|004\[lemonade\]|006\[strawberry\]| &amp;#x200B; This obviously has issues. I don't know how big your project is, or how normalized/complex it needs to be. It's really hard to answer your question in the best possible way without more insight - but this is what I think is the easiest solution that gets your feet on the ground and gets you moving. &amp;#x200B; Another not-perfect option is to change your RELATIONSHIPS table from ingredients.id to something more generic, like a parent table/ parent table id relationship. &amp;#x200B; |id|recipe.id|parentTable|parentTable.id| |:-|:-|:-|:-| |001|001\[lemonade\]|INGREDIENTS|001| |005|003\[strawberry\_lemonade\]|RECIPES|001| |006|003\[strawberry\_lemonade\]|INGREDIENTS|006\[strawberry\]| &amp;#x200B; I'm just trying to give you some options to get you thinking about possible solutions. I would have a hard time seriously endorsing either option here, but these are solutions I've seen used before. Let me know what you think/what other requirements you have and I'll see if I can help with a different solution.
Yes, you are correct! You understand my rambling! The first table was my first consideration, and definitely would get complex. There would be a lot of nulls going around probably half the database. Right now, I have about 2200 ingredients, and 1000-1500 recipes (and I'm adding more each day). The second option looks much more... structured? This is going to be the best answer unless I can come up with another way to do it. It's still not perfect, but so far it's the most organized way I've seen of doing it. I'm still researching too! I'll keep you posted, and thanks for the ideas!
I'm gonna mock up some stuff and test it out! The term 'junction' really helped me shave off false positive hits on Google. I think I'm getting closer to an answer.
Glad I could help. In my experience it really becomes an issue with maintaining the integrity of the parentTable column. Just be careful about data entry (and any way you pull your data out) and make sure you hard code the table names to avoid typos. Good luck!
There are many roles in many companies that might use SQL day to day with varying levels of complexity. If you want to work as a software engineer, it's important to know some sql to understand how to interact with transactional databases. If you're working in BI or analytics, you need a deeper knowledge of sql for advanced querying. College is considered but not a hard requirement in a lot of places. I'm a data engineer so I've been writing SQL for the better part of 5 years. What level of experience do you have?
Currently work for a small company. When I started I didn't have a ton of experience. However, I did know my basics from some oracle stuff i did in the past. I was upfront with how I am terrible with syntax. My resume definitely helped. My college helped me get into the door but wasn't the sole reason for me to get hired.
Thanks for the response. I have zero experience. I actually have been watching intro YouTube tutorials this morning. It seems like something I’d like to put time into learning. I am curious if it’s something you need to spend years studying in order to do professionally? 
This is a very wide topic. Personally when I went through college 20 years ago the database class was basically two courses on MS Access with very little SQL. Even today I think most DBA's are accidental with few if any colleges having degrees focusing on database theory and development. From this it's my experience that most IT shops will weigh experience and college together. They might bring in a college grad with the aptitude to learn SQL and DB development the same as a seasoned DBA with little to no schooling. For me personally I came to my current job with about 15 years experience as a DBA and a college degree, but mind you what i learned in the late 90's in college really doesn't apply much today as it's so different plus most of it back then was theory. &amp;#x200B; The other angle are certification exams, but many just think someone with lots of certs is just a good test taker. It has to be a balance of all this along with good character to find the perfect fit for a DBA.
I had about 2 years experience as a SQL developer and 0 college. I have a bachelors in computer engineering but you’ll never touch SQL in that degree. My first job I had 0 sql experience but it was 75% help desk IT, we just so happen to have access to our database and I wrote stored procedures for reports to get my feet wet. 
That looks useful. The claim_ref is a string though.
Thanks again! I have been really good about my coding practices. Often reworking something into simpler, cleaner code with a lot of comments and very, overly descriptive names because I have a habit of losing myself in my code. For my first time with Python and SQL I feel mighty accomplished getting this far. The database works as is and the interface is easy to use and that's about all I can hope for. I DID edit my comment to give you more details. I apologize for the back and forth, I'm at work and bouncing between having a minute and getting called out for troubleshooting (database is unrelated, lol)
Do you have other programming or database knowledge? Excel/Power BI knowledge? If so, it won't be that hard to pick up on a casual basis and shouldn't be too difficult to learn more advanced topics.
It's impossible to suggest how long it would take, really. In my experience that the single most important thing I did when learning SQL was to learn what to pair it with. If you can do something as (relatively) simple as running a SQL query in Excel and importing the results into a table, you're already going to possess a skill that's foreign to 90% of the workforce. I use Excel as an example because it only took me about four months of active SQL studying before I landed my first two full-time job offers, one right after the other, and the biggest selling point for these shops was the fact that I could combine a skill I taught myself (SQL) with a skill I was already advanced in (Excel) to create something useful. Excel's ubiquity lends itself to this pairing very nicely, but anything that can complement SQL (Tableau, R, PeopleSoft, Python, C#, ETL tools, whatever) is good too. Make sure you understand what each job role entails too. Read up on DML, DDL, DCL, and other "subcategories" of SQL. There are very few jobs that are going to require you to know all of these realms of SQL.
Don't program, although I am an ETL Developer. I use a GUI tool, and when it does something stupid or performs horribly, I dive in and use my SQL skills. Got this job with "1.5 years" of experience, in reality that job I listed had maybe 2 months of actual experience max, but I knew how to sell myself properly, with projects and cherry-picked items from that job. College played exactly 0% into any post-grad job I've ever had. I was a computer engineering major.
not sure i follow the whole thing but maybe try fixing your join order? Right now you're joining to all chassis_Id in Bike_Order regardless of whether it's relevant to an order that you're looking for or not. ... from Bike B left join ( select bo.order_nr, bo.chassis_id from Bike_Order BO inner join Orders O on BO.order_nr = O.order_nr ) bb on B.chassis_id=Bb.chassis_id 
Where did this go?
I'm not looking for an order, i'm looking at the orders to find out if each bike is available in a certain time period, or if they've already been rented out.
`UPDATE my_table SET ID = ID - 1;`
I work for a fairly large company and write SQL practically everyday whether it is ad hoc or ETL. I initially got my start as an analyst with no background expect for some self-education. I got certified in Teradata (what my company uses as their DB). I was able to move into a more senior role just through continued self-education and constant use. &amp;#x200B; There are definitely some knowledge gaps with having no formal background but Google/Stackoverflow/team mates are wonderful things. 
Thank you.
Reading through your edit and a few of your other comments. I wouldn't entirely dismiss the idea of having an INGREDIENT entry for a RECIPE... every solution will have issues you'll need to work out. And...while it would cause some repetition issues if you maintain your structure - it would give some benefits. I'm gonna stick with your lemonade example - because that's where I'm stuck and I'm simple minded. You mentioned you'd have some lemonade already made in case you needed to make strawberry lemonade - you'd be able to add a "Quantity on hand" column to the ingredients table (if that provides you any value, I don't know). You could even remove the RECIPES table altogether, and just include them in the ingredients table. You could have a boolean column indicating whether it's a complex ingredient or not - but really just it's existence in the RELATIONSHIPS table would indicate it's a complex "ingredient." It's probably bad practice, and telling somebody to combine tables sounds like blasphemy, but maybe it offers you value. I don't really know if RECIPES has anything else of value in it, but if it's just over-complicating things: hire a lawyer, hit the gym, and drop that table.
The answer was already given but I think a bigger question is: Why would you want to do this and/or why do you care? Gaps in an auto_increment ID are meaningless (and expected in many cases depending on workflow). The only thing that matters is uniqueness and (sometimes) that ID's generated later are larger than earlier ID's.
CTEs can only be used once. You could look into table variables to get round only having read only access
I write a lot of SQL in my job working with large data sets. College was not really a consideration of me getting my first role at my company which required SQL. Experience was given a lot more weight. Then interview test questions around SQL. You can find low level, or entry level positions which allow you to grow your experience. Note: while I write a lot of SQL, my job is software engineer.
I write a lot of SQL in my job working with large data sets. College was not really a consideration of me getting my first role at my company which required SQL. Experience was given a lot more weight. Then interview test questions around SQL. You can find low level, or entry level positions which allow you to grow your experience. Note: while I write a lot of SQL, my job is software engineer.
You are right mate. But I think one day I'll be using it. It's good to know.
Out of pure interest, what is the ‘GUI tool’ that you use ?
The amount of studies that teach SQL are really limited. Which is a bit of an embarrassment as any discipline involving research would be served with the students knowing SQL. So due to this low overlap between SQL in the labour market, and SQL in education, college really isn't that important.
I knew nothing. Only knew enough to talk about it in the interview. Learned everything while working. 
Senior SQL dev here. Everything I learnt was 'on the job' or home learning. I first came across SQL about 4 years into IT work when I got an app support job for an app with a SQL backend. Basically I had to learn basic stuff like backups / restores / straightforward querying and joins etc. From there I moved between a few similar contract jobs, each time focusing a bit more on SQL and around 2 years in I started studying toward the MS-SQL 70-461 exam (disclaimer: I never bothered taking the exam as employers never seem to care). I can highly recommend any of the Itzak Ben-Gan books for home learning, he breaks things down in a very concise and readable way. Fast forward another couple of years and I became frustrated with support work and fired out CV's for developer jobs thinking it would come to nothing. Got a few calls back, interviewed and had a few practical tests, aced them (they were lot simpler than what I had prepared for!) and next thing I know I'm a SQL dev. A few years on from that I have worked on some genuinely challenging projects and enjoy going to work, which I never thought was an option ;) As with most computer related stuff, all the knowledge you need is out there online. Whether you go to college to learn it really depends on how you learn and whether you can structure your study time well unprompted in my experience.
I write SQL for my job (Data Analyst) and I had next to no formal experience writing it when I started. You can pick it up quickly though once you understand generally what each function is doing
Thank you for the suggestions! I found a solution by using cross joins. I'm including a sample here for anyone else who might have this same predicament. This is NOT an efficient process. Luckily, I am selecting 6 to 7 students at a time, per grade level, per school building, so it only takes about 30 seconds to run this query. First off, you need a table with a column for your entry ID (student ID) and a column for each demographic and each option within those demographics. -- MySQL CREATE TABLE `entries` ( `student_number` int(11) NOT NULL, `male` int(1) NOT NULL, `female` int(1) NOT NULL, `free` int(1) NOT NULL, `reduced` int(1) NOT NULL, `fullpay` int(1) NOT NULL, `asian` int(1) NOT NULL, `black` int(1) NOT NULL, `hispanic` int(1) NOT NULL, `indian` int(1) NOT NULL, `multiracial` int(1) NOT NULL, `white` int(1) NOT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8; Then, your data in the table needs to reflect a "1" in the appropriate column for each demographic. INSERT INTO `entries` (`student_number`, `male`, `female`, `free`, `reduced`, `fullpay`, `asian`, `black`, `hispanic`, `indian`, `multiracial`, `white`) VALUES (907, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0), (601, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1), -- and so on ; Finally, run your cross join queries, with random selection of one combination. This query will find a random combination of 6 students where there are 3 male, 3 female, 1 free lunch, 5 full paid lunch, SELECT e.* FROM entries e CROSS JOIN (SELECT CONCAT('^(', e1.student_number, '|', e2.student_number, '|', e3.student_number, '|', e4.student_number, '|', e5.student_number, '|', e6.student_number, ')$') AS regex FROM entries e1 CROSS JOIN entries e2 CROSS JOIN entries e3 CROSS JOIN entries e4 CROSS JOIN entries e5 CROSS JOIN entries e6 -- need 3 male, 3 female WHERE e1.male + e2.male + e3.male + e4.male + e5.male + e6.male = 3 AND e1.female + e2.female + e3.female + e4.female + e5.female + e6.female = 3 -- need 1 free, 5 full paid AND e1.free + e2.free + e3.free + e4.free + e5.free + e6.free = 1 AND e1.fullpay + e2.fullpay + e3.fullpay + e4.fullpay + e5.fullpay + e6.fullpay =5 -- need 2 black, 1 hispanic, 3 white AND e1.black + e2.black + e3.black + e4.black + e5.black + e6.black = 2 AND e1.hispanic + e2.hispanic + e3.hispanic + e4.hispanic + e5.hispanic + e6.hispanic = 1 AND e1.white + e2.white + e3.white + e4.white + e5.white + e6.white = 3 -- remove all duplicate combinations (e.g. 101 and 101) AND e1.student_number not in (e2.student_number,e3.student_number,e4.student_number,e5.student_number,e6.student_number) AND e2.student_number not in (e1.student_number,e3.student_number,e4.student_number,e5.student_number,e6.student_number) AND e3.student_number not in (e1.student_number,e2.student_number,e4.student_number,e5.student_number,e6.student_number) AND e4.student_number not in (e1.student_number,e2.student_number,e3.student_number,e5.student_number,e6.student_number) AND e5.student_number not in (e1.student_number,e2.student_number,e3.student_number,e4.student_number,e6.student_number) AND e6.student_number not in (e1.student_number,e2.student_number,e3.student_number,e4.student_number,e5.student_number) ORDER BY RAND() LIMIT 1) s WHERE e.student_number REGEXP s.regex The results will be 6 rows with all fields from the entries table. The regex generated is used to select only those students. There is probably a more efficient way to do some of the things I've done above. Happy to learn of any efficiencies anyone might be able to offer!
I’m curious as well. I’d wager Tableau or something of its type. 
SQL is slowly becoming a must have for data analyst positions. Especially at smaller companies. I got my first DA job with **No** actual SQL experience. I just brought in a certificate of a course I've completed on udemy. Now I have less than a years experience but querying databases every day, you get good pretty fast. &amp;#x200B; I do have a math degree so people probably figured I was smart enough to learn it once on the job. 
A GUI tool is like an IDE.. But you don't actually type anything. You drag and drop stuff onto a workspace and then manipulate the variables. Talend is a good example. You drag and drop components, and the tool develops code on the backend. You can see the code, find errors, see how the code is working. But you can't exactly edit it. You can edit the graphic interface.
Looks like you need this (it's sqlserver, yes?): insert into #results select d.IndividualDate from #perf p cross apply lft_get_dates('d', p.first_order_dt, p.perf_dt) d `cross apply` allows you to write subqueries (and table function calls) for each row in source table
Fancy! Hadn’t used that one before. It works! Thank you. 
Thanks. I'll look into table variables.
Working as Data Analytics using sql everyday. Studied physics in college. 0% from college being used other than analytical skills and ability to figure shit out 
If I understand you correctly, you'll need to do two things: 1) Do an ALTER TABLE to add the column with the correct data type. 2) Once you have the empty column, do an update on it referencing the values you want to bring over. Basic syntax (t1 is the table with : UPDATE Orders SET Orders.CustomerName = Customers.CustomerName FROM Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID
I work as a back end bi developer for a fortune 500 company. I have a liberal arts degree. After working most of my 20s at a crappy job, I went back to school at a local community college (online). I took maybe 5 SQL classes before I got frustrated enough at work to start looking. I got my first SQL gig from a large staffing firm. Was there for 2 years then moved into data warehousing. I prefer relational databases, but most work (and better paying) work is in data warehousing. If you want to get into the SQL world call a text staffing from and see what qualification you need. Expect no benefits while you're contracting for 6 to 12 months. 
&gt;UPDATE Orders &gt; &gt;SET Orders.CustomerName = Customers.CustomerName &gt; &gt;FROM Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID I get back this error : `Error 1: could not prepare statement (1 near ".": syntax error)`
 RecordNumber | Zipcode | ZipCodeType | City | State | LocationType | Latitude | Longitude | Xaxis | Yaxis | Zaxis | WorldRegion | Country |LocationText | Locatio | Decommisioned | TaxReturnsFiled | EstimatedPopulation | TotalWages | AvgWages | Notes | \+--------------+---------+-------------+------------------------------+-------+----------------+----------+-----------+----------------+-------+-------+-------------+---------+---- &amp;#x200B; | 128 | 08889 | STANDARD | WHITE HOUSE STATION | NJ | NOT ACCEPTABLE | 40.6 | -74.76 | 0.19 | -0.73 | 0.65 | NA | US | White House Station, NJ | NA-US-NJ-WHITE HOUSE STATION | false | 4691 | 8570 | 401312434 | 46827.6 | | | 129 | 07095 | STANDARD | WOODBRIDGE | NJ | PRIMARY | 40.55 | -74.28 | 0.2 | -0.73 | 0.65 | NA | US | Woodbridge, NJ | NA-US-NJ-WOODBRIDGE | false | 10018 | 17272 | 528315021 | 30587.9 | | | 130 | 07481 | STANDARD | WYCKOFF | NJ | PRIMARY | 40.99 | -74.16 | 0.2 | -0.72 | 0.65 | NA | US | Wyckoff, NJ | NA-US-NJ-WYCKOFF | false | 8079 | 15208 | 896273759 | 58934.4 | | | 131 | 10451 | STANDARD | BRONX | NY | PRIMARY | 40.84 | -73.87 | 0.21 | -0.72 | 0.65 | NA | US | Bronx, NY | NA-US-NY-BRONX | false | 19434 | 31477 | 471446942 | 14977.5 | | | 132 | 10452 | STANDARD | BRONX | NY | PRIMARY | 40.84 | -73.87 | 0.21 | -0.72 | 0.65 | NA | US | Bronx, NY | NA-US-NY-BRONX | false | 33517 | 56277 | 723508523 | 12856.2 | | | 133 | 10452 | STANDARD | HIGHBRIDGE | NY | NOT ACCEPTABLE | 40.84 | -73.87 | 0.21 | -0.72 | 0.65 | NA | US | Highbridge, NY | NA-US-NY-HIGHBRIDGE | false | 33517 | 56277 | 723508523 | 12856.2 | this is the table contents 
I use SQL every day and I’m in supply chain. I’ve been using it for Reporting and analysis. Eventually It just gave me access to everything because I could do it instead of bugging them. I used access for a few years prior
I thought tableau was for visualization? I think Informatica would fit the bill as a GUI ETL tool.
ok, so a) what do 'orders' represent b) how do you record that a bike is rented out c) how do you track inventory/bikes available.
Do cascading rollbacks ever happen in serializable transactions? I don't think they do because otherwise, it's not serialized isolation. You can't have a dependency between two serialized transactions because the isolation level forbids it. Am I all wet?
I don't think you're supposed to post your homework to Reddit.
im just asking how to create this specific table. something small i been stuck on. its for an extra credit assignment. not homework
I work for a company that hosts a wide catalog of searchable data. I use SQL every day to manipulate new data we're adding to make it more useful to the customers. (Lots of temp tables, update statements, and the like.) When I got this job, I had absolutely zero SQL knowledge/experience. 
SELECT o.OrderID, o.CustomerID, c.CustomerName, o.EmployeeID, o.OrderDate, o.ShipperID FROM Orders o INNER JOIN Customers c ON o.CustomerID = c.CustomerID;
Depends on the use case
If you want help, format your table, at the very least. Also, if what you have vomited onto the screen is your actual relation-heading, that is a terrible, terrible fucking heading. I'm willing to believe your instructor gave that to you, which is all the more maddening. There so many normalization anomalies I don't know where to begin. If I had intention of helping you it'd take at least an hour of correspondence before we could actually answer your question... except, you haven't asked a question. Just to be even more of a dick, here's you create the "table" you asked for: ``` CREATE TABLE State_info ( state TEXT, "average wage" NUMERIC, "total population" INTEGER ); ```
Can you expand upon your thoughts?
If you have to rely on a tool to write SQL for you, you don't know SQL.
You came to complain but still help? you could of just left the post empty and it wouldve been cool. i came to this community because im still new to SQL but i didnt know i would be dealing with fuck faces like yourself. homework or not, i didn't try to make a complicated request. So you my dude, can go fuck yourself you retarded ass neckbeard. Ppl like you are toxic to communities.
Much less the theory behind it...
DBCC CHECKIDENT (‘your_table’, RESEED, 0) but your table must be empty first. 
No. You didn't ask a question. You copy-pasted your assignment; extra credit or not. You didn't format your vomit. You want help, you act like you want help. Don't shit on the floor and make statement that someone else should clean it up.
lol if you say so. I guess you got nothing else to do but "help" and "clean up my vomit". Neck beard.
jesus god. &gt; I'm getting an error - how do i solve this? what's the error?
[Lookup Error](http://ora-00920.ora-code.com/) ORA-00920: invalid relational operator &amp;#x200B; I have an understanding of what this error means but I can't debug it.
There is no one size fits all answer to that question. It depends on the end user, application, intended audience, and time and resources ... etc etc the question on its own doesn’t have enough context for me to give my 2 cents.
Something to do with the following line? and employee.employee_number, ehr.ehrtitles.rec_id not in
How do I write this correctly?
I have been looking for a better job that I will enjoy. Your description of your job sounds great, what do I need to look for while searching to find one like yours? 
 SELECT HRID, sum(months) tenure FROM ( SELECT distinct ehr_reports.employee.EMPLOYEE_NUMBER HRID, ehr.ehrtitles.rec_id, max(EHR_REPORTS.EMP_JOB_HIST.MOS_COHORT) as months FROM ehr_reports.employee employee JOIN ehr_reports.emp_job_hist on (employee.REC_ID = EHR_REPORTS.EMP_JOB_HIST.EMPLOYEE_REC_ID) JOIN EHR_REPORTS.EMP_ORGANIZATIONS_HIST hist on (hist.EMPLOYEE_REC_ID = employee.REC_ID) JOIN EHR_REPORTS.ORGANIZATION_REF on (ehr_reports.organization_ref.ORGANIZATION_ID = hist.ORGANIZATION_LV) JOIN EHR.EHRTITLE_STRUC_REF on (EHR.EHRTITLE_STRUC_REF.REC_ID = EMP_JOB_HIST.EHR_TITLE_LV) JOIN EHR.EHRTITLES on (ehr.ehrtitles.rec_id = EHR.EHRTITLE_STRUC_REF.EHR_title_id) JOIN EHR.CAREERSTP_JOBFUNCTIONS on (EHR.EHRTITLE_STRUC_REF.CARSTP_JOBF_ID = EHR.CAREERSTP_JOBFUNCTIONS.REC_ID) JOIN EHR.COHORT_REF on (EHR.COHORT_REF.REC_ID = ehr_reports.emp_job_hist.COHORT) JOIN EHR.CHANGE_REASON_REF on (EHR.CHANGE_REASON_REF.CHANGE_REASON_id = emp_job_hist.change_reason_lv) JOIN EHR.JOB_GROUP_REf on (EHR.EHRTITLE_STRUC_REF.JOBGROUP_ID = EHR.JOB_GROUP_REF.REC_ID) JOIN ehr_reports.emp_job_hist job1 on (job1.employee_rec_id = employee.REC_ID) WHERE EHR_REPORTS.EMPLOYEE.HIRE_DATE &lt;= sysdate AND (ehr_reports.employee.termination_date &gt;= sysdate or ehr_reports.employee.TERMINATION_DATE is null) AND employee.employee_number, ehr.ehrtitles.rec_id NOT IN (SELECT DISTINCT ehr_reports.employee.EMPLOYEE_NUMBER, ehr.ehrtitles.rec_id FROM ehr_reports.employee employee JOIN ehr_reports.emp_job_hist on (employee.REC_ID = EHR_REPORTS.EMP_JOB_HIST.EMPLOYEE_REC_ID) JOIN EHR_REPORTS.EMP_ORGANIZATIONS_HIST hist on (hist.EMPLOYEE_REC_ID = employee.REC_ID) JOIN EHR_REPORTS.ORGANIZATION_REF on (ehr_reports.organization_ref.ORGANIZATION_ID = hist.ORGANIZATION_LV) JOIN EHR.EHRTITLE_STRUC_REF on (EHR.EHRTITLE_STRUC_REF.REC_ID = EMP_JOB_HIST.EHR_TITLE_LV) JOIN EHR.EHRTITLES on (ehr.ehrtitles.rec_id = EHR.EHRTITLE_STRUC_REF.EHR_title_id) JOIN EHR.CAREERSTP_JOBFUNCTIONS on (EHR.EHRTITLE_STRUC_REF.CARSTP_JOBF_ID = EHR.CAREERSTP_JOBFUNCTIONS.REC_ID) JOIN EHR.COHORT_REF on (EHR.COHORT_REF.REC_ID = ehr_reports.emp_job_hist.COHORT) JOIN EHR.CHANGE_REASON_REF on (EHR.CHANGE_REASON_REF.CHANGE_REASON_id = emp_job_hist.change_reason_lv) JOIN EHR.JOB_GROUP_REf on (EHR.EHRTITLE_STRUC_REF.JOBGROUP_ID = EHR.JOB_GROUP_REF.REC_ID) JOIN ehr_reports.emp_job_hist job1 on (job1.employee_rec_id = employee.REC_ID) WHERE employee.hire_date &lt;= sysdate AND (employee.TERMINATION_DATE &gt;= sysdate or employee.TERMINATION_DATE is null) AND emp_job_hist.start_date &lt;= sysdate AND (EMP_JOB_HIST.END_DATE &gt;= sysdate or EMP_JOB_HIST.END_DATE is null) ) GROUP BY (ehr_reports.employee.EMPLOYEE_NUMBER, ehr.ehrtitles.rec_id) ) GROUP BY HRID;
Agreed. A basic google search, or your textbook, would’ve solved this question easily. You want to be spoon fed. I bet you do need that extra credit 
&gt;I get the same error here? &gt; &gt;I am using Toad Data Point.
parens and (employee.employee_number, ehr.ehrtitles.rec_id) not in (select empNum, recID from .......)
Yeah i did that and found a result. Thanks. No not spoon fed. Just wanted assurance from the community since mySQL can be confusing sometimes. but i guess this place can be really toxic with people like you. Could just ignore the post or msg the Mod but i guess ya got nothing else to do
Does the error give a position? Or the text around where the error occurred?
I didn't change anything except the formatting. Your original post was unreadable.
Reddit isn’t your teacher. Neither is stack overflow. Microsoft docs, oracle docs.. that’s your professor. Otherwise you’ll be a copy paste master 
[removed]
U are right on that point. Bruh i just came here for help on a question. Its no big deal
I just dont like when the other guy had to come in insulting. Its uncalled for
Just got rejected for an ETL Developer role due to incorrect answers on my technical test. My confidence in my SQL skills has died. 
How did you get the Analyst role? 
Tableau is a BI tool not an ETL tool if you're using Tableau as your ETL or Data warehouse something's not right.
I was lucky enough to be taught SQL as part of my job, I knew literally none of it when I came on. 
I’m not using Tableau at all. It was just a guess. I should have read his original statement more closely I guess. 
As DBA, I much prefer scratch SQL. While it can be horrendous, nothing F*€#$ up performance quite as much as a GUI tool.
Ab Initio? 
Yeah, oracle needs that parens to do that line correctly.
He may be referring to "Tableau Prep" which is an ETL tool. A pretty nice one at that. I use Tableau Prep, AWS Glue, and Matillion for different use cases. In my past position, I used Datastage or Hive QL. I'll use SQL to profile the data and also to test my ETL programs, and test the reports they feed. 
Really? I assumed Tableau prep was meant for data profiling.
I find that to be true of many things in my work. SQL, that god-awful cisco GUI, sometimes even MacOS gets in its own way and the command line is so much better and straight forward for the job you're doing.
My company had replied in clunky reports written against a transactional no-sql database, but we also had a data warehouse in SQL server nobody was leveraging. I was in a more project management type role and working towards my masters in CS (also did CS for undergrad). I'd built a database driven web app before so I was pretty comfortable with SQL I set up SSRS and started building reports that were way better than anything else we had. They let me keep doing it and eventually I moved on to a database analyst role at another company.
I learned this really early on. I've had instance with postgres where something will fail to insert for one reason or another (human error on syntax generally or an attempt to enter duplicate data) and when it fails postgres will just skip that number and never use it again. I figure if that's the way postgres is written it's probably fine.
Just a trade-off. Handcrafted goods and homemade meals are better products than factory goods and McDonalds, but they don't scale to demand and they're typically not user serviceable. If the requirement is I'm hungry and need to eat fast to get on my way, scratch queries won't get it done. If I need to prepare the best meal possible cuz I'm gonna have to eat it every day for the rest of my life, Alteryx isn't getting it done. If I need to feed 10,000 hangry toddlers, a lecture on set theory and windowing functions isn't gonna fly. If I need to be able to recreate my recipe no matter what kitchen I'm in, Microstrategy is too constricting. Just tradeoffs. 
Yes started doing in web application development. Had a real knack for queries and started doing more, Now developing ETL packages using SSIS from our data sources, Have been gaining knowledge in data and system architecture. Just started my AWS cloud architect professional cert test prep. My skills are always evolving some of the queries and SPs that I wrote last year suck compared to now. I also am building skills daily, I work on a high paced project that has a tendency to either make you or break you, 5 years in and still not broken some days I want to cry in my pillow. In my project most of us have at least a BS degree. I started out with zero degree and earned it while working.
It's for that too. I'll be honest, I'm shocked by just how powerful it is. I can't speak to it's performance with regard to Big Data --its likely not a solution for Mass data loads, but it certainly extracts, transforms, and loads.
I'm SQL Dev! I practiced using strata scratch. I also like sqlzoo, datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches. But [stratascratch.com](https://stratascratch.com) has datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. 
College was required, 2 yr degree when i started 16 years ago, now 4 yr is pretty standard. I had a few SQL classes but didn't get hired to do SQL, started as RPG (AS400) programmer, now Data Architect, but did a lot in between, Lotus developer, VB, C#, ETL developer/SQL developer App Dev manager... learned everything on the job or at training, but mostly on the job. So to answer your question, i had very little experience to get this job, i was hired a couple weeks before graduation and prior to that i was doing custom woodwork so yeah zero experience, just that diploma. &amp;#x200B;
Like so? SET @FromDate='2019-03-28', @ToDate='2019-04-02'; SELECT b.model, b.chassis_id, o.order_nr FROM bike b LEFT JOIN bike_orders bo ON b.chassis_id = bo.chassis_id LEFT JOIN orders o ON bo.order_nr = o.order_nr AND (o.fromdate &gt; @FromDate OR o.todate &lt; @ToDate) WHERE ISNULL(o.order_nr)
I inherited a "data warehouse" that was just a load of super elaborate Looker reports running on redshift creating derived tables. You can do that. These reports literally took 20 hours to generate for a not large amount of data and it was completely impossible to audit or check anything at a granular level but you CAN do that. 
You definitely CAN but if you are that's not good practice whatsoever.
I write TSQL. I’m a database administrator who manages some large applications that use MS SQL back end and have a lot of custom stuff. I had very light knowledge when I came onboard. College degree in computer science. I kind of emphasized my experience a bit stronger but I only spoke about what I knew. 100K and learned most of what I know on the job
There is a saying: "You live by the GUI, you die by the GUI"
Oh god no. I was brought in to write an actual ETL process and an actual datawarehouse and generally unfuck it. 
Shrug - so what? A generated primary key value has one and only one requirement: to be unique. Gaps in the values of auto-generated primary keys are nothing to worry about. The value **does not matter**, a value of 1 is just as good as 6452923, -42 or 334567. &amp;#x200B;
Nice hah that's exactly what I'm doing at my job right now, ETL and python.
I knew that I didn't enjoy the responsibility of DBA work so avoided any job adverts that looked like I would be expected to log on any time 24/7 to urgently fix things. In terms of finding something I enjoyed, I was strict on myself that if I saw any 'red flags' I would politely turn down interviews etc. For example a couple of agencies called about interesting jobs but they were linked to arms companies so I turned down the interview on ethical grounds. Other jobs were in industrial estates so I knew that I would find the environment a bit bleak even if the work was interesting. What was key for me was that I was employed while I wss looking for my next job so I didnt have to take the first job that said yes. 
Its totally an employees market out there, in the UK at least. So much more jobs than there are trained SQL people. Scrub up on whatever you got wrong, grt your confidence back and try again :) Also, if asked a question in an interview and you don't know the answer don't be afraid to say 'I would google it / check on stack overflow'. No one knows everything and everyone I work with has to google syntax from time to time. The key thing they will be looking for is approach to problem solving imo. 
college name plays some part, college knowledge not such much
I work as a data scientist in the financial industry and all I did was going through [sqlbolt.com](https://sqlbolt.com) before interviewing. They asked me some simple questions like using where, like, union, subqueries, etc.
I started a new role in my workplace 6 weeks ago within a data reporting branch. I googled sql and used a website to practice prior to applying for the role, got the job and have been learning since. I have the chance to participate in a training course in a few weeks. The on the job learning has been really beneficial. I have been using SAS EG. 
What am I missing?
Fuck that's a big table. 
A table with 742 columns?
Cool, I'll have to give it another look since we have Tableau licenses anyway. 
all\_tab\_columns &amp;#x200B; [https://docs.oracle.com/cd/B19306\_01/server.102/b14237/statviews\_2094.htm](https://docs.oracle.com/cd/B19306_01/server.102/b14237/statviews_2094.htm) 
Yes it is
I work in SAS and pull data from Hadoop. The majority of tables have this many columns. It’s a nightmare 
Star schema fact table?
Same question here. Have always wondered where can I find data that can be queried off a database by performing joins on it. I use mysql at work. But would like to improve. 
I like this idea. Service not starting is good reasons - service account password expired - service account doesn't have permissions - service account missing a right Sa password lost (if used) TCP connections blocked Master or msdb moved incorrectly or missing RBAR? Database or disk corruption Log file/data file full with bad growth settings I might try and think of more 
I teach SQL at a college. I heavily use the Northwind Database version for MySQL [https://github.com/dalers/mywind](https://github.com/dalers/mywind) Then for querying, there are plenty of SQL assignments created for this database. [http://www.geeksengine.com/database/problem-solving/northwind-queries-part-1.php](http://www.geeksengine.com/database/problem-solving/northwind-queries-part-1.php) [http://www1.lasalle.edu/\~blum/c240wks/NorthwindQueries\_AnswersTo7and9.doc](http://www1.lasalle.edu/~blum/c240wks/NorthwindQueries_AnswersTo7and9.doc) &amp;#x200B; In fact, someone already created a tool to do it online: [https://www.w3resource.com/mysql-exercises/northwind/products-table-exercises/#PracticeOnline](https://www.w3resource.com/mysql-exercises/northwind/products-table-exercises/#PracticeOnline) 
If it's in azure and using AG then you have many possible things to break. Windows firewalls, ports not open or my favourite open to certain ips but the src VMS IP has changed for whatever reason (seen it happen), along with IP Changing you could also configure SQL server to listen on a specific IP then have it not on the machine and have them work out why a service that should connect to the server ain't working network security groups, cluster errors. Is performance on your list too?
You could configure the memory grants really low, or cost threshold for parallelism to 0.
Oracle Data Integrator, commonly abbreviated as ODI. It's a very painful tool and I absolutely abhor using it. Rather write everything in my own SQL but our company already shelled out money for licensing so...
Yes! Performance is for sure on my list. I have the $150 dollars to play around in with Azure from the MSDN license. But locally, we can throw a lot more resources at it. 
I'm actually doing this right now! haha! We discovered an old SQL Server laying around, and I'm trying to login. This would be perfect. 
Oh! This is good!
Nice one
They wanted to move our data warehouse to something using Hadoop. They weren't even going to give us a version of SQL that handled joining tables. Luckily the whole thing crashed and burned for bureaucratic reasons related to a reorg.
If you are including performance and operational issues then you could do things like running high resource queries on the DBS to max out the box and have them track it down or RBAR queries and blocking queries
Changing the collation can read to some interesting issues but that might not be relevant
&gt;Wow, that's awesome! Thank you! &gt; &gt;Is there any way to simulate someone screaming data set requests at me while I have to go to the bathroom?
does "regardless of any mismatch" mean that you don't care which values of ColA go on which rows of TableB? this is not going to end well please de-obfuscate your question
Functionally they should be the same to most DBs. However, writing an in for only a one item search could be confusing to someone else coming along in the future who has to figure out the reason for it. That bring said, if you are passing bind parameters and sometimes you use only one value and others multiple, the optimizer should be able to adapt and use an IN effectively
Another option is : [http://sqlfiddle.com/](http://sqlfiddle.com/) &amp;#x200B; but you need to find some data to input first, then you can query around... &amp;#x200B;
It's written that way because it needs to preallocate the auto increment ID so that the insert doesn't block all other inserts. This is precisely why you *shouldn't* care that the IDs are perfectly sequential or not.
Delete any, and every production table you have in the live database. Well see who gets to keep their jobs after. 
&gt;This is precisely why you shouldn't care that the IDs are perfectly sequential or not. &amp;#x200B; That's honestly kind of what I imagined, but I was so new to it when I found out that I just kept moving an accepted it and never really did much research into it.
If you are dynamically generating code, it is easier to always use an IN vs adding another tick to see if one value exists for the soon-to-be applied filter. Especially useful when dynamically generating sub queries.
"Curteze said it was a good training idea!" 
You might also like to look at the [Chinook Database](https://github.com/lerocha/chinook-database): &gt;Chinook is a sample database available for SQL Server, Oracle, MySQL, etc. It can be created by running a single SQL script. Chinook database is an alternative to the Northwind database, being ideal for demos and testing ORM tools targeting single and multiple database servers. There are also many exercises kicking around, just [google](https://www.google.co.uk/search?q=chinook+database+exercises) it &amp;#x200B;
Tell my boss all the time if the coffee isn’t working. Deleting a production table will sure give you a morning jolt. 
Without a shared column or some kind of relationship between the two, you're kind of out of luck, I think. 
Can you let us know what isn't working or are you receiving any errors? You have some weird back ticks in your query as well as missing a comma at the end of it, separating out the parameters... (it looks like it got moved into the single quote for the query?) Try this and see: BEGIN EXEC msdb.dbo.sp_send_dbmail @profile_name = 'NAME', @recipients = 'email@name.com', @blind_copy_recipients = 'email@name.com', @body = 'body text', @subject = 'Test', @query = 'CInfo.co, CInfo.name, CNotes.code, CONVERT(VARCHAR, CAST (CNotes.entryDate as date), 101) as PolicyEndDate FROM CNotes INNER JOIN CInfo ON CNotes.co = CInfo.co WHERE (CNotes.entryDate Between getdate() and DATEADD(day,30,getdate())) AND (CNotes.category = ''WC'') AND (CNotes.code = ''Data'')', @attach_query_result_as_file = 1, @query_attachment_filename = 'results.txt' END
Adventure works and Northwind are free databases. A lot of Udemy classes (as low as 12 bucks) use them, along with Google some exercises with those databases. 
Table is wider than the one JC used in the last supper.
Yep but when you enter the thunder dome you can’t expect everyone to be nice. Best lesson in life for social ability: 3 out of every 5 people who come across you wont like you. May not have merit to their stance, and you may not like it, but they’re entitled to their opinion. 
True. Im seeing that. Didnt know this community would be so anal though over a simple table question tho. Its not that serious. Thunder dome lol....i mean its a SQL reddit 
The corrupt database challenges http://stevestedman.com/zu2vw
I got a table with 742 columns as well. "EGO.EGO_BULKLOAD_INTF" (Oracle EBS) Strange coincidence?? My ERP, Oracle Applications, loves tables with lots of columns. I have 2,721 tables with over 100 columns, 201 tables that have over 300 columns, and 14 tables that have over 500. The top dog in my system has 802 columns. 
To union, simply select the same number of columns. You can do something like: select column1, NULL, column 3, NULL from table union all select column1, NULL, NULL, column4 from other\_table &amp;#x200B; If you're trying to combine different data types like number and text, convert the numbers to text using a function such as to\_char(). 
w3schools.com is pretty good for basic SQL syntax. I would really recommend you investigate in learning database fundamentals and 3NF... having a base understanding of database schemas and design will help you be able to understand what you're doing when writing SQL queries.
Use row_numer, eg Select ... From ( Select row_numer() over (partition by empid order by job start date desc) as rowid, empid, job etc ) x Where x,rowid = 2 Apologies for the sudo code, on phone
If you're OK with the features that aren't available (SSRS, SSAS, replication, etc...), then yep. The DB engine the exactly the same, so you're not going to notice change in query performance.
I think you would find HackerRank intersting. It has over 50+ challenges starting from basic selects to advanced joins. In each challenge you will get a new problem. Once you run/submit your code you will get a feedback from the system whether you did good or not. All you need is an account (free). [https://www.hackerrank.com/domains/sql](https://www.hackerrank.com/domains/sql) 
There is no substitute for installing a DBMS, loading a sample data set and running queries.
 SELECT 'January-March' AS Month , COUNT(*) AS 'Number of burglaries' FROM CrimeData WHERE Type = 'Burglary' AND Month IN ('January','February','March')
Thank you too for the links, will have some sql practice over spring break.
Perfect thanks
I'm sorry I dont understand this line. Could you expand?
sure SELECT 'January-March' AS Month , COUNT(*) AS 'Number of burglaries' FROM CrimeData WHERE Type = 'Burglary' AND Month IN ('January','February','March') UNION ALL SELECT 'April-June' AS Month , COUNT(*) AS 'Number of burglaries' FROM CrimeData WHERE Type = 'Burglary' AND Month IN ('April','May','June') 
So union all let's me add another part to the query that is separate from the one above is that how it works?
My method of learning was to tell people I could do it already and then learn by doing when they asked me to do things. It was a pretty cool path that worked out ok for me because I worked in operations and all anyone who was asking me knew about SQL was that I was magical for being able to pull things down myself without requesting development on a report. 
If you use IN. Doesnt it also include nulls?
 SELECT 'January-March' AS Month , COUNT(CASE WHEN Type = 'Burglary' THEN 1 ELSE NULL END) AS 'Number of burglaries' , COUNT(CASE WHEN Type = 'Larceny' THEN 1 ELSE NULL END) AS 'Number of larcenies' FROM CrimeData WHERE Type IN ('Burglary','Larceny') AND Month IN ('January','February','March') UNION ALL SELECT 'April-June' , COUNT(CASE WHEN Type = 'Burglary' THEN 1 ELSE NULL END) , COUNT(CASE WHEN Type = 'Larceny' THEN 1 ELSE NULL END) FROM CrimeData WHERE Type IN ('Burglary','Larceny') AND Month IN ('April','May','June') tourist in New York: "please, officer, can you ell me how to get to Carnegie Hall?" officer: "sure -- practise, practise, practise" 
This reminds me of a bit of dynamic code I wrote a couple years ago using IN. It worked great until it got loaded into some old SQL Server 2005 db where it took upwards of fifteen minutes to complete. Turns out there's a cut-off point of n-values (I forget the exact number) you can have in an IN before 2005 freaks out. One value less, the query ran in a matter of milliseconds; one value more and it choked to death. Shouldn't be an issue in anything newer, but all the same I started writing my dynamic code to put my values into a variable table and filtered using an INNER JOIN instead of IN (which ran totally fine in 2005). Legacy support is fun.
We won't know how to do the ON statements until we know what columns in each table match up to what. 
Thanks much appreciated 
I got started as an internship during college. They basically needed simple queries and some reports. I think they'd have taken anyone who could write a SELECT and breathe reliably. Six years later at the "same" job, I write stored procedures, functions, audit script performance and indexes, and design table strucutre for new solutions.
this is homework please do your own homework. you're cheating the other students, because you're graded on a curve.