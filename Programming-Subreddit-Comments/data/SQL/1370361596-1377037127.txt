Is the list of tech's in another table? Really you just need to setup a second data source and set @Tech to be a query based parameter 
yes, that is what I mentioned in my original post 
We've had some issues with BC NICs and are going to make sure we go Intel. I also don't think we'll need to trunk anything, but good to know. Thanks
Great. This is exactly what I was looking for. Thanks.
This is a good suggestion. 
I'm interested to know why you want to build two 2-node clusters. Here, we built 2 SQL Servers and 2 clusters - both servers are a member of each cluster. Each SQL Server instance (1 and 2) runs by default on their own SQL Server. In the event of a failover, both instances run on the same host. The upside of this is that you don't have a SQL Server that sits idle all day - you can split up your database load across two servers. A slight downside to this is that each of our SQL instances are configured to use only 1/2 of the server's memory, so that way we can avoid memory contention when both SQL instances are running on the same host. Compensate for this by purchasing double the RAM that you need for each of your servers. FWIW, SQL clustering works really, really well.
Here is [pdf file] (http://datubaze.files.wordpress.com/2010/10/celkos-sql-for-smarties-2005.pdf)
In the instance you outlined, an active/active cluster, you have to pay for two licenses [refer to the licensing guide pdf](http://download.microsoft.com/download/7/3/C/73CAD4E0-D0B5-4BE5-AB49-D5B886A5AE00/SQL_Server_2012_Licensing_Reference_Guide.pdf). If you go with an active/passive cluster, Microsoft only requires one license for the active node. Unlike in 2008, when there were server licenses available, 2012 is a per core licensing structure which adds up when you are talking about 2 32 core servers (you have to license all the cores, even if you only use some). Can you get away with it? Sure, just don't get caught if they do an audit. Assuming you are talking about one site, then a single cluster with multiple instances would be more cost effective. All I can say is be careful with naming, I had an issue recently where the node name happened to match an account name (someone added the account after the node existed in AD). There was a failover due to a network upgrade and then the node was unable to rejoin the cluster. We had to change the node name and then it was able to rejoin. We are also running out of drive letters on the 3 node cluster so we are starting to use mount points. If you really need the processing power, go with a 3 node active/active/passive cluster. That allows you to only have to license two servers, with the third as standby/failover. This is again, assuming the same geographic location. 
I do sadly have to buy 4 x license of 2012 standard at 8 or 12 cores a pop. Going to suck. 
So I dont know if this is the most efficient way of doing it but what I did was SELECT playerid,lastname,firstname,arrival,departure, min(roomrank) FROM table WHERE roomrank in (SELECT min(roomrank) as RoomRank FROM table) GROUP BY playerid,lastname,firstname,departure,arrival 
That will always get you roomrank = 1 (you are aggregating the whole table and you should filter by player). Also the outer min() does nothing. This should work but has a potential problem: SELECT t.playerid,t.lastname,t.firstname,t.arrival,t.departure,t.roomrank FROM table t WHERE t.roomrank = (SELECT min(t2.roomrank) FROM table t2 where t2.playerid = t.playerid) The problem would exist if a player can have multiples of the same roomrank. Then you could get two rows for roomrank = 1. If that can happen, then you need to decide which row you want (Max departure date or something).
I added extra entries to show the problem of multiple roomranks. You'll need to decide what constitutes a "unique" entry. declare @t table (id int identity(1, 1) not null, playerid int , name varchar(20),arrival datetime, departure datetime , roomtype varchar(20), roomrank int) insert @t values (1234, 'John Smith', '06-04-2013', '06-06-2013', 'Mansion', 1) , (1234, 'John Smith', '06-14-2013', '06-16-2013', 'Mansion', 1) -- added by me , (1234, 'John Smith', '06-14-2013', '06-15-2013', 'Mansion', 1) -- added by me , (1234, 'John Smith', '06-14-2013', '06-17-2013', 'Mansion', 1) -- added by me , (1234, 'John Smith', '06-04-2013', '06-06-2013', 'Main Tower', 4) , (1234, 'John Smith', '06-03-2013', '06-06-2013', 'Main Tower', 6) , (5465, 'Jenny Andrews', '06-05-2013', '06-07-2013', 'SkyLoft', 2) /* Showing multiple entries for same person differing on day */ select t.id, t.playerid, t.name, t.arrival, t.departure, t.roomtype, t.roomrank from @t t where exists ( select 1 from @t t2 where t.playerid = t2.playerid and t.arrival = t2.arrival and t.departure = t2.departure group by t2.playerid, t2.name, t2.arrival, t2.departure having min(t2.roomrank) = t.roomrank ) 
I think this will deal with the duplicate issue you call out: select t.playerid, t.roomrank from ( select playerid ,row_number() over ( partition by playerid order by roomrak desc) as OrdNum from table ) t where t.OrdNum = 1 If your rdbms supports them, windowing functions are freakin' cool. 
When you post queries to Reddit, put four spaces before each line and it will look much nicer. SELECT Clients.lASt_name AS LASt_Name, Clients.first_name AS First_Name, Clients.age AS Age, LookupDict.lookup_desc AS CSP, LookupDict1.lookup_desc AS T19_Status, Clients.text12 AS Legal_Status, LookupDict3.lookup_desc AS Disposition, LookupDict2.lookup_desc AS Bed_ASsignment, ClientEpisode.hospital_admit_date AS Admit_Date, ClientEpisode.text2 AS Admit_Time FROM Clients LEFT OUTER JOIN LookupDict ON Clients.referred_source = LookupDict.lookup_id INNER JOIN ClientEpisode ON ClientEpisode.client_id = Clients.client_id LEFT OUTER JOIN LookupDict LookupDict1 ON Clients.dd2 = LookupDict1.lookup_id LEFT OUTER JOIN LookupDict LookupDict2 ON ClientEpisode.dd1 = LookupDict2.lookup_id LEFT OUTER JOIN LookupDict LookupDict3 On Clients.dd8 = LookupDict3.lookup_id INNER JOIN TeamClient On TeamClient.client_id = Clients.client_id INNER JOIN TeamClient TeamClient1 On TeamClient1.client_id = Clients.client_id WHERE ClientEpisode.text2 IS NOT NULL AND ClientEpisode.episode_status = 'ACTIVE' AND Clients.deleted = 0 AND TeamClient1.team_id = @param1 Order By ClientEpisode.hospital_admit_date, ClientEpisode.text2 
post was deleted... what did it say?
Honestly, I would do a quick search online and you will find tons of tutorials on SSMS. If you prefer formal training, Microsoft has a ton of classes you could take. Alternatively, you can have the ruby framework take care of the ORM for you - it just depends on how fancy you want to get. 
&gt; Alternatively, you can have the ruby framework take care of the ORM for you - it just depends on how fancy you want to get. seriously??? That almost sounds like it'd be simpler to maintain, no?
Thanks for the help! Follow up question: So this whole thing is to find out who is coming in each week to receive a gift. They can only receive a gift once during a two month period, June-July; Aug-Sept, Oct-Nov. If a guest comes in June, gets a gift, and then comes again in July they wouldnt receive the gift, however if they come in August, they would receive another gift, and so on. It is hard for me to figure this out because this is the first week I have to run this. But what I thought of doing is dumping the weekly arrivals into one table that has all the arrivals. So next month for example, I run it the first week of July and dump the results into the table with June arrivals, we will call it AllArrivals. I would then run this to get the weekly arrivals. But how can I remove those who already had a reservation during the time period this is ran? SELECT playerid,lastname,firstname,arrival, departure,roomlocation,roomrank FROM AllArrivals WHERE arrival between getdate()-1 and getdate()+6 Would the best way to do this be: SELECT playerid,lastname,firstname,arrival,departure,roomrank FROM AllArrivals WHERE arrival between getdate()-1 and getdate()+6 AND PlayerID not in (select playerid from AllArrivals group by playerid having count(playerid)&gt;'1' ) and then August 1 delete the table and start again? I am hoping to automate this completely so I dont have to remember doing it weekly so is there a way I can do something like CASE WHEN COUNT(playerid)&gt;'1' and DATEPART(mm,arrival) in ('06','07') and DATEPART(mm,getdate()) in ('06','07') then remove them from results or when count(playerid)&gt;'1' and datepart(mm,arrival) in ('07','08') and datepart(mm,getdate)) in ('07','08') etc...
Lazy fix: SELECT a.IP_Address, a.Caption, a.Customer FROM SolarWindsOrion.dbo.Nodes a *LEFT* JOIN ConfigMgmt.dbo.Nodes b ON a.IP_Address=b.AgentIP WHERE a.SLA = 'Standard' AND *b.AgentIP IS NULL*
So thats a running total in the total_downloads column? That was probably the root of the confusion.
Yes, that's a running total. 
Thank you sir, this is exactly what I needed.
No you'll be likely better off than most people applying based on their description. Access is pretty easy point and click. Worst case if there's anything you need to learn, google becomes your best friend. 
The left join is the right answer, here, but you should know how `not exists` works. select a.IP_Address, a.Caption, a.Customer from SolarWindsOrion.dbo.Nodes a where a.SLA = 'Standard' and not exists ( select 1 from ConfigMgmt.dbo.Nodes b where b.AgentIP = a.IP_Address ) What you can see here is that it's doing a subquery, and if you were to look at the query plan, you'd see it does something called a "left (or right) semi-join". Since you're checking *existence*, and not values, you can just bring back a column with a constant value (`1`) instead of trying to read values from the databases. This way, if you have indices on the columns used in the `where` clause, but don't contain other columns, you'll still be able to take advantage of them. It's certainly useful in some cases, and I use it rather often. But, I just thought I'd help with your original question so you could learn a bit extra.
Access is a garbage interface builder for databases. Go learn some VBasic and you'll be fine.
I've figured it out. While I haven't been able to get exactly what I described above, I did this: SELECT product_id, min(date_created_at), max(date_created_at), min(total_downloads), max(total_downloads) FROM table_march_2013 GROUP BY product_id HAVING count(*)&gt;1 and sum(total_downloads)/count(*)&gt;10000; Then I can just pop it into a spreadsheet and manipulate it further to get what I needed. That very last part is just to get the irrelevant items out of the way. 
You can [learn SQL](http://www.microsoft.com/learning/en/us/sql-training.aspx) using Microsoft resources as well. The training paths are broken out by level of expertise and is a good starting point for finding the best resources to learn SQL server. Also, if you get serious about it and want to get certified in SQL, by using the official training materials it is easier to pass the certification exam.
That is glorious. I've only taken a basic database design and management course and joins totally baffle me. Thanks for sharing!
Calling Poe's law on this one.
I'd have to agree - every time I try to build an honest-to-god SQL query in Access, I have to re-write it using the arcane JET SQL syntax that Access uses instead of ANSI SQL. Multi-user support sucks in Access as well as record locking - I've yet to find a way to use just a row lock when updating a record. That said... I've moved all my projects to SQL Server, so it's not a big deal anymore :)
thank you.
Are you still studying 2-3 hours a day? If so, you're going to learn a lot. You can learn quite a bit if you can find good examples of similar problems online. Access might not be ideal but if that's what you're working with, then that's what you're working with.
Thanks! I really need to take an SQL class, I was just kinda tossed into the fire. I'll dig into this more.
Literally, story of my life. You'll be fine. Just be aware of your deadlines and budget extra time for tough stuff
[Fake it 'til you make it](https://medium.com/this-happened-to-me/8f381aa6bd5e)
You'll learn quick doing it everyday. Don't be ashamed of having to google how to do things in SQL every now and then. I've been doing it 5-6 hrs a day for a couple years and still have to google periodically. Find some good sample code for reports and things that already exist in your department and work off those
Access will get a lot of hate in this sub. And much of that hate will be misguided. Nothing beats looking at a really good sample database filled with the answers to questions. Here's a really good sample database to get you started: http://www.filedropper.com/qrysmp00 EDIT: Bonus! Here's a second one I really like... http://www.filedropper.com/accessdataanalysis1
I use the query builder to frame out the SQL, edit it in Notepad++ and copy it back into Access.
It's entirely likely that the people interviewing you didn't bust out a SQL window because people who know SQL well enough to quiz someone else on it are generally not hiring managers. If they said they were looking for SQL people and you know SQL to at least a mildly self sufficient degree you will be fine. It is likely a more valuable talent to them, I'd encourage learning at least a little about access but odds are you just use it to make reports sometimes. Keep in mind a lot of this is from my own experiences, no promises for you.
Honestly, if you're using mostly Access and you really want to wow them, learn some VB.
you will be fine, SQL is a well blazed path at this point and you will be able to google most of your problems. If you are going the route of databases though, remember to ALWAYS make a backup before deleting data !
The sql in access is a bit different to mssql but i have no idea why... 
Or inserting or modifying a table in anyway... 
SQL itself isn't that much of a big deal. 1-2 weeks of exercise will get you accustomed to the syntax and capabilities. Writing optimized queries is a whole different matter and it can take a long time to learn. It is important to understand how the DBMS works, what's fast and what's not and, most important, why. I would suggest using a "proper" DBMS: MSSQL Express, Postgres, MySQL etc. They are all free/OSS.
The SQL Language isn't particularly difficult to understand, once you get the basics you can learn from there. I'll be honest, I have exaggerated my skill set a little in the past and its okay to do this if you have the confidence to give something a go but its sink or swim haha, with IT motivation is half the battle, if you are willing to learn and show aptitude and you are keen to learn that is 75% of the battle. Congratulations on getting your new job. 
From clause: start with the tables you want to join ON clause Define the matching columns in each table you wish to join Select clause Which columns do you wish to return where clause which rows do you wish to return 
&gt;Where Group not 70 **or** not 14 The reason you get 70, is because it's not 14 The reason you get 14, is because it's not 70. Have you tried **and** instead of **or**? Ninja EDIT - alternatively : GROUP NOT IN (14,70)
Use parentheses when using more than just an OR. Consider 'where fruit is not apple or not pear'. It would return both apple and pear because both conditions are met. BFG_9000's alternative works as well, and it's prettier.
My guess is that this is probably a job at a smallish business that cannot afford to have an actual dev staff and DBA on staff (since those things cost an arm and a leg these days). As long as you are a hardworking person who is willing to put in extra hours to learn stuff and aren't an unpleasant person to work with, your lack of technical knowledge will generally be forgiven. Just don't do anything risky once people start using whatever you end up building for them. If you break production data, then you should expect to be fired. A little common sense goes a long way with stuff like this.
The rows and columns you want as the result set. 
Well, you do get all groups which are not 70 **and** 14 at the same time. [Mindfuck ...](http://en.wikipedia.org/wiki/Negation#Distributivity)
select client number, group from table A where group not in (70,14) and ReportDate between .... 
Great work! I have been trying to work on similar with the Schemaverse (see the tutorial) but your interface is a fair bit cleaner and you go into a lot more detail regarding the actual how-to of SQL. I assume this is mysql based? Are you sanitizing inputs at all? Do you run into issues with long running queries or other malicious use?
2 completely different database engines with 2 different flavors of sql http://db-engines.com/en/ranking
&gt; Great work! Thanks! &gt; I assume this is mysql based? Actually, it's based on Microsoft SQL Server, although the lessons so far are fairly dialect-agnostic. &gt; Are you sanitizing inputs at all? Do you run into issues with long running queries or other malicious use? We're using the permissions in SQL Server to make sure the user can't do anything malicious, and we also timeout long queries. It's early days yet, so we might need to do more on this front, but it's worked for us so far.
You are close, but not outputting the salary: Select Salary, Count(*) From Employee Group By Salary Order by Salary Desc
This [article](https://en.wikipedia.org/wiki/Microsoft_Jet_Database_Engine#History) may help to shed some light on the subject. Seems like a mix of supporting legacy applications/users and technical reasons. Particularly this quote: &gt;From a data access technology standpoint, Jet is considered a deprecated technology by Microsoft, but it is an intrinsic part of Windows, and therefore Microsoft continues to support it Luckily, these days it is fairly easy to convert (upsize) an access file/database to SQL Server though.
 SELECT basic , total , 100.0 * basic / total AS percent FROM ( SELECT ( SELECT COUNT(*) FROM Nodes) AS total , ( SELECT COUNT(*) FROM Nodes WHERE sla = 'Basic') AS basic ) AS v 
Looks like the following fixes it. &gt;, 100.0 * basic / total AS 'Percent'
okay, name it sumpin else ;o)
Well, yuh. 
Givable is an abstract class. I'm thinking this is a bad design. Looking at it, I could merge the nav properties into the giveable table (make it non abstract). Then use an int field (with an enum backing) to determine the type. Right now, I do have code that uses or checks for Giveable, then I use the is operator to check the type or use OfType&lt;&gt;. So I'm thinking that would eliminate type checking and opt for a lighter weight if statement. EF Generated the tables: http://i.imgur.com/Xqy3b0l.jpg 
 SELECT id , COUNT(CASE WHEN product='A' THEN 'humpty' END) AS a_count , COUNT(CASE WHEN product='B' THEN 'dumpty' END) AS b_count FROM daTable GROUP BY id 
Not very technical but a great question to end with. Based on today's interview, what concerns do you have with me as a future employee that I can address now? Also stress you understand the importance of a deadline and understand that you know if a deadline is in danger the best thing to do is bring it up ASAP. Good luck.
Hey! Just wanted to drop you a line and say I really like it so far. It's obviously in it's infancy, but it does several things very well which other SQL sites do not. I'm a big fan of not having to load a new page every time I want to try out a query, and the way you've listed the table contents, and provided several examples of how to write each query, as well as dynamically providing more examples per lesson and building on previous examples is great! I'm really looking forward to seeing some new additions, with hopefully some more technical query tutorials / lessons in the future. : d
Aaah, of couse! Thank you
I don't believe there is anything different, the only change I can see off-hand is that you might have to add a ; to the end of the query. 
You'll probably also want a table for item_combination with the following columns: Item1_id, item1_qty, item2_id, item2_qty, result_item_id, result_item_qty
You can run a Windows VM and install [Microsoft SQL Express](http://www.microsoft.com/en-us/sqlserver/editions/2012-editions/express.aspx) for free if you're interested in Microsoft SQL server.
there was a thread here on this subreddit just yesterday... [learn some sql](http://www.learnsomesql.com/lesson/simple-selects) there's also [sqlzoo](http://sqlzoo.net)
MySQL is free and open-source. Here is a link that includes info on sample datasets: http://www.mysqlperformanceblog.com/2011/02/01/sample-datasets-for-benchmarking-and-testing/
ooo me likey
i'm actually going through the sql and sql server courses on lynda.com 7 day trial, gonna get through as much as i can :-) even at 38 bux a month, i learn best off of videos anyway. sqlzoo has also been really helpful for me. 2 weeks ago, i couldn't even write a select statement
If you're broke and desperate, I've seen those linda videos in the cargo holds of some pirates on a bay.
In the production environment!! (That's a joke) Personally I read up on the general internals of rdbms, both the physical and the logical parts of storage were great ways of getting me into the "mindset". Then my new job just threw me into the deep end and F1 and google were my best friends whilst I was getting to grips with the ANSI SQL and T-SQL. But what worked for me doesn't work for everyone. I was also blessed with being very interested in the data I was working with. I'm pretty sure I wouldn't be where I am at now if all the data was about fish.
Thank you!
SELECT \`value1\`, \`value2\` FROM \`table_name1\` AS \`tn1\` INNER JOIN \`table_name2\` AS \`tn2\` ON \`tn1\`.\`id_value\` = \`tn2\`.\`key_value\` INNER JOIN \`table_name3\` AS \`tn3\` ON \`tn3\`.\`key_value\` = \`tn1\`.\`id_value\` WHERE \`tn2\`.\`value4\` = literal_value That should work. Without seeing literally what your query is, I can't help any further than giving you a working sql query. More to the point, when you ask for help, please include: * The exact statement that causes the error. Not a translation, the exact statement. * What you are trying to do, in plain english. * Solutions you have tried but failed at. You do not seem to encapsulate your fields and table names. You should do so, always. I don't know what engine you use, but MySQL likes \` and MSSQL likes [ and ]. Also, avoid one-line SQL statements. Every syntax keyword (SELECT, FROM, WHERE) should get it's own line. Much more readable and easier to spot the bug. Edit: Ok, you're doing it in Access. What DB type does Access connect to? I would assume MS-SQL or Oracle, but assumption is the mother of all bugs. Try executing the query you are trying in the engine itself, instead of Access. Once you get it working there, put it in Access. In the query I gave, replace \` with the respective [ and ]
Check out http://sqlfiddle.com/
But the Windows license to run a VM isn't free, so it kind of defeats the purpose...
If you can, find other people's code. I came into SQL completely blind, but seeing how other people did things really helped me. If I didn't know a command, I'd just Google it! If you don't have anyone that's physically around and working on SQL, jump onto the Microsoft forums and see what questions/answers other people are giving. And yeah, you'll probably want to get adventureworks -- or whatever that giant Microsoft database is called. It's often used as a reference. http://social.technet.microsoft.com/Forums/en-US/transactsql/threads
My brain's winding down, but isn't this a star schema with subscription in the center? I'd think it would be fine.
Giveable would be at the center, since Subscription derives from it. Well I decided to make some changes to it. http://i.imgur.com/uFrsU6U.jpg DLC and Game and treated are combined into one object called app on Steam's CDR. The only real difference between game and dlc was dlc could have a parent app.
I was planning on doing this part in javascript, but I'll think about it
[Coursera has a class on databases](https://www.coursera.org/course/db)
I'm taking this course online right now to brush up on some of the stuff I learned years ago and forgot, and also see what's changed in the world of databases and data management. I think it's pretty good.
As is postgres which is a better implementation tbh
You dont need a vm, you can install it locally.
I don't think codecademy has SQL courses. They could try W3 schools for a quick primer instead.
OP would still need to purchase a Windows license to run the free SQL Express (OP is using a MacBook...).
Ah ok. Well you gotta do what you gotta do
Hey Mods, any chance this could be added to the sidebar? I feel spammy always posting it :P
There's a lot going on here and while you've given specifics, it's really hard to tell exactly how to help. I'd be interested in seeing an execution plan on some problem queries. A few observations... 1. We have no idea of how your NetApp is connected to your database. It could be next to it or across in a different datacenter. This matters because... 2. You must be hitting the disk like crazy. Especially TempDB. Latency is going to matter on something like this. Even if it's hitting your NetApp cache it's still got to go through the controller, over ethernet/other connection and back again. We've had good success putting our TempDB on a local SSD. Don't know if this is possible for you but it helps. I had bad problems with VMware and IO performance. I had to go back to a real install of Windows on the hardware. As always, more RAM doesn't hurt. 
The NetApp is connected to our ESX servers in the same datacenter via three 1GB NICS in an LACP configuration for each controller. The vmdks are all on the same controller, which manages the SAS 15K disks (the other controller manages the SATA disks). Your comments about tempdb are interesting and something we've seen. We're debating filling up our 6 empty slots in our DS4243 with SSD and doing flash pools, but it's not cheap. Let me try to get our DB developer to give me some sample queries and any more information that could help out. I appreciate the response. Thanks.
I don't know the wait messages on the problematic queries. I'll have our DB developer give me as much info. as he can and I'll pass it along. CPU utilization is at max on all 8 virtual CPU (physically 2 socket, quad core). I SnapMirror this volume once every 8 hours to our DR site. We don't use SnapMananger or cloning. I don't put this SQL server in any resource pool since our CPU at the organization, as a whole, is very minimal. But when these queries run, it'll eat up most of the CPU on one ESX host and that's when VMware's DRS kicks in.
It really comes down to what you are doing. Pull a server report of "top 10 queries by CPU avg time". That will help you identify your "big queries". I assume your Max Degree of Parallelism is probably set to 0 if not 4. If your threshold of parallelism is set real low with your MAXDOP set to 0 or 4 this can be a big problem. If you have 196 million rows and you are hitting 96% caching on your netapp device on those disks, it doesn't sound like your performance monitoring software is correct. I have a netapp device, 246 million rows in our largest table. No cloning. 60 gb's of ram. 8 cores. 10 drives in Raid 10 (I believe). I/O is our biggest issue. I just started at this job 2 weeks ago. I went from having a SAN that could sequentially read at 260 mb/s and a NAS that could read/write at 600 mb/s a sec to a NetApp enviroment that reads at 10-75 mb/s a sec. I try not to get involved in the SAN/NAS/DAS side of things because I just couldn't give a shit. I prefer to show my guys benchmarks and have them work it out with the vendor we bought it from.
Hi Krainz, The challenge here is that we may not know all your MS Access terminology and tools. You might get a better response on Stack Overflow. **The big picture** The first step is to create a View query (in SQL we call it a SELECT statement) that contains exactly the records you would like to insert - basically all the columns from your source data, limited to only the 106 rows you want to copy, with a new "Projeto" value. Your end goal is to make an Add query (in SQL we call it an INSERT statement), because you are adding records. **In the end** If you look at the SQL pane for your Add query, it should look something like this (please excuse my non-MS Access syntax here): INSERT INTO Tarefas ([Titulo de Tarefa],[Projeto],[Ordem] &lt;&lt;etc... etc... &gt;&gt;) SELECT t.[Titulo de Tarefa], '&lt;&lt;your new project name goes here&gt;&gt;' as Projeto, t.Ordem, &lt;&lt;etc... etc... &gt;&gt; FROM Tarefas t WHERE t.Projeto = 'Modelo de Empreendimento' Does that make some sense? The WHERE clause is there to limit the statement to only insert the 106 rows one time (and not one time for every other project you've ever created). **A brief warning** I can't tell whether you are copying the name of the Projeto on each row, despite having a table for Projeto. Usually it is important to create relationships between tables so that your Tarefas table would only contain an integer identifier for that table ("AutoNumber" in MS Access). Maybe you've already done that but MS Access is showing us the Projeto name because it is smart? Maybe it's not that important in your case? Best luck and regards, /u/zbignew
Sql bible. 1600 pages. Intense, a long read but teaches you a lot. 
Do you want to copy them OR do you want to change the "projecto" value OR both? You can Update the projecto value, and keep the records as is if you want to. Which leads to the second question, what are you using to change the projecto value? ie: what criteria? By Project? Or all of them need to be changed to the same value?
I want to do both. It is like all there records are steps within the development of the project, and they are all the same for all projects, so I need all 15 projects to have 106 equal separate records associated to them.
If it's the theory you're after (which i assume it is due to you not having any specific DB's in mind) then I'd just use wikipedia as a start point. http://en.wikipedia.org/wiki/Stored_procedures While the theory behind them is the same, the implementation will differ between languages.
As mentioned already memory could be the issue here, but that is (usually) a bandaid to a real problem. Some queries you can try that have helped me in the past; pulled from [here](http://thomaslarock.com/2012/05/are-you-using-the-right-sql-server-performance-metrics/) and [here](http://blog.sqlauthority.com/2009/01/02/sql-server-2008-2005-find-longest-running-query-tsql/). **Buffer Pool** SELECT (1.0*cntr_value/128) / (SELECT 1.0*cntr_value FROM sys.dm_os_performance_counters WHERE object_name like '%Buffer Manager%' AND lower(counter_name) = 'Page life expectancy') AS [BufferPoolRate] FROM sys.dm_os_performance_counters WHERE object_name like '%Buffer Manager%' AND counter_name = 'total pages' &gt; You are looking for a value less than DATABASE_MEMORY_IN_MB / 3600 (plus a little), for example with 32GB(32768/3600 = 9.1), so you'd be looking for around 10-12. If it is significantly larger, then it is a good indicator you are having memory pressure. **Signal Wait Percentage** SELECT (100.0 * SUM(signal_wait_time_ms)/SUM (wait_time_ms)) AS [SignalWaitPct] FROM sys.dm_os_wait_stats WHERE wait_type NOT IN ( 'CLR_SEMAPHORE', 'LAZYWRITER_SLEEP', 'RESOURCE_QUEUE', 'SLEEP_TASK', 'SLEEP_SYSTEMTASK', 'SQLTRACE_BUFFER_FLUSH', 'WAITFOR', 'LOGMGR_QUEUE', 'CHECKPOINT_QUEUE', 'REQUEST_FOR_DEADLOCK_SEARCH', 'XE_TIMER_EVENT', 'BROKER_TO_FLUSH', 'BROKER_TASK_STOP', 'CLR_MANUAL_EVENT', 'CLR_AUTO_EVENT', 'DISPATCHER_QUEUE_SEMAPHORE', 'FT_IFTS_SCHEDULER_IDLE_WAIT', 'XE_DISPATCHER_WAIT', 'XE_DISPATCHER_JOIN', 'BROKER_EVENTHANDLER', 'TRACEWRITE', 'FT_IFTSHC_MUTEX', 'SQLTRACE_INCREMENTAL_FLUSH_SLEEP', 'BROKER_RECEIVE_WAITFOR', 'ONDEMAND_TASK_QUEUE', 'DBMIRROR_EVENTS_QUEUE', 'DBMIRRORING_CMD', 'BROKER_TRANSMITTER', 'SQLTRACE_WAIT_ENTRIES', 'SLEEP_BPOOL_FLUSH', 'SQLTRACE_LOCK') AND wait_time_ms &lt;&gt; 0 &gt; The number should be less than 20-25%; if its not there is a decent chance that there is one following happening. * You have too many users / sessions connected to the database at one time(rarely able to be changed) * There are long running queries that are blocking the CPU time (usually able to be fixed, see query below) **Finding Long Running Queries** *(This can take a while to run, it is also useful without the above)* SELECT DISTINCT TOP 10 t.TEXT QueryName, s.execution_count AS ExecutionCount, s.max_elapsed_time AS MaxElapsedTime, ISNULL(s.total_elapsed_time / s.execution_count, 0) AS AvgElapsedTime, s.creation_time AS LogCreatedOn, ISNULL(s.execution_count / DATEDIFF(s, s.creation_time, GETDATE()), 0) AS FrequencyPerSec FROM sys.dm_exec_query_stats s CROSS APPLY sys.dm_exec_sql_text( s.sql_handle ) t ORDER BY s.max_elapsed_time DESC GO &gt; This should give you the top 10[increase if you want more] most time consuming queries on your server along with their definitions. The times are in microseconds, divide by a 1000000 to get to seconds. You should be looking for... * 1) High execution count with a medium-to-high, AvgElapsedTime greater than a few seconds. * 2) Extremely long running queries, AvgElapsedTime greater than 10 minutes * 3) Extremely large execution count. &gt; The queries that meet these criteria **might** be candidates for optimization, it's not always required or possible. * If they are able to be refactored to reduce the amount of sub-queries / unnecessary joins / etc. * Check to see if queries under have proper indexes, found this fixes most problems; sometimes a single missing index can make orders of magnitudes of difference * If possible, reduce the amount of math or calculations unnecessarily running on the database where they could be running application server side. **Miscellaneous** * Make sure Re-indexing is performed once a week, once a month at minimum * Re-organize Index tasks should run nightly. * If you are using transaction logs / full recovery make sure that they are being written to a separate drive/pipeline then the primary database files, always make sure they are backed up. 
Thinking about it, is the Maximum Memory set correctly in the Server Properties? This might be partially explained if the database was attempting to use the system pagefile space over and above the RAM. It should usually be set to a few GB less than the OS Max, so something around 28-29GB for your situation.
Oh, man I really don't get what you are trying to do.. I think zbignew followed you better, and provided some good instructions and warnings to think about. Using what I've already done above, once you've modified the projecto column in your new table, you can insert the records back into the original table. This is actually nice, because it limits your updates to a temp table; zbignew's solution is good too, as it doesn't touch the original records. 
Check out Lightswitch with VS 2012 - it's pretty slick Access replacement so far.
This sounds simple on paper but it might be a little daunting to query if you're not used to it. -- Assumes table/xml is as described by ziptime above (tier_value, c_percent) -- ((0, 0.05), (100001, 0.10), (150001, 0.12), (200000, 0.15)) etc SELECT TOP 1 totals*c.c_percent FROM commision_table AS c INNER JOIN (SELECT @totalsales AS totals) AS t ON t.totals &lt; c.tier_value ORDER BY c.tier_value
Understood. But for simple departmental/CRUD applications, I think it will do a decent job. I'm just starting to play with it, but I can already tell you that I would MUCH rather inherit a HTML5 client built on C# that some VBA/Access shite.
I understand this applies to all sales associates, so I don't think you need to put the CommisionsLogic column on the associates table. From what I've seen in the other side of this, which relates to pricing, the modifiers are kept on a separate structure and they use qualifiers to determine to whom do they apply, i.e. max items per client, combined promotions, client specific discounts, etc. Even if that is way too much for what you need now, I would recommend to create another table with the lower-upper-discount columns so as of now you only have to maintain as many rows as layers. Now, the timeframe for the sales, assuming is monthly would require the calculation you mention, each end of month accumulating sales per representative and determine his commission for the next month based on current layers. If no need to maintain history, just keep that as a column on the associates names current_commission. If you need to keep record of historical commissions, then another table to keep start-end date and commission to be paid.
Well for one implementing a source code control tool will definately help you. http://msdn.microsoft.com/en-us/library/ms173550%28v=sql.105%29.aspx Not exactly for documenting but tracking changes and organizing them. There are some half decent free/cheap ones out there. As a sole DBA I use a product called SourceGear Vault, it's free for one user, or relatively cheap for more than 1. 
I would put comments in the top of your scripts and use a search utility like grep to find what you need.
Good code is it's own documentation
You'll probably have (let's say) 100 sales people. Those people will probably leverage (let's say) 10 different commission scales. In normalization theory, you'd want to minimize the repetitive data (thing like not storing Country with every record, rather storing country in a country table, and using the ID in addresses, this minimizes bad spelling {since you have to lookup country from a known list} and ensures some consistency) With your data, it's all about managability. Will you (or the next guy) be able to keep solid track of the fact that Bill is on a 100k/10,150K/12,200K/15 scale while Bart is on a 100K/8,125k/10,150k/13,175k/15,200/18 scale. (I intentionally made a bit of a mess with that to demonstrate how easy it is lose track, even at a 2-unit scale) What I'd suggest is "named" scales, if that works with your commissions. If there's say 3 base scales, you can have a scaleName column in the commisions table, and then a matching scaleName in the SalesAssociate table. You might at first glance think scaleID would be more logical, but...in this case, 100/10,150,12/200/15 are all steps in the SAME scale, so they'd need to share SOMETHING so that linking could still happen. Then you can have a second scale with all of its unique steps. This approach still lets you add a step very easily, and let's you adjust the association with an Associate by updating a single column in a single record (pretty good normalization)
Definitely just add something similar to the following to the top of each one and inline notes where applicable. /* -- ================================================ -- Script Name: The title of the script -- Description: A reasonably detailed description -- -- Parameters In: Anything for stored procedure parameters -- Parameter Out: What it returns if anything -- -- Databases Effected: List databases that have updates, selects, etc -- Tables Effected: List the tables and how they are effected, etc -- -- Additional Notes: Anything extra someone might need to know about this -- script that hasn't been covered above, index, procedure changes, etc -- -- ================================================ -- ================================================ -- Author: Name of the Author -- Creation Date: When it was created -- Requested By: Not required, who requested this script -- ================================================ */
Heh, everyone trying to give help to you is being downvoted for it.... Anyhow, I'm not an SQL guru, and I'm too lazy to learn the SQL for this. I'd take these 106 records, copy/paste them into Excel, Find/Replace the data you want (for all 20 projects) and then copy/paste them back into the table. Would probably take less than 5 minutes this way.
Sorry, I'm not the one downvoting. An even easier way for this kind of solution would be with macros.
As mentioned, that's how I saw it handled in a big ERP. As you mention then, you could do something like: qualifier code, qualifier value,lower limit, upper limit, commission_pct where the qualifier would allow to determine the criteria, i.e. territory,high_volume,0,75000,.1 territory,low_volume,0,100000,.075 this would allow to have even more criteria later that can be added up, say, they made an arrangement with a sales rep to give hi additional 1% due to whatever reason sales_rep,12345,0,99999999999,.01 When selection his pct he would be picked twice, once per his territory and once by his id, and he would effectively have the additional pct he was awarded. 
Are these just scripts in a folder somewhere or stored procedures?
[RedGate SQL Doc](http://www.red-gate.com/products/sql-development/sql-doc/) This is a nice tool, that RedGate provides for SQL Server. You could search for an alternative for whatever your needs are that are of a free variety, or have your company pay the fee. 
They are all in a folder, and sub folders within.
The only issue with this is, that there are literally hundreds of .sql files scattered across a folder and while this would definitely help, I would still have the issue of going through many different files to find what I'm looking for
If that is the case, document all them and use a REGEX to search/pull the top comment out of each one with a filename for documentation.
You need to save the code in the gray box as ex1.sql 
~~I ran into the same issue. Its been a while, so i may have the specifics wrong here. Enter the code into notepad±±, or just plain notepad. Save as ex1.db. Then, run sqlite3, ex1.db &gt; ex1.sql.~~ ~~I emailed the author about my confusion, but never received a response.~~ **EDIT: Im mistaken. x0dyssey and follier have it right.** 
Make sure you are running sqlite3 from the command prompt.. use notepad++ to write out the SQL statements. Save the file as ex1.sql in same directory as your sqlite3 executable. Then run the sqlite3 ex1.db &lt; ex1.sql command in the command prompt from the same directory path as the sqlite3 executable; this will create the database file with the one table name ex1.db in the specified directory. You can also add -echo to view the output from sqlite3 while its creating the table. Sqlite3 -echo ex1.db &lt; ex1.sql 
Alternative solution using pivot: http://sqlfiddle.com/#!6/4e426/14 WITH pivotdata as ( select ID,Product,1 as theCount FROM daTable ) SELECT ID,piv.A,piv.B FROM pivotdata PIVOT(COUNT(theCount) for Product in (A,B)) as piv
Thanks for the reply. That's what I ended up doing. It just kind of a pain because the table has 147 columns, is around 1570 characters per row in the file and is fixed width, no delimiters. &gt; Edit -What is this table used for? It seems somewhat odd that a column would arbitrarily be added. It's an "initial load" type table. Taking in raw data from a customer and getting it all into a central spot where it can be worked with. I couldn't tell you why they added that field in the way they did though.
So, I hope this helps. In general on *nix commandline: y &gt; x means "do y, and write results into x" y &lt; x means "execute y with file x" Non-sql example: $ echo "Hello world" &gt; x.txt There is now a file called "x.txt" that contains only "Hello world." $ wc -w &lt; x.txt 2 The command "wc -w" (counts words in a file) is being fed the file "x.txt, and is giving the result. It can get a bit more complicated. $ wc -w &lt; x.txt &gt; y.txt Now you've created a file called y.txt that just contains "2". Anyway, that's all they are doing with this: `sqlite3 ex1.db &lt; ex1.sql` Feeding a file called ex1.sql into the command `sqlite3 ex1.db` More on that command: http://www.manpagez.com/man/1/sqlite3/ `sqlite3 ex1.db` means "execute sqlite3 using database found at ex1.db" In this example, since that database doesn't exist, it automatically creates it using the create table statement you stored in ex1.sql. 
Yeah, its been a while. I'm glad someone put in the right info. 
A specific system like Oracle or SQL Server? Or like a database computer science class covering theory?
I work specifically with MSSQLServer 2008 R2 right now and I have a hunch that knowledge specific to that DBMS is a little more valuable to me. I plan on checking out MIT's free online Database Theory class eventually though.
&gt;Nothing I can do, gotta work with what I got! HA!
There is a guy here on /r/sql who made this game: http://schemaverse.com/ I made a course on [memrise](http://www.memrise.com/course/88361/sql-4) that will teach you the (most frequently used) functions and arguments.
www.w3schools.com is pretty decent for basics. 
My best recommendations besides reading some of the ground-up tutorials is... start reading SQL Blogs and twitter Feeds from SQL Server MVPs and the like. There is a good chance some of the harder problems you will come across have already been addressed to some extent by them. One of my favorites: [http://thomaslarock.com/](http://thomaslarock.com/) Don't take everything really serious either, it's surprising what you can learn by what others make fun of. Example: [DBA Reactions](http://dbareactions.tumblr.com/); if I don't understand one of the mocking style posts I would go an look it up. It's frightening how often I found something wrong just from a quip sentence and a gif.
You could do a nested query select columns from (select..blah blah blah) Or if this is something you will need to do repetitively, create a view for query 1 and just use the view in your second query
Sorry I don't think I clarified well enough. Query 1 always returns multiple values. If I do the nested SELECT it would essentially be doing a WHERE columnA = (SELECT blahblahblah) the nested select is muliple results so it won't work. I essentially want to do this but have it go WHERE columnA = (SELECT result1 from query1) OR columnA = (SELECT result2 from query1) etc.
I think I clarified a bit better to AnonymooseRedditor. I don't think I was explaining it correctly. Each result from query1 has to be used in query 2 to generate another set of results.
Do you want to find multiple values? Because you can use a HAVING count([column])&gt;1 in that case. &gt;I then want to do a query taking the results of 1 of the columns of that table and run another query for each result of the 10 - 20 lines. Could you post those?
Oh in that case what you can do is select result1 from query1 where result1.columna in (select resuilt2.columna from query...)
SELECT * FROM tableA WHERE columnC in (select ColumnA from TableA) If you have to search for a part of the string in ColumnA, you have to use the [contains()](http://msdn.microsoft.com/en-us/library/ms187787.aspx) function.
The way you are phrasing the question in two stages, is prompting people to give subqueries as the answer. But its not the best solution, and there is no reason why you cannot use a JOIN (that I can think of anyway, its been a while since I used MSSQL)... SELECT * from TableB INNER JOIN TableA on (ColumnC LIKE '%'+ColumnA+'%') Or you may need to build the search term like this... SELECT * from TableB INNER JOIN TableA on (ColumnC LIKE CONCAT('%', ColumnA, '%')) Performance will be relatively poor on large datasets (table scans), but this may not be a concern for your database. If it is, then its probably time to rethink your schema design, or look into some full text indexing. 
hmm... it actually seems like no one is getting what I want to do. I need to run multiple queries using the various results from query1. ie. SELECT * FROM tableb WHERE columnc LIKE '%234%' SELECT * FROM tableb WHERE columnc like '%53%' SELCT * FROM tableb WHERE columnc LIKE '%362%' etc. That last value '%XXXXXX%' is going to vary depending on how many results I get from the first query. ie. SELECT columnz FROM tableA WHERE columny LIKE '%teststring%' I get a return of 10 different values which are as follows - columnz 234 53 362 345 7642 etc. So I want to run the same query X times but on each time I want it to use the next value for columnz that it got from the initial query.
- This doesn't require a sub-query then; albeit it can be done with one. - For example: SELECT columnz FROM tableA AS a INNER JOIN tableB AS b ON a.columny = b.columnc WHERE b.columnc IN ('234', '53', '362', '7642', 'etc') &gt; As a side note, don't use **LIKE** with string numbers, especially not with wildcards. You will end up with more rows then you expect. If columnc had a value of '753' it would be included under your **WHERE columnc like '%53%'**, which is probably not intended. - I'm not entirely convinced you are taking the correct approach to the problem. This looks to be an attempt for a linear solution to a set theory question. - &gt; Edit2 - Each row returned for columnz can be used, with a sub-query suggested above. SELECT 'Do something with ' + columnz + ' here.' FROM (SELECT columnz FROM tableA AS a INNER JOIN tableB AS b ON a.columny = b.columnc WHERE b.columnc IN ('234', '53', '362', '7642', 'etc')) AS z 
Should do what you want, there are other ways to do it but this is the easier to understand. &gt;- UPDATE tableA SET a.column_to_update = c.update_to_value FROM tableA AS a INNER JOIN (SELECT max(change_date) AS mTimestamp, customer_id FROM tableB GROUP BY customer_id WHERE change_type='etc' AND customer_id = 55555) AS b ON a.customer_id = b.customer_id INNER JOIN tableB AS c ON b.customer_id = c.customer_id AND b.mTimestamp = c.change_date &gt;- **Edit** - This could easily be added to a stored procedure, the only different is that change_type = @changetype and customer_id = @customer_id **Edit 2** - As dcolley99 mentioned, this can be done simpler, mine is just broken up to allow you to step through each part to understand what it going on. 
Sounds like you just need a SELECT MAX(changetime) WHERE @a=customernumber AND @b=customertype. You can put this inside a parameterized stored procedure to pass in a and b. Alternatively use a CTE so preselect the data based on the predicates then select the max from that. Put a nonclustered index on changetime covering number and type for better performance. Then pass in the resulting value into your update clause via a temp var. Alternatively for a set of customers select the customer number, type and max changetime group by max changetime, export into table variable andfeed into your update clause for a set based approach. 
Nope still not getting it, and in both your examples you're assuming I already know what the results are for Query1. I want Query 2 to process itself including the results of Query1, not have me manually input x numbers of lines. Yes, I need to use a like because I'm actually searching for x number (about 20) string values inside another string value. so what I'm doing is query 1 is searching for the column_name of all rows that have a column_value string of '%testingtesting123%' I then want to do a new query I want to get all results of the Table where column2_value contains a string of the results of column_name (which at this point is approx. 20 distinct names. Edit - and I'm not saying it requires a subquery that's just the way I'm assuming I'd have to do it. Otherwise can I dump the results of query1 to a temp table and then tell query 2 to run based on a counting integer where it can use the value from row1 in the first query, row2 in the second query, row3 in the third query, etc. and then just UNION them all at the end?
When trying to use LIKE and parameters you will have to use a prepared statement. You can either create you statement string with something like: stmt = ... + "WHERE columnc like '%" + @results + "%'" LOOPING HERE OVER CURSOR ON QUERY1 stmt = stmt + " OR columnc like ' % " + @results + "%'" END LOOPING PREPARE stmt EXECUTE stmt Or you can do your UNION's in a prepared (or multiple) prepared statements. I'm pretty sure neither of these will perform well.
So why isnt my original suggestion working for you? SELECT * from TableB INNER JOIN TableA on (ColumnC LIKE CONCAT('%', ColumnA, '%'))
check **HAVING** too... 
Because there's more than 1 result from the first query. The first query returns approx 20 results. You can't tell the query to use LIKE columnA when columnA means 20 different things. wait a min - I think I'm getting the logic here (*finally*) I don't think I noticed the inner join before. so basically I'm selecting from query 2 but in the WHERE it's pre-joining the data so it's already limiting the options with the 2 string searches? Let me try it out.
I think this might be getting at what I need to do. So looping will proceed to the next row in the original query results?
It won't work. You need a prepared dynamic query to do LIKE predicates with parameters.
Yes. Unfortunately I don't know cursor syntax very well, or I would have it done by now.
perfec tthnx. I'll investigate designing a loop. The biggest problem I find when you're not an SQL guru is figuring out how to design your questions into google effectively. If I knew how to properly explain what I wanted to do I probably could have just googled the answer :)
Could try using a CTE: WITH MaleEmployeesCTE AS ( SELECT E.ID AS EmployeeID FROM Employees AS E WHERE E.Gender = 'M' ) SELECT * FROM Profile AS P JOIN MaleEmployeeCTE AS MEC ON MEC.EmployeeID = P.EmployeeID WHERE P.FirstName LIKE 'JO%'
I second this. Having is a frequently overlooked and a great method to address issues like this, when one needs to do post-group filtering.
*Lite* beer.....
You got your peanut butter in my chocolate! You got your chocolate in my peanutbutter!
Some marketing person thought this was ingenious. 
That was delicious 
 SELECT a.Author , t.[Year Published] , t.Title FROM ( Authors AS a INNER JOIN [Title Author] AS ta ON ta.[Au_ID] = a.[Au_ID] ) INNER JOIN Titles AS t ON t.[ISBN] = ta.[ISBN] WHERE t.[Year Published] = ( SELECT MIN(Titles.[Year Published]) FROM [Title Author] INNER JOIN Titles ON Titles.[ISBN] = [Title Author].[ISBN] WHERE [Title Author].[Au_ID] = Authors.[Au_ID] )
"Hey, thank you for the help but i cannot get it to work as i need it to, if i just use the entry you posted and run it then it asks me for Authors.Au_ID while i need it to display all the authors / First year of publishing by said author / books published within that year. I tried tweaking it but sadly my SQL skills are subpar."
oh, crap, i made a booboo in the last line of the query, change `Authors.[Au_ID]` to `t.[Au_ID]`
"changed it to a.[Au_ID] and that worked like i needed it to" "Thanks for the help - This is exactly what i needed! ( changed Authors. [Au_ID] to a.[Au_ID] )" Anyway thanks for helping him. I'll go pretend I know what all this means. :D
aaargh, a booboo on top of a booboo thanks for covering for me yes, it was supposed to be `a.[AU-ID]`
Hi there, I'm doing the 1Z0-051 SQL Fundamentels 1 exam on monday. Do you mind telling me where you got the actual practice exams? Or perhaps you still have them and don't mind sharing them? :)
 diffdays + ':' + (diffhours % 24) + ':' + (diffminutes % 60) The mod will just give you the remainder.
http://www.reddit.com/r/SQLServer/comments/1g3p02/very_old_ashtontatemicrosoft_sql_server_commercial/ Twice in one week? 
oh... oh my god... that was the worst thing ever...
You didn't say if you want this to run auto-magically whenever an change is made to the change log. If you do and the change log is a table, you could put a trigger on the change log table. A trigger will make sure your customer table is kept up to date with the most current data from the change log. Basically the trigger will look at each insert or update into the change log table and update the customers table. MSDN: http://msdn.microsoft.com/en-us/library/ms189799.aspx sqlteam.com: http://www.sqlteam.com/article/an-introduction-to-triggers-part-i
SQLServerCentral.com has some good articles called "sql server stairways": http://www.sqlservercentral.com/stairway/
I'm pulling the information over through ODBC via MS Query in Excel. The column names are not in any of the rows, just the column name itself. When I want to bring over data, I just specify the fields that I want to bring over and the actual SQL code is written by MS Query. I get the column headers at the top and the data records below it. Excel recognizes the column names as the header. Is there a better way to be importing the data into Excel?
I haven't used MS Query in Excel more than once and that was a long time ago. I believe there is a setting in your ODBC setup that should say: "suppress column headings" or "include column headings" I would try to change that and see if you get better datatype recognition. As for better ways of importing into Excel ... I would suggest that depending on what your needs are Excel might not be the best tool for the job. If you are looking at analyzing data, summarizing data or creating reports; there are many reporting options available that can use JDBC connections and SQL to help. Most require some (or lots of) SQL knowledge to get the data.
You might ask in /r/sqlserver. I use BIRT where I can javascript the DataSet to update the SQL text by inserting the " IN ('a','b') " before it executes. Perhaps SSRS has a similar feature that avoids a TVF? 
Remove the group by run_val1. You should only group by name.
just as an FYI this is what I was doing before but couldn't figure out how to join the SELECTS - SELECT owner_name, COUNT(owner_name) count_adhoc, jobrun_adhoc FROM jobrun JOIN jobmst ON (jobrun.jobmst_id = jobmst.jobmst_id) JOIN owner ON (jobmst.jobmst_owner = owner.owner_id) WHERE 1 = 1 AND jobmst.jobmst_type &gt; 1 AND jobrun_proddt = '2013-06-01' and jobrun_adhoc = 'Y' GROUP BY jobrun_adhoc, owner_name SELECT owner_name, COUNT(owner_name) count_sched, jobrun_adhoc FROM jobrun JOIN jobmst ON (jobrun.jobmst_id = jobmst.jobmst_id) JOIN owner ON (jobmst.jobmst_owner = owner.owner_id) WHERE 1 = 1 AND jobmst.jobmst_type &gt; 1 AND jobrun_proddt = '2013-06-01' and jobrun_adhoc = 'N' GROUP BY jobrun_adhoc, owner_name ORDER BY owner_name
k. problem still stands though, it's returning 2 values for frank. I want any instances where a name shows up twice to merge that into 1 row. frank 2 4 instead of - frank 2 0 frank 0 4
That can't happen. If you have select name, aggregate(), aggregate() from table group by name you only get one row for each name.
I thought Excel might be easier, but you just save as CSV (which you already have) then [do this](http://blog.tjitjing.com/index.php/2008/02/import-excel-data-into-mysql-in-5-easy.html) LOAD DATA LOCAL INFILE ‘C:\\temp\\yourfile.csv’ INTO TABLE database.table FIELDS TERMINATED BY ‘;’ ENCLOSED BY ‘”‘ LINES TERMINATED BY ‘\r\n’ (field1, field2);
I'll give it a go tonight and get back to you. Thanks so much for the assistance!
you probably need to adjust the terminated by to a comma and your strings may not be enclosed in double quotes. Look at the csv in notepad to be sure. The biggest problem I've seen with these CSV imports is non-enclosed strings that have commas. "ABC, Inc." destroys a CSV if it doesn't have double quotes.
I just created and ran this and it works just fine for me. What parameters were you passing it that were causing it to error?
Try replacing the COUNT with SUM SELECT first_name, SUM( CASE WHEN run_val1 = 'N' THEN 1 END) runno, SUM(CASE WHEN run_val1 = 'Y' THEN 1 END) runyes FROM run FULL JOIN mst ON (run.mst_id = mst.mst_id) FULL JOIN owner ON (mst.mst_owner = owner.owner_id) WHERE run_date = '2013-06-01' AND run.mst_type &gt; 1 GROUP BY first_name ORDER BY first_name If your query is more complex, because of data not shown here, then you can always wrap your first query in a CTE and do the sum over that query to aggregate your data.
where did you get BULK INSERT from? try using the LOAD DATA INFILE statement instead
&gt; Try replacing the COUNT with SUM that produces exactly the same result
I ran this on a single table that has a boolean field, using your count case method, and it worked just fine. One row per item. Trim the code down, and return mst_id, runno, runyes. Then add the join to mst and return mst_owner. Then join the owner and return first_name. Find out at which join you're getting duplication, and why. First_name may not be the best group by either, since there might be two franks at some point. Try SELECT owner_id, first_name, runno, runyes GROUP BY owner_id, first_name ORDER BY first_name
This is essentially what I'm already doing and it's not working. AGGREGATE isn't a function I'm using a variation of an aggregate which is COUNT(DISTINCT) http://msdn.microsoft.com/en-us/library/ms173454.aspx
I could kiss you! We have a winner! thank you :)
noted. I'll get around to learning them more. I know the last training course I had explained them as venn diagrams I'll see if I can pull them up again.
Never-mind! Figured it out. WHERE EventTime &gt;= cast(getdate()-7 as date)
This is exactly what I told you to do. Changing Count to Sum is irrelevant (though it might give you a better number). He removed the second grouping level. That's why you only get one instance of each name.
Thanks for the heads up. The regional database system I am running is MySQL, unless I'm misunderstanding things or thinking too naively. What version of SQL does this RDBMS use?
it uses the mysql version more precisely, you can run this query to find out which one -- SELECT VERSION()
You ensure there's a single identity column on the table. Create a job that runs on every midnights and saves the MAX(identity) into a different table keyed on a tableID, date, identity. Then you make a function which translates dates and a tableID into a clustered identity value and scan on those date ranges using the cluster itself. Or do the easy thing and just seek on the EventTime using DATEADD. Preferably with a supporting index. ;)
I'm pulling data from a database I have read only access to. 
~~Your datediff doesn't specify the datepart.~~ Also I restricted it to the user's logins. You may want to undo that. SELECT userid, DATEDIFF( ( SELECT MAX(userdate) FROM logindata WHERE userdate &lt; t.userdate AND userid = t.userid ), userdate ) AS days_since_last FROM logindata AS t ORDER BY userid EDIT: Nevermind, this is MySQL. Try the additional filter and change the group by to order by and see if it helps.
Thank you, but it still gives the same error. order by helps in getting the results in the format I need. It runs well for a table with 1000 test rows but with 90,000 rows, it just breaks.
What are you going to do with those 90k rows? If you can do the next level of aggregating in the SQL or filter out the values you don't care about you'll see better performance.
SQL Server Management Studio doesnt work with mysql.
this is a sql file, you want to use SOURCE myfile.sql not load data..
Duh. My bad.
I have learned that today! You have been very patient with me. Thank you. I am now taking the SOURCE myfile.dump approach. I am trying to source a 3gig dump file. However, my command line cursor is just hanging on a new line. I opened up a new instance of MySQL and ran the SHOW PROCESSLIST statement. My database has SLEEP under the command window. Either it's taking forever to read the query, or something is up.
No worries. I started with SSMS until I learned it doesn't work MySQL. It's a process :/
I'm not sure what the issue was. After leaving it alone for a while it started working.
Are you using SQL server? If so, you can use the NTILE(10) function and PARTITION BY the groups you mention above ORDER BY the value. You'd then either use a subquery / CTE to get the min value for the 9th tile. 
I think you'll have to make a subquery for each year or get some fancy variable going or just union everything together (copy paste is easier imo): select TOP 10 PERCENT year, city, Min(waittime) from #waittime2 where year = 2013 group by year, city order by waittime desc UNION select TOP 10 PERCENT year, city, Min(waittime) from #waittime2 where year = 2012 group by year, city order by waittime desc ect
Thanks, I can do that easily. Will try tomorrow.
Thanks, will give this a try too.
Not sure if this will help but what I would do is probably use a cross join and sub query and filter by the months something like this. Cross Join (Select sum(numdays) as totaldays from tableC where month in ('march','April','may')
Yes.
These are two different things... Excel will seem funny, if you learn SQL.
Yes. I have no clue about excel ( compared to others) pivot tables, lookup tables.. no idea how to handle them in excel
Excel as in spreadsheets? SQL is very different, like entirely different. You'll have no problems picking up SQL regardless of what you do/do not currently know. Optimising it is another thing though I guess. 
Thinking it over, think you may need a full outer join, which Access doesn't natively do. Description in fix here. http://www.databasejournal.com/features/msaccess/article.php/3516561/Implementing-the-Equivalent-of-a-FULL-OUTER-JOIN-in-Microsoft-Access.htm
or just don't join to anything and do it as a subselect. It prevents duplicating the SUM from table A if there are multiple revenue deposits in a month. select A.custmerID ,SUM(A.Balance) /((select sum(B.revenue) from B where A.CustomerID = B.CustomerID and B.month IN ('JAN', 'FEB', 'MAR')) / (Select sum(C.numdays) as totaldays from C where C.month in ('JAN', 'FEB', 'MAR') ) from A where month(A.invoiceDate) in (1,2,3) group by A.customerID 
Relational SQL is basically [Venn Diagrams](http://en.wikipedia.org/wiki/Venn_diagram). Excel is a calculator that can remember all 20+ steps you take to get an answer. They don't have anything to do with each other except that they are both necessary for some business processes. You need the SQL to get the correct data and none of the unnecessary stuff. Then you load it into Excel and do additional calculations to see how your business is performing and how it could be better.
Pivot tables are...well pivot tables and lookup are joins, but at a record by record level. Using a pivot table in excel requires a large list of data, then it's pretty much drag and drop. To do a lookup or join, just put in =vlookup( and let the intellisense guide you. That all being said, I'd rather write some code than do either of these...
here's a similar question: can you learn internal surgery without knowing the art of makeup
That said, I work for a company that treats SQL like server based, enterprise Excel. :(
You can do this with either a CASE statement and remove the LabourType from the GROUP BY. Try doing this for each of the selects you have in parentheses: CASE WHEN LabourType = 1 THEN ActivityStart ELSE NULL END as 'Travel Start', 
I get the feeling we are missing a bunch of requirements and your test case is too simple. If not, use badEVIL's suggestion. Why are you grouping if there are no aggregate functions? Do your agents usually only have one record of each labourType for each activity? 
What badEVIL said, this is something like what you want: SELECT dbo.Personnel.ShortName AS Engineer, dbo.ActivityPersonnelCosts.ActivityStart, dbo.ActivityPersonnelCosts.ActivityEnd, dbo.ActivityPersonnelCosts.LabourType, dbo.ActivityPersonnelCosts.ActivityID, CASE WHEN LabourType = 1 THEN ActivityStart ELSE NULL END AS 'Travel Start', CASE WHEN LabourType = 0 THEN ActivityStart ELSE NULL END AS 'On-site', CASE WHEN LabourType = 0 THEN ActivityEnd ELSE NULL END AS 'Off-site', CASE WHEN LabourType = 2 THEN ActivityEnd ELSE NULL END AS 'Travel End' FROM dbo.ActivityPersonnelCosts INNER JOIN dbo.Personnel ON dbo.ActivityPersonnelCosts.PersonnelID = dbo.Personnel.UID WHERE ActivityID = 28387 GROUP BY dbo.Personnel.ShortName 
yup... sql server [LAG](http://msdn.microsoft.com/en-us/library/hh231256.aspx) and [LEAD](http://msdn.microsoft.com/en-us/library/hh213125.aspx)
It sounds like your not all the way set up on the data base end. You need your database to exist somewere. The most common way to start out would be to host the sql server locally put your data into it then connect to said server (the server is not visual studio or sql manager those programs hook into the host server.) The program normally gets installed with sql lite for beginners. I'm on a phone or I would attempt to provide links. Hope this helps
You need this program to make and hold your databases, and then link them into your program in Visual studio. http://msdn.microsoft.com/en-us/library/ms174173.aspx That is the best way to make databases. You don't make your databases in VS you make them outside and then operate on them in VS. Mind yo I think you can make simple DBs in visual studio, its just a more primitive system.
This. It comes along with the SQL Server setup, or you can install the Express version here: https://www.microsoft.com/betaexperience/pd/SQLEXPMS/enus/ 
Perfectly normal, when you apply the diffs or diffs then tlogs you will use with recovery on the last step to bring it back to life
One of my colleagues throws Excel/csv files into a small table in a personal DB and uses SQL to query/manipulate the data. It's kind of funny actually.
Great, thank you!
There's the undcumented [sp_msforeachdb](http://weblogs.sqlteam.com/joew/archive/2008/08/27/60700.aspx) but I don't know how robust it is or how well it works. I've not used it myself.
Reading your question again I realize that you're trying to search across tables, not across databases. My apologies! 
No worries thanks anyway
This query will generate a query that will find the defined value in all the columns of the database that are of a particular data type. After you run this query, you can copy and paste the first column of the query into a new query window. You'll have to remove the last 'UNION ALL' from the statement in order for it to work. You might have to remove some lines from the query if you don't have permission to query some of the tables in the database. I had that problem when I tested it. I've included a filter to remove tables generated by Microsoft as well as a filter to restrict it to user tables (non-default tables). You could probably tweak it to look in a particular schema, but I didn't feel like putting that much work into it. ;) It's a little resource intensive, but if you use it sparingly it should work fine. Hope you like it. DECLARE @searchvalue VARCHAR(30) = 'value' select 'SELECT [' + c.name + '], ''' + o.name + '.' + c.name + ''' FROM [' + o.name + '] WHERE [' + c.name + '] = ''' + @searchvalue + ''' UNION ALL ', o.name, c.name, o.* from sys.all_objects o inner join sys.all_columns c on o.object_id = c.object_id inner join sys.types t on c.system_type_id = t.system_type_id where t.name in ('varchar', 'nvarchar') AND O.is_ms_shipped = 0 -- Ignore system tables AND O.type = 'U' -- Ignore views and such order by c.name
When trying to run this query I am getting the following error message: Msg 139, Level 15, State 1, Line 0 Cannot assign a default value to a local variable. Msg 137, Level 15, State 2, Line 3 Must declare the scalar variable "@searchvalue". any ideas?
if you have a specific column, then you can use sp_msforeachtable. you'll want to query schema to see if that table has column, or you'll get a bunch of errors, but at the same time you can just ignore them and view results tab for all the ones that have that column in management studio 
Here's something I quickly wrote. It's messy, but seems to work in my limited testing. Warning, painful on a large db. declare @s nvarchar(max), @tbl nvarchar(500),@objid int, @schema nvarchar(500) declare s cursor for select name, object_id, schema_name(schema_id) from sys.tables open s fetch s into @tbl, @objid,@schema while @@FETCH_STATUS=0 begin SELECT @s='declare @columns nvarchar(max), @sql nvarchar(max) select @sql=''DECLARE @value nvarchar(255) select @value=''''INSERT VALUE HERE'''' SELECT '''''+@schema+'.'+@tbl+''''' as tbl, * FROM '+quotename(@schema)+'.'+quotename(@tbl)+' WHERE '' select @columns=coalesce(@columns+'' LIKE ''''%''''+@value+''''%'''' OR '','''')+''CAST ('' + QUOTENAME(name) +'' AS nvarchar(255))'' from sys.columns where object_id='+cast(@objid as nvarchar(25))+' select @sql=@sql+@columns + '' LIKE ''''%''''+@value+''''%'''''' print @sql exec sp_executesql @sql' exec sp_executesql @s fetch s into @tbl, @objid,@schema end close s deallocate s 
Did you get the DECLARE part of the statement when you copied it? It sounds like you're missing the variable declaration.
Might be a db version issue. If pre-MSSQL 2008 you will have to do DECLARE then a SET or SELECT to set the variable.
Yes, it starts with DECLARE @searchvalue VARCHAR(30) = 'value' select No results, just the message posted above. Sorry
Hello This ended up giving me all of the available tables, and their list of columns, on the "Results" tab. Then on the "Messages" tab it gives me something like the following: *(0 row(s) affected) DECLARE @value nvarchar(255) select @value=*'THIS IS THE VALUE I INSERTED' *SELECT* 'SCHEMA.TABLE NAME HERE' *as tbl, * FROM [dbo].[HRApplicationSource] WHERE CAST ([ID] AS nvarchar(255)) LIKE '%'+@value+'%' OR CAST ([AppSourceDescription] AS nvarchar(255)) LIKE '%'+@value+'%' OR CAST ([EffectiveDate] AS nvarchar(255)) LIKE '%'+@value+'%' OR CAST ([ExpirationDate] AS nvarchar(255)) LIKE '%'+@value+'%'* Is there anyway to filter these results to only show where the *(0 row(s) affected)* is &gt;= 1 row? Thanks!
My bad, I should have pointed out. I have SQL Server 2005.
Also, finally got this message: *The query has exceeded the maximum number of result sets that can be displayed in the results grid. Only the first 100 result sets are displayed in the grid.*
This Worked for me USE DATABASE NAME DECLARE @myCursor AS CURSOR, @table as varchar(100), @schema as varchar(10), @column as varchar(100), @sql as nvarchar(max), @value as varchar(max) --SET THE VALUE HERE SET @value = 'value' DECLARE @results TABLE(table_name nvarchar(100),schema_namee nvarchar(10),column_name nvarchar(100)) SET @myCursor = CURSOR FOR SELECT t.name As table_name, Schema_name(schema_id) AS Schema_name, c.name AS column_name FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID WHERE c.name LIKE '%creator%' ORDER BY schema_name, table_name OPEN @myCursor FETCH NEXT FROM @myCursor INTO @table, @schema, @column WHILE (@@fetch_status = 0) BEGIN SET @sql = 'SELECT TOP 1 * FROM '+'['+@schema+'].['+@table+'] WHERE ['+@column+'] LIKE '''+@value+'''' EXEC(@sql) IF(@@ROWCOUNT &gt; 0) BEGIN INSERT INTO @results VALUES (@table,@schema,@column) END FETCH NEXT FROM @myCursor INTO @table, @schema, @column END SELECT * FROM @results 
Also if any one knows how to increase the performance would be really help full, and another question how do I avoid showing all the selects from the EXEC (@SQL) 
Don't think you can do DECLARE @searchvalue VARCHAR(30) = 'value' Try DECLARE @searchvalue VARCHAR(30) set @searchvalue = 'value'
It sounds like you want to download data into your computer, and typically you use some scripting language (Python, C++, etc) to do that. Once you have the data, integrate it into your database (import/export, ETL tools), and then you can use SQL to manage the database. As an economics student, I suppose you use statistical analysis softwares (R, SAP, Stata, etc) to create your models, run regressions, etc. SAP uses T-SQL, which is the same version of SQL that MS SQL Server uses, so you might want to use that approach instead. Also, what is your project about?
Perhaps you are talking about a SQL Server Agent job? SQL Server Agent allows you to create jobs that run on a schedule, and you can run multiple steps with a great deal of control over how the job executes the steps. To create a new SQL Server Agent job, connect to the instance of SQL Sever in SSMS, expand the SQL Agent node, and then right-click on the Jobs node to add a new job. Existing jobs can be edited. One of the tasks within a job will permit you to run a T-SQL script, and you can paste the script within that task and specify the database it should run under. If this isn't what you had in mind, please clarify so we can help. Best of luck to you!
I am more into T-SQL myself, but why not just swap the dates around to obtain the positive number then? CHECK_in_date - Check_out_date =? -(Check_out_date - CHECK_in_date)
Dates in Oracle (as in most computer systems) are internally represented as a floating point value offset from some date epoch. The integer part of this are days since that epoch, the decimal places represent fraction of whole days. So number of days between two dates (including fractional part) is simply the newest date - the oldest date. The number of whole days is that figure truncated. SELECT pat_id AS "Patron", book_num AS "Book", trunc(check_in_date - check_out_date) AS "Days Kept" FROM checkout
Wow, that was way to easy, that was it. You guys will be hearing more from me, that is a fact! Thanks
I knew this sub would help up votes for all!
Something like this? data: http://research.stlouisfed.org/fred2/data/STLFSI.txt BULK INSERT CSVTest FROM 'http://research.stlouisfed.org/fred2/data/STLFSI.txt' WITH ( FIELDTERMINATOR = '\t', ROWTERMINATOR = '\n' )
Hello This requires you to enter in a column name. Also, where is the value you are searching for entered in this statement? I need a query to run that searches for the value, without the need of the column or table name.
Thank you So when I changed what you mentioned above, the query ran. But can you explain to me what I am looking at. It looks like it has basically given me a list of all columns, all tables. For example, when I replace 'value' with 'Immediate Response' (a value I am search for), there does not seem to be any indicator showing me if it is in one of these tables or not. Unless I am missing something. Thanks again Sephian!
Thank you So when I changed DECLARE @searchvalue VARCHAR(30) = 'value' To DECLARE @searchvalue VARCHAR(30) set @searchvalue = 'value' the query ran. But can you explain to me what I am looking at. It looks like it has basically given me a list of all columns, all tables. For example, when I replace 'value' with 'Immediate Response' (a value I am search for), there does not seem to be any indicator showing me if it is in one of these tables or not. Unless I am missing something. Thanks again!
They will ask some basic technical questions to test your knowledge, nothing too difficult. They might give you a case study and ask how you'd approach dealing with it. You also want an example of a project that didn't go well and how you dealt with it. Script out all of this stuff out ahead of time and practice it in the mirror. Make sure you're smiling during the interview as part of it is also them enjoying your personality.
Grats!!
Seems silly to me that this wasn't implemented years ago but hey, so be it. We still set variables in separate statements for backwards compatibility though. No guarantee our clients are all on the same version :-\ 
MS Access is a low level piece of shit barely useful for personal desktop databases. SQL Server, Oracle, and other Enterprise RDBMSes are designed for bulk data storage with FAST access. If you run a company off Access you are asking to lose all your employee's time and probably also all your data. Also Access uses SQL as a query language (some changes from TSQL but basically the same). So you might be making a confused argument.
[Formatted] CREATE VIEW v_employees AS SELECT cod_emp, name, surname, salary, department_code FROM employees WHERE department_code=30 WITH CHECK OPTION; Which of the following commands is correct? a. INSERT INTO v_employees VALUES (10, ‘Ion’, ‘Daniel’, 1000, 505); b. SELECT * FROM v_employees WHERE salary &gt; 505; c. UPDATE v_employees SET department_code= 505 WHERE department_code = 30;
What does the index you want to use look like?
 I have indexes as follows : KEY `response_id` (`response_id`), KEY `question_id` (`question_id`), KEY `question_id_2` (`question_id`,`response_id`), KEY `choice_id` (`choice_id`) Looking at my query, I would assume it would use the two indexes 'response_id' and/or 'choice_id' - why isn't this the case?
I am not very familiar with mysql syntax. Is this four different indexes? I am in no way an expert on indexing but I would try and add choice_id to question_id_2. When getting information from a table, each look up can as far as I know only use one index. The optimizer might perform an index scan followed by additional look ups for values missing in that index. However if the values are not covered by the index and the amount of rows is deemed too big then it's cheaper to perform a table scan instead. If possible in mysql only on the leaf level since it's not for filtering, in mssql one would use the INCLUDE keyword. Read some more here: http://dba.stackexchange.com/questions/28592/must-an-index-cover-all-selected-columns-for-it-to-be-used-for-order-by
WITH CHECK OPTION contraints modification through the view in such a way that rows can't fall "out of the view". The insert fails because any row with department_code &lt;&gt; 30 or null falls outside of the view The update fails because the row would go from being in the view to not being in the view. Any updated row must still be in the view. A select on a view can't be incorrect unless it tries to get columns that aren't specified in the view. A result can be empty but still correct.
I do most of my stuff in SQL Server, but this should apply to MySQL as well. You could create a view that calculates those aggregates for you. It could include all the values for the Players as well as a couple aggregate columns that are populated from the Tasks and Categories values. The view would then create the 'easy access' that you want without having to maintain stored aggregate values.
We have done something like this before. Set up your table with your default codes having blanks (or zeros) in the customerid field. When you create your link to the drop down use a view (or a select statement) that displays the default choices along with the customers own choices. SELECT clientstatus WHERE customerid = '' UNION SELECT clientstatus WHERE customerid = current customer When the customer wants to create a new status, they will just be entering their custom status and their customerid.
How about allowing all clients access to status's with a client id of zero or their own? Then you can just assign a zero to the default status's. P.S. it made me really uncomfortable typing **status's** - should it be **statii** or maybe **status'**
Thanks a lot that seems like a sensible approach. The only issue is having a nullable foreign key seems like bad practice? Any suggestions?
It looks like mySQL does not have Materialized Views/Materialized Query Tables/Index Views as other DBMS have. I would suggest what /u/CrimsonSmear said with regards to using a view, unless you are seeing performance problems. A quick search showed me someone's work for creating the required trigger and/or stored procedures in mySQL to achieve these links: * https://code.google.com/p/flexviews/ * http://www.fromdual.com/mysql-materialized-views
Thank you so much! I never knew what views were, I guess they really are important.
the only way would be to have a "dummy" system customer with id -1 and you can use -1 for all of your global statuses. The problem with doing something like this is that you have to remember that there is this dummy -1 customer and exclude them from reports and what not.
Thanks! Materialized views seem interesting, but I don't have a huge amount of data, so I think I'll be ok with regular views.
statuses
Hey Chicagoan here as well. You're lucky to find a place that gives reimbursement for your SQL classes. I too looked for a long time around chicago to no avail. I pretty much just taught myself using the books online through amazon and the tutorials there as well. They worked well for me, and as with all things you only get out what you put into it. http://www.amazon.com/Inside-Microsoft%C2%AE-SQL-Server%C2%AE-2008/dp/0735626030/ref=sr_1_9?ie=UTF8&amp;qid=1372191271&amp;sr=8-9&amp;keywords=sql+server+2008 EDIT * - go hawks!
I think what I am going to do is have a nullable FK to clients via clientid. I will then add a bit column called [default]. This will be set to 1 for default statuses and 0 for any additional entries that customers want to create. The SP that will gather the customers statuses will look like: select statusid from clientstatus where default = 1 union select statusid from clientstatus where default = 0 and clientid = @clientid Any other suggestions would be great. Thanks a lot everybody !
Let's take that idea one step further, and don't even add the dummy records to the database... Just have two tables and merge them on the way out like so. Just make sure the autonumber seed value for the clientstatus table starts at like 1000 or something like that, so that you will have unique id's. SELECT status as clientstatus, statusid as clientstatusid FROM status UNION SELECT clientstatus clientstatusid FROM clientstatus WHERE customerid = current customer EDIT: This will make performance a lot better, as you can have a unique index on clientstatusid and customerid, clientstatusid
Skip the 'when matched' part. Just run the 'when not matched' code.
Since you don't want to do anything when the records match, there is no reason to use a merge, just do an insert. The purpose of the merge statement is to combine insert/update/delete operations. Here are two alternatives that will only insert records that are not already in table A: Use an exists clause: INSERT tableA (codeID, docID, codeTypeID) SELECT codeID, docID, codeTypeId FROM tableB b WHERE NOT EXISTS ( SELECT codeID, docID, codeTypeID FROM tableA a WHERE a.codeID = b.codeID and a.docID = b.docID and a.codeTypeID = b.codeTypeID ) Left join: INSERT tableA (codeID, docID, codeTypeID) SELECT codeID, docID, codeTypeId FROM tableB b LEFT JOIN tableA a ON b.codeID = a.codeID and b.docID = a.docID and b.codeTypeID = a.codeTypeID WHERE a.codeID IS NULL 
What version of SQL? T-SQl: where Documentos.Documento like ('Protocol 1', 'Protocol 2', 'Protocol 3') where Documentos.Documento like ('(2) to-do list',' (3) building project') where Loteamentos.Nome like '%project%'
only the third of those three LIKEs will work
 Select @@version for Microsoft SQL SELECT VERSION() for mySQL select * from v$version; for oracle Run them all and find out which one works. Libre Office is just a client. The server version is kind of HUGE if you want SQL help. 
Thanks! I'll give it a try.
Why not just use *LIKE '%protocol%'* or *LIKE '%(%'* for the first two?
I'm connected to a mdb MSAccess database
It's kind of different. 'project' always appears with the P in uppercase, as in 'Project in Las Vegas' (it is the name of the project). However, in protocol there are both occurrences of 'protocol' and 'Protocol'. 
yes, that's fine do a search on **supertype/subtype** and you will see however, the subtype tables should not have their own, different PKs instead of FK_Component, the PK_ID of the subtype table also is FK to PK_ID of the Component table
If it's case sensitive you can do OR between the likes for each of Project / project
He said he wanted three new queries. Each line is a separate addition to his existing SQL.
Do you have everything working, or do you need some more help?
You could try it as where table.user2 like case when @techID = '&lt;ALL&gt;' then '%' else @techID end tad hacky but might get the job done edit: Just realised how old this post was, stumbled on it while I was searching for something. 
It worked, thank you so much.
 Select dateadd(dd, datediff(dd,0, myDateTime), 0) dates , CONVERT(TIME, mydatetime) times , values from mytable Then load it in Excel or your report design tool and make the Pivot/Crosstab. The first column truncates the time for day grouping. The second returns the time as a string for the time grouping.
did hope to do it directly in sql server, but i will try this. thanks!
It's easier this way. In SQL Server(http://msdn.microsoft.com/en-us/library/ms177410(v=sql.105).aspx) you have to know all the columns ahead of time. So it wouldn't be bad for a Mon-Sun set of days, but would suck for a Jan1 - Dec31 set. Here's a [sample](http://stackoverflow.com/questions/11985796/sql-server-pivot-dynamic-columns-no-aggregation) I found of how you can do it with dynamic columns.
thanks. but i fear i need to do some excel magic like THLycanthrope did suggest, since it is more flexible in the moment. i will try your solution as soon as i put it into the program and can build the query dynamically.
nope, no ssrs, but i do need to automate it quite soon and then excel might be no longer an opinion. 
Pretty much all my data set looks like is Customer ID: Unique 8 digit dumber Group- 47 distinct groups, text such as "United" Contract ID- 6 digit number referencing a contract
It's not very clear what you are trying to do, but I'm assuming your boss is suggesting using a cross join query to compare two datasets looking for differences. A cross join is extremely inefficient for finding differences between these two datasets, requiring at worst O( n^2 ) comparisons. So 1000 records in each table could result in 1 million row comparisons! You can look for differences using O(2N) comparison (one table scan of each table) using the following approach, where columns in the table are col1...coln : select col1, col2, col3, ......coln, count(src1) CNT1, count(src2) CNT2 from ( select a.*, 1 src1, cast (null as number) src2 from TABLEA union all select b.*, cast (null as number) src1, 2 src2 from TABLEB ) group by col1, col2, col3, ......coln having count(src1) &lt;&gt; count(src2) So, for example : select CUSTOMER_ID, GROUP_ID, CONTRACT_ID, count(src1) REV_EXISTS, count(src2) HIST_EXISTS from ( select a.*, 1 src1, cast (null as number) src2 from REVENUE a union all select b.*, cast (null as number) src1, 2 src2 from HISTORIC_REVENUE b ) group by CUSTOMER_ID, GROUP_ID, CONTRACT_ID having count(src1) &lt;&gt; count(src2) Will show you differences in the following form... CUSTOMER_ID GROUP_ID CONTRACT_ID REV_EXISTS HIS_EXISTS ------------------------------------------------------------ 2 x 1 0 1 2 xx 1 1 0 3 x 1 1 0 3 x 2 0 1
There is some special character in there, Notepad is correcting it, almost guarantee it. You'd need to look at the hex codes to see what it was.
Like the damn single and double quotes in Outlook that aren't really single or double quotes. I hate helping someone over email and I have to type in Notepad then paste it into my email.
`CASE` is what you are looking for; `CASE` for 0 Also, when asking for help with SQL, don't say 'it looks like'. Give us the actual query you use. Please.
is that all i need in the statement? because im getting a Syntax error missing operator in query expression case when "what you typed"
SELECT [DK Rev Query].[PAYOR#], [DK Rev Query].PAYERNAME, [DK Rev Query].GROUPBY, [DK Rev Query].PAYORTYPE, [DK235 Total AR Query]![SumOfBALANC]/([DK Rev Query]![SumOfNETREVAC]/92) AS DSO FROM [DK Rev Query] INNER JOIN [DK235 Total AR Query] ON [DK Rev Query].[PAYOR#] = [DK235 Total AR Query].RAINSP GROUP BY [DK Rev Query].[PAYOR#], [DK Rev Query].PAYERNAME, [DK Rev Query].GROUPBY, [DK Rev Query].PAYORTYPE, [DK235 Total AR Query]![SumOfBALANC]/([DK Rev Query]![SumOfNETREVAC]/92);
I don't have Access cause ewwwwwww. Seriously, get off of Access if you have any choice about the matter (you probably don't). Anyways, something like this: SELECT [DK Rev Query].[PAYOR#], [DK Rev Query].PAYERNAME, [DK Rev Query].GROUPBY, [DK Rev Query].PAYORTYPE, ( CASE WHEN [DK Rev Query]![SumOfNETREVAC] = 0 THEN 0 ELSE [DK235 Total AR Query]![SumOfBALANC]/([DK Rev Query]![SumOfNETREVAC]/92) ) AS DSO FROM [DK Rev Query] INNER JOIN [DK235 Total AR Query] ON [DK Rev Query].[PAYOR#] = [DK235 Total AR Query].RAINSP GROUP BY [DK Rev Query].[PAYOR#], [DK Rev Query].PAYERNAME, [DK Rev Query].GROUPBY, [DK Rev Query].PAYORTYPE, ( CASE WHEN [DK Rev Query]![SumOfNETREVAC] = 0 THEN 0 ELSE [DK235 Total AR Query]![SumOfBALANC]/([DK Rev Query]![SumOfNETREVAC]/92) ); This is probably not 100%, but is close. Also, my god man, it's called a newline. Use it. Indentation also. And if you put four spaces before every line in a block, you get the code lines you're reading now. Cheers.
this is the simplest and most elegant solution 
That doesn't fix the problem OP is having... They will still get a "division by 0" error.
it should actually produce a NULL result, not an error did you test it? i don't have a copy of sql server here at the moment
Mostly do Oracle myself, but I would expect that to replace null totals with 0 not the other way around. Hence why I would still be expecting the divide by 0 errors. I'm not going to get on the instances at work till tomorrow so I'm in the same boat.
Well, after the "END" you need the FROM clause enumerating your tables and join criteria. The WHERE clause is optional.
I think you're confusing nullif() with isnull(). Nullif compares the arguments and if equal evaluates to null. So in this case if TableB.total is 0, then the expression evaluates to null, resulting in a null final value.
Few things to consider: * Make sure you indexes have all been rebuilt * Update statistics and make sure whatever is running the query has permissions to use it * Indexes can slow large scale operations down if they don't cover the correct columns or the data distribution isn't as expected * LEFT JOINs are inherently time consuming, I would look here for the biggest performance gain. Many times you can remove a few LEFT JOINs (especially from tables of 1.57 billion rows) and it will drastically increase speed. It is also worth attempting to get the smallest data set going into large joins to prevent unnecessary transactions
Thanks for your reply. A couple of follow-ups if you don't mind: * What does it mean to update statistics, and how do I do it? I'm very new to this. In fact, I used this tool for the first time a couple of days ago. * How can I tell if my indexes are covering the correct or incorrect columns? I figured indexing on the join keys would be a safe bet. * The tool should automatically use the indexes that I've built, correct?
no no no... the NULLIF function as written above replaces a zero with NULL (whereas COALESCE and IFNULL work the other way around, replacing NULL with a specified value) NULLIF(*column*,0) means "if *column* equals 0, result is NULL" this means that the division is not divide by zero, which would be an error, but divide by NULL, which produces NULL (not an error)
Post the query and what you're trying to achieve. Look at execution plans for the query too, the query could be inefficiently written.
In a similar vein to coldchaos' reply, how many rows are in your result set? If its not 1.5 billion or so, I'd look at refining the query so you don't waste time retrieving rows that are excluded, either due to the nature of the joins or other conditionals. Only grab the columns you're actually using, on that note. Also a prime suspect for slowness is sorting - in general it's usually not worth sorting anything other than the final result. I'm certain that Mssql has an explain-analyse like feature, if you haven't already used it, it should go a long way towards identifying the problem.
What does it mean? Update statistics sorta re-compiles a query (translate what the code says to what needs to be done). If the data in a table changes characteristics (more of that type of data, the range of these values gets spread out more) the information about how to solve a query gets old. Maybe plan A isn't as good as it used to be any more. Running UPDATE STATISTICS TABLE_X makes sure that table_x will not suffer from old information about how to solve queries. check the estimated query plan for your query. You can show it by [clicking this button](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcROoiUH9oE_Qayz5umOni-_AlcefNcYbD10lVTtH8K61WbqJCAa1g) Is my index covering it? The execution plan shown there is a probable candidate for the plan the query will actually use when executed. There is a button to include the actuall plan aswell up there somwhere. Anywho, data flows in the mark up of your plan from right to left. So on the right you might find one of your big tables. You can see the join operations, filtering, sorting and that sort of stuff. If you put the mousepointer over a table read operation you can see how the operation works. When using an index the index name will show up. If no index can be found by the optimizer that would increase the performance of your query a full table scan will be used to make sure everything returned is within the scope the query defined. Wether index a, b, or c would make the query faster or not is an estimate from the optimizer so it might not be 100 percent correct but it's damn good at what it does. So then, short on what indexes can be used for. When reading data from a table, one index is used. If there is no index, the entire table is read. If all the data you need in your query is present in the index, no data is read from the table at all. If all information about WHAT ROWS is present in the index but not all data, the optimizer might opt (hehe) to use the index anyway and perform RID look up to find the missing data for each row. If this operation is deemed to be too expensive VS to just scan the whole table, the optimizer picks the cheapest option (read everything). To avoid RID look up, put the columns you need to look at as either an indexed column or an included column. Index vs include: Indexed columns are present on every level of the index while included columns are just on the leaf level. This means that if a column is used in the where clause or the join-on clause of a query, having it as an included column doesn't help. However if that column is only used in the SELECT part, ie what you actually output it does help. The tool should automatically... What is "the tool"? Did you mean the optimizer? Well only if the indexes actually help. As I described earlier indexes and usage can get pretty technical (but once you get a greater understanding of the fundamentals it get's easy and fun really fast). I know this answer is pretty technical so post your questions and I'll try and answer them.
http://pastebin.com/4DHHxw01
The 3-hour run produced a table with about 7700 rows. I ran the execution plan feature, and that told me that the last drop table was taking up 75% of the time, which didn't look right. I removed that drop table, and now each of the 25 steps says that it takes 4%.
this here? http://www.microsoft.com/en-us/bi/powerpivot.aspx
will look into it, thanks!
Very good explanations to the questions.
I think TOP "is" what you're looking for, along with an ORDER BY SUM(Invoice.Value) to put the records you want at the top. Sorry if I'm totally off base … I'm in a very boring training session at work right now.
Here is the section of the query that is said to be running slow that has been formatted a little better(few small changes on my part also). SELECT table2.date AS Date, table7.CName AS CName, table7.Segment AS Segment , COUNT(DISTINCT table0.CustomerKey) AS NumDelivered , COUNT(DISTINCT table1.CustomerKey) AS NumSent, COUNT(DISTINCT table5.CustomerKey) AS NumRedeemed, COUNT(table2.transactiON_id) AS TotalTransactiONs, SUM(table2.DC) AS DC, SUM(revenue) AS TotalNetRevenue, COUNT(DISTINCT ( CASE when table2.Type= 3 THEN table2.transaction_id ELSE NULL END ) ) AS NumONline, COUNT(DISTINCT ( [CASE statement checking One column] ) ) AS G , COUNT(DISTINCT ( [CASE statement checking One column] ) ) AS BR , COUNT(DISTINCT ( [CASE statement checking Ont column] ) ) AS PromoRed , COUNT( ( CASE when table2.promo IS NULL THEN NULL ELSE table2.transaction_id END ) ) AS Redeeming, SUM(ISNULL(table2.promo , 0)) AS PromoRevenue, COUNT(DISTINCT table4.CustomerKey) AS NumUniqueOpen , COUNT(DISTINCT table3.CustomerKey) AS NumUniqueClick, SUM(table4.EOCount) AS NumTotalOpen , SUM(table3.ECCount) AS NumTotalClick, COUNT(DISTINCT table2.transactiON_id) AS NumRedemptiONs INTO CustomReportingAggregate1 FROM TCF_30 table2 LEFT OUTER JOIN FEBSE_Sub table0 ON table2.CustomerKey = table0.CustomerKey LEFT OUTER JOIN FEOE_Sub table4 ON (table0.CustomerKey = table4.CustomerKey AND table0.ECKey = table4.ECKey) LEFT OUTER JOIN FCS_Sub table1 ON table2.CustomerKey = table1.CustomerKey LEFT OUTER JOIN FECE_Sub table3 ON table2.CustomerKey = table3.CustomerKey LEFT OUTER JOIN CM_Sub table5 ON table5.CustomerKey = table2.CustomerKey LEFT OUTER JOIN vDimProduct_Sub1 table6 ON table2.t_id = table6.T_Id LEFT OUTER JOIN DCR_Sub table7 ON table3.CampaignReferenceKey = table7.CampaignReferenceKey GROUP BY table2.date, table7.CName, table7.Seg; GO &gt; It's difficult without knowing the schema or how they relate but here are some things I see: * Aggregate functions might be a symptom to having too many rows, as the entire query has to finish / be stored before calculating them. * From a quick look, it seems like the first couple joins **might** be replaceable with INNER JOINS, specifically anywhere you are performing a DISTINCT but don't require the NULLs from the LEFT JOINS &gt; Also worth a shot, try running the following with all the aggregate functions and the insert removed into and see how long it takes to run. My suspicion is it might only take a few minutes. SELECT table2.date AS Date, table7.CName AS CName, table7.Segment AS Segment , table0.CustomerKey AS NumDelivered , table1.CustomerKey AS NumSent, table5.CustomerKey AS NumRedeemed, table2.transactiON_id AS TotalTransactiONs, table2.DC AS DC, revenue AS TotalNetRevenue, CASE WHEN table2.Type= 3 THEN table2.transactiON_id ELSE NULL END AS NumONline, [CASE statement checking One column] AS G , [CASE statement checking One column] AS BR , [CASE statement checking Ont column] AS PromoRed , CASE WHEN table2.promo IS NULL THEN NULL ELSE table2.transaction_id END AS Redeeming, table2.promo AS PromoRevenue, table4.CustomerKey AS NumUniqueOpen , table3.CustomerKey AS NumUniqueClick, table4.EOCountAS NumTotalOpen , table3.ECCount AS NumTotalClick, table2.transactiON_id AS NumRedemptiONs FROM TCF_30 table2 LEFT OUTER JOIN FEBSE_Sub table0 ON table2.CustomerKey = table0.CustomerKey LEFT OUTER JOIN FEOE_Sub table4 ON (table0.CustomerKey = table4.CustomerKey AND table0.ECKey = table4.ECKey) LEFT OUTER JOIN FCS_Sub table1 ON table2.CustomerKey = table1.CustomerKey LEFT OUTER JOIN FECE_Sub table3 ON table2.CustomerKey = table3.CustomerKey LEFT OUTER JOIN CM_Sub table5 ON table5.CustomerKey = table2.CustomerKey LEFT OUTER JOIN vDimProduct_Sub1 table6 ON table2.t_id = table6.T_Id LEFT OUTER JOIN DCR_Sub table7 ON table3.CampaignReferenceKey = table7.CampaignReferenceKey 
Any reasons why youre learning it first? Just curious.
This is what i came here to say. I might suggest avoiding indexs JUST THIS ONCE as although he should do that it isnt a direct answer to his question and hes obviously new so i think that might be for another day.
Pretty much what Kush1234 said, although if you don't have any unique identifiers triggers won't be easy *** &gt; With Triggers, assuming there is an ID in both tables CREATE TRIGGER master_collection1quantity_push ON Master AFTER INSERT AS BEGIN INSERT INTO collection1(Quantity) SELECT collection1quantity FROM Master AS M LEFT JOIN collection1 AS c On M.cid = c.cid AND m.collection1quantity = c.Quantity WHERE c.cid IS NULL and c.Quantity IS NULL END; GO *** &gt; Create a Stored Procedure (**recommended**) -- Create the procedure, this only has to be done once CREATE PROCEDURE sp_master_collection1_quantity_insert @quantity=0 AS BEGIN -- Other parameters and inserts can be done here INSERT INTO Master (collection1quantity) VALUES (@quantity) INSERT INTO collection1 (Quantity) VALUES (@quantity) END &gt; Calling the stored procedure created above EXEC sp_master_collection1_quantity_insert @quantity=50
Are there any records with 0 miles?
Why do you have the same information in both tables?
Agreed that this isn't best practice, was assuming that OP was maintaining a legacy system or vendor application that do this kind of stuff more often then should be legal.
Gah, I had forgotten about that. Yes, there are. I'm guessing it's a divide by zero problem? Any way to work around that? Could I just do WHERE Invoice.Miles &lt;&gt; 0? edit: Looks like that did the trick! Thanks a lot!
This is my first time doing this so im not really good. The master has collection1, collection2, and collection3 quantites ( 3 separate columns ). I had created 3 different tables for each quantity as well and that's where the problem is. Any advice would be much appreciated.
You will lose the rest of the invoice if you filter them out. You might want to do something like making the distance very small .1 or 1 mile. Sum(Invoice.Transportation)/Sum(IIF(Invoice.Miles=0,0.1,Invoice.Miles)) AS [Total Cost Per Mile]
Can you create collection1, ..., collectionN as views rather than tables? Then they'll auto-update, and you can index them if you need performance out of them.
Wait. I assume that the collection1, 2, and 3 tables all have more than just the one field? Can I ask what purpose you're trying to achieve? It might help us help you figure out a more gooder solution.
Your model is wrong, simple as. You shouldn't be having to synchronise data between these tables. The only time you'd denormalise like this or duplicate date is to address performance reasons (when millions / billions of rows are involved). You should create a view to achieve what you want for collection1, then you only have to insert into Master. 
thanks guy
That may be a solution I'll have to consider. Right now I think I'm going to just filter out the 0 mile invoices. The reason they're listed as 0 miles has to do with the company that compiles this data for us. I've already told my manager about it and it'll be fixed when they finalize the rollout of their new database. I'm basically just making "mock-up" queries to show them how this all works; the actual numbers aren't quite as important right now. Thanks!
Why can't you do it in a union? Edit - If you want to be tricky about if you could left join the main table to two sub-query(one for each) and then concat the columns together from either or with a NULL to empty string in the SELECT with a WHERE condition that subquery1 and subquery2 can't both be NULL. This only works if you have a unique column in the table though.
You can use the DENSE_RANK() Function to rank the rows and then use an inline view to pull what you need without the need for a join or a union. Off the top of my head, but something like this should work: SELECT * FROM (SELECT *, DENSE_RANK() OVER (PARTITION BY STATUS_CODE ORDER BY CLOSE_DATE DESC) RANK FROM T1) DATA WHERE STATUS_CODE = 'O' OR (STATUS_CODE = 'C' AND DATA.RANK &lt;= 10) 
&gt; The following assumes you have a database schema something similar to this TABLE Pictures (Picture_pk int, picture_data, ...) TABLE Picture_Tags(Picture_pk int, tag char(255)) *** &gt; Creating a stored procedure and a temp table that holds your search parameters CREATE TEMPORARY TABLE search_terms( stag char(255) ) ; /* Insert values of the tags here, one per row */ *** &gt; Creating the stored procedure that will perform the search DELIMITER // DROP PROCEDURE IF EXIST perform_search // CREATE PROCEDURE perform_search() READS SQL DATA BEGIN /* Here is where you will define how the search terms interact */ SELECT p.* FROM Pictures AS p INNER JOIN Picture_Tags AS t ON p.Picture_pk = t.Picture_pk LEFT JOIN search_terms AS s ON t.tag = s.stag WHERE s.stag IS NOT NULL END // DELIMITER ; *** &gt; Running the stored procedure CALL perform_search() ; 
You are suffering from not knowing even what things you need to know to address the problem. http://sql.learncodethehardway.org/
Coldchaos' approach would be the simplest approach however as jokle mentioned you will need to have a common column between the two tables. not necessarily named the same but containing the same data within each table. The below link may be of some assistance in explaining the join function you will need. http://www.w3schools.com/sql/sql_join_inner.asp
 $tags = explode( ' ' , $_GET['tags'] ) $sql_string = "SELECT `ID`, `Dir` FROM `TableName` WHERE ("; $first = true; foreach($tags as $_tag) { if(!$first) { $sql_string .= " OR " } else { $first = false; } $sql_string .= " `Dir` LIKE '%".$_tag."%' " } $sql_string .= ") AND 1=1;"; This solution is probably sql injectible if you don't handle it right. Security is far more than I care to explain though; however, this query, given the table you have, will get any row where `Dir` is like any word (as deliniated by space) in the get request named 'tags' (?tags=foobar%20foobaz%20fuckme)
Please note this is a super simplified and filled with so much *cringe* that I don't particularely want to talk about it. You really should have a more advanced DB set up than this; such as a fkey relationship with a tag table.
**Edit** I am dumb, the only difference in this solution from molx is the sort order, i totally ignored the first part of his predicate /u/molx is pretty close in his solution. I have provided a dummy set of data using a with clause, this is just for my testing. row number or dense rank will work the same in this situation as long as you partition by status code. Your predicate needs to be the status you want to include all of(Status_code = 'O') OR &lt; the ranking column(or R &lt;= 10) because if you have more than 10 open items you want to see them all, not just the first 10. Also you can add an arbitrary sorting order to the status columns by adding a decode. in this example I made the status code "O" sort to the top. with t1 as( select DECODE(MOD(level,3), 0,'O','C') STATUS_CODE, sysdate + level t1date, level row_num from dual connect by level &lt;= 30 ) select * from( select t.*, row_number() over(partition by status_code order by t1date desc) r from T1 t ) where (STATUS_CODE = 'O' or R &lt;= 10) order by decode(status_code,'O',1,2)
Curious.. if you run this stored procedure and @quantity has a default value of 0 if it were unintentionally called without a parameter it would just insert zeroes right? I assume it would be ideal to have some logic in the SPROC to exit the procedure in the event of zero or default it to NULL? 
 select t2013.companyname, t2013.employeeID, t2012.compayname, t2012.employeeID from table2013 t2013, table2012 t2012 where t2013.employeeID = t2012.employeeID and t2013.companyname &lt;&gt; t2012.companyname
You should probably ask in /r/sqlserver since this is a DB specific issue. However, my instances are similar and running as local system.
This is actually pretty damn clever. I don't generally think of doing calculations on pure mathematical data in SQL, but it is certainly a powerful tool for doing so it seems!
The "apply" join function might help you here because it is best used when the join on statement isn't simple and is more efficient when joining larger data sets. Basically, you could do your aggregation in the sub-query within the apply statement (see below) then have it reference back to your base data (table indicated after "from"). The two CTEs below simply populate some arbitrary values. I used the union statement to perform both examples in the same query. The option statement just defines a max recursion depth since my CTEs populate data via recursion. ;with cteA (ID, Value) as ( select 1,1 union all select (a.ID + 1) , (a.Value + 20) from cteA a where a.ID &lt; 1000 ), cteB (ID, Value) as ( select 2,2 union all select (b.ID + 2) , (b.Value + 50) from cteB b where b.ID &lt; 1000 ) select * from cteA a outer apply ( select b.ID, sum(b.Value) SummedVal, 'FirstApplyGroup' GroupID --join all matches with aggregate from cteB b where b.ID = a.ID --your join on statement and 1=1 --any other filter statements group by b.ID ) c union all select * from cteA a outer apply ( select top 1 b.ID, b.Value, 'SecondApplyGroup' GroupID --join top match based on order by statement from cteB b where b.ID = a.ID --your join on statement and 1=1 --any other filter statements order by b.Value desc --get latest/largest value ) c option (maxrecursion 1000) There is an excellent article about the apply command and its performance [here](http://sqlblog.com/blogs/rob_farley/archive/2011/04/13/the-power-of-t-sql-s-apply-operator.aspx).
That is correct, this was meant to be an guideline not a complete solution. -- Create the procedure, this only has to be done once CREATE PROCEDURE sp_master_collection1_quantity_insert @quantity=NULL IF @SalesPerson IS NULL BEGIN --Print or raise error here RETURN END
Well, I suppose I could use a union. I just thought there had to be a more elegant solution out there. Union kind of feels like brute force to me. And, I need the solution to be very fast. The query will be run as a sub-report that is ran in a monthly job that creates invoices for customers. The overall job is a memory hog and is rife with issues. Every change to sub-queries poses a risk of knocking over the whole house of cards. I am not allowed to devote time to rewriting or improving the house of cards as a whole. But, every time a task is assigned that impacts part of it, I try to improve performance a bit, be more elegant in design, and hope for the best. I've now got a couple of different queries to test and see which performs best. Thanks for your time.
Ah, good advice. Thanks.
I'm not entirely convinced that union will be any less efficient. As long as the status = 'C' query is executed first it will only have to retain 10 rows. Partitioning might be more expensive as it will have to maintain a count for all 'C' status rows. **Edit** - I don't have an Oracle system to test it on, but on a 1.6 million row SQL Server table the I/O was pretty close but the partition query took seventy times the CPU utilization. Not sure if Oracle will be the same */shrug*.
So, I've seen the connect by level thing and the over partition thing before but I've never actually used them myself. I have googled these some in the past and I just don't quite get what is happening. Can you explain both of these a little more? I understand that the connect by level gives you 30 test payments but not how it does it. The over partition thing I understand even less. thanks! BTW, both your solution and /u/molx had the same cost on explain plans. Yours was way better at first but I had to add the payment date to the sort and that brought it down even with /u/molx. 
**CONNECT BY** So because dual only has one records we can abuse the connect by clause to force that record to keep connecting to itself until it reaches an nth generation of that record. by using: select 'test' data from dual connect by level &lt;= 3 we are telling oracle to build a structure like this: 1 'test' data --level 1 +-2 'test' data --level 2 +-3 'test' data --level 3 the "data" associated to the root and children is same because it is the same record. try this same example with a table with two records and see what happens? use this example: with two_recs as( select 1 data from dual union select 2 from dual ) select data, level from two_recs connect by level &lt;= 2 What do you expect to see? 2 records? 4 records? Answer at bottom. **ANALYTIC FUNCTIONS** So analytic functions are processed after the result set is retrieved this means to actually work with the result of an analytic function we have to wrap it in another query to restrict the rows based on the result of the function. Function(arg1,..., argn) OVER ( [PARTITION BY &lt;...&gt;] [ORDER BY &lt;....&gt;] [&lt;window_clause&gt;] ) OVER Whenever you see the *OVER* keyword after a function you know you are dealing with an analytic function. This tells the function to be applied after the result set is retrieved and is further controlled by its other options. PARTITION BY Think of partition by as a group by for the analytic function. In the examples we gave you earlier the *PARTITION BY status_code* is saying "get the rank/row_number for these rows retrieved but rank the different status codes in their own groups" that way status code "C" and "O" will have their own set of ranks. You might be asking now, how does the function know how to issue which record which rank? well in comes the *ORDER BY* clause ORDER BY order by is used to determine in what order the analytic function is applied. so when we say *ORDER BY t1date desc* it finds the record with the most recent date and processes it first, in this case it will result in a rank/row_num of 1. So the next step is why the outer query? Well in order to use the result of your analytic function we have to wrap it in another select so that we can have our predicate utilize the result from the analytic function. Remember i said earlier that analytic functions apply after the rows are retrieved so it is impossible to actually window a query based on an analytic function at the same level the analytic function is resolved. To test your skills, how would you get a cummulative outstanding invoice total from this dataset? 1. order from oldest to newest 2. do not include records that received payment -- with test_data as( select * from ( select level invoice_num, 100* level billed, decode(mod(level, 2),0,100* level,0) paid, add_months(trunc(sysdate,'MM'), level -1) invoice_date from dual connect by level &lt;= 10 )order by decode(invoice_num,1,3, -- scramble the order 2,6, 3,7, 4,1, 5,5, 6,10, 7,2, 8,4, 9,8, 10,9, 0) ) select * from test_data desired output would look like this: INVOICE_NUM BILLED PAID OUTSTANDING -------------------------------------- 1 2 0 2 3 2 0 4 5 2 0 6 7 2 0 8 9 2 0 10 **ANSWERS** 1. 6 records in total! you get 2 roots, 1 for each row, and then 2 children for each root, 1 for each row. This happens because the rows are not bound to any relationship so every row gets connected to every other row similar to a cartesian query (select from two tables without a join and every record in one table will join to every record in the other table) 2. -- select INVOICE_NUM, BILLED, PAID, SUM(BILLED-PAID) over(order by INVOICE_DATE) OUTSTANDING from test_data where billed &lt;&gt; paid order by invoice_date
I would suggest you post the error, and possibly some example code. In short, there are a ton of different circumstances which would describe the behavior you're seeing. Why do you need to use EXEC() or sp_executesql for example?
This may be a parentheses problem with your "or" piece of the statement. I'm not sure what your requirements are but putting the section piece into an "in" instead of using the "or" may solve your problem. OAOutput.Section in('Totals','Potential Revenue') 
Exactly what I was thinking when reading, putting the dots in, not a problem, but yea, knowing where the dots go is the hard bit!
I know what is my IPAddresses Range :) 
Are you saying this must only work for a /24? So what happens when you have to handle something like /16? The only solution I see is to make what ever is writing this string put in the actual IP-address instead of anything else. Just as you can't unscramble an egg you can't deduct which of more than one possible parameters was actually used in the client that input the value into your table. Anyway, sticking with simple solutions, if you know your range and you are only dealing with 256 possible IPs you could just hardcode it like this: SELECT '192.168.1.' + LEFT(t.ip,LEN(t.ip)-7)
Yeah, OP went a little crazy with the parentheses. Kills readability and can really muck up a where clause. Here's what a query should look like: SELECT OAOutput.LineID, OAOutput.Name, OAOutput.Section, OAOutput.P1, OAOutput.PropID, OAOutput.VersionID FROM OAOutput WHERE OAOutput.Section IN ('Totals', 'Potential Revenue') AND OAOutput.PropID = [Property] AND OAOutput.VersionID = [PropVersion] ORDER BY OAOutput.Section, OAOutput.SortOrder;
Agreed. I don't understand what possible advantage could accrue from storing an IP address - backwards - as an integer. OP "knows his IP range" but someone else looking at the code - like his successor or his replacement while he's out on PTO - may not. And like you said - if it's always the same /24, why bother storing that information at all? 
OP, I suggest that you convert the IP address values to the standard IPv4 format. You are introducing unneeded complexity to your data which will require additional logic to translate the so called "ip address" into a format that can be used. If someone besides yourself attempted to use that data, it wouldn't be clear what that value is. I know that I would be WTF about that column. I guess it would help to know what generated the "ip addresses" as they are right now?
Thank you for posting this. I tried to post something similar earlier but Alien Blue ate my post and I didn't have the heart to retype it all. There's some serious Business WTF going on here when the person who apparently owns the IP range and the data, doesn't have enough basic SQL knowledge to extract an IP address from a highly non-standard format that only works for certain undocumented "magic numbers". I cannot imagine whose decision it was to store an IP address like this. It's not like it's an internationally recognized and codified standard or anything, and to be of any use whatsoever it'll have to be converted back to - guess what - a properly formatted string, just like the one it was presumably "extracted" from.
OP, I'm sorry to be a jackass about this but this if you're acting in any kind of enterprise IT capacity, this is a completely unacceptable attitude. You may know the data but no one else does. Storing the data in this format - deconstructing a universally recognized standard format into an obscure format that cannot be properly interpreted without undocumented insider knowledge - introduces needless confusion and will complicate any efforts to maintain, report off of, and utilize this data. Storing this kind of data in a format like this should never have gotten past QA. This is particularly troublesome as it's blindingly obvious that in order to use the data it's going to have to be converted back to the properly formatted string from which it was originally obtained and - forgive me - the task appears to have fallen to you who do not have the basic SQL knowledge of how to do this. Which raises the question of who in your enterprise DOES have this knowledge, and also of how the decision was reached to encode the data in this way in the first place. Please, PLEASE seek the advice of a more experienced IT person to assist you in correcting issues like this. Where there's smoke, there's fire, and this kind of thing is a Giant Flashing Red Light.
if he wants to keep the OR, it looks like this: WHERE (OAOutput.Section = 'Totals' OR OAOutput.Section = 'Potential Revenue') AND OAOutput.PropID = [Property] AND OAOutput.VersionID = [PropVersion]
TL'DR- im only manipulating th data for my own needs. the addresses are like this bm'cos of the vendor side, not me. the vendor is a nac company- access layers' portnox. and im a third (or 4th..) party integrator. im only making select on the data and pull it into an Arcsight db. :-) 
So you're saying: * the sa password is the same in both environments? * developers develop and test as sa? * he rolled out files to the production environment without telling anyone, or getting a sign off? If you want to lock out the sa account, you can. Just go to Properties and select Disable and Deny. But make sure that someone else is able to log in as an administrator. We have an AD group for the DBAs that authenticates with Windows auth, and we changed the default password to sa and didn't share it out. No one logs in as sa. The STIGs for SQL Server 2005 set out by the Dept. of Defense recommend renaming the sa account, changing the default password, and not using it for anything. I would also recommend using different passwords for each SQL server. Also, your devs really have no business having accounts on your production boxen, and if they do, they should be in an appropriate role that doesn't give them wide-open access to backups, audits, change data capture, etc. Deployments should be handled by the DBAs after some kind of rollout process.
i cant choose mate. this is the way the data is stored. not my choise. im only making selects and pull it to an outside db - Oracle Arcsight db. 
Yuck. Well, at the very least you can document: - the "formatting" used by their product - that all IP address are expected to come from the same /24 - that if an IP address comes from a different /16 or /24, that you cannot guarantee that the IP address you pull is the correct one, as you have to guess the length of each octet. The guys scripting up the ArcSight monitoring scripts will need to know this information.
Really? petrus2: "So, I'd like to lock out a certain type of connection from my SQL server -- specifically: ApplicationName = .Net SqlClient Data Provider LoginName = sa" BananaRepublican73: "If you want to lock out the sa account, you can. Just go to Properties and select Disable and Deny." Sounds to me like that's exactly what you asked for.
He wants to lock the specific application when the user is sa. IMO, it's a bad idea if it's even possible.
Woops. I guess I had half an answer.
I think what he's asking is not to just lockout the sa account, but to lock out the sa account where it's a connection from .Net SqlClient Data Provider. I think this is possible, but I don't remember how to go about it. Trace? 
No. I want to lock out only those connections that have LoginName = sa and ApplicationName = .Net SqlClient Data Provider
Maybe email the guy and tell him "Don't do that anymore."
As suggested before, change your sa password and create accounts with adequate permissions that you can assign them. You shouldn't have to lock out the connections, you're just making things harder for yourself that way.
I apologize. I misunderstood. 
If you do not intend to rename and disable the sa account, you can accomplish what you want by setting up a logon trigger and then query dm_exec_sessions to look for the login/application pair, originating IP address, etc. [This] (http://ifcsong.wordpress.com/2011/04/11/using-logon-trigger-to-bloc-remote-sql-server-connections-from-sql-server-management-studio-or-other-applications/) blog post should point you in the right direction. 
Believe me, we had a couple of heated phone conversations and email exchanges.
Thanks!
managers? stake holders?
I'm fairly new to learning SQL, so I apologize if this is unhelpful, but it seems like the EXCEPT/INTERSECT commands may be of some use to you. You can use them while joining the two tables to see what is the same and what is different between the two tables. You can read more about their use here: [Intersect/Except](http://msdn.microsoft.com/en-us/library/ms188055.aspx) I hope this helps. Good luck!
I think you need more information - ProductID. If Client:Contract is a one-to-many relationship, you need to know the product for each ContractID. Otherwise, how would you know what's considered "changed"? There is nothing that uniquely identifies a Client:Product relationship over time, and so there is no way to know from looking at the data whether the ContractID has ever changed. EDIT: What you'd ideally want is this: SELECT x1.Client, x1.Company, x1.ContractID AS [ContractID_2012], x2.ContractID AS [ContractID_2013] FROM ( SELECT t1.Client, t1.Company, pc1.ProductID, t1.ContractID FROM 2012 t1 INNER JOIN ProductsToContracts pc1 ON t1.ContractID = pc1.ContractID ) x1 INNER JOIN ( SELECT t2.Client, t2.Company, pc2.ProductID, t2.ContractID FROM 2013 t2 INNER JOIN ProductsToContracts pc2 ON t2.ContractID = pc2.ContractID ) x2 ON (x1.Client = x2.Client AND x1.ProductID = x2.ProductID) WHERE x1.ContractID &lt;&gt; x2.ContractID EDIT 2: some typos.
It will tell you which ContractIDs didn't exist last year. It won't tell you what the ContractID was last year. If a new contract was created this year, it would also be returned by this query.
Try the following: select a.* from 2013 a where company in (select company from 2012) and not exists (select 0 from 2012 b where a.company = b.company and a.client = b.client and a.contract_id = b.contract_id) I'm assuming client, company &amp; contract_id form a primary key here. So, if they come up in this query either a client or contract has changed.
 SELECT Field1 , MAX(Value) AS maxvalue , Value / ( SELECT SUM(m) FROM ( SELECT Field1 , MAX(Value) AS m FROM daTable GROUP BY Field1 ) ) AS calculation FROM daTable GROUP BY Field1
Hey thanks a lot. I just had to change Value to maxvalue at the start of the calculation field and it worked.
could you show the real query please
[Specs](http://msdn.microsoft.com/en-us//library/ms177523.aspx) See *FROM &lt;table_source&gt;* section. &gt; UPDATE t FROM t INNER JOIN u ON t.k=u.k SET t.x = u.x will first perform the join and update each **x** of **t** with the value of **x** of **u** from the *same row of the join*. EDIT: Typo.
BananaRepublican has the right of it with a login trigger, but it would be easy to change the application name in the connection string and get in. Plus, login triggers can be dangerous in the "stop people from logging in" way. I have to agree with the other options - remove Sysadmin rights, change passwords, and let the owner/stakeholders of the system make the decision to grant or deny this access (helped along by lots of evidence of the cost in time to clean up after this guy). 
Yeah sorry I should clarify the time stamps... So I'm looking at a month's date range, with daily data. Essentially within that date range, I want to flag the row where the datum changes from one day to the next, this happens infrequently with the dataset I am using. IE the value stays the same from June 1st til June 8th (one timestamp per day), but on June 9th the value changes. I want to select June 9th, and then the next time a change happens as well... So on and so forth.
The easiest way to do this would be a stored procedure with a few conditions/changes to what you have there, although there are various ways to do this. &gt; Definition Edit - Noticed this one is slightly incorrect, fixed in response below. CREATE PROCEDURE sp_recent_vehicle_maintenance @VehicleID AS BEGIN SELECT e.* --Change this later when you know what you need FROM (SELECT VehicleID, MAX(CheckOutDate) AS most_recent FROM RecordsDB.dbo.Encounter WHERE VehicleID = @VehicleID GROUP BY VehicleID) AS md INNER JOIN RecordsDB.dbo.Encounter AS e ON md.VehicleID = e.VehicleID WHERE e.CheckOutDate &gt;= DATEADD(d,-14, md.CheckOutDate) AND e.EncounterID != md.EncounterID END &gt; Sample Execution EXEC sp_recent_vehicle_maintenance @VehicleId = 920 Edit - Still don't know entirely what you want, but if you want ALL records for all vehicles within 14 days of the last change the *INNER JOIN* to a *LEFT JOIN* and reverse the order of join
col is whatever column you want to check to see if it is "yes". table is the name of your table that contains col.
There's something I forgot to mention: relationships. I have one table for projects and one for tasks. Each task is linked to a project through the field 'proj_code'. I'd like to know the percentage of completion of each project. The tasks table has ALL the tasks from all projects.
Then you need to add a group to aggregate under: select proj_code, sum(iif(Done="yes",1,0))/count(*) as yesratio from Tasks Group By proj_code 
Yes, it is saying it is a syntax error for something something subquery with the parenthesis. BTW I'm using a pre-made database model.
Post what you have and the error if you can copy it.
The field in the report: http://i.imgur.com/Le5gqbu.jpg The error: http://i.imgur.com/wiIxe2q.jpg "The syntax of the subquery is incorrect. Check that syntax and put the subquery between parenthesis." A part of the table: http://i.imgur.com/mNEbrjO.jpg The 'Projeto' is a special field that searchs for the project in the project table. To create relationships in the other queries I've been using Projeto.ID with no issues.
I've changed the syntax, now: http://i.imgur.com/ZOva29A.jpg It says the calculated expression is not valid for web-compatible reports.
Here are the steps to find where it fails: Select [Realizada] From Tarefas Select [Projetos.ID] From Tarefas Select [Projetos.ID] From Tarefas Group By [Projetos.ID] Select [Projetos.ID], [Tarefas.Realizada] From Tarefas Group By [Projetos.ID] Select [Projetos.ID], SUM(IIF([Realizada],1,0)) as yesCount, COUNT(*) as totalCount From Tarefas Group By [Projetos.ID]
 SELECT t.lastupdate , t.thevalue FROM ( SELECT DATE(lastupdate) , MAX(lastupdate) AS latest FROM daTable GROUP BY DATE(lastupdate) ) AS m INNER JOIN daTable AS t ON t.lastupdate = m.latest 
Thanks, that helped. only had to replace the DATE() function, that one didn't work for me. But... CAST(lastupdated as date) as lastupdate ... did the trick. But would you mind to explain me WHY it works? Why does MAX in the sub select gets all dates and not just the latest of all dates?
Just want to add a solution I got at stackoverflow: select Max(LastUpdate),MAX(TheValue) from YourTable Group by CONVERT(VARCHAR(10),LastUpdate,111)
it's probably only a coincidence that the maximum value for each date in your sample data happens to occur on the maximum timestamp for each date of course, if that always holds true for your data, then fine... but what if it doesn't? in the general case, obtaining a column value on a groupwise max requires using a subquery (or window function like /u/Quadman's solution) 
That was my first question, too. The grouping by date should solve it... it groups by date, gets only the highest date and max(thevalue) is only to make it available, but it should be only value by that time anyway. edit: Damn, you are right. Ok... so I guess I have to stick to the more complicated solution.
That is not a good way of doing it; use a loop and a temp table, it will make your life much easier. CREATE TABLE #temp_count ( cnt int , dttm datetime) GO DECLARE @looper int SET @looper = 0 WHILE (@looper != 10) BEGIN INSERT INTO #temp_count(cnt, dttm) SELECT count(*) FROM database.dbo.table WAITFOR delay '00:00:15' SET @looper = @looper + 1 END SELECT cnt, dttm FROM #temp_count DROP TABLE #temp_count GO 
SOLVED! See edit. 
Wild guess: You can't just skip the primary key. Try to name the fields you want to enter something: INSERT INTO Logins (loginname, realname...) VALUES (...)
Stored procedures are just a convenience, the query is the same. Updated for all vehicles. SELECT e.* --Change this later when you know what you need FROM (SELECT VehicleID, MAX(CheckOutDate) AS most_recent FROM RecordsDB.dbo.Encounter GROUP BY VehicleID) AS md INNER JOIN RecordsDB.dbo.Encounter AS e ON md.VehicleID = e.VehicleID WHERE e.CheckOutDate &gt;= DATEADD(d,-14, md.most_recent) AND e.EncounterID != md.EncounterID ORDER BY VehicleID
Thanks! Sadly I'm working with sensitive production data, and won't be able to create or drop tables without oversight. I might just end up putting the code into a C++ program and have it hold the values and do the comparison.
The table being created is a temporary table, it will be allocated and dropped from tempdb without effecting any live database. But a pair array in code could easily perform the same task.
Insert and Delete triggers would be another good way to monitor what goes in and out...
- &gt; Insert and Delete - &gt; Triggers - &gt; good way - **[no, NO, NO!](http://25.media.tumblr.com/f011f3a20a946d1b3ae4fcb1884b6c80/tumblr_mjtr3uzSQA1s373hwo1_400.gif)** - &gt; on sensitive production data - **[UNACCEPTABLE](http://24.media.tumblr.com/a5eb5f75488784ace0717623327eaabb/tumblr_mk6euwlIZf1s373hwo1_400.gif)**
I believe the .frm files only store the table structure, not the data itself. The data is separately stored in the .myd files. If you have those two files, you should be able to rebuild the database (after a rebuild index operation). EDIT: [See here for instructions](http://stackoverflow.com/questions/879176/how-to-recover-mysql-db-from-myd-myi-frm-files) if you have these other files. But if all you have is the .frm file, then you've already lost the data. Sorry. 
Alright then. I saw a post while on google that, maybe by chance, if i rebuild the table structure with dummy data, that MySQL just might be able to rebuild the table. Not sure if it will work but i might as well try it on one of my tables. But all i have is the frm files. Might as well give it a shot.
Yea, this would be the opposite case. If you have myd, but lost the frm, you can stash the myd, create tables with identical structure, move the myd back and your data should appear.
I think you might be hooped. On a lighter note, [this](http://25.media.tumblr.com/4104101e96495e5c1380d80e9b46a550/tumblr_mmbtaeb1I71s373hwo1_400.gif) is how I feel when this kind of stuff happens.
Whatever you can do to break this task down into smaller pieces will help you make progress. If you have 1.5 TB of indexes and 1TB of temp space, you will run out of space again. There are no hard and fast numbers but you should have multiples (X times) the space available as the space used in a system to do these operations. Can you break the problem down by disks? by table? If you had a spare you could restore there and mitigate the problem of running out of resources before job(s) ends. If you could ask a huge fast disk you would be in a better standing me thinks.
Do an online rebuild/reorg, if tempdb is a problem and you can't easily expand the drive that tempdb is on temporarily add an unlimited file to tempdb on a drive which has enough capacity. 
I watched the cursors video (as that one is free) to get a feel for the course. Gary, you start off saying that: you should only use cursors on certain conditions. You even explain why and that's good, but then you proceed to use/show a cursor where a SELECT (DISTINCT) query should suffice. Why not show something where you actually need to use a cursor for?
good point dr trunks, I know what you mean, rather than an 'engineered' condition, a real world usage.
The suggested solution of joining a table to itself to check this is very inefficient and could result in a worse case scenario of n^2 record iterations. If 1000 records, then possibly 1,000,000 comparison iterations. The most efficient way is to use analytic functions which result in only n iterations (one table scan). By partitioning by mydate, next_mytimestamp will be null if no next timestamp for the same date exists, so the where clause predicate will ignore these records. In all other cases, differences will be returned as per your requirements. select * from ( select mydate, mytimestamp, lead(mytimestamp) over (partition by mydate order by mydate, mytimestamp) next_mytimestamp from table) where mytimestamp &lt;&gt; next_mytimestamp
It's hard to say without a little more data. I assume the "trans" table is a list of individual transactions? Any there can be many per month? Also the Account_SelfPay table, is that a kind of "employee table"? Is the assigned date field in that table a month? or a day of the month? I assume you are trying to get the sum total of transactions for each account by month for 4 months starting with the month stated in "Assigned date"?
Very good questions. My apologies for not clarifying what information is in the tables/fields TRANS table - customer transaction information. A customer may have multiple transactions in the same month across multiple Client_ID's. All individual transactions appear in this table including posting date, transaction type, transaction amount etc. Account_SelfPay table - Customer account information. The Assigned_Date is a complete date in MM/DD/YYY format. It represents the date the account was received. Your assumptions are correct about the sum of transactions for each month, for 4 months based on the month/year assigned. What i'm hoping for to come back in excel is roughly in the below column format. Client_ID - Assigned Year - Assigned Month - Month 1 trans(sum) - Month 2 trans (sum) etc. I hope that helps! Thanks for responding. Warm regards Jason
The relatively easy, but poor performance, way would be to create a sub-query for each months sums using DATE_ADD creating m1, m2, m3, m4. Something like the following pseudo-query. SELECT Client_ID, m1.sum, m2.sum, m3.sum, m4.sum FROM disc--Distinct list of customers LEFT JOIN m1 ON Client_ID LEFT JOIN m2 ON Client_ID LEFT JOIN m3 ON Client_ID LEFT JOIN m4 ON Client_ID WHERE m1.Client_ID IS NOT NULL AND m2.Client_ID IS NOT NULL AND m3.Client_ID IS NOT NULL AND m4.Client_ID IS NOT NULL **NOTE:** This only works for customers who have had transactions in all of the last four months. If you want to include them all you will have to do a bit of work with the WHERE condition. 
Rebuild each index separately if/where possible. You can list them with [this query:](http://stackoverflow.com/questions/425475/sql-server-listing-all-indexes) SELECT i.name AS IndexName, o.name AS TableName, ic.key_ordinal AS ColumnOrder, ic.is_included_column AS IsIncluded, co.[name] AS ColumnName FROM sys.indexes i INNER JOIN sys.objects o ON i.object_id = o.object_id INNER JOIN sys.index_columns ic ON ic.object_id = i.object_id AND ic.index_id = i.index_id INNER JOIN sys.columns co ON co.object_id = i.object_id AND co.column_id = ic.column_id WHERE i.[type] = 2 AND i.is_unique = 0 AND i.is_primary_key = 0 AND o.[type] = 'U' --AND ic.is_included_column = 0 ORDER BY o.[name], i.[name], ic.is_included_column, ic.key_ordinal You can either do it manually from SSMS or use **ALTER INDEX 'index_name' ON ' table_name' REBUILD** for each index. 
I do most of my work in SQL Server, so my syntax for MySQL may not be perfect, but if I understand the problem correctly, I would alter the query to return: TRANS.Client_ID, ACCOUNT_SELFPAY.AssignedDate, sum(case when month(TRANS.Post_Date) = month(ACCOUNT_SELFPAY.Assigned_Date) then TRANS.Transaction_Amount else 0 end) as 'SameMonth', sum(case when month(TRANS.Post_Date) = month(date_add(ACCOUNT_SELFPAY.Assigned_Date, INTERVAL 1 MONTH)) then TRANS.Transaction_Amount else 0 end) as 'MonthPlusOne', etc., changing the "INTERVAL 1 MONTH" to 2, 3, 4, etc. until I reached the number of columns I needed. You would then need to group on the client ID and the assigned date.
* Defrag each partition individually * Set sort_in_tempdb = ON only if you have enough space in tempdb for each partition and if tempdb resides on faster &amp; decicated storage devices than the actual user databases * If parallelism is being limited at the instance-level (to something like 1) you may want to look into setting the index rebuild MAXDOP option to something higher, use at your own risk and please understand the effects of doing so within a production environment
Thanks for the code. I was looking for something along these lines.
I wasn't aware of that mindflux, I just use it as a platform for content, I'll take a look because as you say not ideal
Are you sure this is MYSQL? THe whole solution revolves around returning a value that can be the bucket. for example, on MSSQL to get months, I often use SELECT datepart(yyyy,TRANS.assigned_date)*100+datepart(mm,TRANS.assigned_date) as AssignedMonth It returns things like 201302 and 201211 Just that bit gives you an expression you can then group on, and sum the transactions that have that assignedMonth value.
Sorry for the delay in my response. I may be wrong in it being MY SQL. I was under the impression it was but now I question that. I'll validate the platform Monday to be sure. I appreciate you taking the time to write that all out. I'll post what I find out. I suppose this is why our database team cringe when one of us "operations" guys request access for something. Warm regards Jason
I'm no longer positive it's MYSQL based on some of the feedback i have been reading. This datepart formula fascinates me. I ran into a wall not that long ago with a field that should be a date but has a format of 060113 for 6/1/13. I couldn't find a solution to query values between dates as it seems to read that field as if the next expected value would be 060114. Thanks for the reply! Warm regards Jason
No worries, let me know if it does turn out to be MS SQL Server I'll double check the query and update it to make sure it follows the T-SQL syntax rules. 
Just to get it right, the final result should be: TheCount 4 2 3 1 Right? 
article is three years old but still makes some excellent points, including... &gt; I want to be very, very clear about this: ORM is a stupid idea. i love it when someone is this forthright 
the numbers aren't important what's important is I need a query to get the count between where the numbers are different. it could be 9 instances of 1 between the 2 2's or 1 or 15 all with different time stamps but what I want is the number of times 1 occurs between 2 2's (the time will always be counting up).
I've yet to use an ORM, but I've been using SQL (mostly DB2/400) for over 18 years now, and you know what? It works for me.
That's not a point it's an opinion, and he does a terrible job of backing it up. ORMs are slower? So what? That's like saying SQL is slow because it's transactional. Learn to scale horizontally with your application. ORMs provide no benefit. Really? Here's a few great things about SQL. * Validation * Consistency (use a trait in PHP for DateTime in your entities with an ORM like Doctrine and you don't need to worry about someone naming columns inconsistently with things like added_on, created_on, date_added, etc... Even better I can throw an interface on there and type hint my way to knowing what objects have a date I can use in my classes.) * Migration (I can easily migrate by comparing my schema to the entities in my codebase) * Separation of code and schema. I don't care what your table is called, I just need to know the class that I'm accessing it from. * Database agnostic. Postgres? Sure. MySQL, No problem. MongoDB, alright... * Caching. Very easy these days. You are writing code, let your data objects guide your decisions, not the database. When people just write straight SQL they tend to have databases and queries that lack consistency and maybe have one or two small variations from place to place that really reduce the ease at which you can update parts of your codebase. You also run into people treating SQL as a programming language, which I would avoid because it's much easier to scale your application servers than your database servers. Pure SQL is fine, not very complicated, and sometimes the best tool for the job but simply throwing out ORMs because "they are slower than raw SQL" or "you just don't know SQL (please)" is short sighted.
We wrote an in-house orm, worst decision ever. As the dba I support the idea of things like entity framework and totally agree that the application should drive the data objects not the database defining the application. Also, i r drunk. 
thank you! Let me try it out.
just tried it out and the values I'm getting aren't jiving and I'm trying to figure out why. I think it's because I have to CAST the Time from a datetime field as it doesn't allow to do &lt; or &gt; with datetime. If it would let me do greater than for the datetime it should be fine. I guess I can add as well that a date will always end with ITEM = 2 so if it doesn't find an ITEM=2 for the current date that means it's a new day (if that makes sense?). Ya, my bad as I did TIME but I didn't explicitly state that it's actually a DATETIME field. :( edit - I think we might have it? still numbers are off but changed the 2 top DECLARES - DECLARE @lowerTime DATETIME and DECLARE @nextTime DATETIME It's giving much more accurate numbers but it's still off. investigating further I still think it's due to calendar rollover somehow... UPDATE - It works! You rock I hope you're making lots of money working with SQL because you're damn good!
Well, converting it shouldn't be a big problem. :)
thank you again! Now that it's getting the info I need I'm just going to look into tweaking it a bit to work it into a larger query I have.
ORM isn't safe ground for a DBA. They're 2 distinctly different models which require two different approaches when designing and managing. I don't understand why people keep feeling the need to compare apples with oranges. They're both amazing when done right, and you can eat them together but they're not the same goddamn fruit!
I don't really know if I understand you 100% correctly, but I think you should take a deeper look at dynamic sql. There you can build your query string and you can use variables (like a table name) to complete it. My second "I don't know": Usually I do my SQL stuff with MS SQL, but Google told me, that there is dynamic SQL at MySQL, too.
I never said it was an easy way out. It's an easy way in. If you're terrible at OO design, you just make a bunch of objects and let ORM figure it out. If you're terrible with data, just make a bunch of tables and hope your ORM can autogen the right classes. Just out of curiosity, are you saying that ORM's don't make things easier?
The main opposition I have to ORMs(especially custom built) is that they seems to make developers lazy and think less about what they are designing; usually leading to performance issues or hidden implicit errors. They also add another layer or abstraction that makes diagnosing the issues more cumbersome. That said properly integrated ORMs can drastically decrease turn-around time. .NET Entities for example are easy to implement and follow their execution path, along with sticking more closely with regular database integration/structure. 
If you're using Hibernate, it does all this for you. Can you derive the subtype table name from the supertype type field? Otherwise, you'd need to keep a map somewhere of the type to table name mappings. 
There is no SQL question. Post in /r/sqlserver or maybe an SSRS forum. If that happened in my reporting platform (BIRT or eRD) I would blame the font. It appears the font metrics and control's right alignment do not agree about where the right edge of the space is.
Not a SQL question. Try /r/sqlserver. The answer will depend heavily on how your SQL Server is configured and the worst case usage load you might need out of it.
You were correct. It is MS SQL. Sorry for the confusion. Thanks GloamingGramercy Jason
Click on the cell and check the Padding properties. By default it should be 2pt. Change the right padding to 3pt or reduce it to 1pt. If that doesn't do it then check the cell's border properties and look for thick borders.
No I'm not using Hibernate, It was just an example of something that did it. This project is with node.js. The supertype table will contain the subtype in the type column. I can get a working result set by doing a LEFT OUTER JOIN on all of the known subtype tables (even though there will obviously be null columns), I'm guessing that's just massively bad performance wise though. 
Does MS Access support the coalesce function (or isnull funtion)? Returns the first non-null result: COALESCE([MILESTONES]![Date1],[MILESTONES]![Date2],[MILESTONES]![Date3]) 
 IIf(MILESTONES!Date1 Is Null, IIf(MILESTONES!Date2 Is Null, MILESTONES!Date3, MILESTONES!Date2), MILESTONES!Date1)
r3pr0b8 - thank you the above code did work. 
I believe you could make use of the SWITCH statement as well for this if your nested If's ever get too lengthy.. http://www.techonthenet.com/access/functions/advanced/switch.php 
I'd include year for clarity, I imagine something like the below should get you a handle to group by: SELECT CONVERT(VARCHAR(6), GETDATE(), 112) AS [YYYYMM] With that said, getting the group/summary is the easy part, first you decide which columns you will group by, and which columns you will aggregate. Columns you want to group by are included in both the select statement the group by statement (doesn't always have to be the case, but in practical applications it always is). The trick is adding a grouping member for month, which we create via the above conversion. We only have one aggregate column Transaction_Amount, and it needs to be summed, so we use sum(Transaction_Amount). Aggregates are in the select statement but not the group statement. SELECT TRANS.Client_ID , TRANS.Account_ID , CONVERT(VARCHAR(6), TRANS.Post_Date, 112) AS PostMonth , sum(TRANS.Transaction_Amount) as Transaction_Amount_Sum , CONVERT(VARCHAR(6), ACCOUNT_SELFPAY.Assigned_Date, 112) as AssignedMonth FROM DataRepository.dbo.ACCOUNT_SELFPAY ACCOUNT_SELFPAY , DataRepository.dbo.TRANS TRANS WHERE ACCOUNT_SELFPAY.Account_ID = TRANS.Account_ID AND ((TRANS.Client_ID='STRIPCLUB01') AND (ACCOUNT_SELFPAY.Assigned_Date Between {ts '2012-01-01 00:00:00'} And {ts '2012-03-31 00:00:00'}) AND (TRANS.Transaction_Code In (1,2))) Group by TRANS.Client_ID , TRANS.Account_ID , CONVERT(VARCHAR(6), TRANS.Post_Date, 112) , CONVERT(VARCHAR(6), ACCOUNT_SELFPAY.Assigned_Date, 112) Also, seems your using implicit joins instead of explicit joins, shouldn't make a performance difference but most people find explicit joins to be a bit easier to read. 
Cool, sorry for the delayed reply my internet is down and I forgot to check for reddit msgs from my phone, I'll dig through my T-SQL refs and update that code
As THL mentioned, not really a direct SQL question; although there are [some metrics](http://thomaslarock.com/2012/05/are-you-using-the-right-sql-server-performance-metrics/) you can perform to check to see if your system is being limited by memory or by CPU. I find CPU is rarely the bottleneck, more often it is the server memory, network/connection limit, SAN that are wall in front of your server. The easiest/best way to determine this is to run performance monitoring on your current system when there is a heavy load. Another factor is how dynamic your applications that run off the server are, if they are internally developed there is usually the ability to optimize, vendor solutions on the other hand can be tricky and you might just be stuck throwing hardware as software problems.
Not a SQL question. Please post in /r/oracle The installer you used to install the 11g instance should have a remove option. 
Alright, thanks for the advice.
What you have is returning everything from the right of the dash. Try the following for the left side: SUBSTRING("COL."Number" FROM 1 FOR LOCATE('-',"- "COL."Number")) I'm never great with substring indexing so +/- 1 to each of those parameters should get you what you need.
Hi Smileython, thank you - the 1 works and in case anyone else was inerested the full code to remove the dash is SUBSTRING("COL."Number" FROM 1 FOR LOCATE('-',"- "COL."Number")-1)
Yeah, I figured you needed the -1 , but I was too lazy to double check with actual code. Glad it worked!
[This](http://stackoverflow.com/questions/2968042/sql-query-for-finding-rows-with-special-characters-only) looks interesting: WHERE Email LIKE '%[^0-9a-zA-Z ]%' You just need to build your list for [valid characters](http://en.wikipedia.org/wiki/E-mail_address#RFC_specification) and this returns failed validations. 
You could create a user defined function that would iterate through the characters. That way, you could use the set based rather than cursor method. But, I think you would still need to iterate through each char in the string to check to see if it is a valid char. function pseudocode Pass in strval get lenth of strval if the length of strval is &gt;0 then for each char What puts the email address into the database? Wouldn't the app need to have some sort of validation prior to getting to the data layer? Or is this existing data and you just need to make sure that the email addresses are valid?
Hmmm.... I didn't consider using the ^ to exclude valid chars.
The following UPDATE statement with the IN clause should work if you don't need to do anything complicated UPDATE data_tbl SET secure = 'Y' WHERE acct_no IN (SELECT acct_no FROM access_tbl) Optionally you can add an additional WHERE clause to not change those that are already secure. AND secure &lt;&gt; 'Y' 
Yep this. Just be careful with updates. Do this in your test environment first or run the Select and double check the rows being changed before you do the update: SELECT secure, acct_no From data_tbl WHERE acct_no IN (SELECT acct_no FROM access_tbl)
Deleted bad answer
if your database supports transactions add SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED SET XACT_ABORT ON BEGIN TRAN [query here] ROLLBACK this will simulate the entire operation and then rollback transactions so you can see how the query goes. After it returns no errors and rows affected match the count you get from a select tring mentioned above, then take out the SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED and change ROLLBACK to COMMIT this will commit the db changes. I recommend when you are not sure, or are developing a new prov, or migrating data from one db into another, as you eventually will, performing sql development within the TRANS with isolation level set to read uncommitted will prevent a ton of tears and locking rows as well as save a ton of system resources.
Noob here, is that regex? 
Anyone have any idea around foreign characters? The statement works for all normal English characters, but when dealing foreign or Unicode characters, it is not working as expected. I added the N'' before the like to search for Unicode, but anything like " é" is not being found and correctly rejected. Thanks for the help!
Coldchaos, Thanks. I put together this query to disable nonclustered indexes, and then rebuild or reorganize everything else in seperate executions. I'm assuming if you run a query via EXEC SYS.SP_EXECUTESQL that it is considered it's own transaction and own SPID, but we'll see. --Drop temp table, index, create new IF OBJECT_ID('TempDB..##IndexPartitions') IS NOT NULL DROP TABLE ##IndexPartitions CREATE TABLE ##IndexPartitions ( [ID] INT IDENTITY NOT NULL PRIMARY KEY, [Status] tinyint DEFAULT '0' NOT NULL, [Table_Name] NVARCHAR(60) NOT NULL, [Index_Name] NVARCHAR(100) NOT NULL, [Index_Desc] NVARCHAR(50) NOT NULL, [Index_Fill_Factor] INT NOT NULL, [Partition_Number] INT NOT NULL, [Rows_Count] BIGINT NOT NULL, [Avg_Fragmentation_in_Percent] FLOAT NOT NULL, [Page_Count] BIGINT NOT null ) CREATE NONCLUSTERED INDEX idx_clustered ON ##IndexPartitions ([Table_Name], [Index_Name], [Partition_Number]) --Gather index stats INSERT INTO ##IndexPartitions SELECT '0' AS [Status] , B.name AS [Table_Name] , C.name AS [Index_Name] , A.index_type_desc AS [Index_Desc] , C.fill_factor AS [Index_Fill_Factor] , A.Partition_Number as [Partition_Number] , D.rows AS [Rows_Count] , A.Avg_Fragmentation_in_Percent , A.Page_Count FROM sys.dm_db_index_physical_stats(DB_ID(),NULL,NULL,NULL,'LIMITED') A --trying Limited mode for quicker scans INNER JOIN sys.objects B ON A.object_id = B.object_id INNER JOIN sys.indexes C ON B.object_id = C.object_id AND A.index_id = C.index_id INNER JOIN sys.partitions D ON B.object_id = D.object_id AND A.index_id = D.index_id WHERE C.index_id &gt; 0 and A.avg_fragmentation_in_percent &gt; 19 order by avg_fragmentation_in_percent desc SELECT * FROM ##IndexPartitions ORDER BY avg_fragmentation_in_percent DESC --Declare temporary variables DECLARE @ctr INT DECLARE @task NVARCHAR(10) DECLARE @sSQL NVARCHAR(MAX) DECLARE @Status TINYINT DECLARE @Table_Name NVARCHAR(60) DECLARE @Index_Name NVARCHAR(100) DECLARE @Index_Desc NVARCHAR(50) DECLARE @Index_Fill_Factor INT DECLARE @Partition_Number INT DECLARE @Rows_Count BIGINT DECLARE @Avg_Fragmentation_In_Percent FLOAT DECLARE @Page_Count bigint SET @ctr = 1 SET @sSQL = '' --Disable nonclustered indexes to free up disk space. --They will be re-enabled when they are rebuilt WHILE @ctr &lt;= (SELECT MAX([ID]) FROM ##IndexPartitions) BEGIN SET @sSQL = '' SELECT DISTINCT --@Status = [Status], @Table_name = [Table_name], @Index_Name = [Index_Name], @Index_desc = [Index_Desc] --@Index_Fill_Factor = [Index_Fill_Factor], --@Partition_Number = [Partition_Number], --@Rows_Count = [Rows_Count], --@Avg_Fragmentation_In_Percent = [Avg_Fragmentation_in_Percent], --@Page_Count = [Page_Count] FROM ##IndexPartitions WHERE [id] = @ctr IF @Index_Desc = 'NONCLUSTERED INDEX' BEGIN SET @sSQL = @sSQL + 'PRINT N''Disabling index ' + @Index_Name + ' on table ' + @Table_Name + ' partition ' + CAST(@Partition_number AS VARCHAR(10)) + ' as it is NONCLUSTERED.''' SET @sSQL = @sSQL + N' ALTER INDEX ' + @Index_Name + ' on ' + @Table_Name + ' DISABLE ' END PRINT @sSQL EXEC sys.sp_executesql @sSql --Increment the Counter SET @ctr = @ctr + 1 END --Reset dynamic sql SET @ctr=0 WHILE @ctr &lt;= (select MAX([ID]) FROM ##IndexPartitions) BEGIN SET @sSQL = '' SELECT @Status = [Status], @Table_name = [Table_name], @Index_Name = [Index_Name], @Index_desc = [Index_Desc], @Index_Fill_Factor = [Index_Fill_Factor], @Partition_Number = [Partition_Number], @Rows_Count = [Rows_Count], @Avg_Fragmentation_In_Percent = [Avg_Fragmentation_in_Percent], @Page_Count = [Page_Count] FROM ##IndexPartitions WHERE [id] = @ctr --Determine wether to reorganize or rebuild IF @Avg_Fragmentation_In_Percent &lt;= 20 --------------------------------------------------------------------------------------------- begin SET @task = 'REORGANIZE' END ELSE BEGIN SET @Task = 'REBUILD' END --Log to messages SET @sSQL = @sSQL + N' PRINT N''' + CONVERT(NVARCHAR(MAX), GETDATE(), 100) + ': Index Maintenance: ' + CAST(@task AS NVARCHAR(MAX)) + ' index: ' + @Index_name + ' on ' + @Table_Name + ', partition ' + CAST(@Partition_Number AS NVARCHAR(MAX)) + ' as it has ' + CAST(@Avg_Fragmentation_In_Percent AS NVARCHAR(MAX)) + '% fragmentation.''' --Build the reindex query SET @sSQL = @sSQL + ' ALTER INDEX ' + @index_name + ' ON ' + @Table_name + ' REBUILD ' IF @Partition_Number &gt; 1 BEGIN SET @sSQL = @sSQL + ' Partition = ' + CAST(@Partition_Number AS NVARCHAR(10)) end --Commit --PRINT @sSQL EXEC sys.sp_executesql @sSQL SET @ctr = @ctr + 1 --Increment the counter END 
This is one I use: &gt;SELECT [E-Mail] FROM [Contacts] WHERE (PATINDEX('%[ &amp;'',":;!+=\/()&lt;&gt;]%', [E-Mail]) &gt; 0) AND ([E-Mail] IS NULL) OR (PATINDEX('[@.-_]%', [E-Mail]) &gt; 0) OR (PATINDEX('%[@.-_]', [E-Mail]) &gt; 0) OR ([E-Mail] NOT LIKE '%@%.%') OR ([E-Mail] LIKE '%..%') OR ([E-Mail] LIKE '%@%@%') OR ([E-Mail] LIKE '%.@%') OR ([E-Mail] LIKE '%@.%') OR ([E-Mail] LIKE '%.cm') OR ([E-Mail] LIKE '%.co') OR ([E-Mail] LIKE '%.or') OR ([E-Mail] LIKE '%.ne')
Looks like two pieces of broken code. The second piece says SELECT and GROUP BY but not FROM. Start over.
I have played around with AdventureWorks2012LT some. After modifying some of the data to make it more useful for queries (for some reason MS made all the dates the same) and also thinking about modifying the design for shipping, I realized I was essentially turning it into an existing dataset the professor has for the full-blown database design class that she uses for homework assignments. There's also no book for students to follow along with and I would have to create a huge amount of tutorials using AdventureWorks. While the online games that teach SQL look interesting, they don't fit the academic environment that I need. Students would not be happy to be pointed to a game after paying $1500 to take a class. Does anyone know of ready-made tutorials using AdventureWorksLT?
Student also may not be happy to be pointed to ready-made tutorials you found for free on the internet after paying $1500,- If you don't like the dates, you can change those yourself. If you need fake history you can use this query: &gt;update table set [datetimecolumn] = (dateadd(dd, rand() * -100, dateadd(mm, rand() * -100, dateadd(hh, rand() * 100, dateadd(MINUTE, rand() * 100, getdate())))))
Try to mention the RDBMS engine you're using. Looking at the error message Im guessing its MS SQL Server (but you posted in /r/sql which is a generic reddit covering MSSQL, MySQL etc etc) Also just post the full query rather than trying to simplify it. The syntax error refers to a '.', but there isnt one anywhere near the CallSTartDt_d so its probably in the SQL before the bit you pasted 
MySQL doesn’t need a license for personal, or learning use. That said, MariaDB is essentially the same, and is fully Open Source. Other Open Source relational databases include PostgreSQL, Firebird, and SQLite. Complexity-wise, from simplest to most complex: SQLite, MySQL/MariaDB, Firebird, PostgreSQL
Postgres is much closer to SQL Server, Oracle and DB2 in terms of functionality than mysql. Additionally Microsoft SQL Server Express Edition is free and I believe Oracle probably offer something similar 
CJ Date's book are what I learnt with. and celko's sql for smarties might have some good example 
Neither SQL Server nor Oracle are open source.
Thanks. I actually think I made an error in export as I did different export and imports today and it worked , even after dropping 
Although he said open source, the mention of licensing makes me think he might have the impression that all proprietary software costs money, and just needs something free. Also sounds like for the purpose of the class, an express edition vs open source wouldn't matter. I'd suggest [SQL Server 2012 Express](http://www.microsoft.com/en-us/download/details.aspx?id=29062), it will do everything you need to start out and is easy to use. Downloading the AdventureWorks sample database and tinkering around with it wouldn't be a bad idea, either.
Many SQL books have sample databases for which the scripts to create the database are available online, and are used for query examples and problems in the book. T-SQL Fundamentals by Itzik Ben-Gan starts from basic to more advanced SQL/T-SQL, obviously catered to SQL Server, and I always see it recommended. I also have an O'Reilly 'Learning SQL' book, which has good non-proprietary examples but I think its database was designed for MySql, so the scripts might need to be modified. I'm not sure what your options would be for a book that covers both SQL and Excel with a sample database/exercises though, sounds like you are looking at more of a BI focus, and not sure how beginner-friendly those would be.
I think all he means is free software, not really open source
apex.oracle.com Create an account with Oracle and click the "Request a free workspace" button. Once logged into the workspace click on "SQL WOrkshop" icon. From here you can click on the Object browser to look at the tables in your database schema or you can click on SQL Commands to issue statements against the database.
schema = user. they are one and the same. Some schemas only have rights to query and work with other schemas and do not actual contain any objects. that is how a schema could be leveraged as a "user". I had the same headache when I was first starting with Oracle.
What kind of rubric do you have for the class structure? I'm learning SQL server at the moment and would be glad to help you put it together. When do you need it put together? Another thing you might be able to do is ask your college's CS department for a sample database. Maybe use something simple like a customer list for CRM or something like that. To focus on SQL Server, I highly suggest getting in touch with the folks affiliated with sqlpass.org. A lot of them have written books and conducted seminars to teach introductory information to people who are new with SQL Server. Shoot me a pm if you want any help!
Wow, that's an awesome explanation. Any other tips you can give a beginner DBA? 
I was looking at the T-SQL fundamentals book last night and that could be something I have the professor get to review it. I found the script for the sample database and it is much simpler and straight forward than adventureworks. The biggest thing is finding a book resource to lean on that is updated when new versions of software come out. It is time consuming to update custom made material. Another option I learned about today was the Microsoft IT academy. There is a entry level cert for database fundamentals that should have curriculum already made that might be within that subscription service.
At the end of your class, will your students be able to pass the exam?
If i understand correctly, I think you should look into adding **constraints** to your table. link: http://msdn.microsoft.com/en-us/library/ms189862(v=sql.105).aspx 
Don't use cursors or loops unless you absolutely must. Set based logic, and proper indexes are your friend. Add constraints and a proper clustered index on that table, and use a parametrized SP to do the insert. Maybe try/catch and return a nice error message for your VBScript to interpret. My main DB here has 1.2B rows and I'd be fired if a 1 record insert took three minutes. 
Using a basic joins could easily do this, not sure exactly how the difference are detected but you can try something like this: INSERT INTO events.dbo.eventLog (eventID, worldID, mapId, state, dateTimeStamp) SELECT eventID, worldID, mapID, state, dateTimeStamp FROM External_Logs AS tl LEFT JOIN events.dbo.eventLog as el ON tl.eventID = el.eventID AND tl.worldID = el.worldID --etc etc WHERE el.eventID IS NULL --etc etc
The problem with your method is that if you have multiple transactions with the same date for a given customer, you're going to get more than one row, which may be undesirable. Also, there's no reason you can't use the CTE/row_number method to accomplish the second task, you'd simply use where row &lt;= 5 (or however many purchases you wish to see per customer).
Yeah, you get the same problem if you say "show me the top ten customers by number of purchases"; what do you do if there's a tie? You can solve the issue you described with a simple join using a tie-breaker like the PK, too [although this works on the assumption that the pk is strictly monotonic in the same direction as the created_at column]: SELECT * FROM purchases p1 INNER JOIN purchases p2 ON p1.customer_id=p2.customer_id AND p1.created_at&gt;=p2.created_at AND p1.id&gt;=p2.id GROUP BY p1.customer_id HAVING COUNT(p1.customer_id)=1 Of course, now it's uglier than the window function, but at least it's mostly still portable... [except the part where the columns other than p1.customer_id aren't well-defined, but all p1.* columns will in practice be the same. Solve that more group by clauses, or using a DB that allows that minor transgression]
You don't run into that problem with the row_number approach. I think you might be confused about how it works. While dense_rank will return the same number for ties, row_number will not - if there are only two purchases, both with the same date, one will be numbered 1 while the other is numbered 2. So, if you use row_number in your CTE and then return only rows where row is 1 or less than or equal to 10 or whatever approach you might like, you're only going to get exactly the number of rows you specify (or fewer, if they don't exist).
Oh, I get that. I'm just pointing out that row_number isn't portable, at all, and offering up a portable-but-maybe-not-quite-as-good-but-probably-still-fine alternative.
The regular database class should be able to, but I doubt these students would be quite ready. If some of the content could be used though that would be great. I think classes in the IS field should be aimed with the goal of certifications so graduates have proof beyond taking a class they have a baseline level of knowledge about a subject. Does anyone have any knowledge about Microsoft IT Academy or MTA certs?
http://www.microsoft.com/learning/en-us/mta-certification.aspx That's Microsoft's url for MTA certification. I would get in direct contact with them and see if I could strike up a deal for your students. It would be really nice to get that type of certification along with their schooling. Often, students get the short end of the stick in college.
I've spent a few hours looking around at that stuff today. I finished my undergrad in the same program in 2011 and feel like an entry level database cert and .NET development would have helped tremendously find the job I really wanted instead of what I got. It all worked out though since I'm essentially getting paid to get a masters now. I will have my professor or college IT director contact Microsoft to get more info. What is your background in IT if you mind me asking?
Dang you know what, might not work. So if the last entry has the same "state" as what I'm trying to put in, I won't put it in. If it changes, I put it in. But then if it changes again, and the new state happens to match one previously entered, that's fine (there's only a few states and they cycle through). I'm wanting to build a history of these cycles. I was going to put in a unique constraint against eventId,mapId,worldId,state but it won't work because there will be duplicates for sure. Is it possible in the constraint to use part of the datetime stamp, such as make sure it isn't duplicated on the same day but after that not care? I could use the date, but my datetime column is system specified (not supplied in the query)...can I compare just a date data type to that column or do I need to add a date only column to include in the constraint?
You should put an index on eventId,worldId,mapId,state and change the query as such (no need to order and take top) it should take only marginally longer than an insert without check INSERT INTO events.dbo.eventLog (eventId,worldId,mapId,state,dateTimeStamp) SELECT 'D20268C3-301F-4DD5-930D-2417B356EE05', '1016', '23', 'Preparation', '2013-07-09 16:35:31.000' WHERE not exists (SELECT * FROM events.dbo.eventLog WHERE eventId='D20268C3-301F-4DD5-930D-2417B356EE05' AND worldId='1016' AND mapId='23' AND state = 'Preparation')
Hey, thanks for the alternative query. I don't like it quite as much for the tie break reason you guys discussed, and also because unless you've got a very smart optimizer the subquery method is probably less efficient. But it's good to know alternatives. Do you know if any major databases don't support some version of window functions?
If there is a tie, then you don't really know the most recent record. Pretending that one comes before the other invalidates whatever reason you have for this query returning the full row of data.
sqlite doesn't, and I don't think mysql does. Access probably doesn't, but that also fails the 'major database' test. As the other guy said, in the case of a tie, picking one at random (which is what your Window does) isn't really valid in most cases, either 
Does the class material favor a specific db? Does the teacher or teacher's assistant have more experience in one db over another? You may need his help or he might be able to show you some tricks that are "off menu". What do you want to do with the knowledge you get from the class? * Are you looking to get into db admin, developer or the bi field in the corporate world? * Do you want to go independent/small business? * Do you want to only have basic sql syntax knowledge? * Don't have any desire to pursue databases or knowledge management in the future? Although all the db's mentioned in the comments support standard sql syntax, some add their own spin on top of standard sql. For example, since I know standard sql, I feel confident I can be productive in any of the mentioned dbs even though I have been a ms sql guy for many years. Not listed in any specific order: * MySQL: http://www.mysql.com/ * Postgres: http://www.postgresql.org/ * MariaDB: https://mariadb.org/ * Oracle Express: http://www.oracle.com/technetwork/products/express-edition/downloads/index.html * MS Sql Express: http://www.microsoft.com/en-us/sqlserver/editions/2012-editions/express.aspx 
This is for a school project and there really isn't any rules for what we can and can't do. My professor suggested that I make a table for both of those queries and then calculate the percent change but that seems like way too many steps. Would saving those as variables be the easiest way? 
Well, if this is the only calculation you need to do, then saving it to variables and calculating would be enough (heck, you could just do it all in one query without variables, but take baby steps). However, it sounds like you got to do this for several time periods. I would probably do what your professor suggested, as it is just good habit.
 select (sum(s.SALES) / sum(case to_char(o.ORDERDATE, 'YYYY') when '2004' then s.SALES else null end)) PCT_CHANGE from SALES s join ORDERS o on o.ORDERNUMBER = s.ORDERNUMBER where o.ORDERDATE &gt;= '1-Jan-2003' and o.ORDERDATE &lt; '1-Jan-2005' 
[SQL Fiddle](http://www.sqlfiddle.com/) You can pick your DB platform and no special client needed, you just use your web browser. 
Awesome! That worked. 
 This is what I get for trying to accomplish something half an hour before quitting time on a Friday. It turned out to be tons less complicated than I thought it should be. Go figure.... Thanks again. 
Glad to be of service.
I never properly thanked you for your help. I haven't been able to work through much of this (I got swamped at work) but I still plan on running through this and getting the full knowledge from experiencing the ins and outs. Be that as it may, I sincerely appreciate your efforts to educate me and assist with my initial problem. Thanks very much!
Anytime. It helps me find holes in topics I think I understand. Its all part of the endless learning cycle we signed up for as a developer/programmer/tech person
This is not a subreddit for posting job offers. /edit : the account is a throwaway; no use in educating, just downvote.
You can always use a [server logon trigger](http://msdn.microsoft.com/en-us/library/ms189799.aspx). Keep in mind that server login triggers can potentially prevent any account from authenticating if they fail for some reason. CREATE TRIGGER freaking_idiot_dev ON ALL SERVER FOR LOGIN AS BEGIN IF APP_NAME() = '.Net SqlClient Data Provider' AND ORIGINAL_LOGIN() = 'sa' BEGIN RAISERROR('God dammit, idiot dev.',16,1); ROLLBACK; END END Above code is *untested*.
Here's what I ended up doing: I added a new date only column, and made it part of my unique constraint: ALTER TABLE events.dbo.eventLog ADD CONSTRAINT uc_eventLogDupeCheck UNIQUE (eventId,worldId,state,date) I changed the insert query to a simple insert, adding in the date as the last value. Also modified my vbs to resume next on an error (simply because at this point I don't care). Takes two minutes each run, I dumped the table and ran from scratch, did the initial 85k rows in two minutes, each subsequent run takes only two minutes and adds the expected number of rows. If I schedule it to run every 5m or so I expect the first time it runs tomorrow it'll import all 85k rows again, but I'm fine with that (I'll set up some pruning at some point to dump data after 14 days or so). Thanks for all the advice!
&gt; Select [Realizada] From Tarefas Sorry for taking so long (work), but in that first one MSAccess asked me to put it within parenthesis. I did id, and then It gave me that 'not web compatible' message. Same thing to the others. **Looking at how the report has been built, it looks like I have to first create and save a separate query, and THEN select data FROM it.** Does this make any sense to you?
Don't understand why you'd ever want to do it this way, why not look at the columns and figure out which ones you have to drop. Better yet, why drop them at all? Query to list all the columns / data types for a table: USE -- Your database here GO SELECT sycol.name 'Column_Name', syt.Name 'Data_Type', sycol.max_length 'Max Length', sycol.precision, sycol.is_nullable, ISNULL(syi.is_primary_key, 0) AS 'Primary Key' FROM sys.columns sycol INNER JOIN sys.types syt ON sycol.system_type_id = syt.system_type_id LEFT OUTER JOIN sys.index_columns syic ON syic.object_id = sycol.object_id AND syic.column_id = sycol.column_id LEFT OUTER JOIN sys.indexes syi ON syic.object_id = syi.object_id AND syic.index_id = syi.index_id WHERE sycol.object_id = OBJECT_ID('/*Your table name here*/')
&gt; &gt; Ask your DBA [There are ways to find this out](http://ask.sqlservercentral.com/questions/16078/schema-and-role-permissions-for-all-users-in-a-dat.html) but they require access to the system tables. The excel connection might just iterate through them all for you, or there is some hidden Microsoft voodoo I don't know about.
A better (and easier) way would to query the INFORMATION_SCHEMA views.. select * from INFORMATION_SCHEMA.COLUMNS where yadda yadda
Was just a subset of a query that I use to get other/more information. Either will work.
Thanks for the info, looks like I have some more reading to do.
I think you have to use VBA instead and build a SQL String. I don't think you can accomplish this with a basic Access query, as the operator is not something you can modify as far as I know. 
You can set filtering in the object explorer by right clicking on the category and clicking filter.
There was a standford opencourseware on relational databases with a focus on SQL. The instruction all came in the form of videos (which I find to be infuriatingly information sparse. Give me walls of text, please.) but it included exercises and quizzes whose answers were checked by SQLite.
article ends with... &gt; If you write this non-engineering focused SQL tutorial or know of one that exists please let us know! try [sqlzoo](http://sqlzoo.net)
&gt; Give me walls of text, please. I find walls of text a little intimidating, I prefer demonstrations. 
SQL is a wall of text though.
I've created a visual representation of what I'm trying to explain: http://imgur.com/pES1vGr
I think a self join on date and &lt;&gt; amount would get you going in the right direction. You could then combine the columns using some string functions. 
 ;WITH temp2 AS (SELECT DISTINCT t.MyDate, (SELECT Item_No + ', ' FROM #temp1 t2 WHERE t2.MyDate = t.MyDate FOR XML PATH ('')) AS Item_Nos FROM #temp1 t) SELECT MyDate, SUBSTRING(Item_Nos, 1, LEN(Item_Nos) - 1) FROM temp2
There isn't an aggregation function to concatinate results, so you aren't going to get anywhere with group by. There might be some trick you can do with partition or pivot, but I have my doubts. Odds are you will need a stored procedure and a cursor.
I just had to do a similiar query last week, I rebuilt it for you. DECLARE @txt VARCHAR(8000) SELECT @txt = COALESCE(@txt + ',', '') + [item no_] from [temp1] group by [Expiration_DT], COALESCE(@txt + ',', '') + [item no_] select @txt, [Expiration_DT] from [temp1] group by [Expiration_DT] ah damnit, I didn't have to do the group by doesn't seem to work :( edit: I tried some more querying: declare @date datetime declare temp_cur cursor for select distinct mydate from [#temp1] order by MyDate open temp_cur fetch next from temp_cur into @date while @@FETCH_STATUS = 0 begin DECLARE @txt VARCHAR(8000) = null SELECT @txt = COALESCE(@txt + ',', '') + [item_no] from [#temp1] where mydate = @date select @txt, MyDate from [#temp1] where Mydate = @date group by mydate fetch next from temp_cur into @date end close temp_cur deallocate temp_cur Hope this helps, maybe someone else can expand on this to get it into 1 result. Instead of selecting you could also INSERT it into a result table and select from there.
I came across a similar result with some google searching after sending in the question. I have never heard of "FOR XML PATH," so I'm not sure what that does, but it works. Can you explain?
In case anyone else runs into this problem in the future, a solution that I came up with: email COLLATE Latin1_General_BIN LIKE N'%[' + NCHAR(128) + '-' + NCHAR(65535) + N']%' searches for all unicode characters
 CREATE TABLE #temp1(Item_No varchar(100), MyDate datetime) INSERT INTO #temp1 SELECT '20', '2013-07-10' UNION ALL SELECT '10', '2013-07-10' UNION ALL SELECT '5', '2013-08-16' UNION ALL SELECT '9', '2013-08-16' SELECT STUFF((SELECT ', ' + item_no FROM #temp1 WHERE (MyDate = t2.myDate) FOR XML PATH('') ),1,2,'') AS Item_Nos ,myDate FROM #temp1 t2 GROUP BY t2.MyDate drop TABLE #temp1
pivotting is best done in the application layer, not via sql 
Connect to the MySQL database through Excel. Pivot. (Advanced level: Powerpivot) ??? Profit. 
If you just want it to auto increment, you leave the ID_Number column name out of the Insert into and select and it will auto populate the value. Also be sure to remove the set identity insert on/off declarations, you only need them if for some reason you were wanting to force your own set value into that column.
You must be confused as to which column is setup as the auto-identity column for that table. If ID_Number is really the auto-identity column, you would not be receiving that error message. That error is basically saying: "Your insert statement provides a value for an auto-identity column, don't do that." Since we didn't provide a value for ID_Number, and we still got the message, I'm thinking you need to re-inspect that table. If you really want help, open up SQL Studio. Browse out to the table in the left column. Right Click -&gt; Script Table as -&gt; CREATE To -&gt; Copy To Clipboard. Paste the "CREATE TABLE" statement here in a comment so we can all take a look.
It's a hacky way to get SQL Server to output multiple values of a column into one long concatenated string. FOR XML PATH is *supposed* to be used for outputting your column values, each wrapped in its own XML element, returned as a large single XML value. The ('') says to make that XML element a blank string, therefore "concatenate all of those rows into a single value"
As mentioned you can use FOR XML, if you want to do some additional operations on the data you can perform it similar to below; less dynamic but easier to follow/control data. SELECT t.eid, MAX( CASE rownum WHEN 1 THEN t.tag_value + '; ' ELSE '' END ) + MAX( CASE rownum WHEN 2 THEN t.tag_value + '; ' ELSE '' END ) + MAX( CASE rownum WHEN 3 THEN t.tag_value + '; ' ELSE '' END ) + MAX( CASE rownum WHEN 4 THEN t.tag_value + '; ' ELSE '' END ) + MAX( CASE rownum WHEN 5 THEN t.tag_value + '; ' ELSE '' END ) + MAX( CASE rownum WHEN 6 THEN t.tag_value + '; ' ELSE '' END ) AS tags FROM (SELECT rownum = ROW_NUMBER() OVER ( PARTITION BY a.event_id ORDER BY event_name ), a.event_id AS eid, b.tag_value FROM events AS a LEFT JOIN event_tags AS b ON a.event_id = b.event_id) AS t GROUP BY t.eid) AS f &gt; Note this example has a maximum of **SIX** values in a list, you can add additional ones if required.
Sorry just an updated I realised I was missing a bracket, but now my question has changed. &gt;SELECT t1.[Disposition_Desc], t2.[CallStartDt_d], avg(datediff(Day,t3.[CallStartDt],t2.[CallEndDt])) as [Average Call Time], count(*) as [Total Calls] FROM [TCC].[dbo].[TCCDisposition]t1 inner join [TCC].[dbo].[TCCAgentDispoDetail]t2 on t1. [Disp_Id]=t2.[Disp_Id] inner join [TCC].[dbo].[TCCCallDetail]t3 on t2. [CallId]=t3.[CallId] where t2.[CallStartDt_d] between '2013-03-01' and '2013-06-30' and [Disposition_Desc] in ('1030_Sales - New Client','1030_Sales - Online Help') &gt; Group by t1.[Disposition_Desc], t2.[CallStartDt_d] It appears that this isn't actually pulling the average call time as it returns figures such as Average Call Time 151 Total Calls 3994, I know this might be asking a bit much but can't anyone see where and if there is an error in my "logic". Neither of these figures seem correct. I doubt there were 4000 sales calls an each one lasted 2.5 mins (if that's what it's stating) 
This might not be much help, but aren't you calculating the difference in days between calls rather than the time? ie datediff(minute, t3.[CallStartDt],t2.[CallEndDt]). As for the number of sales calls, you could try counting the callid to make sure you're counting distinct id's. (select distinct ...)
Unfortunately I can't see a table where there is a "Duration" column, so I am trying to make do but taking the difference between the start and end of the call. There are two columns I can find that relate to the start/end one is CallendDt and then CallEndDt_d I believe one includes a time stamp i.e. CallEndDt 2011-08-18 07:00:04.000 whilst the other returns the date only i.e. CallStartDt_d 2011-08-18 00:00:00.000. I tried select distinct but it still doesnt seem to calculate it properly as Total Calls is still 2000. I am expecting it to be several hundred at the very most. 
T-SQL does not have an equivalent to MySQL's GROUP_CONCAT() aggregate function or Oracle's LISTAGG(). There is a widely popular FOR XML PATH('') hack, but since this behavior is both undocumented and unsupported it's not guaranteed to remain intact between versions. That is to say, it should always be considered deprecated. It would be more correct to write your own CLR aggregate function.
How about something like this SELECT t1.[Disposition_Desc], t2.[CallStartDt_d], avg(SUM(datediff(min,t3.[CallStartDt],t2.[CallEndDt]))) as [Average Call Time], count(*) as [Total Calls] FROM [TCC].[dbo].[TCCDisposition] as t1 inner join [TCC].[dbo].[TCCAgentDispoDetail] as t2 on t1.[Disp_Id] = t2.[Disp_Id] inner join [TCC].[dbo].[TCCCallDetail] as t3 on t2.[CallId] = t3.[CallId] where t2.[CallStartDt_d] between '2013-03-01' and '2013-06-30' and [Disposition_Desc] in ('1030_Sales - New Client', '1030_Sales - Online Help') Group by t1.[Disposition_Desc], t2.[CallStartDt_d] I have summed the call times to then take the average for each Disposition_Desc.
Disclaimer: Microsoft cooks all kinds of voodoo into MSAccess, so there may be a better way from an access perspective vs SQL. From SQL I'd do a sub query on each table with a conditional select statement to determine if the table contains at least one instance of your search term. Union each subquery and do an additional conditional select, ie CASE when Count(thing) &gt;= 3 foo else bar 
Your approach is simply the wrong way to address this kind of scenario. Simply create a view on the table which does the modifications you want and refer to the view. Easy.
The "inserted" virtual table can't be updated, only referenced. Try to use CREATE TRIGGER [dbo].[SetFields] ON [dbo].[tblMyTable] FOR INSERT AS BEGIN UPDATE t SET Col1 = 100, Col3 = '100_' + CAST(Col2 AS VARCHAR(50)) FROM tblMyTable AS t INNER JOIN inserted AS i ON t.primarykeycolumn = i.primarykeycolumn WHERE Col1 = 0 END Alternatevly, create an INSTEAD OF INSERT trigger (a table can only have one, and it is executed instead of doing anything else) CREATE TRIGGER [dbo].[SetFields] ON [dbo].[tblMyTable] INSTEAD OF INSERT AS BEGIN INSERT INTO tblMyTable(col1,col2,col3...) SELECT CASE WHEN i.Col1 = 0 THEN 100 else i.Col1 END ,i.Col2 ,CASE WHEN i.Col1 = 0 THEN '100_' + CAST(i.Col2 AS VARCHAR(50)) ,... FROM inserted AS i END 
I can't use a view... I'm "stepping in the middle" of 2 processes... both of which look for the original table name, so I can't change it to a view.
This is what I tried and it works like a charm. --drop table [dbo].[tblInfoManual] create table [dbo].[tblInfoManual] ( Id int identity(1,1) primary key clustered ,flngFileKey int ,flngSequence varchar(50) ,fstrRecordKey varchar(54) ,fblnActive bit ) GO CREATE TRIGGER [dbo].[SetFileKey_1] ON [dbo].[tblInfoManual] INSTEAD OF INSERT AS BEGIN INSERT INTO tblInfoManual(flngFileKey,flngSequence,fstrRecordKey,fblnActive) SELECT CASE WHEN i.flngFileKey = 0 THEN 100 ELSE i.flngFileKey END, i.flngSequence, CASE WHEN i.flngFileKey = 0 THEN '100_' + CAST(i.flngSequence AS VARCHAR(50)) ELSE i.fstrRecordKey END, fblnActive FROM inserted i END GO insert into tblInfoManual(flngFileKey,flngSequence,fstrRecordKey,fblnActive) values (33,'abc123--$$ 33','aaaaaa',0),(0,'abc123--€€ 0','bbbbbb',1) select * FROM tblInfoManual
Which DB engine?
MyISAM
you cannot "loop" in sql in fact, your table is badly in need of redesign
Isn't there like a subquerry way?
You can loop in Some databases. Which database which version of database are you using? Also plz post proper table structure, I am confused regarding what your table looks like. 
Use a LEFT JOIN with a GROUP BY instead of subquerying. &gt; And next question, how can I improve my query? Firstly there shouldn't be any need to store this in an interim table. Just work it out on the fly each time you need it, performance will be more than adequate and if it isnt, then cache it at the application layer 1) Change your schema to store results as a tinyint instead of 'win', 'loss' etc. 2) Since its only a single value, use IFNULL instead of COALESCE (fairly negligible, but more readable) 3) Decide if you are trying to count up wins and losses, or tot up the resulting points. For example you are counting wins, but then are using it to derive 3 points per win, so the column does not represent wins, but 'points from wins'. Which is fine, except... COALESCE(SUM(CASE WHEN cup_scores.result='loss' THEN 0 ELSE 0 END), 0) is always going to return 0, so you cannot tell how many losses they have had from this data. 
I don't use Access, but this might work: select project, year, sum(budget) from (Select project, 2014 year, case when budgety1t = 2014 then budget1t when budgety2t = 2014 then budget2t when budgety3t = 2014 then budget3t when budgety4t = 2014 then budget4t when budgety5t = 2014 then budget5t else 0 end budget From projects ) rollup Group By project, year ; If you have more than 5 columns, this will get very tedious. You could do something similar with self joins and/or unions, but this seemed the simplest version.
You could use cursors to loop through the records, but they are horribly inefficient. I really wouldn't use them except for one-time things like a major data extraction or load. The parent poster is 100% correct that your table needs to be redesigned. Essentially what you have now is a spreadsheet in a db table. 
Access syntax for the case statement would be nested iif statements iif(budgety1t = 2014, budget1t, iif(budgetyt = 2014, budget2t ....
Searching the unioned subquery is the way to go. SELECT COUNT([SEARCHTERMCOLUMN]) FROM ( SELECT [SEARCHTERMCOLUMN], "HARDCODEDTABLENAME" FROM TABLENAME1 GROUP BY [SEARCHTERMCOLUMN] UNION ALL SELECT [SEARCHTERMCOLUMN], "HARDCODEDTABLENAME" FROM TABLENAME2 GROUP BY [SEARCHTERMCOLUMN] UNION ALL .... ) U WHERE COUNT([SEARCHTERMCOLUMN]) &gt;= 3
Ouch. Still, if it works...
SSIS doesn't like temp tables very much, you need to define the output of the temp table or make sure that the connection remains persistent through out the workflow task. There is a RetainSameConnection property in the connection manager. if you set that to true, you should be good to go. That said I'm speculating without looking at the error output. 
you can also have the first column in the temp table be a nvarchar(max) and it (should) be able to fit everything. 
normally I do, but I thought it was "safe" to do it without because it was "the same table" that was being inserted into. I think I'll just add the columns to my inserts by default now. thanks.
Being kind of pressed for time I decided to just give up on the ssis package and instead make a sqlcmd batch file to output. Thanks for the comment though. I'll play around with that connection property and see what I can learn. 
additionally, SSIS pretty much only works well with global temp tables (i.e. ##tmp) set your connection manager to retain same connection have an execute sql task that selects your stuff into ##tmp then have a data flow that pulls from ##tmp and into your output destination set delayvalidation to true on the data flow make sure you create the global temp table first in mgmt studio so ssis can grab all the schema stuff it needs for your data flow's source you can do much the same to write data to a global tmp as a destination 
Why is it Access 'developers' always end up in this subreddit? http://www.reddit.com/r/msaccess/ is that way. First off, SQL doesn't do loops. Maybe you can loop through your dataset once it's pulled into a real programming language, but that's outside the scope of this subreddit. Second off, my god, do you know what a relational database is? You need to separate the budget totals out of the Projects table and relationally join them. I would get more specific, but I can't, because you aren't describing your issue, you're describing what you want to know. Please provide the exact table definition for Projects (Every single column, and what type of data it holds) and include exactly what each column is for. Past that, please explain what this database is being made to do; what is the data for, and how is it going to be used? At least explain the difference between budget_y1t and budget_1t. Also, don't use abbreviations in column names. It's horribly bad practice now that we no longer live in the days of less than 1 meg of RAM.
oh, I didn't think about breaking it up into tasks like that. If I could do that and then just have the data flow read it out and drop the table afterwards then that would work. Thanks.
select LEVEL from dual connect by LEVEL &lt; 6 Loop, first 5 integers in SQL, Oracle. You can loop with treewalks in Oracle and recursive CTEs in most modern RDBMS.
And start a transaction but don't commit until you are sure the answer is correct.
I'm actually trying to achieve something similar. Cognos can pull directly from SOAP to FM. You have to configure the webservice using "Virtual View Manager". Then you can access the "view" in FM. 
Woops, I'm sorry. I clearly did not read the error message that you were getting :) Glad you figured it out!
ok, so if my query is actually joining the text reference number field to 4 other tables with number reference number fields.... would i have to type that expression in the criteria 4 times for each different table? the reference number is called the same thing in every table except the table that has it in text format btw Thanks
also, when i did the following cstr([Table A]![table A reference number name]) it does not create the j oin to table 2 
Nah. You should be able to hard join your four other tables to the table that has the numeric version of it.
It won't create any visible context clues that there is a join, and I am not even sure it is technically a "JOIN" but it will operate in that fashion.
 SELECT a.*, b.* FROM FirstTable a INNER JOIN SecondTable b ON b.ReferenceNumber = cast(a.ReferenceNumber as varchar) CAST the numeric one to a string. Doing the opposite (converting the string to INT) could possibly result in it barfing.
nothing to add other than fuck Access
If you're on 2008 or higher (I'm sure it was 2008 it came in with), could the merge statement work for this function, not sure if would be faster or not tho.. You can elect to do noting on a given branch in a merge..
I think this is all you need. Double check the numbers and then delete the old columns. I'm pretty sure MySQL lets you get away with minimal grouping, however, if it complains about the groups, add all the other non-aggregate columns from the SELECT (name, dept, paygrp, etc) into the GROUP BY. SELECT Employee.EmpId, PRTran.TranDate, Employee.Name, Employee.Department, Employee.PayGrpId, PRTran.TranAmt AS "Net Pay", Employee.StdUnitRate as "Rate" , (SELECT sum(RptEarnSubjDed) FROM PRTran b WHERE (PRTran.TranDate=b.TranDate AND (PRTran.EmpId=b.EmpId AND EarnDedId='401K'))) AS "Gross Wages", SUM(CASE WHEN EarnDedId='401K' THEN RptEarnSubjDed ELSE 0 END) GrossWages, (SELECT SUM(Qty) FROM PRTran c WHERE PRTran.TranDate=c.TranDate AND PRTran.EmpId=c.EmpId) as "Hours", SUM(Qty) Hours, (SELECT SUM(Qty) FROM PRTran d WHERE PRTran.TranDate=d.TranDate AND PRTran.EmpId=d.EmpId AND EarnDedId='OT') AS "OT Hours", SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE 0 END) OTHours, (SELECT SUM(Qty) FROM PRTran e WHERE PRTran.TranDate=e.TranDate AND PRTran.EmpId=e.EmpId AND EarnDedId='OT') * Employee.StdUnitRate / 2 as "OT Extra $", (SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE 0 END) * Employee.StdUnitRate / 2 ) OTExtra, (Select ROUND((SELECT SUM(Qty) FROM PRTran d WHERE PRTran.TranDate=d.TranDate AND PRTran.EmpId=d.EmpId AND EarnDedId='OT') / (SELECT SUM(Qty) FROM PRTran c WHERE PRTran.TranDate=c.TranDate AND PRTran.EmpId=c.EmpId), 4)*100) as "% of Hours" ,ROUND(SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE 0 END)/SUM(Qty) ,4 ) *100 PctOfHours FROM BLANKED.dbo.Employee Employee JOIN BLANKED.dbo.PRTran PRTran ON PRTran.EmpId = Employee.EmpId WHERE PRTran.TranDate='2013-07-16 00:00:00' AND PRTran.TranDesc='Net Pay' GROUP BY Employee.EmpId, PRTran.TranDate ORDER BY Employee.Department, Employee.EmpId
I'm still playing with different versions of this, what I'm actually doing now is dumping the data into a raw table, then using a sp to pull it and do the comparison/inserts to the proper table. With indexing and all that it takes about a minute to run through all 85k rows of raw data.
I did end up using the stuff function and it worked like a charm.
Glad to help. I only used STUFF for simplicity/scalability. I wanted to chop of the first two chars (&lt;space&gt;&lt;comma&gt;) You could have also used SUBSTRING in this context, but not knowing the end length I always hate specifying a big number like an ass. BUT, you could have gotten away with the below. ALSO, I'm sure there was a more elegant solution to the problem. This was just my raw reaction to the thing. SELECT SUBSTRING((SELECT ', ' + item_no FROM #temp1 WHERE (MyDate = t2.myDate) FOR XML PATH('') ),2,9999) AS Item_Nos ,myDate FROM #temp1 t2 GROUP BY t2.MyDate drop TABLE #temp1
Thank you so much for replying. Ok, so I tried with this code and started getting divide by zero errors. I took out the majority of the code and just tried the following: SELECT Employee.EmpId, PRTran.TranDate, Employee.Name, Employee.Department, Employee.PayGrpId, PRTran.TranAmt AS "Net Pay", Employee.StdUnitRate as "Rate", SUM(PRTran.Qty) as "Hours" FROM BLANKED.dbo.Employee Employee JOIN BLANKED.dbo.PRTran PRTran ON PRTran.EmpId = Employee.EmpId WHERE PRTran.TranDate='2012-07-31 00:00:00' AND PRTran.TranDesc='Net Pay' GROUP BY Employee.EmpId, PRTran.TranDate, Employee.Name, Employee.Department, Employee.PayGrpId, PRTran.TranAmt, Employee.StdUnitRate, PRTran.RptEarnSubjDed, PRTran.Empid ORDER BY Employee.Department, Employee.EmpId The problem I founf is that with this SQL: SUM(PRTran.Qty) as "Hours" returns 0 (as do all the other select statements that use the case/else). In the original SQL: (SELECT SUM(Qty) FROM PRTran c WHERE PRTran.TranDate=c.TranDate AND PRTran.EmpId=c.EmpId) as "2Hours" --&gt;returns 177.33 SUM(PRTran.Qty) as "Hours" --&gt;returns 0. Any suggestions? 
Do you also get a lot more rows than you'd expect? Try removing the PRTran.TranAmt from the Select and the Group By. 
Ok, I think I finally have it. Well, not perfect, but much closer to a real SQL statement. SELECT Employee.EmpId, Employee.Name, Employee.Department, Employee.PayGrpId, SUM(CASE WHEN TranDesc='Earnings' THEN TranAmt ELSE '0' END) AS "Gross Wages", SUM(CASE WHEN TranDesc='Net Pay' THEN TranAmt ELSE '0' END) AS "Net Pay", Employee.StdUnitRate as "Rate", SUM(Qty) as "Hours", SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE '0' END) AS "OTHours", (SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE '0' END) * Employee.StdUnitRate / 2 ) as "OTExtra", ROUND(SUM(CASE WHEN EarnDedId='OT' THEN Qty ELSE 0 END)/SUM(Qty) ,4 ) *100 PctOfHours FROM BLANKED.dbo.Employee Employee JOIN BLANKED.dbo.PRTran PRTran ON PRTran.EmpId = Employee.EmpId WHERE PRTran.TranDate='2012-07-31 00:00:00' AND ((TranDesc='Earnings' or TranDesc='Net Pay') ) GROUP BY Employee.EmpId, Employee.Department, PayGrpId, Employee.Name, Employee.StdUnitRate ORDER BY Employee.EmpId Thank you for the help.
you should be able to grab a lot of information from this -- * [Comparison of different SQL implementations](http://troels.arvin.dk/db/rdbms/)
I can see value in doing this as an exercise. However when I started learning SQL, the advice I generally ran across was that it's best to pick one SQL platform initially and branch out once you have a deeper understanding, rather than try to learn multiple syntaxes simultaneously.
agreed. learning should be organic. I had very, very basic mySQL knowledge before I started my current job (around 3 years ago) now I'm about waist deep in MSSQL and branching into Oracle. If it weren't for the 3 years of MSSQL I would be completely "WTF?!" when it came to Oracle. Completely different strictly from a querying standpoint. SSMS is completed different than SQL Developer. I now find that I'm more comfortable working within command line.
This. A lot of the stuff you had on there was ANSI SQL that should be the same across the board. The real differences will come when comparing vendor specific things like t-sql and p-sql. Try to learn one enough to the point where you know what questions to ask Google when learning the next one.
It's really kind of a mess. Your CREATE TABLE statement will run only in MySQL. It's not valid for any other dbms. There are a *lot* of differences in CREATE TABLE syntax--the syntax diagram for Oracle's CREATE TABLE statement takes over 50 pages to print. Your ALTER TABLE statements are all incomplete. None of them should run; if they did, they wouldn't do anything useful. For column alias, standard SQL is select purch_amt as total from sales; That will run on all SQL dbms. Your Oracle column alias is actually a table alias. I didn't look any further.
They're not exactly supposed to run. It's meant to show the different syntax. Thank you for the input!
If you get the syntax right, the statements will run. If you get the syntax wrong, you can't compare them.
What system are you running it on now, and what were you running it on before?
Just typing this out helped me come up with an easier way to do this. Here is my basic query: SELECT update_date, colB, colC FROM thirtyMinUpdateTable WHERE colB = 'some criteria' AND update_date = (select max(update_date) from thirtyMinUpdateTable where 1=1) Thanks for being my rubber ducky!
sqlservercentral and dba.stackexchange.com are highly recommended
 select UPDATE_DATE, COLB, COLC from ( select UPDATE_DATE, COLB, COLC, dense_rank() over (order by UPDATE_DATE desc) DRNK from thirtyMinUpdateTable where COLB = 'some criteria') where DRNK = 1 Will always return the most recent data if there is data in the table.
This won't work if colB = 'some criteria' is not the most recently updated date.
Open the DDL (data definition language) script in a TOAD editor and click the "Execute as script" button.
Which one is the ddl file? I only have a dmd file and when i try executing that i get the invalid "SQL statement" error.
piece of cake in mysql, using the GROUP_CONCAT function you neglected to mention which dbms this is for
Try one of the "SQL Developer Data Modeler 3.1 - Sample DDL Scripts" on the web link you sent. DMD is a data modeller file.
In SQL Server, use CONVERT instead of CAST A cast statement is dependant on the locality settings on your OS, the CONVERT isn't. e.g. Locality settings are for Europe CAST('06/15/2015' AS datetime) will throw an error as it is looking for the sixth day of the fifteenth month. CONVERT(datetime, '06/15/2015', 103) won't throw an error, as the 103 datetime data type is for European dates and will swap the 6 and 15. I hope I'm making sense.
The easiest/most dynamic SQL Server equivalent to GROUP_CONCAT is to use [FOR XML](http://stackoverflow.com/questions/451415/simulating-group-concat-mysql-function-in-microsoft-sql-server-2005). SELECT table_name, LEFT(column_names , LEN(column_names )-1) AS column_names FROM information_schema.columns AS extern CROSS APPLY ( SELECT column_name + ',' FROM information_schema.columns AS intern WHERE extern.table_name = intern.table_name FOR XML PATH('') ) pre_trimmed (column_names) GROUP BY table_name, column_names
The most efficient way to implement this will depend on your DBMS. That being said, using triggers and an audit table is probably the only way to go with this. With certain DMBS you can identify recently updated records, but I am not aware of any that provide column level granularity. 
so what do you know at the time the stored procedure is executed? Do you have the updated record and primary key? If you have the new record, is it coming in as a record type param or one param for every field? Also, Oracle? SQL server? mysql?
This is exactly the type of thing have been looking for. Much thanks!
Will do! Thank you for the suggestion!
Thank you for this. I am so confused because I keep finding conflicting sources.
record type. It's MSSQL
 SELECT pk_object, MAX(ship_date) FROM Your_Table_Here GROUP BY pk_object
Well, if your existing SQL server dies you would have a failover for the remaining 20DB's. Add in log shipping or a cluster and you have some decent redundancy. If you have the budget it can add some redundancy. However it sounds like the new app would be fine with it's DB running on the existing SQL server.
also, how do you want to handle the situation when the field is updated to null?
I take issue with your comment that running two SQL Servers on the same box is bad! Multi-instance installations are perfectly fine, they would only cause a problem if you failed to configure them properly.
This is the statement I'm having issues with: SELECT Max([order_ups_delivered_time])as dtime, [order_ups_status], [order_ups_tracking], [cart_item_id], [order_ups_dock_received] from (cart_items INNER JOIN orders ON cart_items.order_id = orders.order_id) where ([order_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date()) group by cart_item_id UNION ALL select max([shipment_ups_delivered_time]), [shipment_ups_status], [shipment_tracking_code], [cart_item_id], [shipment_ups_dock_received] from (cart_items INNER JOIN shipments ON cart_items.shipment_id = shipments.shipment_id) where ([shipment_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date()) Group by cart_item_id; 
Reasons that I've come across for having a seperate instance: *Application requires a different server collation *Segregation of sysadmin groups (eg. one dept is admin for dbs on one instance etc ...) *Application supported only on one particular version / edition / patch level (eg. not the current one) *Any other particular config option that would impact your existing databases *Ease of administration. If things are in seperate instances it's easy to manage them performance wise (you could also use resource governer if you have it) 
I'll set default values for all fields, and he can only change it according to validation, null is not applicable.
I'll go with something similar. I'll create table which will behave as audit and have additional field (besides field name, field value) which indicates if current value is the one which is applied.
You could do something like this: &gt; You don't need to do a union all, don't need any redundant information ;WITH tempu(dtime, order_status, order_tracking, item_id, dock_received) AS (SELECT Max([order_ups_delivered_time])as dtime, [order_ups_status], [order_ups_tracking], [cart_item_id], [order_ups_dock_received] FROM (cart_items INNER JOIN orders ON cart_items.order_id = orders.order_id) WHERE ([order_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date()) UNION SELECT [shipment_ups_delivered_time], [shipment_ups_status], [shipment_tracking_code], [cart_item_id], [shipment_ups_dock_received] FROM (cart_items INNER JOIN shipments ON cart_items.shipment_id = shipments.shipment_id) WHERE ([shipment_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date()) ) SELECT info.order_status, info.order_tracking, info.item_id, info.dock_received, maxer.dtime FROM tempu AS info INNER JOIN (SELECT item_id, MAX(dtime) as dtime FROM tempu GROUP BY item_id) AS maxer ON info.item_id = maxer.item_id AND info.dtime = maxer.dtime 
Problem I'm encountering is that it stops at order_status and says that I did not include it in an aggregate function.
Are you sure you are doing it correctly? You can't have the order_status in the MAX / GROUP BY query; why I separated them in the above. I tried one very similar to the one I listed just above on a test realm and it works like expected.
To try and flesh it out more appropriately. I have unique objects cart_item_ids. Depending on circumstances they have ups_tracking numbers either from the orders table or the shipments table (hilariously terrible I know.) That then has a delivery and dock time potentially. So in my query I'm getting some cart_item_IDs have two tracking numbers so within each half of the union I will get duplicates. There are no cross union duplicates.
You don't run separate copies of SQL, you run additional databases on the one copy. It makes no sense to ever put two copies of SQL on the server. And from a licensing standpoint it's cheaper to upgrade the server to handle the additional load in pretty much all cases.
Great reply!
&gt; It makes no sense to ever put two copies of SQL on the server. It's quite common to have multiple cluster instances in a windows fail over cluster running on 1, 2 node, or more nodes.
No, you can absolutely run multiple instances of SQL Server on the same box, no problems whatsoever. 
I think one of the key things that often gets over looked in a multi-instance environment is assigning memory limits to the sql instances. 
I looked for literature describing a single host with multiple sql instances that can fail to each other, and I couldn't find anything. It doesn't seem possible as the failover cluster architecture operates at layer 3, not layer 7. I get that you might want to install different versions side by side on a single host though, for compatibility purposes. I would give some grief before taking that route though. 
EDIT: this link (http://pastebin.com/DVduSLxp) kept my formatting, so spare your eyes and skip the rest of the comment. Let me write back to you what I think your situation is before I give you my idea of the best solution. You have a list of Order Numbers and sometimes there are multiple line items under the same Order Number. Because of this your table will have the same Order Number twice and different shipping dates. I recommend using this. It basically takes the query you have now, and uses that as your FROM table, but sorts by OrderNumber and then by ShippingDate and creates a new field that numbers the rows by OrderNumber. SELECT OrderNumber, ShippingDate FROM ( SELECT OrderNumber, ShippingDate, ROW_NUMBER() OVER(PARTITION BY OrderNumber ORDER BY OrderNumber, ShippingDate DESC) AS RowNumber FROM OrderLine l INNER JOIN OrderStatusCodes s ON l.LineStatus = s.StatusCode WHERE OrderNumber IN () ) AS TempTable1 WHERE RowNumber = 1 So you would get something like this as your TempTable1: OrderNumber ShippingDate Row_Number 111111 7-1-2013 1 111111 6-15-2013 2 111111 5-24-2013 3 222222 7-2-2013 1 333333 6-20-2013 1 333333 5-1-2013 2 And your where clause is going to only return records with the highest ship date. EDIT: it looked very pretty before saving the comment :(
I know two bits of stuff all about XML, but I'm not seeing how querying the table structure through the information_schema tables will give me the data I want
I was going to suggest this method as well. I have something similar in which I had to use a CTE and row_number Worked like a charm
Regardless of whether this will be the final most efficient method I use I very much appreciate this response and will try to get this query to run to see which will function the best and to teach myself more. Thank you for your help this is a great way of thinking about it. I've been running very raw SQL statements and then modifying my outputs in excel for the last 24 months so I'm trying to cut back on front side calculations wherever possible and thinking in this next step is a great growth experience. 
I ended up doing this below statement but it's breaking on the FROM somewhere. I took many of your concepts and have been attempting to implement again. I didn't fully grasp that I could only make a group by function properly if it was running a function on everything but one field. SELECT dtime, Tracking, [cart_item_id] AS [Item ID], [order_ups_dock_received] AS [Dock Received Time] FROM( SELECT (max(order_ups_delivered_time) AS dtime, order_ups_tracking as Tracking FROM orders WHERE ([order_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date())) as ta FROM ((cart_items INNER JOIN orders ON cart_items.order_id = orders.order_id) INNER JOIN ta on orders.order_ups_tracking = Tracking) UNION ALL SELECT dtimeb, Trackingb, [cart_item_id] AS [Item ID], [shipment_ups_dock_received] AS [Dock Received Time] FROM( SELECT (max([shipment_ups_delivered_time) AS dtimeb, shipment_tracking_code as Trackingb FROM shipments WHERE ([shipment_ups_delivered_time] Between DateAdd("d",-120,Date()) and Date())) as tb FROM ((cart_items INNER JOIN shipments ON cart_items.shipment_id = shipments.shipment_id) INNER JOIN tb on shipment.shipment_tracking_code = Trackingb);
I believe the query was written on 2008 and I now run it on 2012. We ran it on '12 w/o an issue on another computer. Though he did have 08 installed, which I do not. Not sure if that makes a difference. 
I do use the CONVERT command The syntax roughly goes as: convert(Datetime, [db.DATEDIM]) AS WE_DATE Not sure if I used the right brackets here, but we are trying to convert a field from one database from text to date. Again, the syntax works flawlessly on other machines, just not my own.
I'm not doing your homework for you, but when you create a table, you generally want to a specify the column names and types. 
Yeah! How do I do that? I've been trying to figure that out. Do I put that after CREATE TABLE table_name (column names and data types go here?) 
http://www.w3schools.com/sql/
I found a list of things that can cause that error to pop up. They also include a workaround that might help you out in this situation. http://www.sql-server-helper.com/error-messages/msg-241.aspx Check it out. Hope it helps.
You might want to look into spiceworks. It's free and it will do pretty much exactly what you're looking for (and more). 
number of milliseconds since 1970-01-01 00:00:00
Use Spiceworks (free) or PDQ Inventory for this. Both have a database you can query should you want to.
oh, wow... that's so deep, man
Or, use lansweeper, it's free and you host it yourself,
Its number of seconds, not milliseconds
wanna bet? :)
You basically want three tables: machines (machine_id, hostnamw, maybe installed ram or whatever), software (software_id, name, version, purchase_dare, etc) and machine_installed_software (machine_id, software_id, license_key). Then you can do a join between the tables for counts and which machine has what on it. 
i was able to run this against master, but soon as i try on my actual DB, i get incorrect syntax near '(' for the line containing (db_id('database_name') 
This looks like SQL Server? In which case you can call OBJECT_NAME (object_id [, database_id ]) Also note you should try to get what you need from the INFORMATION_SCHEMA views rather than system tables, where possible
I'm not sure what the size of your architecture / network are; but if the majority are running Windows you might want to look into using System Center Configuration Manager(*SCCM*) to do this; it's meant for managing deployments / etc but it also does a good job of keeping track of software licenses and updates.
Depends on what data you want from the GAMES table. Let's say you only wanted the game_title, game_esrb_rating columns, you could technically do a JOIN and add those two columns. SELECT g.game_title, g.game_esrb_rating, r.review_title, r.review_rating, r.review_body FROM reviews r JOIN games g ON g.game_id = r.game_id WHERE g.game_id = {$page_id} You're then returned a dataset that contains the columns game_title, game_esrb_rating, review_title, review_rating, review_body. Otherwise, you would have to query both tables separately.
that would return only those games that have a review use a LEFT JOIN (games as the left table), and you will get all games, plus their reviews if any
Since the odds are you are not using sqlite, here's a SQL server solution. I think this will work in MySQL, too. select T.Fruit from Test as T order by substring(T.Fruit,len(T.Fruit)-6,6)
+1 At the very least, divide that 'column' into two: color and fruit and prevent headache you are having and headache chunkyks mentions... 
dont worry, i knew better to change that ;). what i found out, when i copy paste, the characters don't always copy over correctly. fixed by manually typing out the statement
So, what platform are you trying to do this with? SQL comes in different dialects.
Something like this (untested).... select c.CLASSID, count(a.STUDENTID) as STUDENTS, count(case when a.STATUS is null then 1 else null end) as "NULL", count(case when a.STATUS = 'FAILED' then 1 else null end) as FAILED, count(case when a.STATUS = 'DONE' then 1 else null end) as DONE from TABLE1 c join TABLE2 s on s.CLASSID = c.CLASSID join TABLE3 a on a.STUDENTID = s.STUDENTID where c.CLASSID in (1,2) -- Remove this where clause if class filtering not required group by c.CLASSID
for all those of you submitting solutions which sort on only the last 6 characters, you are gonna mess up when the results contain both tomatoes and potatoes mysql has a wonderful function that makes it trivial... ORDER BY SUBSTRING_INDEX(fruitname,' ',-1) this splits the string using a space delimiter, and sorts on the last complete word, regardless of length 
the view is better, since your function would have an extra call to the database using the id instead of the length function just operating on the name don't forget to assign a **column alias** to LEN(lastname) in the view just curious, but why do you need the length of someone's last name? this is the type of thing that should be handled in the application layer when the raw data is retrieved by the query
Thank you very much. I think I should clarify. I used the length of the last name as an example, but in reality what I am calculating is three more complicated values. In one case it's actually the person's department, depending on a recursive hierarchy tree that crawls up to one of 5 people (department managers) (I already have that hierarchy as a view, all I need is to match the top boss with the dept name); in another case it's whether the person is a regular or contingent worker, which depends on a record of another table (which has several records per employee) that satisfies certain characteristics. There's another case in which there's a table of working history (datestart-dateend) and I account for the total number of years worked for the company, for which I could also use either a view or a function, so that I add that number to a bigger view. Would that change your answer regarding the number of calls?
you lost me right after the word "crawls"
The performance hit is [or should be] totally irrelevant: the reason to do it correctly is to be sure your data is correct, and the fact it's faster is just a side benefit.
you titled your post [MY SQL] but then your sample schema uses IDENTITY and you mention CROSS APPLY, both of which are SQL Server which is it? actually, never mind, this works in both -- SELECT e.name , MAX(CASE WHEN o.type='E' THEN o.name END) AS ceo_name , MAX(CASE WHEN o.type='F' THEN o.name END) AS cfo_name , MAX(CASE WHEN o.type='O' THEN o.name END) AS coo_name FROM employer AS e LEFT OUTER JOIN officer AS o ON o.emp_id = e.id GROUP BY e.name it has the advantage also of working when an employer does not have all three officers
Learn-stage exercises which can be solved under Oracle as well: http://sql-ex.ru/exercises/index.php?act=learn
Thank you so very much! That example helped me a lot i can't thank you enough.
Try to avoid using a scalar function since it will perform row by row ops on the dataset (ie poor performance) The question to ask is whether the logic of this calc would be used in different places. If it isn't, then you may aswell code it straight into your existing view. If you want to use this logic from a few views, then to avoid replicating the logic everywhere, consider using a function which returns a table (TVF), and join onto that it from your view. This way it can still accept parameters to customise it, but performance will be much better than an equivalent scalar UDF
If it were TVF or CTR vs View it might be a tougher argument, however, definitely view better than scalar function. Scalar function is RBAR (row by agonizing row).
Are the dates an actual date field in MySQL or just a VARCHAR? 
 SELECT SALES_T.[DATE], SALES_T.SALES, LY_SALES_T.SALES FROM TABLE1 AS SALES_T, TABLE1 AS LY_SALES_T WHERE RIGHT(SALES_T.[DATE],4) = RIGHT(LY_SALES_T.[DATE],4) AND LEFT(SALES_T.[DATE],4) = LEFT(LY_SALES_T.[DATE],4)-1 EDIT:Fixed from 1-Left to LEFT(x)-1
There are a whole bunch of other columns such as store/subsidiary number in the table, but I won't be using any of them in this specific query. 
it's only elegant insofar as the string functions RIGHT and LEFT to pull out the yyyy and mmdd portions were pretty much forced based on your use of VARCHAR for dates note that this elegance disappears if you wanted to find out sales compared to the same day last week, because then it has to account for month boundaries and use actual date arithmetic by the way, your use of square brackets means this is ~not~ mysql
Yes, but my point being that correctness here would obviate the calculation 
well it probably was, in fact *his* SQL server haha ;) check out this article http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html and it explains to you what joins are. in theory if you want to return all officers from a company you just need to do either a left outer join on the company ID or do an inner join on the officer's company id on the company id (see if you can see the difference there). all of these operations that you want to do require proper structure and etc with keys properly assigned.
Great, thanks. My "main" view has a lot of fields and tables, and since these calculations require themselves some joins or sub-queries, that's why I intended to put them on their own, separate place (SVF or V). I'm not sure I could use TVFs since the value is calculated individually for each employee (i.e. the only parameter I need is the employeeID), so I'd end up with a simple view then. So that brings another question: is it bad that I use views that use other views that use other views, instead of puting all the calculations in one view? I suppose it is bad if I only need a few records of my "final" view and I'm anyway calculating stuff for the first ones, but in reality I will almost always be using the whole set of records, or maybe a big subset, and the main table is only about 2,000 records (would grow up to 10,000 in a very remote case)
Thanks! That's all I needed to ~~hear~~^read . 
What you are trying to do is called a pivot table. r3pr0b8's solution is good because it works cross-platform, but since you are using SQL Server you could take advantage of the PIVOT syntax: SELECT name, E ceo_name, F cfo_name, O coo_name FROM (SELECT e.name, o.type, o.name oname FROM employer e LEFT OUTER JOIN officer o ON e.id = o.emp_id) d PIVOT (MAX(oname) FOR type IN (E,F,O)) p
&gt; So that brings another question: is it bad that I use views that use other views that use other views When sql server accesses a view, it works similar to as if you had dumped the view definition into your query, and joined onto it. This makes it convenient, and performance will not suffer for it in many cases. But if you start nesting views heavily, it does make it more difficult to troubleshoot any performance issues when they do arise. My advice is to keep your views quite specific in their purpose to minimise the amount of work sql has to do to expand the view. Also try to avoid any subqueries in the view if possible, and keep everything 'set-based' (meaning use joins rather than calling scalar udf's over each row). There is one other performance issue with nested views. SQL Server allows you to index a view, in which case it is no longer expanded out as if you'd copy/pasted it into your query. Instead the view is materialised into the database similar to a normal table. This can be useful, but you cannot index a view if it is referencing subviews (this is true of sql server 2000, perhaps its changed but I don't believe so). Having said that, for the sake of 10000 records, I cant imagine using views or even nested views will cause a huge issue
Great! Good to hear that. It seems my intuition led me to the right path all along. Thank you very much for the clear explanation.
This is really quick and can probably be done in a better way, but I like beer. SELECT * INTO #temp FROM ( SELECT 1 AS Item_ID ,'Blue' AS Tag, '' AS [Rank] UNION SELECT 1, 'Warm','' AS [Rank] UNION SELECT 2, 'Green','' AS [Rank] UNION SELECT 3, 'Coarse','' AS [Rank] UNION SELECT 2, 'Fine','' AS [Rank] )t UPDATE #temp SET [Rank] = Ranking FROM (SELECT * ,RANK() OVER(PARTITION BY ITEM_ID ORDER BY ITEM_ID, TAG) AS [Ranking] FROM #temp) t WHERE #temp.Item_ID = t.Item_ID AND #temp.Tag = t.Tag SELECT * INTO #temp2 FROM (SELECT Item_ID ,CASE WHEN [Rank] = 1 THEN Tag ELSE NULL END Tag1 ,CASE WHEN [Rank] = 2 THEN Tag ELSE NULL END Tag2 ,CASE WHEN [Rank] = 3 THEN Tag ELSE NULL END Tag3 ,CASE WHEN [Rank] = 4 THEN Tag ELSE NULL END Tag4 FROM #temp)t2 SELECT DISTINCT t2.Item_ID ,tag1.Tag1 ,tag2.Tag2 ,tag3.Tag3 ,tag4.Tag4 FROM #temp2 t2 OUTER APPLY (SELECT * FROM #temp2 t3 WHERE t2.Item_ID = t3.Item_ID AND Tag1 IS NOT NULL)tag1 OUTER APPLY (SELECT * FROM #temp2 t4 WHERE t2.Item_ID = t4.Item_ID AND Tag2 IS NOT NULL)tag2 OUTER APPLY (SELECT * FROM #temp2 t5 WHERE t2.Item_ID = t5.Item_ID AND Tag3 IS NOT NULL)tag3 OUTER APPLY (SELECT * FROM #temp2 t6 WHERE t2.Item_ID = t6.Item_ID AND Tag4 IS NOT NULL)tag4
Flattening data sucks when you don't have something to aggregate by or if you need to keep column names, otherwise it's just easier to use FOR XML or pivot.
 with data as ( select row_number() over (partition by item_id) ranking, item_id, tag from your_table ) select item_id, [1] as TAG1, [2] as TAG2, [3] as TAG3, [4] as TAG4 from (select ranking, item_id, tag from data) as dataset pivot (max(tag) for ranking in ([1], [2], [3], [4])) as pivot Use the windowing function *row_number() over* to generate a unique rank for tags for each *item_id* Then just *pivot* using the aggregator *max()* 
once again, unnecessary parentheses are unnecessary if i understand what you're trying to do, this will do it -- WHERE trgtskrun_crtdt &lt; @nextTime AND trgtskrun_crtdt &gt; @lowerTime AND ( trgmst_id = 1475 AND tskmst_id = 1578 OR trgmst_id = 1476 AND tskmst_id = 1579 ) since ANDs take precedence over ORs, my rule of thumb is to parenthesize only the ORs in a situation like this
I've done that, and agreed on the parentheses. even going how you posted it still doesn't grab all the info. Anyways, I've found another way to do it that is much simpler :)
Don't be that guy. Please post your solution for the next person that might think they have this issue.
Are you going back and updating, or are you going to fix the entering process to make the assignment when the record is being generated? Our Support center has a round robin based on shifts (we have 3 offices in different time-zones) and cases per hour. A new ticket cannot be created without assignment. Techs can assign to themselves, or pick from a list weighted by the round robin algorithm.
Our company is using ManageEngine and it's underlying MySQL database. It works pretty well for physical and virtual asset tracking. It also has a IT ticketing system for tracking HelpDesk requests. The real advantage is the worm you install on every machine and it reports back what OS and what is installed on it. That way you can poll your infrastructure IP list and know you've got every machine that is logged in. We had a huge MS TrueUp and it worked great.
It's not really a problem with the entering process. This process is really taking records that had incorrect assignments and then trying to reassign them evenly over a team of people.
Why not just sort the records counts per person ascending then keep assigning TOP 1 of the blank records to the first person. In your case it would look like: *** &gt; Start Joe, 3 Nick, 5 &gt; Step 1 (4 blank records remain) Joe, 4 Nick, 5 &gt; Step 2 (3 blank records remain) Joe, 5 Nick, 5 &gt; Step 3 (2 blank records remain) Joe, 6 Nick, 5 &gt; Step 4 (1 blank record remain) Nick, 6 Joe, 6 &gt; Step 5 - END (0 blank records remain) Joe, 7 Nick, 6 *** Alternatively you could perform the same process except calculate an average record for all employees then order by the highest difference from the average on the low side.
have you just tried ranking them in ascending order? The person with the fewest jobs would get jobs assigned until they have one more job than the next guy and then they get bumped down the list. 
there's no solution really I was approaching it wrong. I ended up doing a select into the temp table with all the data and then running updates on that temp table instead of using scalar values to generate the info on the fly.
Query might look something like... WHILE ((SELECT count(*) FROM dbo.PersonAssigned WHERE idTech IS NULL) &gt; 0) BEGIN UPDATE PersonAssigned(...etc..) SET idTech = (SELECT idTech FROM dbo.View_LeastAssigned) --Define View_LeastAssigned as SELECT TOP 1 idTech FROM dbo.PersonAssigned GROUP BY idTech, ORDER BY count(idTech) END 
Ok, that's the exact workaround I was thinking, initially, just wondered if the PIVOT operator would be able to handle it by itself. Seems like an obvious use-case so that that's why I was feeling a bit dumb. Thanks, I'll just do this then.
Yep, sometimes you have to do it the hard way.
I had explored that as an option initially, but keep in mind that the actual query utilizes 100 pivot columns. PIVOT seemed the more efficient choice.
Boom! :D
PIVOT would only be an efficient option if your data was already aggregated into the ranges you require. Since you will have to define all of the ranges somewhere, I would do so in CASE statements as outlined above. This solution also does not force your ranges to be mutually exclusive like HollowImage's does.
If you find yourself doing more than the occasional oddball pivot query I highly suggest moving that data into SSAS. Trust me, you'll thank me later.
I have to ask, SSAS? Google says ["SQL Server Analysis Services."](http://msdn.microsoft.com/en-us/library/ms170208\(v=sql.105\).aspx) Just want to make sure I'm on the right track before I begin researching anything.
Yes, if you build a SSAS cube you can slice the data however you want, create your own aggregations and calculations, and then create Pivot Tables in Excel. If you are not licensed for SQL Server Enterprise though, it may not even be worth exploring. 
Ah ok. This is probably more than what's necessary. Currently out of about 500 SPs in use only one other one uses a pivot.
If I understand correctly, maybe the below might help if you choose to look into an SSIS solution more. Make a DataFlow task and inside create two (or more) DB Sources to pull the specific information that you would normally feed into a temp table. Now that your data is loaded with the source DB connections use a UNION ALL transformation to bring the sets together (you now have your temptable data at this point, without the actual table). You would then link the output of the UNION ALL to a Flat File destination. From here you can add variables and SQL Tasks (with Result Sets to populate said variables) to set various parameters/expressions (file/directory names for instance).
do you actually want to update the rows in tabeA? in that case, you ~must~ have a column that the tables can be joined on database tables don't work like excel sheets, you cannot just "add a new column" by pasting a column of data from somewhere else, because there is no sequence to the rows in a relational table -- you have to have a way to identify which row gets which value also, please mention which dbms you're using
Yeah I would wonder what kind of data is the column that is being appended, such that it doesn't matter which rows the values are added to.
ah sorry. Using MSSQL. Yes I want to update the rows. Basically I have 3 columns in TableB. I want to then insert a new column into TableB with the results from the query in TableA.
ok I did up to here - UPDATE target SET ............ FROM TableB The join I can't do as it's actually 2 temp tables right now trying to get the syntax down so there's no keys whatsoever. The problem is that (and here's where it gets confusing) the PK value I'm using in TableA is different than in TableB but they're from the same column. Think of it in that I have 1 task and 2 subtasks but I need the date from the 3rd subtask to be associated with task #1. I couldn't figure out how to do this with the initial select because how do you - SELECT key, datetime (from 3rd unassociated key) FROM TableA WHERE xlksjdlksjdf Unless I was to do a table variable maybe? hmmm... maybe should rethink how I'm doing it? Anyways, the query you posted minus the PK is successfull but out of the 2000+ DATETIME entries it's just copying the 1st one through every entry so the entire column is the same value instead of 2000+ different ones.
in both tables I have a column called "columnx" TableA contains what is in "columnx" In Table B I have tried creating one full of NULL and UPDATE (doesn't work) OR In TableB I have left no corresponding columnx and tried doing INSERT INTO (doesn't work)
so basically you're saying that there is no data in those tables which will allow you to match which rows go with which rows, in order for the new data to be copied over in an update here's a suggestion: export the tables to excel, and do your copy/paste there, then import the updated table 
well then I could easily just make a PK counting 1 - x for both tables and join that and then delete the column after? I'm going to try that :)
goddamnit though, it works to a point. The column I'm using being datetime seems it's auto sorts it regardless of using a ROW_COUNT or not. I've done the following - CREATE TABLE #TEMP (pkey INT IDENTITY(1,1), datetime DATETIME) INSERT INTO #TEMP SELECT datetime from TABLEb when I then SELECT * FROM #TEMP The order of the datetime is oldest date = row 1 which isn't what I wanted. damn back to the drawing board. Seems any insert into a table with a auto increment key is auto sorting the results. fuckers. I think the only bet I have is to try to figure out how to associate the existing ID with the one 3 levels down somehow. Not sure how I'll do that but I'll think of something. That's the only guaranteed association for this.
&gt; Seems any insert into a table with a auto increment key is auto sorting the results nope sequence is determined by the order that the rows are retrieved by the SELECT portion of the INSERT/SELECT notice, you omitted the ORDER BY clause
I can email you my notes, slides, and textbook from last semester :) Shoot me a PM
Hmm, haven't really looked in to MCSA or prep classes. I'm pretty sure it's not required for any position with my company, so I don't know if something that expensive would be worth it. Is this a common requirement employers seek for SQL experts? 
The stuff I used it for was getting started with SSIS (Visual Studio) - I paid $49 for two months, got all the training materials and completed all the courses. I went from new guy to, literally, leading candidate for promotion in 9 months.
Personally I like the Joes 2 Pros books so I am inclined to suggest; Beginning SQL Joes 2 Pros: The SQL Hands-On Guide for Beginners &amp; SQL Queries Joes 2 Pros: SQL Query Techniques For Microsoft SQL Server 2008, Volume 2 (Sql Exam Prep) also Microsoft SQL Server 2008 Bible &amp; http://www.amazon.com/SQL-Server-Developer-Edition-2012/dp/B007RFXQAM/ref=sr_1_1?ie=UTF8&amp;qid=1375426344&amp;sr=8-1&amp;keywords=microsoft+sql+server+developer+edition Also this; http://blog.sqlauthority.com/ SQL Server developer edition is great because you get a full version of Microsoft SQL Server at a fraction of the cost. I hope this helps. 
I found the Oreilly Head First SQL book to be great in teaching me SQL for my job. it starts you with recipe cards and moves you into basic database creation then onto queries and security and administration, all while being *very* accessible. I'm pretty sure Oreilly or Amazon have an excerpt you can read to get an idea. Outside of that, W3schools was a big help and speaking with other SQL users I work with (report writers, dba's, data analysts). The biggest hurdle I had was learning the data model of our enterprise-wide database. learn to read a data model and get very familiar with what you'll be working in. the SQL queries will almost write themselves.
That's on my to do list to. Do you know if they will have another enrollment where you can receive a statement of completion? It looks like right now you can only go through the material. 
Cool, I know a little bit about stored procedures. just googled dynamic querying, looks like pretty cool stuff. Thank you for the help!
Wow, that's awesome! Looks like ssis and ssrs will be the next stuff I'm learning at work. I'll look in to this site, maybe it'll give a me jump start. Thanks for the help!!
No, it's not common. Certs are generally required for entry level jobs, but aren't often looked at beyond that. They can be useful for getting raises within a company that wants them, though. They can also be good if your resume looks exactly like someone else's, but you have a cert. Where certs ARE very useful is as a framework for learning a subject. The study material is usually a well ordered, well structured, way of chewing through large topics. I would not suggest paying for a prep course for them out of pocket though, they tend to be too much information to process too fast. If your employer is footing the bill, they can be a good way to network and get insight from experienced trainers, though. 
I'm not positive, but I'm sure you can ask them. 
If you want to stay within the SQL code area then most likely stored procedures and Agent jobs would be the next step. [SQLServerCentral.com](http://www.sqlservercentral.com/) is a great resource for these things. If you want to move towards the "servers" then you're talking about DB architecture and design. This area is very different than writing SQL code. DB design is more about data relationships, understanding triggers, foreign key concepts, etc. Beyond that is DB management. Here you're talking about Backups, Replication, Transaction monitoring, load balancing etc.
Read this book: http://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802 Seriously.
Reporting, particularly ad hoc reporting, is a great way to start learning SQL, because you get a chance to try different things out. That's how I started, and today I am a hardcore DBA with many years experience. I learned a shit-ton about SQL while writing reports. In most cases you will be working with a database that is poorly designed at best (at best! Many are not designed at all, but are merely accretions of barely-related bits.). The cool thing is that these are the best sorts of databases to learn to write SQL on, because with a shitty database you can explore ways to make queries run better. With some practice you should be able to manage 2 orders of magnitude improvement pretty regularly, and you should occasionally get 3 or maybe 4. When you do this people will think you are a fucking magician. The way I did this is simple, but requires dedication: look at the plan for every query you write, and see if you can find a way to improve it. Every query. Some you won't be able to improve. (SELECT column FROM table WHERE id=somevalue probably can't be improved much, most times.) Read SQL sites, ask specific questions, and just keep working at it. As you go you will pick up techniques and patterns. As you pick up techniques and patterns your understanding of how a relational database works will grow.
BI is a good way to get paid. I love SSIS and SSRS, still need to learn SSAS but those jobs are few and far between. 
Everything by Joe Celko.
Data warehousing as an introduction to SQL? That seems rather a complicated jumping-off point.
Oh yes, and the T-SQL introductory books by Itzik Ben-Gan are simply the best money can buy. Thorough, graduated, easy to understand...the best of the best.
ALternately (and if you're using Microsoft SQL Server), there's the REVERSE function: http://technet.microsoft.com/en-us/library/ms180040.aspx
I don't think this what you want. The two fields are a combination. Further nested parenthesis is needed or ( (tskid = whatev and tsktrgt = whatev) or (tskid = whatev2 and tsktrgt = whatev2)) should be in the or statement. 
Right on man. That's what I ended up doing. Thanks for the reply. You're awesome
Sounds like you're on the right track
Sorry, I tired to format it and failed. **edit Failing miserably at posting this sorry. PL/SQL
Not to nit-pick too much, but avoid using "when others" whenever you can. It's a catch-all that can bite you (ie: to_many_rows exception). I would use 'when no_data_found', or define the exceptions you expect. 
Thank you kind sir. I've seen Tom Kyte mention this on several of his posts too. I think the reason I use "when others" is because I don't have to explicitly mention all the possible exceptions that could happen. And since I always capture the sqlerrm, I (later) know what caused the exception. I know I should change this habit :( 
 Put 4 spaces before each line you want to format as code. (This and other formatting instructions are in the "formatting help" links beneath comment boxes and self-post text boxes.)
It's simply this: The APPLY operator allows you to invoke a table-valued function for each row returned by an outer table expression of a query. Without this function: if you had a table-valued function to run against a second table, you'd have to use a while loop, cursor, or some other RBAR mechanism. 
If you want a variable number of columns in your pivot this is the only way as far as I've found a solution. The first select @cols "collects" the columns you want to pivot on. The second statement makes the actual query. This is just a copy-paste of what I made (leaving out the business name). declare @cols nvarchar(max), @stmt nvarchar(max) select @cols = isnull(@cols + ', ', '') + '[' + T.[From Date] + ']' from (SELECT distinct convert(varchar, [From Date], 102) as [from date] FROM [Employee Absence] WHERE DATEPART(yyyy, [From Date]) = DATEPART(yyyy, GETDATE()) AND DATEPART(mm, [From Date]) IN (7, 8) AND [Cause of Absence Code] = 'vacation') as T select @stmt = ' select P.* FROM ( SELECT DISTINCT convert(varchar, [From Date], 102) as [From date], [Employee No_], try_convert(int, Quantity) as quantity FROM [Employee Absence] WHERE (DATEPART(yyyy, [From Date]) = DATEPART(yyyy, GETDATE())) AND (DATEPART(mm, [From Date]) IN (7, 8)) AND [Cause of Absence Code] = ''vacation'' ) as T PIVOT ( sum(T.quantity) FOR T.[From Date] IN (' + @cols + ') ) AS p order by P.[Employee No_]' exec sp_executesql @stmt = @stmt
Re-written this response a few times. Yes, if you capture, log and raise (and re-log and re-raise) the error all the way back through to the start, it works, somewhat. The big problem with implementing that way is that it is so easy to do it incorrectly, or for someone else to come in after, copy how without understanding the why. Honestly, the more I think about it, the more I realize why Tom Kyte calls it a bug. It's a simple way for a junior dba/dev to say "I'll catch everything with this, and deal with it higher up", and an experienced dev/dba to say "I know what exceptions to expect, so I can deal with them higher up". 
&gt; I am trying to avoid multiple lines of data why?
Im on my phone so cant type it out. But i think you want unpivot.
If Mark Smith is assigned systems Windows and Mac I want to see that on 1 line, not two lines of data. Sorry if I was not clear 
I can't change the data model. Is there any other way to achieve this?
You can do this by pivoting the results, but the syntax differs according to the db software. What are you using?
Can you double check that link? It's taking me to a page that says &gt;All material from this website has been merged into www.db-class.org. The mentioned link does not work.
I mean, if you brute force every possible value, you could use CASE. CASE WHEN ColumnName = 'foo' then ColumnName END as foo
In the perfect coding world, I agree completely. If you have a very tightly controlled package flow then this can be differed to a much higher level as you know it is going to be caught eventually. I find that is not the case at the overwhelming majority of places I have consulted for. It is not necessary to re-capture, re-log, and re-raise as long as it is captured once, logged, and raised. Yes the raise will change the line number, however, the original error is already captured and logged with the call stack and any important information that is needed. At that point you did your do diligence, you persisted the real error and then caused the error to be propagated to the callee so it is handled however the callee thinks it should or shouldn't be. If the callee decides to log the error again then that is fine, you can see how the error propagated through the app and the first instance of the error is the source of the problem. I would rather have the same error logged 10 times than not logged at all.
It's the same as the difference between an INNER and LEFT OUTER join, afaik. Personally, I work best with examples, so I would suggest mocking up some tables, and exploring the differences with a few test queries. Good luck! 
This article has some good examples: https://www.simple-talk.com/sql/t-sql-programming/sql-server-apply-basics/
Even though you join on the name column between both tables, you still have to specify what table you want the order by to use - Even in a natural join like this. I suppose that they could make the query processor smart enough to handle this situation, but there may be some scenario that I'm not aware of where that could cause a problem.
OK, that hadn't occurred to me to try. I guess it could be useful, and the possibility then makes this necessarily an error. 
What you wrote is really confusing. I'm not even sure what you are asking. &gt;I have a where clause here which returns all BLANK What do you mean that is returns all BLANK? It returns no rows? A column in the result is NULL? &gt;It works What do you mean by this? What is the behavior when "it works"?
&gt; What columns are in the result is independent. this isn't true in any dbms that uses **standard sql** which requires that the ORDER BY clause reference ~only~ columns mentioned in the SELECT clause source: joe celko (*sql for smarties*, 3rd edition, page 328)
TIL I need to study more standard SQL.
no idea... obviously not yours, eh ;o)
&gt; What do you mean that is returns all BLANK? i think he means it returns all library books (judging by the name of the only column he revealed) except he didn't want to say what it is he's actually working on companies can be really prickly when it comes to revealing their data on teh interwebs
Can you create a variable beforehand, then do a fetch and concatenate both DURATION records into the same variable? You'd have find a way to make sure it was in the right order. Does this help?
You can use FOR XML PATH for that, as shown [here](http://sqlandme.com/2011/04/27/tsql-concatenate-rows-using-for-xml-path/) Or you can use CTE and recursive SQL to do the job. Finally, if what you're trying to do is located inside T-SQL code (like a stored procedure or something), you can also use a cursor with some windowed function (like row_number) to identify the matchings rows and process them correctly. Edit : this goes with spatialdestiny's answer. Also, FYI, on Oracle you can use the wm_concat function to do that. I think I've read somewhere that there would be the same function on SQL Server in the next or so release, but I couldn't find the article again (but I didn't searched very far :))
It sounds like you're asking for an explanation on how the WHERE clause works in this case to determine last week. I've broken it down to a few lines and added some comments. The first part: (WHERE ReturnDate &gt;= [Start of Last Week]) is broken out as follows: SELECT dateadd(week,-1,getdate()) --Get date from 1 week ago , datepart(weekday,getdate()) --Get Current Day of Week number (ex: Tuesday = 3) , (datepart(weekday,getdate()) * -1) --Multiply by -1 to get how many days to reach 0. , (datepart(weekday,getdate()) * -1) + 1 --Add one to get to Sunday for that week. , (-1 * datepart(weekday,getdate())+1) --This is actually the previous two steps combined. , dateadd(day,(-1 * datepart(weekday,getdate())+1),dateadd(week,-1,getdate())) --One week ago subtracting the # of days above to get to previous week's Sunday. (2013-07-28 10:25:28.983) , datediff(day,0, dateadd(day,(-1 * datepart(weekday,getdate())+1),dateadd(week,-1,getdate()))) --Strip out the time, '2013-07-28 10:18:27.337' becomes '2013-07-28 00:00:00.000' , dateadd(day,1,datediff(day,0, dateadd(day,(-1 * datepart(weekday,getdate())+1),dateadd(week,-1,getdate())))) -- Add 1 day to it to get Monday of previous week The second part: (AND ReturnDate &lt; [End of Last Week]) is broken out as follows: SELECT dateadd(week,-1,getdate()) --One week ago , datepart(weekday,getdate()) --Current day of week number , dateadd(day,-1*datepart(weekday,getdate()),dateadd(week,-1,getdate())) --One week and 3 days ago = The Saturday prior to your start range (two weeks ago) , dateadd(day,7,dateadd(day,-1*datepart(weekday,getdate()),dateadd(week,-1,getdate()))) --Add 7 days to get the Saturday at the end of last week , dateadd(day,2, datediff(day,0,dateadd(day,7,dateadd(day,-1*datepart(weekday,getdate()),dateadd(week,-1,getdate()) --Add two days to get to Monday of current week. )))) It's a little complicated because it's taking the day number that the query is running and calculating the date range based on that. Hope this helps.
I can only use TSQL at the moment. Could you perhaps give me some templated syntax?
Definitely helpful. Thanks!
Yes, it's really an error. It has no way of knowing whether you are working on t1.name or t2.name in the ORDER BY. Yes, they're going to be the same, but that doesn't matter.
I completely agree and I did not set-up this system. Previously the system was paper and we are in an intermediate solution until a more permanent data storage can be implemented. Funding is an issue so it probably won't happen soon. And yes the data type for the time is varchar.
Aha! I knew I wasn't crazy. This is exactly what I was thinking. It certainly doesn't help that certain versions of Oracle will allow it, while others throw the error. 
Generally speaking, this type of issue is best handled by the application logic rather than the database. SQL really, REALLY hates turning rows into columns and vice-versa because it's indicative of breaking relational logic. Fortunately, you're using Oracle, which has the LISTAGG() aggregate function. There's also the wm_concat() function in older versions, but that is an undocumented and unsupported function so I would avoid it if your version of Oracle supports LISTAGG(). Here's a good answer from StackOverflow: http://stackoverflow.com/questions/1120706/is-there-an-oracle-sql-query-that-aggregates-multiple-rows-into-one-row This question is asked constantly everywhere because while it's not something that the DB should do, apparently nobody ever thought that reporting software would want to do it either. I've never seen any report software that could handle this. It's absolutely idiotic.
Error was that 'name' was ambiguous in the order by clause. DBMS was two different version numbers of Oracle SE. 11.2.0.1 vs 11.2.0.3. The first version has run an equivalent query for months without issue. The new version gives an error every time. I get annoyed when a simple version update changes the way my code executes, which is why I asked the question. Not sure why you would expect the where clause to be ambiguous. 'jobdate' only exists in one table.
&gt; I get annoyed when a simple version update changes the way my code executes i do too "defensive sql" is mostly standard sql, with common sense for example, the only name column in this example's SELECT clause is t1.name, so common sense says code that in the ORDER BY, thus not only following the standard, but avoiding the ambiguous column error (if any) as well 
How did you plan on adding the data? 1 Random value of columnx in table A to each row in Table B? Maybe somethign like this? declare @columnx DATETIME declare col_cur cursor for select top 1 columnx from tableA order by PrimaryKey open col_cur fetch next from col_cur into @columnx while @@FETCH_STATUS = 0 begin update tableB set columnx = @columnx WHERE [PrimaryKey] = ( SELECT TOP 1 [PrimaryKey] FROM tableB WHERE COLUMNX IS NULL ORDER BY [PrimaryKey] -- You need to decide what column you want to sort on ) fetch next from col_cur into @columnx end close col_cur deallocate col_cur
 SELECT cli_spec , COUNT(CASE WHEN ms_decile = '1' THEN uniqueid ELSE NULL END) AS decile_1 , COUNT(CASE WHEN ms_decile = '2' THEN uniqueid ELSE NULL END) AS decile_2 , ... , COUNT(CASE WHEN ms_decile = '10' THEN uniqueid ELSE NULL END) AS decile_10 FROM jm_mr_20130807 WHERE COALESCE(have_email,email) IS NOT NULL GROUP BY cli_spec 
Thanks jdawg! Very creative and it worked like a charm!
[This book](http://www.amazon.com/Training-Kit-Exam-70-461-Microsoft/dp/0735666059) is designed specifically for the test. It covers everything on the test, and some things that aren't specifically on it but may help for other questions. It also comes with practice test software that is very useful. [The test goes well beyond writing queries](http://www.microsoft.com/learning/en-us/exam-70-461.aspx). Be prepared for (not meant to be comprehensive): * UDFs, * window functions, * defining and modifying tables and views, * best use of data types, * pivots * using XML data * CTEs * creating and modifying triggers, stored procs Good luck!
pro tip #1 -- mention your dbms [MYSQL] (or post in /r/mysql) pro tip #2 -- learn formatting select dem.last_name , dem.first_name , ra.service_date from demographic dem join radetail ra on dem.hin=ra.hin where service_code='E140A' join dxresearch dx on dem.demographic_no=dx.demographic_no where dx.dxresearch_code="364"; pro tip #3 -- print the entire error message, it would've pointed you to the JOIN after your first WHERE clause (you cannot split up the FROM clause like that) corrected query -- SELECT dem.last_name , dem.first_name , ra.service_date FROM demographic AS dem INNER JOIN radetail AS ra ON ra.hin = dem.hin AND ra.service_code = 'E140A' INNER JOIN dxresearch AS dx ON dx.demographic_no = dem.demographic_no AND dx.dxresearch_code = '364' 
there is no difference in inner joins in outer joins, the difference is huge
This. I need to learn this concept better.
Holy shit dude. Read the sidebar.
So, when you INNER join, and include some condition in the join, it's exactly the same as putting it in the WHERE clause. When you LEFT join and include some additional condition, it's saying Only show this data when it exists in the table AND fits this additional criteria. But a LEFT wouldn't have filtered the WHOLE ROW off anyways. So, that's the difference.
Thank you! 
I'll kind of compound on what IUseRhetoric said: GROUP BY the rest of the column names, but outer join onto that derived table to "figure it out". So, something like this: SELECT i.title, i.content, u.description, i.created_on, c.name, i.category_id as story_category FROM stories i INNER JOIN story_categories u ON u.id = i.category_id INNER JOIN story_pictures c ON c.story_id = i.id LEFT OUTER JOIN ( SELECT i.content, u.description, i.created_on, i.category_id, count(*) as rc FROM stories i, story_categories u, story_pictures c WHERE i.category_id = u.id AND i.id = c.story_id GROUP BY i.content, u.description, i.created_on, i.category_id ) g ON ( g.content = i.content AND g.description = u.description AND g.created_on = i.created_on AND g.category_id = i.category_id) WHERE i.category_id = 6 AND g.rc = 1 LIMIT 0, 20 (disclaimer: above code not tested, and i'm an oracle guy, so that might not even be okay syntax for mysql)
Thanks!!
If your database is static breaking a chunk or two off into a temporary table and then joining those into the full query would be one way to reduce the complexity for you.
I'm learning SQL myself, so maybe you can explain why you are using (SELECT PostDate FROM Posts WHERE Posts.ThreadID = Threads.ThreadID ORDER BY PostDate DESC LIMIT 1) instead of (SELECT MAX(PostDate), FROM Posts WHERE Posts.ThreadID = Threads.ThreadID) In the same vein, couldn't you just: Select AuthorID from (select AuthorID, MAX(PostNum) from Posts where Posts.ThreadID=Threads.ThreadID) as LASTPOSTER This is of course assuming the highest PostNum within a given thread is the username you're looking for, and any given PostNum only ever has one AuthorID. 
So first a disclaimer, 99% of the SQL I write is for reporting/BI type needs. Nearly every query I do bounces across the entire dataset, and usually in a fairly complex method. My datasets don't change any more often than nightly either. Application developers run into problems I never will, so some of my advice would actually be downright terrible in certain situations. One example I'll run into is to identify the top X accounts by some metric, then calculate a whole bunch of other metrics off that list of accounts, including something particularly taxing like a windowed moving average. I could build the first part, calculate top X accounts, into each statement easily enough, but there's a better way for me. I'll calculate top X accounts, and then insert that into a temporary table. From there, I can continue with the rest of my queries and they'll be quite a bit less complex since I'm joining to a table instead of doing a subquery. Temp tables can also be a huge performance gain in certain situations since MySQL isn't going to make use of indexes in derived tables, while temp tables can be created with indexes. As for an implementation. I'd start by building a temp table for the last poster of each thread containing ThreadID, AuthorID, and DateCreated. From there you have easy left joins to your threads and users table to get the final results.
your sql is fine problem has to be in your php logic
You could use a correlated sub query like select a.ID, a.fname, a.lname, (select fname from table where ID = (a.ID + 1)) from table Doing this from memory so you may have to Google it a bit. As an aside, a slot tournament where people play for other semi-random people sounds like you'd force half your players to use a worse score. 
Thanks, I'll try that. Its called Switch or Stay, 3round and the best combined score wins. 3rd round is the only one that they will move left or right or stay. Its kinda like musical chairs. 
move &lt;?php do { ?&gt; to just after the line $row = mysql_fetch_array($sql); to be like this: $row = mysql_fetch_array($sql); do { $post_id = $row['post_id']; $post_user = $row['post_user']; And think about why. 
Thanks.. Sometimes fresh eyes help. I'd been looking at it for a bit and just really wasn't making sense. You rock man... If I could give you 10 upvotes I would :)
 SELECT Threads.ThreadID , Threads.Title , Threads.OPID , Threads.DateCreated , op.UserID AS op_id , op.Username AS op_name , last.UserID AS last_ID , last.Username AS last_name FROM Threads INNER JOIN Users AS op ON op.UserID = Threads.OPID INNER JOIN ( SELECT ThreadID , MAX(PostDate) AS latest FROM Posts GROUP BY ThreadID ) AS p ON p.ThreadID = Threads.ThreadID INNER JOIN Posts ON Posts.ThreadID = p.ThreadID AND Posts.PostDate = p.last INNER JOIN Users AS last ON last.UserID = Posts.AuthorID WHERE Threads.ForumID = $_GET[f]
Have you thought about removing nested select in select clause and using agg max w/ group by for post date? You are pretty much providing create scripts; I am certain that if you provide table data, enough to work on problem, help would be forthcoming. I'll create them tables for if you provide data...
After a bit of fixing, this gave me everything except the last post date. I changed it to: SELECT Threads.ThreadID, Threads.Title, Threads.OPID, Threads.DateCreated, op.UserID AS op_id, op.Username AS op_name, last.UserID AS last_ID, last.Username AS last_name, (SELECT PostDate FROM Posts WHERE Posts.ThreadID = Threads.ThreadID ORDER BY PostDate DESC LIMIT 1) AS PostDate FROM Threads INNER JOIN Users AS op ON op.UserID = Threads.OPID INNER JOIN (SELECT ThreadID, MAX(PostDate) AS latest FROM Posts GROUP BY ThreadID) AS p ON p.ThreadID = Threads.ThreadID INNER JOIN Posts ON Posts.ThreadID = p.ThreadID AND Posts.PostDate = p.latest INNER JOIN Users AS last ON last.UserID = Posts.AuthorID WHERE Threads.ForumID = $_GET[f] This worked! Thank you for giving a direct, clear, and useable solution.
dude, no last post date is simply p.latest (sorry for the typo) you ~don't~ need that subquery in the SELECT clause, it's redundant
But it's not selecting p.latest. I just ran it through phpMyAdmin to confirm. That field is never returned. Or are you saying I should just write p.latest at the top with the SELECT portion? Edit: Ignore that. I just did what I said above, and it worked.
What version of OBIEE? Can't you do a pivot? It is really simple in 11g.
Can you elaborate with an example?
 select count(src1) as CNT1, count(src2) as CNT2, COL1, COL2, COL3 from (select COL1, COL2, COL3, 1 as src1, to_number(null) as src2 from TABLE1 union all select COL1, COL2, COL3, to_number(null) as src1, 2 as src2 from TABLE2) group by COL1, COL2, COL3 having count(src1) = count(src2)
Another important factor is the time it takes to compute the hash function. Most hashing libraries now allow you to pass in a number of iterations to run the hash. This helps prevent brute force attacks. Take a look into BCrypt it is a pretty good salted hash and has implementations in most languages. Also, make sure the website is using SSL and for redirects from HTTP to HTTPS or else their passwords will just be passed around in plain text.
This is the idea, yeah. There's a couple of things there I haven't seen before, so I'll give that a go. Thanks!
OK. Here is my guess as to how to do this (I don't fully understand your business requirements). You need to know the max "Payment" date and sum of payments. Then calculate the totals for each fee group BEFORE the last payment date. And separately for any fee after that date. Then you can subtract by priority, fees-interest-principle from the older fees. Then add the remainder to the new fees to get the balances. HOWEVER, I know with my home loan that extra payment in a month goes directly against principle. I'm not sure if a combination of overpayment and under or deferred payments corrupt your totals.
So whats wrong with this query? Can you post the output from it, and explain which bit is lacking? Its not very clear from your question, but whatever your requirements are, I'm sure it is possible to do. Have you considered using a cursor to loop over the payments and loans for a given user, in date order?
The problem is if the customer makes a payment which is more than the fees and interest combined, it comes off of the principal which in turn effects the amount of interest in the next row. As far as a second payment going directly towards principal, I wasn't told that that was the case so I'm going to assume it's not, like I said, not a finance guy, the only loans I've ever had are student loans.
Here's the data, some data removed and manipulated to protect the innocent. So basically what I need to see in each row is "Principal Due", "Fees Due", and "Interest Due" but these are going to change every time there is a payment row. Because that payment will first go towards existing fees, then interest, and then principal. So now if someone makes a payment and it deducts some from the principal total then the interest is going to be affected because the total principal is now lower. So basically my running total fields are completely wrong because they don't take any of that into account.
 amount,daysBetween,effectiveDate,effectiveDate2,interest,type,runningTotalPrinciple,runningTotalFees,runningTotalPayments,runningTotalInterest,amountDue 0,0,1/5/2009,NULL,0,PRINCIPAL,0,NULL,NULL,0,NULL 150,0,1/5/2009,1/5/2009,0,PRINCIPAL,150,NULL,NULL,0,NULL 100,0,1/5/2009,1/5/2009,0,PRINCIPAL,250,NULL,NULL,0,NULL 35,0,1/5/2009,1/5/2009,0,FEE,250,35,NULL,0,NULL 59.4,21,1/26/2009,1/5/2009,5.178075,PAYMENT,250,35,59.4,5.178075,230.778075 200,10,2/5/2009,1/26/2009,4.43835,PRINCIPAL,450,35,59.4,9.616425,435.216425 35,4,2/9/2009,2/5/2009,1.77534,FEE,450,70,59.4,11.391765,471.991765 78.57,14,2/23/2009,2/9/2009,6.21369,PAYMENT,450,70,137.97,17.605455,399.635455 250,3,2/26/2009,2/23/2009,2.07123,PRINCIPAL,700,70,137.97,19.676685,651.706685 35,11,3/9/2009,2/26/2009,7.59451,FEE,700,105,137.97,27.271195,694.301195 92.54,14,3/23/2009,3/9/2009,9.66574,PAYMENT,700,105,230.51,36.936935,611.426935 35,14,4/6/2009,3/23/2009,9.66574,FEE,700,140,230.51,46.602675,656.092675 90.42,14,4/20/2009,4/6/2009,9.66574,PAYMENT,700,140,320.93,56.268415,575.338415 35,14,5/4/2009,4/20/2009,9.66574,FEE,700,175,320.93,65.934155,620.004155 50,2,5/6/2009,5/4/2009,1.47945,PRINCIPAL,750,175,320.93,67.413605,671.483605 200,1,5/7/2009,5/6/2009,0.936985,PRINCIPAL,950,175,320.93,68.35059,872.42059 87.49,11,5/18/2009,5/7/2009,10.306835,PAYMENT,950,175,408.42,78.657425,795.237425 35,14,6/1/2009,5/18/2009,13.11779,FEE,950,210,408.42,91.775215,843.355215 80.78,14,6/15/2009,6/1/2009,13.11779,PAYMENT,950,210,489.2,104.893005,775.693005 35,14,6/29/2009,6/15/2009,13.11779,FEE,950,245,489.2,118.010795,823.810795 200,11,7/10/2009,6/29/2009,12.476695,PRINCIPAL,1150,245,489.2,130.48749,1036.28749 80.26,3,7/13/2009,7/10/2009,3.402735,PAYMENT,1150,245,569.46,133.890225,959.430225 35,14,7/27/2009,7/13/2009,15.87943,FEE,1150,280,569.46,149.769655,1010.309655 88.11,14,8/10/2009,7/27/2009,15.87943,PAYMENT,1150,280,657.57,165.649085,938.079085 35,14,8/24/2009,8/10/2009,15.87943,FEE,1150,315,657.57,181.528515,988.958515 89.8,15,9/8/2009,8/24/2009,17.013675,PAYMENT,1150,315,747.37,198.54219,916.17219 35,6,9/14/2009,9/8/2009,6.80547,FEE,1150,350,747.37,205.34766,957.97766 80.14,21,10/5/2009,9/14/2009,23.819145,PAYMENT,1150,350,827.51,229.166805,901.656805 35,7,10/12/2009,10/5/2009,7.939715,FEE,1150,385,827.51,237.10652,944.59652 88.11,21,11/2/2009,10/12/2009,23.819145,PAYMENT,1150,385,915.62,260.925665,880.305665 35,7,11/9/2009,11/2/2009,7.939715,FEE,1150,420,915.62,268.86538,923.24538 87.23,21,11/30/2009,11/9/2009,23.819145,PAYMENT,1150,420,1002.85,292.684525,859.834525 35,7,12/7/2009,11/30/2009,7.939715,FEE,1150,455,1002.85,300.62424,902.77424 86.16,21,12/28/2009,12/7/2009,23.819145,PAYMENT,1150,455,1089.01,324.443385,840.433385 86.16,0,12/28/2009,12/28/2009,0,PAYMENT,1150,455,1175.17,324.443385,754.273385 35,7,1/4/2010,12/28/2009,7.939715,FEE,1150,490,1175.17,332.3831,797.2131 85.11,21,1/25/2010,1/4/2010,23.819145,PAYMENT,1150,490,1260.28,356.202245,735.922245 35,7,2/1/2010,1/25/2010,7.939715,FEE,1150,525,1260.28,364.14196,778.86196 84.08,21,2/22/2010,2/1/2010,23.819145,PAYMENT,1150,525,1344.36,387.961105,718.601105 35,7,3/1/2010,2/22/2010,7.939715,FEE,1150,560,1344.36,395.90082,761.54082 83.07,21,3/22/2010,3/1/2010,23.819145,PAYMENT,1150,560,1427.43,419.719965,702.289965 35,7,3/29/2010,3/22/2010,7.939715,FEE,1150,595,1427.43,427.65968,745.22968 82.08,21,4/19/2010,3/29/2010,23.819145,PAYMENT,1150,595,1509.51,451.478825,686.968825 35,7,4/26/2010,4/19/2010,7.939715,FEE,1150,630,1509.51,459.41854,729.90854 81.11,21,5/17/2010,4/26/2010,23.819145,PAYMENT,1150,630,1590.62,483.237685,672.617685 35,7,5/24/2010,5/17/2010,7.939715,FEE,1150,665,1590.62,491.1774,715.5574 80.17,21,6/14/2010,5/24/2010,23.819145,PAYMENT,1150,665,1670.79,514.996545,659.206545 35,7,6/21/2010,6/14/2010,7.939715,FEE,1150,700,1670.79,522.93626,702.14626 79.24,21,7/12/2010,6/21/2010,23.819145,PAYMENT,1150,700,1750.03,546.755405,646.725405 35,7,7/19/2010,7/12/2010,7.939715,FEE,1150,735,1750.03,554.69512,689.66512 78.33,21,8/9/2010,7/19/2010,23.819145,PAYMENT,1150,735,1828.36,578.514265,635.154265 35,4,8/13/2010,8/9/2010,4.53698,FEE,1150,770,1828.36,583.051245,674.691245 74.52,21,9/3/2010,8/13/2010,23.819145,PAYMENT,1150,770,1902.88,606.87039,623.99039 25,5,9/8/2010,9/3/2010,5.671225,FEE,1150,795,1902.88,612.541615,654.661615 99.52,1,9/9/2010,9/8/2010,1.134245,PAYMENT,1150,795,2002.4,613.67586,556.27586 35,4,9/13/2010,9/9/2010,4.53698,FEE,1150,830,2002.4,618.21284,595.81284 104.7,21,10/4/2010,9/13/2010,23.819145,PAYMENT,1150,830,2107.1,642.031985,514.931985 35,7,10/11/2010,10/4/2010,7.939715,FEE,1150,865,2107.1,649.9717,557.8717 75.1,21,11/1/2010,10/11/2010,23.819145,PAYMENT,1150,865,2182.2,673.790845,506.590845 35,7,11/8/2010,11/1/2010,7.939715,FEE,1150,900,2182.2,681.73056,549.53056 300,11,11/19/2010,11/8/2010,15.731485,PRINCIPAL,1450,900,2182.2,697.462045,865.262045 73.45,10,11/29/2010,11/19/2010,14.30135,PAYMENT,1450,900,2255.65,711.763395,806.113395 35,7,12/6/2010,11/29/2010,10.010945,FEE,1450,935,2255.65,721.77434,851.12434 84.05,21,12/27/2010,12/6/2010,30.032835,PAYMENT,1450,935,2339.7,751.807175,797.107175 35,4,12/31/2010,12/27/2010,5.72054,FEE,1450,970,2339.7,757.527715,837.827715 35,3,1/3/2011,12/31/2010,4.290405,FEE,1450,1005,2339.7,761.81812,877.11812 86.22,21,1/24/2011,1/3/2011,30.032835,PAYMENT,1450,1005,2425.92,791.850955,820.930955 35,4,1/28/2011,1/24/2011,5.72054,FEE,1450,1040,2425.92,797.571495,861.651495 81.85,21,2/18/2011,1/28/2011,30.032835,PAYMENT,1450,1040,2507.77,827.60433,809.83433 35,10,2/28/2011,2/18/2011,14.30135,FEE,1450,1075,2507.77,841.90568,859.13568 87.39,21,3/21/2011,2/28/2011,30.032835,PAYMENT,1450,1075,2595.16,871.938515,801.778515 35,7,3/28/2011,3/21/2011,10.010945,FEE,1450,1110,2595.16,881.94946,846.78946 83.05,21,4/18/2011,3/28/2011,30.032835,PAYMENT,1450,1110,2678.21,911.982295,793.772295 35,7,4/25/2011,4/18/2011,10.010945,FEE,1450,1145,2678.21,921.99324,838.78324 100,9,5/4/2011,4/25/2011,13.758885,PRINCIPAL,1550,1145,2678.21,935.752125,952.542125 82,12,5/16/2011,5/4/2011,18.34518,PAYMENT,1550,1145,2760.21,954.097305,888.887305 35,7,5/23/2011,5/16/2011,10.701355,FEE,1550,1180,2760.21,964.79866,934.58866 84.99,21,6/13/2011,5/23/2011,32.104065,PAYMENT,1550,1180,2845.2,996.902725,881.702725 35,7,6/20/2011,6/13/2011,10.701355,FEE,1550,1215,2845.2,1007.60408,927.40408 84.77,21,7/11/2011,6/20/2011,32.104065,PAYMENT,1550,1215,2929.97,1039.708145,874.738145 35,7,7/18/2011,7/11/2011,10.701355,FEE,1550,1250,2929.97,1050.4095,920.4395 83.69,21,8/8/2011,7/18/2011,32.104065,PAYMENT,1550,1250,3013.66,1082.513565,868.853565 35,4,8/12/2011,8/8/2011,6.11506,FEE,1550,1285,3013.66,1088.628625,909.968625 79.49,21,9/2/2011,8/12/2011,32.104065,PAYMENT,1550,1285,3093.15,1120.73269,862.58269 25,5,9/7/2011,9/2/2011,7.643825,FEE,1550,1310,3093.15,1128.376515,895.226515 35,5,9/12/2011,9/7/2011,7.643825,FEE,1550,1345,3093.15,1136.02034,937.87034 30,0,9/12/2011,9/12/2011,0,FEE,1550,1375,3093.15,1136.02034,967.87034 219.75,21,10/3/2011,9/12/2011,32.104065,PAYMENT,1550,1375,3312.9,1168.124405,780.224405 35,4,10/7/2011,10/3/2011,6.11506,FEE,1550,1410,3312.9,1174.239465,821.339465 35,3,10/10/2011,10/7/2011,4.586295,FEE,1550,1445,3312.9,1178.82576,860.92576 81.02,21,10/31/2011,10/10/2011,32.104065,PAYMENT,1550,1445,3393.92,1210.929825,812.009825 35,7,11/7/2011,10/31/2011,10.701355,FEE,1550,1480,3393.92,1221.63118,857.71118 79.51,21,11/28/2011,11/7/2011,32.104065,PAYMENT,1550,1480,3473.43,1253.735245,810.305245 35,4,12/2/2011,11/28/2011,6.11506,FEE,1550,1515,3473.43,1259.850305,851.420305 150,10,12/12/2011,12/2/2011,16.7671,PRINCIPAL,1700,1515,3473.43,1276.617405,1018.187405 75.66,11,12/23/2011,12/12/2011,18.44381,PAYMENT,1700,1515,3549.09,1295.061215,960.971215 35,10,1/2/2012,12/23/2011,16.7671,FEE,1700,1550,3549.09,1311.828315,1012.738315 86.63,21,1/23/2012,1/2/2012,35.21091,PAYMENT,1700,1550,3635.72,1347.039225,961.319225 35,4,1/27/2012,1/23/2012,6.70684,FEE,1700,1585,3635.72,1353.746065,1003.026065 80.61,21,2/17/2012,1/27/2012,35.21091,PAYMENT,1700,1585,3716.33,1388.956975,957.626975 35,10,2/27/2012,2/17/2012,16.7671,FEE,1700,1620,3716.33,1405.724075,1009.394075 85.8,21,3/19/2012,2/27/2012,35.21091,PAYMENT,1700,1620,3802.13,1440.934985,958.804985 35,7,3/26/2012,3/19/2012,11.73697,FEE,1700,1655,3802.13,1452.671955,1005.541955 81.62,21,4/16/2012,3/26/2012,35.21091,PAYMENT,1700,1655,3883.75,1487.882865,959.132865 25,2,4/18/2012,4/16/2012,3.35342,FEE,1700,1680,3883.75,1491.236285,987.486285 35,5,4/23/2012,4/18/2012,8.38355,FEE,1700,1715,3883.75,1499.619835,1030.869835 30,0,4/23/2012,4/23/2012,0,FEE,1700,1745,3883.75,1499.619835,1060.869835 217.75,21,5/14/2012,4/23/2012,35.21091,PAYMENT,1700,1745,4101.5,1534.830745,878.330745 25,3,5/17/2012,5/14/2012,5.03013,FEE,1700,1770,4101.5,1539.860875,908.360875 35,4,5/21/2012,5/17/2012,6.70684,FEE,1700,1805,4101.5,1546.567715,950.067715 30,0,5/21/2012,5/21/2012,0,FEE,1700,1835,4101.5,1546.567715,980.067715 40,0,5/21/2012,5/21/2012,0,FEE,1700,1875,4101.5,1546.567715,1020.067715 93.87,21,6/11/2012,5/21/2012,35.21091,PAYMENT,1700,1875,4195.37,1581.778625,961.408625
Just one customer for now yes, eventually I'll have to sum it up and show it for all customers. And yes I'm using SQL Server, unfortunately stuck with 2005... 1) Yes principal is anytime the loan amount increases for whatever reason (It seems that its not just when the customer increases it but certain fees go towards principal instead of fees as well) 2) Yes 3) Yes Thanks a ton, if you can solve this I owe you some reddit gold! EDIT: there is a datetime, for some reason it copied over with just the time which is always 00:00.0 because we only store the date, I'll try to get a better format for you. EDIT 2: Edited the data above with dates. 
So you really need to analyze payment to payment. But also carry over the fee totals. What database do you have? I see you can use CTEs. Does your DB also support recursive CTE (like SQL Server)? With recursion, you can have a CTE that tracks the account balance and continually updates for each payment. Then you just need the max balance row plus the newest fees to see the current balance. I made a mock up of how I think it might work. Except it seems like the principle should be counting down, so double check the calculations. WITH balance ( SELECT account, paydate, &lt;startingfees&gt; f, &lt;startinginterest&gt; i, &lt;startingPrinciple&gt; p FROM journal j WHERE paydate = &lt;get the first payment&gt; and type = 'payment' UNION ALL SELECT account, paydate , (j.amount - b.f - (select sum(amount) from journal where type = 'fee' and account = j.account and paydate between b.paydate and j.paydate) f , (j.amount - b.f - (select sum(amount) from journal where type = 'fee' and account = j.account and paydate between b.paydate and j.paydate) - b.i - (select sum(amount) from journal where type = 'interest' and account = j.account and paydate between b.paydate and j.paydate) ) i , (&lt;same idea for principle&gt;) p FROM journal j Inner Join balance b ON WHERE paydate = &lt;MIN greater than b.paydate in same account&gt; and type = 'payment' and j.account = b.account ) Select b.account , (b.f + (select sum(amount) from journal where type = 'fee' and account = j.account and paydate &gt; b.paydate)) f , (b.i + (select sum(amount) from journal where type = 'inerest' and account = j.account and paydate &gt; b.paydate)) i , (b.p + (select sum(amount) from journal where type = 'principle' and account = j.account and paydate &gt; b.paydate)) p From balance b where b.paydate = (select top 1 b1.paydate from balance b1 where b.account = b1.account order by paydate desc)
I'm pretty sure it is not not possible/feasible to do this level of text comparison, parsing AND updating in SQL. The SQL would simply be to find the strings that are different. After that you have to parse character by character (or word by word) to find the changes and add the appropriate tags so that your formatting tool can render underlines and strikethrough. 
I'm not looking to insert/update the data within the SQL db however I am looking to make the results of the query as such. 
* Why are you using a normal database for something that amounts to source control and would be infinitely better handled by (free) software? (SVN, etc) * Stop using Access, there are many good ways to build reports and none of them involve access(*except maybe label printing, maybe*). * It seems to me like this process is the result of bandaiding a ball of bandaids on top of a legacy system. Fix it right instead of trying to add another bandaid. 
I think your best bet would be to write a VBA script to automate this in access, I don't think you can do this formatting with plain SQL. A reporting platform would work. But when you say "new text" and "deleted text", you're talking about comparing a current result set to a previous one? 
* You can find out about [Subversion HERE](http://subversion.apache.org/); it allows for the submissions and revision of documents with the added bonus that it keeps track of all changes(and who made them) not to mention the ability to go back to almost any previous version in the past. * The SQL Server Reporting Service can do almost anything Access can if done correct, if not Crystal Reports is a good alternative * Sometimes you get lucky if you implement a potential replacement solution that saves everyone time (and money) how fast management will come around. Especially when you tell them you can pull the report of who is changing what or being the most productive from SVN. Not saying these will work for your situations, but it might be a place to start looking.
This works great numerically but I have 2a after 19.
That is correct, comparing 1st version text to 2nd version text. Whatever isn't showing within 2nd version text have strike through applied in the 1st version. The reverse for the underlining. 
The other way I mentioned about should work then.
So in the example you provide, is the loan balance at the point that the payment is made on 06-05-13 275.00? Or is there an outstanding loan balance elsewhere? Normally, in a transaction table, you have +ve and -ve amounts, if the Principle amounts and Payment and Fee amounts offset each other, I would expect one set to be negative. 
 case when right(foo,1) like '[A-Z]%' then left(foo,len(foo)-1)+convert(decimal,(ascii(right(foo,1))-96))/100 else foo end "When the last character of foo is a letter, then take everything other than that letter and add it that letter converted to a = .01, b = .02, c = .03 ... z = .26, else just use foo." This assumes your subcategory only goes up to 'z', but your numbers can go up to whatever.
The one thing that I would be worried about is that this would depend on each individual checking the document in correctly. A document management system is more appropriate in that case instead of a code based source control. I know the differences seem trivial, but the larger the group of people, the more tasks such as checking in changes need automation. Sharepoint, Notes (now really obsolete) maybe even a wiki might be a better option. Plus SVN is good for coders, but the learning curve can be a little steep. I actually implement mercurial because of the ease in learning. Can you image the typical non-technical person trying to learn this?
Well this has been an eye-opener for me. Any other questions OP?
It's not as difficult as you think, I've seen SVN being used by some very non-technical persons to keep track of project documentation. They don't really need to know why/how it works just that when they are finished their sections to update and commit. The odd time there is a conflict it tells them who. Change of process? Definitely, some training also. Pays for itself when a manager asks if anyone has a copy of the old documents from a few months back though or someone deletes a missions critical document from their desktop.
You can probably do a search for interview questions and get some good examples, I usually start off with some of the below (a lot of this may be too simple or too advanced, depending on how far you got with your self teaching): What types of joins are there and how are they used? Are you familiar with database normalization? Can you tell me about different types of indexes? Difference between Function and Stored Procedure? What is a Trigger? What are cursors? How would you go about finding duplicate items in a table? There are more that can get more in depth, and I love to have candidates white board for me the answers to several of those questions (show me how to join this with that). There's some good ones at: http://www.freejavaguide.com/sql-interview-questions.htm but don't get too worked up about them. Be honest about what you know and don't, if you're a little shaky work on the fundamentals, not memorizing the minutia (like a lot of the big lists such as the one provided can get you bogged down in). I have had fewer candidates than I wish (even when interviewing "senior" candidates) who know each of the ACID properties or each of the levels of normalization, but those are good to know if you aren't familiar with them yet (though I would be surprised if you were comfortable working with them for a first job). Good luck! There is a ton of fun to be had in a SQL job (in development or DBA and most places in between). Let us know how it goes :-).
Be able to explain all types of joins very well.
I just want to say I'm glad you're not one of those jerks that over-complicates the interview questions when the actual job doesn't require master-level in-depth knowledge of analyzing execution plans or calculating actual storage space needed given some insane hypothetical specification. Thank you.
BTW, [SQLFiddle.com](http://sqlfiddle.com) is useful for collaborating on this type of SQL problem solving. And of course, it's broken when I try to send you the link, so hopefully it will be working by the time you read this.
know your joins!
This looks elegant as hell. I'll give it a try. Thanks. Edit: It works beautifully
The principal balance would be 275, then we would calculate interest from the principal between the dates, so there would be an interest balance, and if any fees (which in my example there arent at that point) then there would be a fee balance, so in this case that payment on 6/5 would pay off all the interest first, and then the remaining money would go towards the principal. I'm not a finance person not quite sure if this table is set up "normally" (I didn't design it), and the most experience I have with loans is student loans which I paid off in full after college.
Thanks! I'll play with this and see if I can get it working...
i guess a notepad with some cheats is frowned upon
You should also understand Database (schema) design principles: -Normal Forms - specifically - What is 3rd normal form? (Foreign Keys, Primary Keys) -Dimensional Models (OLAP) - What is a cube? a FACT? a DIMENSION? -OLTP Models - What is the different between OLAP and OLTP? -Table vs Column Oriented Designs When defining a database schema - you must always make a trade off between - Data Integrity (normal forms) - Navigational (SQL) complexity (so called "Not" Normal Forms) - Performance - READ Performance vs WRITE Performance The 'correct' SQL required for a specific problem is influenced by all of the schema design decisions made up to that point. When you understand these schema design goals and patterns - specific SQL challenges are easier to tackle. 
You wouldn't understand but thanks for this. I'm in the process of developing an application and this has given me some hard questions to ask before I get too far in :)
Did you take the Oracle SQL exam yet?
i have not
this is a good start: http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=5001&amp;get_params=p_exam_id:1Z0-051&amp;p_org_id=&amp;lang= you used to be able to take this online - open book
perfect. thanks
Also, if you can get your hands on one, the Oracle University book for SQL is really good. 
What's the job description for the position?
SQL dba
Thanks, I received a few errors so Im just going to try and solve why the syntax is wrong. But I get the idea. Again thanks!
In a view. Just name the view Tablename_vw and then inside the view do select fooname as Name, hue as Color from Tablename
I added this update, sorry: &gt;EDIT: Let me clarify a bit. We have a lot of existing sites using column names like "image", "imageMed", "image1" for example. For all of the future websites, I just want to use "imageMed" (these aren't the actual names, just an example), but I can't rename it in the table because it'll break the old sites. Is there anyway I can just give this column a second name?
Synonyms? http://technet.microsoft.com/en-us/library/ms177544.aspx 
I wish mysqls views didn't suck.
Good thought, but can't be used for a column.
select imageMed = image1 
Select imageMed from (Select image from tablename Union Select An_image from other_tablename)
trane_0 has the right idea. Running a server on your laptop actually adds a non-trivial amount of overhead. Someday, you might want to install postgres. Someday, mongo. To do all this stuff properly, set up some VM's that you can spin up and down on your laptop as required.
Umm.. what? Are you saying that you have a query you run against MS SQL and you now want to run it against Oracle, from inside of a .NET application? 
yeah, sorry. I'm (obviously/unfortunately) new to all this and kind of got thrown at this project. but you got it right. 
You're going to have to re-write this in PL/SQL. It's not tough, but you're going to have to implement different ways of calling/parsing return codes from stored procedures. Good luck! 
Your text makes my head hurt, but you probably want to do this: select some_item from some_table where some_condition and not exists (select some_item from some_table where some_other_condition) 
I am trying what you suggested, but the thing is that I need all MPN lines, and only the ITEM line where there is no MPN that also has the same ITEM_NO. All the other data (the items_no and the size and color and barcode) are duplicated. This table has no primary key.
I understand now. This worked select * from IM_barcod where (barcod_id = 'item' and item_no not in (select item_no from im_barcod where barcod_id = 'MPN')) or barcod_id = 'MPN' THANK YOU SO MUCH!!!
Yeah it would be quicker to rewrite.
I see that you found a solution, but since I've already written it out, here ya go: SELECT T1.* FROM tblName T1 INNER JOIN tblName T2 ON T2.item_no = T1.item_no AND T1.barcode_id = 'MPN' UNION SELECT T3.* FROM tblName T3 LEFT OUTER JOIN tblName T4 ON T4.item_no = T3.item_no AND T4.barcode_id = 'MPN' LEFT OUTER JOIN tblName T5 ON T5.item_no = T4.item_no WHERE T5.item_no IS NULL Basically, the first query joins to itself to get all records where barcode_id is "MPN". Then, it unions to a similar query, but instead joins *again* on itself where those same values are null, which excludes all collections containing records where barcode_id is "MPN".
I only had a tenuous grasp on it as it stood. Your clear code and explanation is really helping me. Thank you too!!
I got quite far using the high-performance method I tried, but ultimately it wasnt feasible. So I fell back to doing it the cursor way. 1) *Set up a dummy finance table with the data you provided. This represents the data returned by your original query (although you could prune your query since mine only needs the date, type and amount fields* https://gist.github.com/carpii/6236099 2) *Create a memory table to work with, populate the data with dummy dates and then loop over it generating the compounding interest/balance* https://gist.github.com/carpii/6236105 3) *Just a demo of how the results look from this query. Should be a good starting point even if the results are not correct* https://gist.github.com/carpii/6236117
It's pretty common on my development machines to have multiple versions, haven't run into any issues yet. I just set the services all to manual start and run as needed. VMs cab be nice, but there's a lot of additional space required - I generally only use them if I am trying to mimic a server installation.
There is a way to do this with an aggregate function as well, but I'm on a mobile device so I'll try to write it up when I get to a better interface :)
same here, this is awesome 
Enjoy guys!
Bless you, sir.
Enroled. Thank you.
Awesome. I'm just about to roll out SSRS.
Thank you! 
Thanks. About to inherit an SSRS project and this will help.
 SELECT AVG(x) AS opponent_avg FROM competitions WHERE player1 = 'A' quality of solution on a rough par with quality of question
Being a relative SQL noob, what is the skill level required for SSRS? I'm pretty decent at building queries, but I'm not doing anything really impressive either, like nested statements.
Thanks a lot for this! I think I may have a go at your other SQL course afterwards
I'm on my phone so please forgive my formatting but I think you're looking for something like : "select * from cube A Join agents B On A.agent_name = B.agent_name And B.[level 1] = 'united' ; If you get something huge and take a year, change the join to: Join (select distinct agent_name, [level 1] from agents) B And filter by the same parameters. Take note of the aliases (a &amp; b) and brackets on columns with a space in the name 
 SELECT * FROM dbo.Cube as A JOIN dbo.Agents as B on A.[agent_name] = b.[agent_name] AND b.[LEVEL 1] = 'United'
Hi, from the sidebar: http://www.reddit.com/r/SQL/comments/nlogo/question_regarding_starting_to_learn_sql
This looks great. Much appreciated.
good stuff peacefire
I'll bite. What happens monday? Please don't say you start a new job doing SQL where you said you knew SQL....
you can include literal strings in your select column list. select column1, column2, ..., columnn, 'some text you want on every row' from sometable Or select column1, 'table1' from table1 union select column1, 'table2' from table2 
Sweet! I use SSRS but I have learned by trial and error and messing around. Some formal training will be helpful I think
Thank you! This got me close to what I was looking for!
do you mean using "AS"? select count(w) as tableAcount from A where X is 'NULL' group by W;
W3schools
&gt; this feels kinda hacky it is hacky, but then, so is the requirement :)
Can you add numerical column to specify order and use that instead?
use a CASE expression
So where in SA you from? 
In Tanzania actually, the company that sent me this is from SA, its like a job interview thing.
To the source table? Its sort of a job screening question so I don't think I can add anything.
like this -- SELECT PlayerID , MatchID , SetsWon , SetsLost , CASE ... END AS Outcome FROM ...
&gt; its like a job interview thing i trust that if you get it, you won't be asking reddit to help you with your job every few hours
There are a couple of ways to do it. It depends on what the real point is. If the real point is to just move Cape Town to the end of an alpha order, you can select city, person from customers order by (case when city = 'Cape Town' then 1 else 0 end), city, person; This approach will give you fewer maintenance headaches, because it will work for any number of arbitrary cities. (It refers explicitly to only one city's name: the name you want to move to the end.)
I used this Youtube tutorial, which helped me get out of the gates. From my limited experience, learning the language isn't that difficult, but the most time consuming thing was learning my specific database and the tables. [Part 1](http://www.youtube.com/watch?v=xaRrTBmMp30) [Part 2](http://www.youtube.com/watch?v=qHTK0QYUX_Y) [Part 3](http://www.youtube.com/watch?v=deegPjmasq8) [Part 4](http://www.youtube.com/watch?v=vHE-EeLaYsI)
http://www.w3schools.com/sql/
that college EHB by any chance?
I dunno if it applies to their SQL stuff, but www.w3fools.com
Write up your own questions and practice your answers out loud. You'll give more confident answers.
Look at some of the basic questions in this stack overflow thread http://stackoverflow.com/questions/2119859/questions-every-good-database-sql-developer-should-be-able-to-answer
What a question... I'll throw my 2cents in. Learn it all, just because your current job is only a SQL Server shop doesn't mean you won't need the knowledge later on. For example I've only worked in SQL Server shops but in my current position I'm asked to do some Oracle work and the learning curve is much steeper on Oracle vs MSSQL. For pay, I think that depends on size of company, experience, location and concentration. I do a lot of BI development work I was making 70k with about a year experience. But there is a lot of demand for BI work where I'm at... no idea if that was good but I was certainly happy with it at the time.
ZipRec is a record variable that is the exact same type the cursor returns (because of the %ROWTYPE attribute in its declaration). If you use the cursor, you wouldn't need to fetch it into n different variables (n is the number of columns in the cursor and in this case, the table M_Zip), you could fetch it into the ZipRec variable and refer to various fields of that record like M_Zip.column_name1, M_Zip.column_name2 and so on. A small block to use it could be like DECLARE CURSOR getZip (Pa_Zip_Code M_Zip.Zip_Code%TYPE) IS SELECT * FROM M_Zip WHERE Zip_Code = Pa_Zip_Code; ZipRec getZip%ROWTYPE; BEGIN OPEN getZip('01234'); LOOP FETCH getZip INTO ZipRec; EXIT WHEN getZip%NOTFOUND; -- do work with ZipRec DBMS_OUTPUT.PUT_LINE(ZipRec.a + ZipRec.b); END LOOP; CLOSE getZip; END;
Does it actually have to be a column in Table 1? Could you have a third object, namely a view, instead? Your view could then be something like ;WITH T2Vals AS (SELECT DISTINCT CustID, Date FROM Table2) SELECT Table1.SalesmanID, Table1.CustID, Table1.Date, .....any other table1 fields 'YES' as in_table_2 FROM Table1 WHERE EXISTS (SELECT 1 FROM T2Vals WHERE Table1.CustID = T2Vals.CustID AND Table1.Date = T2Vals.Date) UNION SELECT Table1.SalesmanID, Table1.CustID, Table1.Date, .....any other table1 fields 'NO' as in_table2 FROM Table1 WHERE NOT EXISTS (SELECT 1 FROM T2Vals WHERE Table1.CustID = T2Vals.CustID AND Table1.Date = T2Vals.Date) This assumes you're using a relatively recent version of SQL Server (for the ;With statement) - maybe SQL Server 2008 or greater. If you're using an older version or a different DB or if a view is no good let me know and we can try to find another solution.
This is really helpful, thanks a lot.
A view should work (or namely, creating a temp table of that view should work). My SQL skills are pretty weak but there's nothing in here individually that I don't recognize so I'm going to take some time to try to parse this. I'll let you know how I come out. Thanks!
&gt; I'm looking to add a column to Table 1 okay, alter table 1, add the column, let's call it in_table2, integer... i'm going to assume this is for microsoft sql server (you forgot to mention which dbms)... UPDATE table1 SET in_table2 = CASE WHEN t2.[CustID] IS NULL THEN 0 ELSE 1 END FROM table1 t1 LEFT OUTER JOIN table2 t2 ON t2.[CustID] = t1.[CustID] AND t2.[Date] = t1.[Date] 
This solution, of course, is a one time solution. If you need it to stay up to date as additions are made to table1 or table2 you need to use a view as I suggested in my comment, or associate this UPDATE query to a trigger that runs on changes to table1 and table2.
It's pretty straightforward: &gt; ;WITH T2Vals AS (SELECT DISTINCT CustID, Date FROM Table2) Builds a temporary table (CTE) that holds the distinct list of the CustID/Date pairs from Table 2 &gt; SELECT Table1.SalesmanID, Table1.CustID, Table1.Date, .....any other table1 fields 'YES' as in_table_2 FROM Table1 WHERE EXISTS (SELECT 1 FROM T2Vals WHERE Table1.CustID = T2Vals.CustID AND Table1.Date = T2Vals.Date) Select all of the records from table1 where the custID/Date pair is IN the temp table. For this set of data, we include an extra column called "in_table_2" that gets a value of "Yes" (since these are the ones that a match could be found for). &gt; UNION SELECT Table1.SalesmanID, Table1.CustID, Table1.Date, .....any other table1 fields 'NO' as in_table2 FROM Table1 WHERE NOT EXISTS (SELECT 1 FROM T2Vals WHERE Table1.CustID = T2Vals.CustID AND Table1.Date = T2Vals.Date) Add in the rest of the records for table 2 - the ones where the CustID/Date pair was NOT in the CTE. For these, we also include the in_table_2 column, but here we use a value of "No"
yeah, well... a one-time solution to a one-time problem ;o) the "mashing together parts of a few different tables" nature of table 1 is a giveaway
Fair point.
Yeah - then definitely go with this solution. Quicker and easier than mine (which is more geared towards making this an ongoing solution).
Update: I was able to apply this and generate exactly the list I was after. Thanks!
Thanks again for your help. I was able to get this solution working perfectly. Seems really simple now that I've seen/done it, but I just couldn't quite get there before.
The cursor takes a parameter that it uses in the where clause. So it just runs SELECT * FROM M_Zip WHERE Zip_Code = &lt;THE PARAMETER&gt; If you were looping over the rows in the cursor, you could do it like IonSush said or Use this method: FOR zip_rec IN getZip('01234') LOOP DBMS_OUTPUT.PUT_LINE('Zip is ' || zip_rec.zip_code); END LOOP; 
awesome, thanks!
PL/SQL (oracle) isn't vastly different than T-SQL (microsoft). It will be a good learning experience regardless. Also many larger companies have both running side by side. Views, tables, procedures, triggers, users, and grants also have a lot of overlap. I think they differ greatly on backup/restores, file system, and high availability strategies like replication and clusters.
Administration of SQL Server and Oracle is night and day. Just because you learn generic fundamentals of the SQL language will not help you admin a SQLServer. A lot of the concepts are the same but both you and your employer are going to be in for a reality check when you come back. Buy a a very thick SQL server book and devote yourself to learning it cover to cover. Know the capabilites of SQLServer as well as its limits. Install it somewhere and play with it. Unless their idea of a DBA is "someone who knows SQL", you need something else. Tldr; it will help you grasp the fundamentals but SQLServer database administration is more than writing queries.
 ID prntid Name DateTime 23452 NULL Item1 Time 23456 23452 Item2 Time 23459 23456 Item3 Time SELECT i1.ID , i1.Name , i1.DateTime AS i1_datetime , i3.DateTime AS i3-Datetime FROM items AS i1 INNER JOIN items AS i2 ON i2.prntid = i1.ID INNER JOIN items AS i3 ON i3.prntid = i2.ID WHERE i1.prntid IS NULL
Im not very good at explaining.. sorry :)
After looking a the [time doc](http://technet.microsoft.com/en-us/library/bb677243.aspx) I am not sure if this is exactly what you want. Given that you have a value of 64 I am betting that your Duration field is just the number of seconds and your conversion of '118' to 1:18 is incorrect. I really think [this stack overflow question](http://stackoverflow.com/questions/2316288/efficient-way-to-convert-second-to-minute-and-seconds-in-sql-server-2005) is what you are looking for. TL;DR SELECT RIGHT(CONVERT(CHAR(8),DATEADD(second,&lt;Duration column&gt;,0),108),5) 
Thanks for your help. i worked in out :)
Thanks. I have worked it out
I think that is what you want unless you want if thats not powerful enough you'll probably need to use recursive SQL.
the main function for me is to limit the select top 1 whatever - aka joining a table but only using 1 row to avoid amplification of the table on the left. left - 100 rows, right 2 rows = left outer join = 200 result rows left - 100 rows, right top 1 rows out of 2 = outer apply= 100 result rows. makes your financials less fuck up looking. 
Haha, I really hope you're kidding!
Thank you, that's helpful
&gt; I want just 1 row with the most recent datetime. GROUP BY in the outer query will work if all you want is the MAX(DAteTime), but this technique will give you other columns from the max row at the same time -- SELECT i1.ID , i1.Name , i1.DateTime AS i1_datetime , i3.DateTime AS i3_Datetime , i3.foo AS i3_foo -- if necessary FROM items AS i1 INNER JOIN items AS i2 ON i2.prntid = i1.ID INNER JOIN ( SELECT prntid , MAX(DateTime) AS latest FROM items GROUP BY prntid ) AS m ON m.prntid = i2.ID INNER JOIN items AS i3 ON i3.prntid = m.prntid AND i3.DateTime = m.latest WHERE i1.prntid IS NULL 
the query you posted cannot work your FROM clause references only the ems table, but your CASE erxpression references a different table, ms was this a typo in your posting, or what's actually going on here?
Yup I got it :)
Script if from commandprompt using osql You can then use sp_password or better, ALTER LOGIN But you would need to connect using Windows Authentication I guess, otherwise you will have to enter the current sa password to log into each one
Oh man. I'm too stupid to understand this. Wouldn't this mean that I'd have to type somthing along these lines countless number of times? sqlcmd -Q "ALTER LOGIN sa WITH PASSWORD = 'newpass1'" -S"SERVER1" -Usa -Poldpass1 
Do you have a central management server? That could make this task a lot easier. You could then create a script and run it against all of the servers under the CMS with one click. Also, If you screw up, you've screwed up on all of the servers with one click...
Sorry, I know a lot less than I thought before (which wasnt much..) I kind of assumed the only difference was the backbone and they all accepted the same statements. Edit: And I even totally forgot to prefix it, well.. I screwed this post up royally :(
You also don't have the comment in your select statement.
&gt; Where do I create SaLogins? Most developers will have SQL Server running locally just for playing around. I'd create it there, or on a dev server. &gt; Isnt that unsafe to save old and new passwords on a SQL db? Yes, but you'd be storing them purely to generate the script, then dropping the table (and data). But if the old passwords are all the same, and the new ones are too, you may aswell just copy and paste your single command in a text editor, then amend the hostnames
Yeah, I had it but removed it because it was just getting an unrelated comment and I assumed I had it 100% wrong anyways
Ah, thanks for helping my brainfart. Also how do people do that code formatting on reddit?
What do I do if I don't have the old Passwords in other words I am to only use Windows authentication? 
Yeah, what I thought was create a registered servers group in SSMS, then you can run the alter against all the servers at once (assuming the passwords are the same on all the servers). We do this when we need to deploy a change on multiple servers.
Thats what I'm trying to do. I'm trying to reference, in the concat, a table 'trig' in db 'ms'. trig and ems do not have the same type of data at all. 
&gt; Lines starting with four spaces are treated like code Expand the "formatting help" link to see more options. 
you cannot reference a table like that, it has to be a column furthermore, you can only reference one row, not all of them (hence the "more than 1 row" error message) perhaps if you explained what you're trying to do? are you actually trying to insert 4992 rows for every row of the score table?
&gt; I'm trying to take every value of ems, which consists of an ID and a string of 8 characters, and match them to 4922 other, shorter characters a join will do this -- join each row of ems to every row of trig could you do a SHOW CREATE TABLE for both tables please? 
i don't think you can do that at all, because there appears to be no sequence within the rows that you can use for your "up until" criterion
Not explicitly a SHOW CREATE TABLE, but these are the statements from my notes. CREATE TABLE trig( gramid MEDIUMINT NOT NULL AUTO_INCREMENT, gram VARCHAR(3), PRIMARY KEY (gramid) ); CREATE TABLE ems( emid MEDIUMINT NOT NULL AUTO_INCREMENT, em VARCHAR(255), emscore SMALLINT, PRIMARY KEY (emid) ); 
Could you use inner select statements to do that? Bit rusty but something like the following 2 statements: UPDATE table SET Time = (SELECT Time FROM table WHERE Var = 2 AND ID = 123) WHERE VAR = 1 AND ID &lt; 153 UPDATE table SET Time = (SELECT Time FROM table WHERE Var = 2 AND ID = 153) WHERE VAR = 1 AND ID &gt; 153 
This might be a solution. Still allows for manual running but it's getting somewhere going to investigate this option 
&gt; The sequence will be within the Time. order by will sort that out. it's confusing that your sample data did not reflect this 
i don't see where "Each row in ms.trig has a score associated to it, and I want to add up all of the scores of matches and sum them together"
Oh yeah, I forgot that I did an ALTER and added that in. trig is now: CREATE TABLE trig( gramid MEDIUMINT NOT NULL AUTO_INCREMENT, gram VARCHAR(3), grscore VARCHAR(3), PRIMARY KEY (gramid) ); 
you made it a VARCHAR? eewwwwww... run this first, and check the results before using it in your INSERT -- SELECT ems.emid , ems.em , SUM( CASE WHEN ems.em LIKE CONCAT('%',trig.gram,'%') THEN CAST(trig.grscore AS SIGNED) ELSE NULL END ) AS score FROM ems CROSS JOIN trig GROUP BY ems.emid 
How do I find if I have a CMS? Just ask someone? Assuming I do how do I go forth changing the passwords for the following servergroup Server_1 Server_2 Server_3
If you look at my comment, you can do this simply within SSMS. Go to View -&gt; Registered servers, create your server group if it does not exist, and add the appropriate SQL Server instances (this step will have you log in to them using your windows credentials). Then, Ctrl+N to open a new query connection to all servers in the group. This way you can execute your script on all servers at once. A quick google tells me you will want something like this USE [master] GO ALTER LOGIN [sa] WITH PASSWORD=N'NewSAPassword' GO
Thanks for the response. Let's say that A &amp; B are the Make and Model of a vehicle, and C is the price that it sold for at a dealership. I'd like the 10 most recent dealership sales, per Make &amp; Model. I also have the freedom to concatenate MakeModel if that makes it easier.
Im not sure about limiting it to 10 yet, but here is a starting point Select Concat('Make','Model') as name, Price, Date from table order by name, date desc; A potentially painful way to do it, is select 10 results into a table for each make/model. and then just select all from that table. Essentially something like this: select concat('make','model') as name, price, date into newtable [in databasename] where name = 'ford ranger' order by date desc limit 10; repeat for every combination, save the sql and run it as necessary for an updated list? im somewhat new to sql myself, so im poking around to find out more. 
yea, the big issue of course is limiting results by a group. that's the crux of the problem, and after some extensive googling it doesn't sound like there's a trivial solution. i'll end up making a gigantic table and having to use a rank or rownumber function on the data then grabbing all ranks or rownumbers less than equal to ten
&gt; dbm sorry, it's Netezza. Kind of somewhere between MS SQL and Oracle. 
I tried to do that but it didn't work for me when I was creating the thread, but now it works in this comment. SELECT person FROM people