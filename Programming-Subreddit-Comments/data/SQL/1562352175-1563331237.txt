You want to probably learn MS SQL, and will likely end up working in either it, Postgres, or something like MySQL, depending on the size of the company. In general from my experience most mid to large size companies will have their analytics databases on MS SQL, while their larger transactional databases may be in Oracle. Postgres and MySQL (imo) are more for small companies, but others may have differing experience. In terms of analytics/data science it also seems that more jobs online reference MS SQL as well. You could pretty easily switch flavors from one to the other, so it doesn't really matter. I like MS SQL because there is a large community &amp; amount documentation.
Is there a lot of difference between different SQL software? Is it difficult to switch?
Not difficult, per se, but I've never done it. It would be more annoying because the syntax is different, and some functionality is different. I would accept a job using another flavor of SQL if it was a nice step + raise, but if two jobs were about equal then I would pick the job that used MS SQL. For some flavors it would be harder, for example I think MySQL lacks a lot of functionality that I have come to rely on in MS SQL. It *might* be able to do the same thing, but it would be clunkier and harder to write. In general my appetite to learn another flavor of SQL is about a 0, but then I have no ambition to go down the DBA road. If I did want to become a DBA, then I could absolutely see a massive benefit to being familiar with multiple flavors of SQL, but for analytics &amp; data science I don't see any real value. MS SQL is fairly cheap, and has a lot of robust features.
Thank s might well work. I will have a look at this on Monday. Thanks for the suggestion!
Document databases (aka NoSQL) usually serve a different purpose than relational databases. Many modern stacks and applications are built on them. Transactional/relational databases, especially financial data tracking are usually served better by relational databases mostly because of ACID. Inventory, sales, accounting, etc. tend to fit better here. [This video](https://www.youtube.com/watch?v=v_hR4K4auoQ) meant for people developing on Google's Cloud Firestore that I think gives a good explanation about the differences. For apps like Facebook, Gmail, Instagram where it's super read-heavy, Document DBs are a no brainer. These things are changing and in some cases converging (aka mongo, RDS, MSSQL, Cosmos which all do all of the above) very rapidly in our industry. I try to lean the foundational technologies like ANSI SQL, REST, etc. It makes it much easier to approach the applied side.
Alright. Thank you for clearing all my queries.
Introduce them to Google and Stackoverflow, and teach them how to do what all the rest of us do: Learn how to ask Google, and keep rephrasing your search until an answer pops up in one of the top three links. Also, how to come here, post some sample data, and ask for help / direction. Sans that there are a bunch of photos on Google ([here](https://www.google.com/search?q=sql+cheat+sheet&amp;rlz=1C1CHBD_enUS819US819&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjVyIOeu57jAhUQpJ4KHVtNCcAQ_AUIECgB&amp;biw=1366&amp;bih=667)) for cheat sheets and you might want to look them over to see if any look appropriate/applicable for your purposes.
NoSQL tools are used mainly in the web development space. If someone cares about that data for more than the most basic analytics, it is usually ETL‚Äôed into another environment and remodeled for reporting. Unless you‚Äôre going into development, I don‚Äôt see a large benefit to NoSQL environments in the analytics space. I do see a lot of Python, Jupyter, R, Tableau, and in some pockets Hadoop/Spark. MS SQL Server provides a really solid toolkit for analysts, and T-SQL is still widely used for analytics. PostgreSQL is probably the biggest up-and-comer of the last decade, making a lot of strides into the enterprise, especially with Cloud offerings and derivatives like Amazon‚Äôs Redshift. For smaller databases, MySQL/MariaDB is still widely used, and companies that can burn money to heat the building buy a lot of Oracle. There‚Äôs also quite a bit of MicroStrategy, SAP Business Objects, and Cognos in the Business Intelligence market as well. Having been on the data/ETL/analytics side of the industry for a few decades now, I always say that the tools are always less important than the concepts and ecosystems - every company I‚Äôve ever been at used a different toolkit, albeit with a few commonalities. I suggest picking a toolkit for yourself that will stand the test of time, and get used to learning. Your toolkit should include a good cross platform text editor you can count on for your whole career, a robust and mature programming language, and a free, runs-anywhere, RDBMS that you can experiment with. These are tools you can use no matter what your job demands. For me, those have been Emacs, Python and PostgreSQL, but for you they may be different. Best of luck!
So you are suggesting I learn foundational tech like ANSI SQL rather than something like MS SQL?
&gt; I suggest picking a toolkit for yourself Toolkits like ANSI SQL or REST?
Very useful sheet. Really appreciate your response thank you
Very useful sheet. Really appreciate your response thank you
It is more annoying than difficult, if you can code in MSSQL you can code in MySQL or Oracle, there will just be small syntax differences thatll drive you nuts for a bit.
Short answer - my original meaning for that comment was looking into investing in tools for yourself, not just the actual tools you will use for your current/next job. Long answer - you may decide SQL and REST may serve you really well, but there are some deeper questions you need to also explore that will tell you if those are right for you: * Are you looking to be a web developer, DBA, data analyst, ETL developer, data architect, business analyst, something else? * What skills are in demand in the industry you want to work in? * What skills are in demand in the location you want to work in? * What skills are foundational to advance to where you want to be? Never assume your entry level job will get you a direct path to where you want without a plan. * What non-technical skills can you be working on? Employers can teach technologies no problem. They can‚Äôt teach communication, people skills, personality, or passion easily.
Not sure I 100% understand what you're trying to do, but the below will accept a SP name and any number of param values, log it all to a table, and execute the target SP with the given values. Caveat with this is that the params **must** be passed in the same order as they appear in sys.parameters. You could enhance this to accept the param name as well in the args string so that you join on actual param names instead of an arbitrary order value and avoid this pitfall. Obviously you'd want to wrap this in a transaction and handle errors gracefully so that they're surfaced back to the user calling the helper proc. &amp;#x200B; -- Create a test DB CREATE DATABASE [testdb]; go -- Switch the DB context USE [testdb]; go -- Create a basic logging table CREATE TABLE dbo.sp_log ( id BIGINT NOT NULL IDENTITY(1, 1), proc_name NVARCHAR(50) NOT NULL, execution_time DATETIME NOT NULL, params NVARCHAR(max) NULL ); go -- Create a sample target SP that accepts a couple of parameters CREATE PROCEDURE [dbo].[Test_proc] (@param1 VARCHAR(50), @param2 VARCHAR(50)) AS BEGIN -- Whatever your other stored procedures do goes here. -- Just selecting the params back out as an example. SELECT @param1, @param2; END; go -- Create the helper proc to handle parsing of sp and its param values CREATE PROCEDURE dbo.Sp_helper (@args NVARCHAR(max)) AS BEGIN -- Figure out where the SP name ends and the params begin DECLARE @paramstartpos INT = Charindex('||', @args, 1) + 2; -- Set the SP name DECLARE @sp_name VARCHAR(50) = Substring(@args, 1, @paramstartpos - 3); -- Set the param list DECLARE @param_list NVARCHAR(max) = Substring(@args, @paramstartpos, Len(@args) - @paramstartpos) -- Declare a table to hold the params and populate it with the parsed values that were passed DECLARE @params TABLE ( parameter_id INT, param_value NVARCHAR(255) ); INSERT INTO @params SELECT Row_number() OVER ( ORDER BY (SELECT NULL)) AS parameter_id, value AS parameter_value FROM String_split(@param_list, '|'); -- Line up the expected params for the target SP with the values that were passed to the helper proc DECLARE @paramvalues TABLE ( sp_name VARCHAR(255), param_id INT, param_name VARCHAR(255), param_value NVARCHAR(max) ); INSERT INTO @paramvalues SELECT pr.NAME AS sp_name, p.parameter_id AS param_id, pm.NAME AS param_name, p.param_value FROM sys.procedures pr JOIN sys.parameters pm ON pr.object_id = pm.object_id JOIN @params p ON p.parameter_id = pm.parameter_id WHERE pr.NAME = @sp_name; -- Put the params in a SQL string for logging and to be passed later during execution of the target SP DECLARE @sqlparamlist NVARCHAR(max) = Stuff((SELECT ',' + ( param_name + '=' + '''' + param_value + '''' ) FROM @paramvalues FOR xml path('')), 1, 1, ''); -- Log the execution INSERT INTO dbo.sp_log SELECT @sp_name, Getdate() AS execution_time, @sqlparamlist AS params; -- Declare the target SP execution code with the proc name and passed params DECLARE @sqlcmd NVARCHAR(max) = (SELECT 'exec dbo.' + @sp_name + ' ' + @sqlparamlist); -- Execute the target code EXEC Sp_executesql @sqlcmd; END; go -- Execute the sample SP with logging EXEC dbo.Sp_helper 'test_proc||testing...|testing2...';
[https://www.w3schools.com/sql/sql\_primarykey.asp](https://www.w3schools.com/sql/sql_primarykey.asp) &amp;#x200B; [https://www.dofactory.com/sql/where-like](https://www.dofactory.com/sql/where-like) &amp;#x200B; [https://www.postgresql.org/docs/9.4/sql.html](https://www.postgresql.org/docs/9.4/sql.html) (click on download and download the whole SQL document to have it in your computer) &amp;#x200B; [http://www.sqlservertutorial.net/sql-server-basics/sql-server-select/](http://www.sqlservertutorial.net/sql-server-basics/sql-server-select/) &amp;#x200B; [https://www.tutorialspoint.com/sql/sql-alias-syntax.htm](https://www.tutorialspoint.com/sql/sql-alias-syntax.htm) &amp;#x200B; [https://docs.oracle.com/javadb/10.8.3.0/ref/](https://docs.oracle.com/javadb/10.8.3.0/ref/)
SQLZoo is a great starting point for beginners. Additionally, refer them to the WiseOwl SQL playlist on YouTube. That should be more than enough to get them rolling.
Well once you learn the foundational stuff the vendor-specific versions become much easier. Imho you will also understand them at a deeper level. For example, Microsoft has an implementation of ANSI SQL they call TSQL. It's ANSI SQL plus some additional keywords and features. Mircosoft also has a number of commercial products that are "SQL Servers." Microsoft SQL Server (mostly for on-premise), Azure SQL, Cosmos DB, and more. Oracle has something similar. Then there's MySQL, Amazon RDS, Mongo, and more. But they all use ANSI SQL and then extend it with their own features, keywords, etc.
Will check these out and refer them to him thank you!
Amazing thank you so much. Very useful links.
Enjoy
In case that is a column of consecutive dates, look up DATEPART in your search engine of choice. Play around with it. That's probably what You need.
`-- Declare # of years you want dates for, the month, the day, and the occurrence of that date within the month` `declare @years tinyint = 15;` `declare @month_name varchar(10) = 'November';` `declare @day_name varchar(10) = 'Friday';` `declare @week_num tinyint = 4;` &amp;#x200B; `-- Build the calendar` `WITH calendar` `AS` `(` `SELECT CAST(getdate() as date) AS [date]` `UNION ALL` `SELECT DATEADD(dd, 1, [date])` `FROM calendar` `WHERE DATEADD(dd, 1, [date]) &lt;= dateadd(year,@years,getdate())` `)` &amp;#x200B; `, month_days as (` `SELECT [date]` `,row_number() over (partition by year([date]),month([date]) order by date asc) as [week_num]` `FROM calendar` `where 1=1` `and DATENAME(weekday,[date]) = @day_name` `and datename(month,[date]) = @month_name` `)` &amp;#x200B; `select [date]` `from month_days` `where week_num = @week_num` `OPTION (MAXRECURSION 0) --Unlimited recursion; limit the number of years in parameter at the top` `;`
The following may sound harsh, but I mean you no offense; just trying to help out. There is no answer to the question you've asked, and it is likely because you don't know enough about this area to ask a well-formed version of the question. To use an analogy, you've asked something like "What is the best tool for water these days? I hear boats are used a lot, but should I learn water hose instead?" The answer to this question is either "What the heck are you talking about?" or "What are you actually trying to do?" To answer what I *think* might be your question: yes you should learn SQL if your interests/career are tending down the path where SQL seems like it might be used. The "flavor" of SQL (Postgres, MySQL, Oracle, etc.) doesn't matter (as long as it's not MS Access SQL.) Most major implementations are close enough to the standard that, if you learn one, you know them all (and Google is enough to fill in the differences as they arise.)
I'm just used to avoiding putting any SQL code in directly in any user app (though this app is company internal &amp; will only be used by 3-4 people so realistically I should not be too concerned with any injection)
Getting out of my area of expertise so I don't have an educated opinion to give you. You could create a sproc to validate whatever is being passed from the app to mitigate against the possibility of injection. For example you might assemble the dynamic query in the app, then pass the query itself down to the database where it is executed as dynamic SQL. Of course you could also mitigate things by heavily restricting the login your app uses to access the database and locking down what tables it can query, restrict it to read only, point it to views that don't have any sensitive columns, mask sensitive columns, etc.
Cosmos DB is non-relational and not connected to SQL Server at all. SQL Server is the classic RDBMS, Azure SQL DB is single-serving SQL Server databases hosted in Azure, Azure SQL Managed Instance is like a ‚Äúlight‚Äù SQL Server limited to 100 databases on an instance but you don‚Äôt have access to the host (so some things don‚Äôt work the same).
If you have a date table, then just use a row_number() and pick the rows where its equal to 4.
I'll take a look at this on Monday. I'd considered a wrapper proc - nice to know it can be implemented. FWIW - I'm creating a logging system for a set of SSRS reports. I'd use ReportServer, but as my reports are deployed to customer sites, I cannot rely on having access to it.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/applehelp] [Reinstalled OSX from Time Machine, postgres error](https://www.reddit.com/r/applehelp/comments/c9l5i9/reinstalled_osx_from_time_machine_postgres_error/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
wow, that looks awesome, can i run that in MySQL?
&gt; I am trying to find a way to write SQL to pull that date. 1... for which years 2... what platform/database are you on
Please don't recommend Oracle to *anyone*. Least of all a beginner.
There are free to use versions of SQL Server (Developer) and PostgreSQL. These are two fairly popular, pretty power relational DBMS. That is what you will primarily run into in the field. Ignore the "NoSQL" stuff, despite what you read here. They are mostly open source and a little flaky and unreliable in my experience.
Why? If I was interested in being a DBA it would be the first thing I'd want to dig into due to the sheer number of Oracle database and servers in production. My company, for example, has hundreds of them. It has *one* MS SQL server. My server. Our DBA group treats MS SQL like its dirty, and wants as little to do with it as possible. I'm not recommending it as my preferred technology. I'm recommending it as a career path.
So... you'd probably want to learn Oracle if you were going to be a DBA. https://db-engines.com/en/ranking_trend/system/Microsoft+SQL+Server%3BOracle
Every experience I've had with Oracle has been negative: cost, performance, connectivity. MS SQL is far and away the better product. The only people I've seen turn their nose up at MS SQL are LAMP nerds. Even if you want to avoid Microsoft, I would still recommend PostgreSQL. It's free to use for a developer.
That's great an all, but it's still the #1 tool by the numbers, and Oracle salaries (based on a quick Google) are also higher. Again: I'm not recommending Oracle as a technology for someone to use, I'm recommending it as a technology for someone to learn [if they are interested in becoming a DBA.] I'm not saying which product is better. I'm saying if you want to go into analytics (my field) that I would recommend MS SQL, as it seems like most analytic shops for mid to large companies are backed by MS SQL, and that in my experience its smaller 'boutique' type companies where you run into things like MySQL or Postgres. Postgres is fine to recommend, and learn, but it might limit you to smaller companies. Not a lot of 100M/year companies are likely going to use it... so it will kind of pigeonhole your employment opportunities.
Standard relational databases: you can't go wrong with Postgresql or Microsoft SQL server. But they do have imitations, that are handled by other software. Graph databases are good for organizing and querying across graphs. Analytical databases are good at running queries similar to SQL across bigger Data sets. Distributed data systems like parquet across distributed file systems can also be queries with SQL, though they don't form a relational database. All of them, and even more, are used a lot in the industry. Depends what you need it for
&gt;queries nice
There are dozens of ways to do this. How are you getting the file? You want to automate the whole process on your end. How you get the file will impact what tools are best for your use case. /R/ETL will be another resource
At some point, sure. I picked up Oracle along the way. But I started out on SQL Server.
I don't think that's a bad idea. From the little I know about Oracle I'd rather start with MS and then migrate to Oracle. Still I'd be trying to do that asap within 2-4 years if I was interested in the DBA track.
Please tell me there isn‚Äôt an actual MS Access SQL...
I find the SQL problems on Hackerrank to be helpful. There is a wide variety of questions in terms of difficulty and data sets. Happy (job) hunting!
Good point, but it depends. I switched into SQL after years working with other technologies, and I set a 2 year goal to progress to a senior level position which I failed by about 6 months. But then I was older at that point and had a lot of professional experience in other areas, plus demonstrable soft skills. I suppose if you're fresh out of college that would be a bit harder to do, *however*, I firmly feel that a senior level position is something you ear by accomplishment, and not by time in the field. If you are a quick learner and begin contributing/taking on the responsibilities of a senior level position, then sure you can switch companies after 2-3 years and get the title bump early. In the beginning of your career title is more important than salary.
MSAccess does have its own flavour of SQL. It is painful and somewhat horrifying.
Probably not due to syntax and function differences between engines. Google for the MySQL equivalents to the SQL Server code above.
It depends, yes, but only if you have plentiful experience. As you said, your background (hard/soft skills) definitely help. Senior definitely &lt;&gt; time, unfortunately, most if not all companies still consider senior directly equal to time spent A) with the company or B) with the position. I've noticed in the technology field over the past couple of years the title "senior" is slowly drifting away only to be replaced by "myField Title 1, 2, 3, 4, ... 11". IMO people focus WAY too much on their title and not on their salary. If your company's pay scale is geared more toward "time spent" versus "knowledge understood" that's where you will see your 20-30% raises by switching companies (or working for yourself). With all that said, 2-3 years is still ridiculously early for any type of "senior; level X" title. There's no way in 2-3 years one can be exposed to and understand a wide enough variety of issues to be considered a senior. I'm sure there are outliers, but, for the *vast majority* it's not feasible. Note: I'm currently a mid-level DBA (\~4 years) and there are some "senior DBA's" which I can run circles around and other mid-senior level DBA's which I can barely hold a candle against. Age gaps are wide as well; it's not like it's some senior DBA who's just coasting into retirement (which is sad, but a very surreal reality).
Access does have SQL and it's as backasswards as you might think it would be. I recommend against it mainly because of its crippling multiple join syntax: https://docs.microsoft.com/en-us/office/client-developer/access/desktop-database-reference/inner-join-operation-microsoft-access-sql .
So the users_id in the items table refers to the user who posted the item? Yes, that works, but it's generally bad form to not name your fields in a more descriptive way. I suggest using posted_by_user_id or something similar. Also, the XREF joining table is correctly modeled for users to liked items. You may want to consider adding a unique constraint to user_id and item_id in your XREF table or making a composite primary key from those fields, unless users can like an item multiple times.
Thank you!
If you want to learn more logic-based SQL challenges as opposed to the general query for certain results I‚Äôd recommend trying codewars. Their site has a wide variety of problems with varying difficulty levels as well
I think that should work. Couple of quality-of-life enhancements I‚Äôd make: &gt; table names shouldn‚Äôt be plural (item instead of items; user instead of users). It‚Äôs just a best practice. &gt; give the id columns more descriptive naming. Ideally, a column would be named the same thing regardless of what table it‚Äôs in. Example: in the users table the identity is named id, but that column in the items and 3rd table is named users_id. Name it user_id in the users table so everything‚Äôs intuitive. Same goes for the other two tables.
Emacs... damn... DAMN!!!!!
I agree with you mostly, especially about what companies consider, but that doesn't mean you can't jump ship to another company after 2-3 years to get the title bump. You are right, to an extent, that most people think too much about their titles and not their salaries, but titles are very important in the formative years of your career and show a demonstrable success, which will enable you to get large raises every time you move to a new company. &gt;With all that said, 2-3 years is still ridiculously early for any type of "senior; level X" title. There's no way in 2-3 years one can be exposed to and understand a wide enough variety of issues to be considered a senior. I'm sure there are outliers, but, for the vast majority it's not feasible. You aren't wrong, but it's a good goal to shoot for. &gt;Note: I'm currently a mid-level DBA (~4 years) and there are some "senior DBA's" which I can run circles around and other mid-senior level DBA's which I can barely hold a candle against. Age gaps are wide as well; it's not like it's some senior DBA who's just coasting into retirement (which is sad, but a very surreal reality). I'm really more speaking to the analytics side of roles.
Also adding on. MS Access SQL is absolutely horrible. Join syntax is awful and requires parentheses for joins, random data errors, pulls incorrect data sometimes when query gets complicated, no window functions, 2gb data limit, crappy editor. Absolutely despise Ms Access and agree with learning any flavor of SQL except this MS Access initially. The MS Access sql syntax is really not too transferable to other query engine ones
Also adding on. MS Access SQL is absolutely horrible. Join syntax is awful and requires parentheses for joins, random data errors, pulls incorrect data sometimes when query gets complicated, no window functions, 2gb data limit, crappy editor. Absolutely despise Ms Access and agree with learning any flavor of SQL except this MS Access initially. The MS Access sql syntax is really not too transferable to other query engine ones
Even if you "jump ship" to another company, does that really mean you're a senior? No; Salary is everything, I work closely with OLAP people and they have ZERO clue about how to optimize shit (this extends to the application team, but, it's understandable at their level). They \[OLAP\] write atrocious queries and then are confused as to why they take so long, This is a both a business and hiring part, but, it's still a real issue. Log shipping? Redoqueues? Blocking? Locking? They are clueless. Guess who pays for it? OLTP. My first DBA job (jr DBA) started at 55K and I'm now at 100K, but, I also deal with ridiculous expectations of monitoring servers outside of being on call (2 on 2 off). Plus, if opsgenie doesn't alert me, I'm still on the hook and it's bullshit. I also have a decent understanding of powershell, python, and GO. I can perform OLAP functions and that of my main role, OLTP, but, the on call portion and "accountability" portion is unrealistic for most companies and it's a struggle. For the record, I've sat in over 50+ interviews including basic white board exercises and the amount of people who come through with "senior" in their title, but, cannot complete a simple whiteboard test is mind blowing. My heart goes out to all DBA's who are expected to conquer the world, but, are paid lower than their counterparts.
Salary *is* everything, but if lets say you start off on day one, and in two years jump ship for a senior title without a senior pay. Cool, because in another two years you jump ship again and get a big raise. Ideally you should be getting a big raise each time you jump, so you should end up with a senior title at the bottom rate. Then as time goes on you start to only seek the maximum pay range. &gt;I work closely with OLAP people and they have ZERO clue about how to optimize shit (this extends to the application team, but, it's understandable at their level). They [OLAP] write atrocious queries and then are confused as to why they take so long, This is a both a business and hiring part, but, it's still a real issue. Log shipping? Redoqueues? Blocking? Locking? They are clueless. Guess who pays for it? OLTP. So I'm quite good at optimizing these things, and architecting the OLAP environment. I'm not ging to get into a back and forth here, but "our groups" general response here, to you, is, "so what? make it faster, do your job." None of us care if our queries are inefficient if they are valuable. If I write a query that takes 72 hours to run, but saves the companies 1 million dollars, or makes the company 1 million dollars... what exactly do you have to say about how long it took to run? Now, personally, I would like to see that 72 hr query run in 72 seconds if possible, and that is the area which I have chosen to specialize. Nevertheless, I think I stand by my statement about 2 years to a senior. It might not apply to most people, but I'm talking about people with a dedicated plan.
I think our definition of "senior" is polarized. You say you're quiet good, but, does this account for changes made from an OLAP team which you do not expect? I give 100% if your queries are inefficient. Lol. Clearly, you work in a WAY different environment than myself as a 72 hour query is laughable, at WORST. You have ridiculously expectations my friend; even with a dedicated plan.
Depends on needs - I prefer SQL Server but I‚Äôm used to it and now mostly deal from an an Analytics perspective. Not sure of current licensing rules and pricing. Postgres is good too
Whatever one you're most familiar with
&gt; does this account for changes made from an OLAP team which you do not expect? What kind of question is this? I work closely with the C-suite executives to produce analytics, and there is no such thing as, "did not expect." -- There's chewing someone's ass out and explaining to my boss how much money we lost because someone fucked something up, and didn't tell us about it in advance. &gt;Clearly, you work in a WAY different environment than myself as a 72 hour query is laughable, at WORST. Model scores, census data, credit data, billions of rows... I had a query once that calculated something like 14.5 *trillion* rows. What do I fucking care if it takes the weekend to run? Are you saying the server isn't stable enough to run it for that long? Do we need to get a better server? Do you know how much money we might save after looking at the results of that 72 hour query? Exactly, now go back to work DBA bitch.
Nah, it‚Äôs not difficult to switch. I‚Äôve used HiveQL, Redshift, Oracle, SQLite, and Teradata all in the past 3 years as an analyst. The biggest difference was HiveQL because Hadoop doesn‚Äôt store data the way a database does so it‚Äòs a whole different paradigm. But the syntax is still largely the same. If you know how to write SQL for one system you can write it in another, you just need to adjust the syntax and get familiar with nuances of each flavour of SQL. I imagine it‚Äôs a different story from the DBA or Admin side, but if you‚Äôre just running queries then just learn any SQL to start.
I'd say it depends on budget more than anything, then once you have that you can look at what you can get in that price range.
agree with this if a user can only like an item once a composite key with user_id and item_id in your xref table makes a lot of sense.
\&gt; In general from my experience most mid to large size companies will have their analytics databases on MS SQL, while their larger transactional databases may be in Oracle. Postgres and MySQL (imo) are more for small companies, but others may have differing experience. I think this \_was\_ the case until recently, but PostgreSQL is beginning to become a competitor. I'm currently working on contract for a large [GOV.UK](https://GOV.UK) department and almost every project is using PostgreSQL. MSSQL does have the edge in some areas and is definitely easier to get up-and-running with, plus being part of the MS ecosystem and being able to plug PowerBI right in. On the other hand, though, PostgreSQL has embeddable procedural programming languages, \[PostGIS\]([https://postgis.net/](https://postgis.net/)), excellent native JSON support, arrays, \[hstore\]([https://www.postgresql.org/docs/10/hstore.html](https://www.postgresql.org/docs/10/hstore.html)). The best part, everyone gets the \[awesome edition\]([https://blog.codinghorror.com/oh-you-wanted-awesome-edition/](https://blog.codinghorror.com/oh-you-wanted-awesome-edition/)) for free. You can't really go wrong with either.
I'd add that the foreign key columns would better be named in singular (item_id instead of items_id), since they refer to a single user/item. Also you might consider things like category and maybe color or even size to occupy their own table(s) as well and reference them in the items table.
I will have to develop an estimating database for a company, so would Postgres be able to handle the magnitude of the data(i.e. is Postgres good for content heavy db) ?
Okay, but if I decide to use Postgres and mySQL, then that would be at no cost since these are open-source? Also, I just wanted to know if open-source dbms can be used for company data?
Additionally to my question: if anyone has estimating db development experience, can you please share a few pointers in terms of any tips (e.g. how to get started, what resources to use etc.)? I am a beginner in databases but would like to eventually build my skill level so that I can develop a cost estimate database for my company.
Imo those are both reasonable options, but there are countless factors. If the database isn't going to be huge Microsoft's SQL Express is free up to like 10gb. With the time a project like this I would start looking into industry standards for a product that handles similar workflows either for ideas or to purchase as you may find that to be more cost effective.
Yeah, I don‚Äôt think it‚Äôd have any issues there. I used it years ago for the backend of an internal web app with ~80 concurrent users. No big issues
Depends on if you use mysql community or one of the paid versions, same with postgress and enterpriseDB postgress, The big difference between free and commercial is the support. If you don't need that then you're good to go (and there are plenty of decent community support groups too). If however this is going to be a key app for your business then it may be worth investing to have that backup.
Why on earth would they come up with a separate flavor of SQL? At this point, just embed Express with Access - it‚Äôs not like they charge for it.
Microsoft be Microsoftin'
It was nothing! Did it work¬ø¬ø
There is tool called SQL search that is absolutely incredible.
In mysql, I like information_schema for just about everything.in Oracle, there are a lot of views like all_tab_columns to see what is there.
The whole SQL Toolbelt is amazing
See if this site can help you with an example you can use. https://www.connectionstrings.com/sql-server/
Specifically APEX SQL Search. (It is a utility that works on top of SQL Management Studio)
Hi thanks for your comment. That was the website that i mentioned it just mentions stuff about drivers and i was quite confused. I will look again those thanks.
I like to find the *main* tables and see how they're related. With most systems this gives me a decent feeling for how things fit together. When I say main tables, I'm talking `order`, `customer`, `employee` etc. Sketching them and the key relationships on a piece of paper is usually a pretty solid start. I much prefer this over reading docs which are often out of date or incomplete.
Sorry about that. Try something like this: Driver={SQL Server Native Client 11.0};Server=myServerAddress; Database=myDataBase;Uid=myUsername;Pwd=myPassword;
`information_schema` is actually standard, most RDBMS offer it.
Thats all good ill try that one. Thanks for the reply :)
Hi. I have done a compilation of some code i have used to connect to SQL DB\`s from excel i VBA. Im not sure if it will cover your case but i guess its worth a shot. Had alot of trouble when i was starting out with this. To make it work i have to activate"Microsoft ActiveX Data Objects 2.8 Library", believe this could be other versions aswell. I always have to activate this if I want to setup a connection from a new excelfile. (Might be a given allready but rather include it to make sure you dont get errors cause of it.) &amp;#x200B; I dont know how this compares to best practise but its done the job for the needs i have had. &amp;#x200B; `Dim adoCN As ADODB.Connection` `Dim rs As ADODB.Recordset` `Dim sConnString As String` `sConnString = "Provider=SQLOLEDB.1;Password=Passord;Persist Security Info=True;User ID=user;Initial Catalog=database;Data Source=server"` &amp;#x200B; `adoCN.Open` `sConnString` &amp;#x200B; `'Check state of connection.` `Debug.Print "Connection state: " &amp; adoCN.State` &amp;#x200B; `sSQL = "SELECT * FROM Table"` &amp;#x200B; `Set rs = CreateObject("ADODB.RECORDSET")` `rs.ActiveConnection = adoCN` [`rs.Open`](https://rs.Open) `sSQL` &amp;#x200B; `'Do stuff with the reply from your query here` `ThisWorkbook.ActiveSheet.Range("A1").CopyFromRecordset rs` &amp;#x200B; `rs.Close` `adoCN.Close` `Set adoCN = Nothing`
It'd help to know what database you are using and what your role is ( are you an application developer, SQL developer, DW developer, Data Engineer, DBA, etc... ?) &amp;#x200B; In any case, there's a bunch of ways, but depends on what database you are using. Either way I usually go straight to the metadata to profile data models. Also, most good SQL IDE's out there have database explorers. &amp;#x200B; System tables and views, index usage stats, and query logs all give a good overview of what data is there and how it's used. There's pretty much always some good queries you can find on Github or Stack Overflow for any type of database that can help you graph out dependency hierarchies for tables and for programmatic artifacts (stored procedures and UDFs etc..). &amp;#x200B; Automatic ERD tools help a ton too and come built in with many SQL IDE's.
Isn't that a SQL Server only tool though? OP didn't mention what type of database he is using.
I would get some output that has all ready been verified and see where all these data is coming from what table. This shows you the relationships between tables and shows you the main tables being used.
Thanks i will give this a look. Seems like allot of people struggle with this .
A lot of that might come down to what country you're in, too. I guess based on what someone else said about "never recommending Oracle," and then coming to the conclusion myself that if I did want to learn Oracle, I'd probably prefer starting with MS SQL and then switching over to it.... based on that kind of logic, maybe MS SQL is the best flavor for people to start with.
Would complain about inconsistent name. One table has create at, the other one says created at. I also dislike that dates are stored as varchars, prices are integers.
If you want a visual representation of a diagram AND you have MS Visio, you can reverse engineer the relationships of the tables. There is a "reverse engineer wizard". This feature was taken out for some versions, but I believe it is back in. Alternately, in SQL Server you can query [system databases](https://www.mssqltips.com/sqlservertip/1781/list-columns-and-attributes-for-every-table-in-a-sql-server-database/) to produce a list of all schemas, tables and columns. This gives you a big result set and not necessarily immediate insight, but it sure is handy to have a list like this while you're searching for fields and learning where stuff is.
I loved using schemaspy for relational databases. It gives you a lot of info.
All of this, and keep consistent naming conventions. The posted_user_id really would annoy me. Just use userid for every user id column.
&gt;n Okay thank you
Thats actualy not the best idea. Tables can have more references to the same table, but each relation has its own meaning. Naming the column after that meaning makes perfect sense. It doesnt need to contain the "_id" suffix tho, may be something like "posted_by", but its a matter of preference i guess...
I'd just suggest a little change in what you wrote. And that would be not to create separate tables for seller and buyer feedback. Instead have one table with columns description and rating and another column lets say "source" of type enum [buyer;seller] that would name whether the feedback is from seller or buyer. It will be easier to set up as well as extendable to allow more "roles" to place their feedback, and it also allows to work with feedbacks disregard of what role they originate from...
Thanks for the comprehensive answer to the high level question. The specific situation that raised the question came after seeing several Redshift DWs that had several different collections of data from several different methods of ETL including Blendo and Segment. The latest one was a Panoply instance that had several data sources ingested. I was just wondering if there's a generic way to get a high level understanding of a DW's contents, how things can/should be joined, and ideally the amount of non null values, cardinality, etc. Thanks again. UDFs sound interesting and I'll check out the RS friendly tools.
I understand what you are saying, and for a schema this size it doesn't matter. But when you have very large databases with hundreds of tables small differences in naming conventions like that will be really annoying. Plus he uses table_id for every other fk column but that one. I know it's a petty complaint, but he was looking for criticism :)
Let's take a step back here. *Why* are you developing this software? Your company is apparently in the construction business, not the software business. So why are they choosing to create software (not their core competency) instead of buying something off the shelf? Do they build cranes, cement trucks, jackhammers, saws, and man-lifts as well? Or do they purchase/rent/lease these tools? My point is this: Software to do this already exists. It may *appear* to be expensive but that's only if you ignore the fact that your time doesn't cost the company money. And more importantly, if you ignore the fact that bugs in this software or downtime of the software for whatever reason doesn't cost the company money too. Don't build things that fall far outside your company's core competency - buy them.
Oracle sure didn't when I tried.
With well defined OLTP databases and complete replication, you can ideally just query the metadata tables for the relationships (or use an auto ERD feature), and the data will be properly normalized with no duplicates or other violations. But.. often there's no explicit foreign keys, incomplete replication from source to the Data Warehouse, or just poorly designed data models. At almost any level of data quality though you'll still need to understand the application and data model of the source system. No real way around that. You'll need to interview subject matter experts, review source system diagrams, ask developers, and try to do aggregates on the data before you know where the issues lie (if there are relational design issues). There's no easy way to get an immediate, universal understanding of data. It is as dynamic and idiosyncratic as humans much of the time because humans introduce the data issues. That kind of where the art form comes in. But for stuff like cardinality you can look look at the execution plan (at least for SQL Server and Postgres). You still have to write queries and examine tables, but it can be automated if you want to gather info and have a picture of your db that is ongoing, or that you can use to auto-generate HTML docs (or whatever). For instance, in my data warehouse (Postgres) I have a suite of PLpgSQL UDF's that collect and catalog data into custom metadata tables with info about source replicates and their counterparts (with stuff like datatype conversations and mappings). It autogenerates all the metadata automatically and modifies certain staging tables according to the changes. Google around and you'll find specific scripts you can use that will give you the info you want. If you want you can use that info to automate stuff like documentation.
Assuming you‚Äôre using MSSQL https://docs.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-union-transact-sql?view=sql-server-2017
The naming conventions seem weird for some attributes (personally I like using the suffix naming convention), some datatypes should be changed (ex; Prices I usually make those DEC(19,4) something like that), also make sure every table is at least 3NF (unless there's a reason for it) which looks good. Also the table names I would just make it "feedback" instead of "user\_feedback" unless I plan to have employee\_feedback for example.
That's good, it makes it a strong relationship and prevents data anomalies from occurring - this is also an example of a table in 3NF
Union All Select Column1 as Name1 From Table Where Gender = ‚ÄòF‚Äô Union All Select Column2 as Name1 From Table Where Gender = ‚ÄòM‚Äô
It is, however, a convenient tool for building a user interface since it doesn't have too steep a learning curve, training is inexpensive and it's easy to find affordable expertise. Used as a front end for SQL Server it is reasonable.
I know I'm replying very late but I was busy. I was not able to find any way of closing the connection properly and I also Googled a lot but fortunately I figured out what happened. I used sql magic in my jupyter notebook to connect to DB2 instance and that connection was not closed. After a couple of days later, the instance on the notebooks disconnected. I've figured out that I can just stop the kernal of the notebook to close the connection immediately.
I thought about it as well, your suggestion should work as well üëç
Indeed; Teradata, DB2 and Oracle are notable exceptions.
I will gladly take a look when you support MSSQL. :)
I use draw.io for quick sketches of databases, where I can also add text comments and share it with colleagues
Thanks but that wouldn‚Äôt have worked since I need to calculate the percentages of each group. I ended using an order by clause which works great for calculations üëç Thanks anyway!
it looks like you are generating this -- INSERT INTO 'snetworks' ( 'Amazon',) VALUES ('1') there are two things wrong -- the table and column names should not have single quotes around them (the value 1, too, probably), and there should be no dangling comma
not sure why you need separate rows here's a solution for a single row -- SELECT 100.0 * COUNT( CASE WHEN EMP_SEX = 'F' THEN 'F' ELSE NULL END ) / COUNT(*) AS "Percentage of Female Employees" , 100.0 * COUNT( CASE WHEN EMP_SEX = 'M' THEN 'M' ELSE NULL END ) / COUNT(*) AS "Percentage of Male Employees" FROM EMPLOYEE holler if you'd like to see the two row solution, it's uglier
&gt; I've figured out that I can just stop the kernal of the notebook to close the connection immediately. This is a terrible, kludgy way to "resolve" this problem. There has to be a better way.
* Where is this all getting run from? IOW, on what machine is Task Scheduler running? Whose credentials are being used by these steps? * Why four separate processes (on two different platforms - Task Scheduler and SQL Agent) when a single Powershell script, Agent job, or SSIS package could do it all? * You have a dependency chain here and if #2 takes too long or fails, what will #3 &amp; #4 do? * How are errors handled? Who is notified if any step fails, and have you documented the process so that anyone can who isn't you can support it, pick it up where it broke, fix it, and restore/resume? Who are the business owners who have to be informed if something breaks? * Why FTP? It's one of the worst protocols remaining from the early days of the Internet. Use SFTP at a minimum.
* At present this is running on a server in my lab space, as my company didn't want to invest in hardware, now they seen it working they will invest in a server for everything. * This is where I lack skills/time, I had issue with permission with job agents, powershell scirpt in one go? Point me in direction where I could this? FTP import is a ssis package. * #3 has retry steps as this takes second to run the export, if #4 fails I have to kick off again manually as my company tool wasn't designed for this method (I think) . * Any steps fail go through to my email and are checked by me daily.. at present customer who cannot get into our system report back to business and emailed across to me. We have processes in place now... No one can support this until I've trained once my solution has been validated by central support. * Double checking my FTP it is sftp with encryption key.
Hi people, I have been given this screening test as part of an interview. I'm hardly a novice when it comes to SQL but I can write queries just fine in a number of ways, however dates are a big weakness of mine. I thought I jump in here to get some insight from some of the more experienced users here. Feel free to have a look. Thanks!
&gt; At present this is running on a server in my lab space, as my company didn't want to invest in hardware, now they seen it working they will invest in a server for everything. A whole server for one small new process? This is a waste. There should be a server available that it can be placed on. &gt;This is where I lack skills/time, I had issue with permission with job agents, powershell scirpt in one go? Point me in direction where I could this? FTP import is a ssis package. Can't give much as you haven't described much of how this thing all works or what your issues were. You can add additional steps/stages/whatever to your SSIS package to do these things. Is this all running under your own account? You need service accounts for this. &gt;if #4 fails I have to kick off again manually as my company tool wasn't designed for this method (I think) OK, have you documented *how* to do this? Including permissions requirements, other software needed, etc.? &gt;Any steps fail go through to my email and are checked by me daily If I'm in your support engineer group, this is a hard stop right here. There needs to be better alerting. At the very least, something to a shared mailbox that's monitored. Preferably something that goes to an alerting platform like PagerDuty or VictorOps if it's business-critical. &gt;at present customer who cannot get into our system report back to business and emailed across to me Your customer should not have to alert you when your own internal processes fail. This is a very bad look for your organization. Your organization should be proactively telling them (after getting an alert, or even automatically when the failure happens) "hey, we hit a snag overnight, we're working on it and expect resolution within X hours." And no one should be emailing you personally - again a shared, monitored mailbox. *Even if* this isn't being turned over to another group, that should be the setup because as it's currently designed, you can't ever take vacation or spend a week in the hospital - it all depends on you having access to your mailbox and no one can take over for you.
Honestly I agree with you on every point. We're a small 40 person company in the UK with the most of support in the Netherlands. At present it's me saying exactly the same, we need better monitoring, no zero points of failure, more proactive checking our system and get to problems before our customer alert us. Unfortunately some people in my company are not invested, which have driven me to say "it can be done but we need X Y Z first before we continue landing this solution" It falls on deaf ears...
I know it's not the ideal way to do it but I'm not a data engineer and I was learning SQL through a course which didn't have much content on making connections to databases so I was pretty much clueless the entire time. And as I said it was just a temporary solution. The funny thing is even the TA of the course suggested the same thing when I posted in forums lmao.
If it's MySQL \`tableName***\`*** is the correct syntax.
&gt; If it's MySQL `tableName` is the correct syntax. it's even correct without the backticks at all!! who knew!! INSERT INTO snetworks ( Amazon ) VALUES ( 1 )
Breaking news! Scientists have discovered water is wet!
It sounds like you're the only support for this solution. Be it as it may, you will need to simplify this process so that one script performs all the work. As another has mentioned PowerShell will work. I won't go into explaining the technical steps here as there are infinite resources online that you can reference. My main point is that the group you will be presenting this to will certainly care more about if this solution can be supportes and troubleshooted in your absence. So reduce the numerous triggers to one script that calls all your various components. Document each step clearly and in plain language within the script. Then you can simply show them how that script is called, the steps it performs, and how those steps are outlined.
That's what I've been thinking this afternoon how to make this solution simple to understand/document. My main problem chatting to my work colleague is getting other members of the team involved/trained. There is a very common problem "if I agree to this it's my problem" which isn't the case for me but the culture at my work place. Best course is start with powershell for each step, then document before presenting to central.
&gt; There is a very common problem "if I agree to this it's my problem" which isn't the case for me but the culture at my work place. Management needs to change this culture. If management won't change it, you need to find a new gig.
100% agree I'm putting in my all but the culture needs to change.
If you can't change the company you work for, change the company you work for. It's impossible for someone at the individual contributor level to change company culture.
That's not uncommon. Especially when it comes to ETL solutions as they often must be highly customized. But you're job here is as the engineer. So just let that be your focus. The skill/support gap is somebody elses issue. If You're trying to solve both then you're bound to become frustrated and unsatisfied with your job.
The driver will depend on what's available on each PC. Pretty much every Windows PC has \&gt;Driver={SQL Server}; The other drivers like "Driver={SQL Server Native Client 11.0};" tend not to be installed, unless you've already installed SQL Server Management Studio on that PC.
https://www.w3schools.com/sql/ https://sqlzoo.net Two resources that have been recommended to me by people. Specifically SQLzoo which has assessments ranging from easy to hard. In my experience a lot of it will range from basic extraction and simple joins to more complex questions where multiple joins and potential aggregate functions are needed. Good luck OP!
To do that you should make a prepared statement on your environment and pass it the variable
And Postgres will require quoted object names if they have capital characters and were quoted when created.
What does that look like?
WOW!! This is exactly what I was looking for!! I was trying to switch from two rows to two columns. Thank you so much!! Before you came along, with help, I was able to create this query which adds a bit of context: &amp;#x200B; SELECT EMP\_SEX AS Sex,COUNT(\*) AS "Sex Count Total",(SELECT COUNT(EMP\_SEX) FROM employee) as "Total Sex",CONVERT(DECIMAL(18,2),((count(\*))/1.0/(SELECT COUNT(EMP\_SEX) FROM employee)/1.0)\*100) AS "Percentage of Sex" FROM employee GROUP BY EMP\_SEX; &amp;#x200B; I'll rework my code to combine the best of both! THANK YOU!!
&gt;That's great an all, but it's still the #1 tool by the numbers Which numbers? Oracle is definitely in use by some very big players, but I would have thought by most metrics it would have been on the smaller side. Definitely agree though that Oracle salaries are higher for DBAs, but I think that has a lot to do with Oracle being more difficult to DBA.
I linked to an article. Seems by most metrics it is the largest player in the industry. Hard to find jobs data but if you go to Indeed.com and search for "Oracle DBA" vs "MS SQL DBA" the total full time jobs on the left side seems to suggest in the few cities I checked that there are between twice to ten times as many.
What have you done so far? Show us your work and where you're stuck and we'll take a look.
Linux version is now up and running! We are also working on getting support up for other databases soon! Please stick around! :\]
Thanks for the interest! I'll be sure to let you know when we support those databases!
Thank you! I'll definitely update you when we begin to support that database!
When I want to do this, I use a templating language like jinja in my sql script, then I use a python script to render the template and execute the SQL statement. Alternatively, there's an open source project called \[dbt\](https://www.getdbt.com) that makes it really easy to templatize sql scripts and has native support for referencing environment variables from within those templates. It also has a bunch of other features that might be useful depending on your projects.
I guess it's a regional thing too. In my home (small country with smaller younger businesses) Oracle is found in some government departments and large financial institutions, but the rest is MSSQL as far as the eye can see.
I assume everyone's an American on the Internet.
I've used Strata Scratch, HackerRank, and Datacamp to learn SQL. If you're trying to a job and interviewing, youtube is great to learn how to prepare for technical interviews. I like this one ([https://youtu.be/n6gM265zG68](https://youtu.be/n6gM265zG68))
Have you looked at [querying the ReportServer database](https://docs.microsoft.com/en-us/sql/reporting-services/report-server/report-server-executionlog-and-the-executionlog3-view?view=sql-server-2017) for the values passed to the SP from within the report rather than logging the SP execution itself?
Please do not recommend w3fools - that page is full of errors.
Column names should not be quoted - and if you need to quote them, you have to use double quotes in (standard) SQL. `'Amazon'` is a string constant. `"Amazon"` is a column name.
That would be my preference, however as I make an SSRS based product, I don't always have the ability to query the ReportServer DB.
Thanks for the tips this was really helpful. I am currently using linked tables on access however i want to run stored procedure on the sql server database by creating macros in VBA that i am going to assign to a button. If i have any other problems ill be sure to ask for your assistance. Thank you.
+1 for w3 schools. I'm sure the other is good too Some things I like to question knowledge on for BI roles are: Having Aggregates Isnull/coalesce Outer/cross apply Stored procs vs views Temp tables I bother less with inserts/updates/deletes as they don't really apply as much for these kind of roles For BI, I've never had the luxury of having a BA to get requirements, so it's also important for them to be decent people persons and understand how to talk to people, manage expectations and know what questions to ask
Dude why did you right join? Why not left join with two join criteria?
I work as a web dev and don't even know what I'm looking at. I'm gonna try to follow through the videos. Thanks!
Wow this worked like a charm! thanks for this! gave me exactly what i needed and the CTE / recursion part makes more sense to me now. thanks!
I blew through the W3C test in five minutes with a perfect score. Very simplistic, but if you can't do that then I don't want you. And honestly, it would trip some of the people I work with. I looked at your second link above and the first question is good but my problem is experience. I work in DB2, SQL Server and Oracle. To answer that question you have to know which SQL engine you are working with. If I'm talking to an HR person my response will go completely over their head. If I'm talking to an experienced DBA who has worked with these various systems (s)he will understand.
https://sqlite.org/cli.html
MySQL has the LAST_INSERT_ID() function that will return the ID that was generated for the last insert statement that your current session/connection made (in the case of multiple inserts in the same statement it returns the first generated ID), so you should be able to do SELECT LAST_INSERT_ID(); immediately after the insert and be sure you got the right ID.
Thank you! I didn't know LAST_INSERT_ID existed! This seems like the answer to my doubt.
Your `results` CTE is confusing... how are you deciding which date/value pair to use? You're doing `LIMIT 1`, but you do not have an `ORDER BY`, so your `results` query can return different results randomly if there is more than 1 row with a matching `fruit_name`... What you should do is remove the `LIMIT 1` and instead use a window function to get the first row of each fruit_name based on how you want to do that... http://www.postgresqltutorial.com/postgresql-window-function/ HOWEVER, since you're using Postgres, you can just use `DISTINCT ON` to get 1 row with the latest date in it: WITH results AS ( SELECT DISTINCT ON (fruit_name) date, value FROM fruits ORDER BY fruit_name ASC, date DESC )
 SELECT 100.0 * COUNT(CASE WHEN Trips_Completed &lt; 10 AND Accept_Rate &lt; 0.9 AND Rating &gt; 4.7 THEN 'ok' ELSE NULL END) / COUNT(*) AS percentage FROM Courier
It will usually evaluate everything within the statement before it does the conversion to decimal, so it does all of the multiplication and then division using integers (since that's the datatype that COUNT uses) and then converts the result to decimal. Splitting it up so that you cast each of the counts first should give you what you are looking for. SELECT cast((SELECT COUNT(*)*100 FROM "Courier" WHERE "Trips_Completed" &lt; 10 AND "Accept_Rate" &lt; 0.9 AND "Rating" &gt; 4.7) as decimal) /CAST(COUNT(*) as decimal) FROM "Courier";
&gt;end of file key combination Sorry...should've included a sample command prompt: &amp;#x200B; (sqlite) C:\\filepath chinook.db SQLite version ## sqlite&gt; .tables &amp;#x200B; All this does is index to the next line and give another sqlite&gt; prompt
This is highly confusing - what is the actual, underlying problem you are trying to solve here? Do you want to get the result of quantity \* value for the latest date of a fruit? Then you don't need the CTE or the cross join or something similar. In Postgres you can do that in one single query using distinct on: select distinct on (fruit_name) fruit_name, "date", quantity * value from fruits where fruit_name in ('apple', 'pear') order by fruit_name, "date" desc; If you want a solution using standard SQL, you can use a window function instead: select fruit_name, "date", quantity * value from ( select fruit_name, "date", quantity, value, row_number() over (partition by fruit order by "date" desc) as rn from fruits ) x where rn = 1; The `distinct on ()` solution is typically faster in Postgres
Sounds like an empty database.
Downloaded SQLite with the CLI tools on Windows and the [Chinook sample DB](http://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip). What are you seeing instead of the below? `cd /Downloads/SQLite` `C:\Users\pixelbaker\Downloads\SQLite&gt;sqlite3 Chinook.db` `SQLite version 3.28.0 2019-04-16 19:49:53` `Enter ".help" for usage hints.` `sqlite&gt; .tables` `albums employees invoices playlists` `artists genres media_types tracks` `customers invoice_items playlist_track` `sqlite&gt;`
Thank you very much but I seem to be getting a ridiculously large number for some reason when trying to implement this. I'm not sure what the issue is but there is another response that seems to be working. You also taught me a simple lesson in using a decimal to force a specific data type so thanks for that.
This has worked great, thank you! I'll keep in mind the multiplication taking place before the conversion in future.
I think this works. Thank you! &amp;#x200B; Only additional thing is where should I add the where statements to choose the date range and the company?
The only certs worth getting are the ones from the DBMS vendors themselves. Is there even a cert for Postgres? Get the cert for the platform you work on/want to work on. Or don't. There are a *lot* of internationally-known experts on these platforms who haven't bothered with a cert because it just hasn't been necessary for them.
This looks like it works, Thanks! &amp;#x200B; Only additional thing is how should I add the where statement to choose the company and date range? I added it to the end but it looks like the Sum function is summing the total amount across every line which is diluting the results to zero.
i have literally had interviewers laugh at me when i asked about certifications. get the ones the guys paying you want you to have - hopefully, they'll pay the fee.
In Postgres, you could use arrays for that: select f1.* from fruits f1 where f1.quantity in (10,12) and exists (select 1 from fruits f2 having array_agg(f2.quantity) @&gt; array[10,12]); The drawback is, that you need to specify the quantities twice, but that could be removed by using a CTE: with input (q) as ( values (array[10,12]) ) select f1.* from fruits f1 cross join input i where quantity = any (i.q) and exists (select 1 from fruits f2 having array_agg(f2.quantity) @&gt; i.q)
There is no such thing as "tempdb logs" in Postgres. A transaction deleting 100 million rows isn't more costly than a transaction deleting 1 row in Postgres. The only drawback of a DELETE in Postgres is, that the space won't be freed up immediately.
Nice tutorial! A few thoughts on some of the things I wish I had learned/understood earlier when learning SQL, if you don't mind. I've seen you already have an example using multiple aggregates in a single query, but maybe it wouldn't hurt to combine examples 4, 5, 6 into: SELECT Department, MIN(Salary), AVG(Salary), MAX(Salary) FROM Employee GROUP BY Department; and make a point about how you can only select things directly only if they're in the `GROUP BY` clause, otherwise they _must_ me aggregated. This is obvious to a non-beginner, but the errors you get when you fiddle with the grouping columns without updating the selection can be quite confusing to beginners.
Depends what you want to do. For the purposes of querying, minor. For DBA stuff the differences become more apparent.
One query/report that accepts your parameters and returns the appropriate data. How is this data delivered to the consumers of the information?
The querying basics are similar on most databases. There are however syntactic gotchas, especially in some common functions you'll use - for instance the functions for date math can vary greatly from one db to another. (Your database, Postgres, has a convenient interval type that's nice for dealing with date math that you won't find everywhere.) Some may have you write identifiers differently - SQL Server writes them like **\[this\]** while in Oracle/Postgres they are like **"tHiS"** if you want to preserve caps. MySQL uses backticks like **\`this\`**. But when you get to the advanced stuff that's where you'll find more differences. Some databases invent their own SQL extensions - Oracle has a lot of these such as the MODEL clause as one example off the top of my head. Some lag in keeping up with the SQL standard so you may not find common table expressions (aka the WITH clause) in some databases like MySQL for example, until the recently released 8.0. And as far as I know MySQL still doesn't support FULL OUTER JOIN which, to be fair, isn't used all that often. Some databases are more lax about things like data type conversions, while others force you to be explicit. In general I find Postgres tends to err on the explicit side, while MySQL is very lax. One interesting thing about MySQL is you can do this: **select sum(a), b, c from mytable group by b** and it will not fail, while it WILL fail in every other database I've seen. This is because **c** has to be included in the select in most databases. MySQL will just choose an arbitrary value for **c**. And to add to the complexity, I was recently told this behavior is now tied to a database flag that defaults to disabled in MySQL 8.0. &lt;sigh&gt; :) Anyway, just my two cents.
Essentially OUTPUT allows you to pass values into a stored procedure or other chunk of code and get a value back out. Your code is saying: 1. Declare @HasReadmission because I'm going to use it later. 2. Execute this stored procedure with this @EpisodeOfCareID. Process that procedure's code using the @EpisodeOfCareID and pass back the value it provides. Store that value in my @HasReadmission parameter. From there you can use that parameter's value to do additional processing in the parent procedure or code.
look into full outer join (and you probably would want to use the "bom" table twice)
&gt;which means I'm setting myself up to create ~300 queries+ Why is that? Like /u/pixelbaker said, if you parameterize your queries (instead of having a unique query for each combination of criteria), you should have a minimal number to write and maintain.
It sounds like your joins are fine - if they're all left joining to MAIN then from the joins alone you'll get all the records from MAIN. I wonder whether you have something in your WHERE clause referencing STORAGE? To keep the left join working, you need to allow for NULL in the storage field. E.g., SELECT * FROM main m LEFT JOIN storage s on m.main_Id = s.main_id WHERE s.field = 'value' Will never return a row from main that doesn't have a value in storage, because we've specified we only want records with a s.field value. So instead you'd need to do something like: SELECT * FROM main m LEFT JOIN storage s on m.main_Id = s.main_id WHERE s.field = 'value' or s.field is null If this doesn't help, can you post your query?
Thanks and you're correct I have a constraint with s.main_id. It's a varchar field because we had to allow it to be either integers or TAR%. So the constraint in my query is: s.main_id not like 'TAR%' since I only want it to attempt matching on when the field contains integers (which matches with m.main_id. Other valid values in that field could be TAR12345 but if I leave out this where constraint, it produces an error "conversion failed when converting the varchar value 'TAR12345' to data type int. So how do I allow it to still find NULL if it doesn't match with this in mind?
Have you tried altering the WHERE to: (s.main_id not like 'TAR%' or s.main_id is null) If that doesn't work, you can get around the type conversion issue by explicitly converting to varchar in the join: FROM main m LEFT JOIN storage s on cast(m.main_id as varchar) = s.main_id
&gt; Something to do with the "As percentage" bit. If i remove that seems to work as I expect it should. sorry, that's unpossible assigning a column alias to a select expression does not change the result
filter conditions on the right table in a left outer join belong in the ON clause, not the WHERE clause SELECT main.* , storage.main_id , storage.userid FROM main LEFT JOIN storage ON storage.main_id = main.main_id AND storage.main_id NOT LIKE 'TAR%'
Sounds like you have something in your where clause.
There are built in functions that check whether fields are certain data types, like ISNULL and ISNUMERIC. You can instead do ISNUMERIC(that field) =1
ANSI (American National Standards Institute) SQL is the standard that everything adheres to. The various implementations of SQL, like postgresql, Microsoft SQL Server, MySQL, etc. all follow it. So, the "standard" stuff you're learning now like SELECT, JOIN, UPDATE, DROP, etc will give you the same results everywhere. In addition, the various implementations "extend" (or add features) that are only available on their product. This is usually through implementation-specific keywords or back-end features. For example, Microsoft has "T-SQL" adds in-query functions like cast, convert, and date() that are not in the ANSI standard.
It‚Äôs for a journal submission so likely the consumers will never see the database or raw data, just see my interpretation of the results. I‚Äôm just trying to save a bit of time and learn how to structure queries that are a little more complex.
Parameterizing the query will let you run it repeatedly without having to make copies of it. That's what you're after, right? Minimizing the duplication of work (which is a good goal).
How often do you need to run these 300 queries?
Looks like that did it, thanks everyone.
Thanks this also works in the ON clause.
If you can generalize and post your current query to return one set of combinations we can help you parameterize it into a query that works for all of them.
Just once
Once and then never, ever again? Or once every quarter? Once every year? Basically you have a query like this: select blah from places join otherplaces where things = @conditions And you want to run that "shell" query 300 times for each @condition that you have? Is that about correct?
hi [r/StrataScratch](https://www.reddit.com/r/StrataScratch/) is cloesed
I finally remembered the word, ROLES there are three roles in the website how can I set it up without creating three login pages OR having the person select which role they are.
You would have the login page, then the page that they are redirected to would have If statements checking their user type and displaying whatever data is within the If statement
/agree with what others have said. I would suggest following ANSI as you learn, because that will be the most portable. Also, once you get comfortable with the concepts, moving between flavors of SQL becomes easier. I learned exclusively in Oracle but have recently taken some work in MYSQL, and have been muddling through fairly well (With a Google window constantly open to check syntax and a fair amount of cursing mind you).
For pure SQL the differences are only ever a quick Google search away. Any skill with one dialect is fairly transferable to tither dialects. Postgres is my favorite dialect. Most flexible, feature rich, etc. The differences are more pronounced for the procedural SQL extensions such as PLpgSQL (Postgres), PL/SQL (Oracle), T-SQL (Microsoft SQL Server), and others. However, they are still somewhat similiar. For instance, the Postgres one is modeled after Oracle's and both are based on the Ada programming language. So..Skill with one directly translates. PostgreSQL's PLpgSQL is better though. Again, more flexible, feature rich, more advanced data types (like arrays). T-SQL is more noticeably different both in syntax and how you approach it. Examples.... T-SQL is not based on Ada. Also, for T-SQL the procedural runtime is one and the same with the standard SQL runtime as opposed to other languages that have separate runtimes and incur a minimal context switching cost. On the other hand T-SQL has terrible performance when using cursor for loops and scalar functions are similarly unperformant. So with T-SQL people use stored procedures over functions, while Oracles language has both, and Postgres has had only UDFs up until recently (though this was not a feature loss as PG UDFs served dual purpose functionality).
So I won't be available at all tomorrow until late, but if my previous message is approximately what you want to do, and you want to automate this process then you are describing a loop with parameters. You might be able to do it without dynamic SQL, not sure. For example, you might take all the conditions you want/need and load them into Excel, then take that file and load it into your database as a table. You could hardcode this in the query if you are unable to load data to the database itself, and using Excel generate the script necessary to copy and paste it into SSMS. Anyway, once you have your "table" (permanent, or temp) then you would join to it for each condition, and run a loop that increments for each condition. As each iteration of the loop runs you can dump it out with some additional formats that will allow you to see which iteration of the loop the data is from. So something like this: declare runid = 1 while loop select runid, stuff from table join othertables a join #parametertable b where things = b.condition set runid = runid + 1 end That sounds like what you're after. If I were you and you still need help after Googling this for better syntax then I would repost this in the morning and ask how to write a LOOP. People are probably going to tell you that you don't need to use a LOOP, and that they are bad, but if what you're explaining to me is your use case then you want a LOOP. I'm pretty sure you can avoid dynamic SQL.
^ if you do have 300 sets of distinct conditions you have to filter for, I don‚Äôt really see a way of this not sucking. One way or another you are going to have to program 300 sets of conditions. But maybe I‚Äôm not understanding correctly
It's simple, you just use a LOOP. You can either load all 300 conditions as a flat file into your database and join to the table, or hard code all 300 and INSERT VALUES. You can pretty easily use Excel to write a formula if you structure all of your conditions so that it outputs the necessary SQL for you to copy and paste.
Yes, my point being at some pint you have to enter the 300 conditions into said flat file. If he has to manually create that file and then write a loop, is that really going to be faster than writing a where clause w/ 299 or statements? Even if it is, my point still stands. OP has to manually code in the 300 conditions. That part will suck
A lot faster to enter them into a file and then be able to modify / maintain them, then running 300 queries and forgetting which one you're on, etc. The name of the game is sustainability and replicability. Would you rather hit F5 300 times and wait... 90-600seconds per result, or just enter it all into Excel, use a SLEEP function, hit F5 once, and then come back tomorrow and get your results? &gt;Even if it is, my point still stands. OP has to manually code in the 300 conditions. That part will suck He already knows what they are, and probably has them in a format which is transposable to the format necessary to run a LOOP.
Yes I agree, if OP is going to repeat, it‚Äôs worth the trouble of doing it write in an automateble fashion. But OP said it‚Äôs for a publication, so it sounds like a one-off thing. As you said it would also be easier if OP already has them in a workable format. But it sounds like what he currently has is a bunch of queries.
Even if its for a one off, you load your file and make sure you are happy with the parameters, then run it. You do not run all 300 separately and hope you can copy/paste properly. It isn't a proper way to handle your request. You write a LOOP, and they are rather simple to do. Writing 300 parameters isn't difficult, and as I said, they are already written. You just need to copy &amp; paste them into Excel and then write a formula that will output the SQL syntax you need to insert them into a #table, or just load the *xlsx/*csv file straight to the database. I encounter this problem often writing models, and this is the elegant way to solve it.
Like, lets reframe this entire conversation. A colleague or peer comes to me and says, "Hey, I have this deliverable and I found a solution, but I need to "write" 300 queries, run them all one by one which takes 5 minutes, and then save the results. I'm a highly paid data scientist/analytic developer, I don't have time to do that." Response: "OK, great point, you definitely don't have time to do that. So copy everything to Excel, write a LOOP, and schedule a job to run overnight so we have the results in the morning. Then go work on some other shit." How you gonna argue that, or stand by your original point? If loading the parameters to Excel (which makes it repeatable) takes less time than manually switching them and waiting for the results.... then what? That's your job. Go do it. Take the better soul stuff on down the road. Go copy the parameters into Excel and take 4 hrs, then schedule the job to run over night, and then spend the last 4 hrs of your day doing something else. We ain't got time for that.
Lol as I already acknowledged several posts ago, a LOOP (or ‚Äúloop‚Äù as most people without the caps lock on would say) would be the best way to solve this problem. But it doesn‚Äôt seem like OP has ever used a loop before. If he/she had why why would they be here? This is a classic use case for a loop, so I‚Äôm going to go ahead and give OP the benefit of the doubt and assume if they were comfortable writing loops, they would have considered that option. And again, OP literally said he/she is doing this ‚Äújust once‚Äù for a publication. So I am not sure why you keep suggesting all of these enterprise level solutions. I‚Äôm on an analytics team at a tech company, your suggestion is sound in that context but seems like a tad bit of overkill in this case.
Lol what are you talking about? As I already conceded, in a business setting your solution would be the correct one. But OP isn‚Äôt in a business setting. So I‚Äôm not sure why you keep pontificating in this fashion. No one is advocating for manual processes in business settings
I write all functions in SQL using CAPS, it's a hard coded habit that isn't ever going to die. &gt;But it doesn‚Äôt seem like OP has ever used a loop before. If he/she had why why would they be here? So we should probably tell them how to do it? &gt;This is a classic use case for a loop, so I‚Äôm going to go ahead and give OP the benefit of the doubt and assume if they were comfortable writing loops, they would have considered that option. This is incongruent with your previous statement. It is a classical case for a LOOP, but we can assume the OP has no idea that such wizardry is possible. &gt;And again, OP literally said he/she is doing this ‚Äújust once‚Äù for a publication. So? Is it worth it to learn a new technique, which will be faster, or to just stagnate and waste your time when a better option is available? This seems like such a nonsensical statement to make in terms of career development. What are you going to do the next time you need to write 30,000 queries? &gt;So I am not sure why you keep suggesting all of these enterprise level solutions. Because I'm a fucking professional, and none of the other solutions make sense. &gt;I‚Äôm on an analytics team at a tech company, your suggestion is sound in that context but seems like a tad bit of overkill in this case. Yeah, well, when your company is looking to find someone to be your boss's boss's boss, give me a call.
https://www.youtube.com/watch?v=AUphDV3Pjww
Lol wow dude you are so angry, I‚Äôm not even going to attempt to unpack all that. Take a deep breath. I will address your comment about my statements being incongruent. Perhaps I didn‚Äôt make myself clear. My point was that this situation is sort of an obvious use case for a loop. So either a) this person doesn‚Äôt know what a loop is, or b) they do and just didnt think to apply a loop to this issue. Due to how obvious it would be to use a loo for someone with coding experience, I‚Äôm going to go ahead and assume this is case a. I‚Äôm which case, it may be a tall call for OP to go from 0 coding knowledge to writing loops. It may just be faster to do things by brute force. Anyway, I‚Äôll end this by again conceding for about the 5th time that your approach is more scalable, efficient, and reliable. But call it a gut feeling, I don‚Äôt think OP wants to learn how to write loops. If they do, hey more power to them, they should absolutely do it. Learning to code is an invaluable skill, as I‚Äôm sure we can both agree. But if they don‚Äôt see themselves coding in the future, seems like a lot of effort for little reward. Not everyone is trying to be a ‚Äúfucking professional‚Äù... btw, only noobs write sql with caps. There‚Äôs syntax highlighting for a reason. And if you were qualified to be my boss‚Äôs boss‚Äôs boss you‚Äôd be the VP of a fortune 100 company, not writing angry reddit posts.
I'm not angry. I just started a new job this morning, and haven't been on Reddit all day. I'm having a few beers right now. Don't take things so personally and you'll go further. &gt;Perhaps I didn‚Äôt make myself clear. My point was that this situation is sort of an obvious use case for a loop. So, to make myself clear, we (as a community) should probably recommend using a loop to him, no? &gt;So either a) this person doesn‚Äôt know what a loop is, or b) they do and just didnt think to apply a loop to this issue. So we should educate them, no? That is purpose of this place, no? &gt;Due to how obvious it would be to use a loop for someone with coding experience Are you advocating that they use a loop or not? Are you willing to help them write it? Because I am. I consider that a constructive use of my time relative to my own career development. &gt;Anyway, I‚Äôll end this by again conceding for about the 5th time that your approach is more scalable, efficient, and reliable. But call it a gut feeling Oh boy... &gt;But if they don‚Äôt see themselves coding in the future, seems like a lot of effort for little reward. Not everyone is trying to be a ‚Äúfucking professional‚Äù... btw, only noobs write sql with caps. I have certifications in COBOL, FORTAN, and PERL.
Oh, I have an RPG certification, too.
Mmm you seem a little angry when you say things like ‚ÄúI‚Äôm a fucking professional‚Äù and I could be your ‚Äúboss‚Äôs boss‚Äôs boss‚Äù. Anyway, I‚Äôm genuinely very impressed by your mastery of archaic languages. I‚Äôm sure you get paid handsomely to consult at financial services companies which are still operating on technology from the 80s... in all honestly, I‚Äôm sure you actually are a skilled dev. You seem competent probably wouldn‚Äôt be here if you were not. The comment about caps was more of a playful barb aimed at an obviously worked up redditor, based on my general observation that those who do not eventually stop using caps for functions are generally not the sharpest knives in the drawer. To answer the more direct questions you posed: again using a loop would be the best way to do this. And yes, you should point out new techniques to people here. If you go back to my original post, I never said ‚Äúwhy are you wasting your time with telling this person about loops?‚Äù . My original point is that OP seems to be looking for a quick and dirty solution to this problem. And. Regardless of if they use the more scalable approach of using a loop, or just doing it manually, they are going to have to go through the obviously tedious task of imputing the conditions into some workable format. Anyway, this thread has really gone of the rails. But my point was imputing that list of conditions will suck. And maybe the can find a way to get some numbers they need without actually accounting for all 300 cases
I can't speak to DBA jobs, but for analytic roles these certifications are worth nothing.
I am a fucking professional. https://www.youtube.com/watch?v=xMJO3PL-ux0 I deal with guys like you all the time at work. It's not as funny at work when I'm calling you these names in public meetings. I'm not really angry with you, I'm disappointed. I'm let down because of your mediocrity. &gt;I‚Äôm sure you get paid handsomely to consult at financial services companies which are still operating on technology from the 80s I primarily work in SQL, Python, Tableau, SAS, and SPSS now, but cool story. One time a company I used to work for was hiring a RPG developer and they were advertising the position for 5K more than I was making at that point in my *junior* career. So I told my manager they had to pay me more or I'd go work in another department. They paid me more. I'm not even 40 yet. &gt;again using a loop would be the best way to do this. Cool story. &gt;My original point is that OP seems to be looking for a quick and dirty solution to this problem. You misspelled loop. &gt; they are going to have to go through the obviously tedious task of imputing the conditions into some workable format. So they can run a loop? &gt;Anyway, this thread has really gone of the rails. Because you're talking a lot and not saying anything. &gt; But my point was imputing that list of conditions will suck. Like I said, this likely already exists, and probably would have taken less time than you've invested in this conversation so far... so.... maybe as a personal career development thing you want to re-examine the way you approach problems? &gt;And maybe the can find a way to get some numbers they need without actually accounting for all 300 cases That isn't how modeling works. That's something DBA's say to analytic developers that makes the analytics team thing that DBA's have no idea what the fuck they're talking about.
Lol dude you are so self diluted ‚ÄúOne time a company I used to work for was hiring a RPG developer and they were advertising the position for 5K more than I was making at that point in my junior career. So I told my manager they had to pay me more or I'd go work in another department. They paid me more. I'm not even 40 yet.‚Äù Talk about cool story lol
It's a really cool story. You're talking about a loop. Say the word with me. LOOP. LOOP. What are you a Trump supporter? Jesus chris.
Dude this gold, please keep it coming.
Do you have any more chestnuts about times you got big boy raises? I can tell them to all of the other devs I work with when I teach them about the groundbreaking magic of loops
Holy shit you just edited your last post to include ‚Äústraight baller, son‚Äù. I rest my case
When you can recommend a loop to business users to provide valuable insights get back to me.
A minute ago I thought I was angry? Am I not angry now?
When did I say you weren‚Äôt angry? You‚Äôve seemed angry this hole time
[I'm upstairs, I'm listening to my Will Smith CD](https://www.youtube.com/watch?v=QFcv5Ma8u8k).
[mockaroo](https://mockaroo.com/) might be helpful
Awesome, thanks
&gt; unpossible
As many other have mentioned, there is a standard. However, I prefor not to refer to it as ANSI SQL, because the standard is acutally an international standard managed by ISO. However, ANSI still does the secretary. From standard's perspective, there is a set of mandatory features ("Core SQL") and optional features. On top of that, extensions are permitted. The standard just defines the result of SQL syntax the standard itself defined. It does not way what happens if you use syntax outside the standard syntax. Unfortunately, not even the mandatory features are available in all dialects. One commentor mentioned the quoting of identifiers (names of tables, columns, ...). This is really an excellent example of how the standard has not reached its goal of unification. When you look close enough, you will find plenty of differences between the different dialects, unfortunately. Regarding learning: PostgreSQL is a pretty good base as it is rather close to the standard. If you know more about dialects are different, have a look at my website [https://modern-sql.com/](https://modern-sql.com/) where I explain interesting SQL features in detail and also provide compatibility information.
Can you recommend any good starting material for Python specifically using it the way you mentioned? I've been working with SQL daily for over a year but it would be nice to pair it with Python. It's not lack of reading material that's the problem, its the sheer abundance that makes it hard to know where to start
Hi, this is a really useful piece of information, thank you. Do you know if there is a similar way to also pull through scheme of stored procedures and functions? Based on their original column name (from the tables they come from) not the renamed ones?!
Unless you‚Äôre using every field in the client app, `select *` is wasteful.
thanks
Honestly, sometimes the best way is to pick up side projects and then research as you go. This way you'll pick up the relevant bits as needed rather getting lost in the avalanche of materials. As you're doing a project you might wonder if you can do something, Google it, and then find out that you can in fact or that there's a better way you ever thought of. There's a ton of blog posts out there. Many are for Python with Postgres. Charles Leifer and some others have some good ones for Python and SQLite. DataCamp has some Python and PG stuff, I believe. Pandas to_sql() and read_sql() methods are kind of my bread and butter. For my general workflow for side projects... I do about 90% of my real data work in SQL and then about 10% using Pandas DataFrames. The first 5% is loading the data from raw files, CSV, JSON, API calls, etc..and maybe I'll do an DF.apply() to clean this or that then all DF.to_sql() to store the data in a SQL db and maybe create a couple views or SQL UDFs to properly sort and catalog the data. From there I run my analytical queries to get whatever aggregates or data shaping I need then I'll pull the data back out with DF.read_sql() to do some plotting, to train a ML model, or to ship the reshaped data to a new destination. You can actually do most of the data cleaning in Pandas (if you have small data and only 1 or so datasets that don't need Joins). Pandas has done a decent job of emulating SQL functionality, but I still use SQL, because I like to keep my data in one place and keep historical lots. Processing with Pandas tends to be a bit more empheral, and I don't want my data in a bunch of CSVs scattered around and disconnected from other data that may be relevant when joined. As someone who comes from data warehousing, I understand the value of collecting and curating data rather than doing a one-and-done job with Pandas and throwing away the results.
Yes, you can use SQL Server 2017 Developer. It's free to use for learning. They say: "SQL Server 2017 Developer is a full-featured free edition, licensed for use as a development and test database in a non-production environment." Download [here](https://www.microsoft.com/en-us/sql-server/sql-server-downloads).
I would download SQL server express or a dev instance of SQL server.
Thanks! I appreciate that
I have managed to download Server Management Studio. Thanks.
SSMS **is not** SQL Server. It's just a client for it. You still need to download and install SQL Server Developer Edition.
Right ok, I'll retry.
&gt; Microsoft SQL Server doesn't require Java Runtime or SDK out of the box, so I'm not sure what's going on there If OP tripped into the SQL Server 2019 CTP releases, the Machine Learning features now support Java and will require the runtime or SDK. OP could also use Express Edition, it'll likely be sufficient for their needs.
Sql server isn't a table product, Java is a free download
What's the actual data type of the column? Also, use `DATEADD` - it handles time zones and leap years correctly.
This doesn't work? &amp;#x200B; `SELECT * FROM TABLE WHERE DATEDIFF(DAY, DATE_COLUMN, GETDATE()) &gt;= 30`
Thanks for the clarification, makes sense.
you nailed it right here hahah I've never used a loop before. It is for a one time thing so I'll never have to run this query again. Probably overkill to rewrite all the parameters when the original query is so short so I just have a shitty few hours of work ahead probably. Learned a lot coming here tho- my sql game needs work. Also thanks for being the first person I saw to not assume I'm a dude.
how did we get here
Thank you for putting in the time and doing this!!! Very helpful :)
Data type of the column is also date/time and it stores it in the same format. I'm essentially copying over from one table &gt; to another table. As for using DATEADD - how would I utilize it? My query with it doesn't seem to return any results : `CONVERT(date, a.DateField) = CONVERT(date, DATEADD(day,+30, GETDATE()))`
`cast(substring(DATE_COLUMN, patindex('%,%',DATE_COLUMN)+2,len(DATE_COLUMN)-1) as date)` That will solve your data scrub based on sample format.
No problem üëç
&gt; Data type of the column is also date/time and it stores it in the same format. sorry, no SQL Server stores datetimes internally in a format which you wouldn't recognize what you're seeing is a **display** format &gt; I'm essentially copying over from one table &gt; to another table. okay, so just plain date arithmetic should do it WHERE DateField) &gt;- CURRENT_DATE + INTERVAL 30 DAY
I think it's because you join on pszprokeyi
look into outer joins
You have count(**pszprokeyi**)"#Zuweisungen" . You can't count nulls. Try SUM(CASE WHEN **pszprokeyi** is null THEN 0 ELSE 1 END) as "#Zuweisungen" &amp;#x200B; Also you have this in the inner joins. JOIN psz ON shokeyi=**pszshokeyi** AND shovarkeyi=pszkavkeyi JOIN pro ON **pszprokeyi**=prokeyi AND pszkavkeyi=prokavkeyi AND proetykeyi=1 &amp;#x200B; Try LEFT JOIN psz ON shokeyi=**pszshokeyi** AND shovarkeyi=pszkavkeyi LEFT JOIN pro ON **pszprokeyi**=prokeyi AND pszkavkeyi=prokavkeyi AND proetykeyi=1
that extremely hard to debug -- use table qualifiers, man!! so instead of ON shokeyi=pszshokeyi AND shovarkeyi=pszkavkeyi you should write ON sho.shokeyi = psz.pszshokeyi AND sho.shovarkeyi = psz.pszkavkeyi you might *think* you've sidestepped this issue because it appears that the name of the table *is actually embedded in the column name* but that is an awful convention and it will bite you as for your actual problem, you want `LEFT OUTER JOIN` not `JOIN`
I work in SQL Server, Oracle and DB2. You can copy and paste basic SQL statements from one to the other and other than table naming conventions everything will pretty much work. Each one has strengths and weaknesses. As noted in other replies the biggest differences are in the custom functions as well as the more extended functionality.
&gt; You have count(pszprokeyi)"#Zuweisungen" . You can't count nulls. &gt; Try SUM(CASE WHEN pszprokeyi is null &gt; THEN 0 &gt; ELSE 1 &gt; END) as "#Zuweisungen" perhaps you do not realize, but **those give the same results **
Yes, 2017 is version 14.%
If you want to speed up your replication after you get it set up, i highly recommend checking out nitrosphere
Do I need to do anything else, like connect, customise or install ssms? Thanks
BETWEEN ‚ÄòYYYY-MM-DD HH:MM:SS‚Äô AND ‚ÄòYYYY-MM-DD HH:MM:SS‚Äô
Thanks for the reply, but what I meant was that it only looks at the hour between specified dates. E.g. every row on any day between two dates that fall between 10 and 11am.
As of 2016, SQL Server does not ship with SSMS included in the installer. You need to install SSMS or Azure Data Studio separately.
Yeah sorry, I couldn't get the screenshot on in the previous thread. ü§™
let's start with this -- WHERE timestamp_column BETWEEN '2011-04-15' AND '2011-05-15' presumably you want to include the data from May 15? then you should write this -- WHERE timestamp_column &gt;= '2011-04-15' AND timestamp_column &lt; '2011-05-16' then append this -- AND TIME(timestamp_column) &gt;= '10:00:00' AND TIME(timestamp_column) &lt; '11:00:00'
On MS SQL Server, the easy way to do it would be; &amp;#x200B; SELECT * FROM #SomeDates WHERE CAST(SomeDate AS TIME) BETWEEN '10:00' AND '11:00' While I'm sure that the MySQL code would be quite similar, the problem is that this is not SARGable and fairly expensive. Anyone have any ideas on how to express this in a SARGable fashion? If I think of something, I'll post it :)
I'd say the same thing. Get SQL Express as I believe you can do everything except some very advanced stuff in there.
Off the top of my head, if you sign up for the Developer Essentials program (free), you can download and use it as a developer. As in not for production.
üëç
You're right. OP didn't mention his SQL engine so this is a solution. I swear Oracle tripped me up in a T-SQL procedure once with an ORA-xxxx code and this was my solution. But count should work in 99.9999% of the situations.
&gt; My question is will I basically be embarrassing myself by showing this? if it gets the correct result, **absolutely you will not**
The temp tables can probably be replaced by CTEs and there's more than likely some indexes or changes you can make to improve the speed... but if it works, it works. Everyone has to learn somewhere. &amp;#x200B; Before you show it off, get the actual query execution plan for it and see if there's any obvious bottle necks... then when you go to show it, you can already point out "There's something up with the join on XYZ... seems to take a long time and I'm not sure why."
While there may be some things that you can do to improve the efficiency of it, unless there are performance concerns because you are running it in a busy production environment the most important part is that it returns the right data. You can always tune it later if needed. Back when I was first learning SQL I wrote a horrid query with several nested subqueries against a huge table to calculate intervals between things, which took ~50 minutes to run. it barely worked because the reporting database was log shipped from prod and so was restoring for 2-5 minutes every hour, leaving me a 55 minute window to work with. The important part was that it worked and pulled the needed data so management was happy (it also only needed to be ran once per month). I got a nice pat on the back and moved on to other projects. Skip three months into the future to a time where I was both more familiar with the table in question and with SQL in general. It was time to run the query again but as I had some extra free time I decided to try and re-write it to be more efficient, especially as they were wanting to be able to pull the data more frequently. The end result was that I used some window functions and cut the execution time down to ~3 minutes.
to add on, IF you do wind up embarrassing yourself showing this, the only result is you now know that higher up is a fucking asshole and you should start looking for a work environment that encourages employees learning and asking for feedback on new knowledge.
Thanks, story made me feel better about sharing it. For whatever reason I get intimidated when I'm trying to show someone my queries who I know could do it better lol.
use a website like imgur to upload images and link the image in the reply comment.
Since you are in Excel already why don't you use a Pivot Table. Get all the data back and then let the Pivot table do the work. Set up a query that has all the combinations and then use that as your data cube.
Theres a typo in the code screenshot of code at the top of the link.
That's a T-SQL library, not a "SQL" library ;) In Postgres for example there is no need for a "median table" or passing comma separated string - you would simply create a custom aggregate that can be used just like any other aggregate. For the median this wouldn't even be needed ([although there is an implementation for it](https://wiki.postgresql.org/wiki/Aggregate_Median)) because it can be calculated using built-in aggregates. To calculate the average and median of 5 numbers, you would simply run: select avg(x), percentile_disc(0.5) within group (order by x) as median from unnest(array[1,5,9,8,7]) as t(x)
ML services and SSAS I believe..
The SQL server agent doesn't come with express either last I checked.
Thanks a lot for the info :) Also changed the name to T-SQL, somehow that slipped my mind. lol &amp;#x200B; As for the MedianTable, that's just using a data type, since in SQL Server you can't send a table variable to a function without it being a defined type. It could have been solved without it of course, it was just a method of solution. Over time I will add different solutions/approaches to the same problem. Since older and newer version of some DBMSs don't support certain modern functions (for example: SQL Server's string\_split function didn't exist prior to 2016). If you have any suggestions on what you think would be a nice problem to tackle and put up in that library, please do share!
&gt; Developer Essentials [Linky Linky](https://visualstudio.microsoft.com/dev-essentials/#software)
thank you for the answer!
Nope. I used express in production for about a year before we finally got standard. You can call processes through sqlcmd and batch files with task scheduler. It's not ideal, but it works
Eh. Weirdest thing but that happens a lot in the MSSQL world. I've heard plenty of folks just refer to their SQL Server instance as a "SQL database" and then refer to other dialects by their formal name. It's peculiar to people that use other dialects, but most SQL Server folks don't even notice that their doing it. (I'm a consultant so I use pretty much whatever the client has). It's just a symptom of Microsoft naming things ambiguously. It's a marketing trick that Apples known for. iPhone, Apple Watch, etc.. These are generically named so that people are inclined to think of their product first as the de facto standard for a specific product category. It's a shortcut bid for brand dominance. The idea is so that people will start using the brand name in place of the product category or as a verb to carry out an action associated with particular product category. Example... People used to "Xerox a document" rather than copy a document. Many people ask for a "Kleenex" rather than a tissue. You throw your trash in a "dumpster" (which was also once a brand name instead of a generic item). I've took a few marketing / consumer psych classes in my undergrad......
Alcohol and weed.
It will honor both INs. It‚Äôd work the same as if you just put everything into one single IN.
Try this: [SET DATEFIRST](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql?view=sql-server-2017)
Sorry that's not exactly what's happening here and also not what my test suggested. This isn't an Additive situation where both IN's have different values To give a more concrete but hypothetical example I based my test on, say the main IN is AND UserName IN ('John', Bob', 'Roger') Now because of how the software is designed the Names filter will only have those 3 names instead of the full list of names available. Also the data of all 3 employees show up on the main form Now if the user goes to the filter list and if they check Roger and hit filter the second IN will wind up being AND UserName IN ( 'Roger' ) So you wind up with WHERE 1=1 AND UserName IN ('John', Bob', 'Roger') AND UserName IN ( 'Roger' ) The desired behavior and the one that my test brought up was that only Roger showed up in the list of results. So I'm trying to determine whether that is a fluke or the behavior I should expect 100% of the time. Because if it's the latter then I can proceed with writing the rest of my filtering code, but if there's possibility of it failing then I have to rethink how I deal with keeping the main clause but still honor the filtering.
Does this work with views, though?
You can expect it all the time, if you check the execution plan for IN it basically converts it to an OR clause e.g. WHERE 1=1 AND (UserName = 'John' OR UserName = 'Bob' OR UserName = 'Roger') AND (UserName ='Roger')
Thanks, that takes a load off my mind and means I can probably finish my filtering system tomorrow and start thinking about the search function.
I'm picking your solution DATEADD(dd, -(DATEPART(dw, d1.date)-3), d1.date) is failing on Monday and Tuesday: e.g., Monday is day 1, so datepart - 3 returns -2, and subtracting negative two (i.e., ADDING two) is returning the NEXT Wednesday rather than the previous one. I'm sure there's a more elegant solution but something like DATEADD(dd, - case when (DATEPART(dw, d1.date)-3) &lt; 0 then 5 + DATEPART(dw, d1.date)) else DATEPART(dw, d1.date)-3) end, d1.date) And of course it goes in both the select and the group by.
* What does your execution plan say? * Are your tables indexed and keyed?
I think your answer key is wrong - it should be B and E.
There doesn't seem to be anything syntactically complicated that would be confusing the optimiser, so it's probably simply a case of adding indexes. There's a lot to be said for doing this properly, but you could find out whether this is the case by just putting single-field indexes (unique where appropriate) on all of the IDs used in joins and the where clause. I do question why you've got a group by in there? That's not actually valid syntax is most SQL implementations, and I don't know what MySQL actually does there - does it return only one message per inbox ID?
I'm sorry, but 30 seconds across 20M rows and you think thats bad? There isn't much to improve upon in your query, but you could possibly improve your data model. IN() tends to be kind of sucky, so you could dump the keys you are looking for into a table/#table and then throw an index on it before joining. Additionally you could look into clustering around user_id if that's even possible, or you could look into partitioning your data, but overall 30 seconds against 20M isn't bad in my opinion and I'm curious why you want/need to improve that?
for a sec I was wondering why you were answering yourself...
i mean i'm kind of drunk, but, this is kind of fucking wrong? but also i'm drunk and i might be almost as wrong as the question fundamentally kind of the thing to do is to write the correct thing and to pray that the bullshit corporate test hasn't eaten its own feet jesus, they call an entity relationship model an ERD? which means diagram? really? look i can't help you with the [bullshit meaningless certifications](https://www.reddit.com/r/java/comments/5mnjfi/i_have_been_looking_into_oracle_certifications_is/). these are written by the same assholes that don't know english and somehow write the ACT here are some things you need to know 1. i've been hiring for 20 years and i've literally never cared for an entire straight second whether someone had an oracle cert 1. see #1 1. the dba at work (all jobs, not current) legitimately looks down on people who've done this 1. no really see #1 students and projects can have multiple of each other? fine, who cares create table FuckYourStudents(id bigint not null auto_increment, stuff fields data); create table FuckYourClassesToo(id bigint not null auto_increment primary key secondary key maybe even tertiary key this column seems important quaternary key); create table FurthermoreFuckTheRelationshipsBetwixt(student bigint not null, project bigint not null, foreign key(obvious shit) references (other obvious shit)); ***BuT tHaT's NoT oPtImAl*** oh, isn't it? how do you define optimal? speed? disk space? what's the cardinality of the students? the classes? what's the density of the relationships? more importantly on what fucking planet will students' and classes' "optimality" matter on a not-1980s machine "BUT WE WANT TO KNOW IF YOU CAN CREATE THE IDEAL STRUCTURE" well, s/he can't, because this question contains far too little information what if the classes are permanent, like oxford, and you're indexing thousands of years? what if the classes are ephemeral, like livestream, and you're indexing thousands of opportunities? you don't even fucking know what's "optimal" from this diseased, half-baked question someone should get into a time machine and slap the author. hard . like they can't even decide on how capitalization works, so it couldn't be that the question is written by a no-detail idiot, surely the reader is at fault . who the fuck knows? probably the author of this extra-wrong question decided there would be more students than projects, and decided that because of that, "the only correct way" would be to build the relationship in that directipn they would be immediately fired as a DBA for making claims without metrics but yeah, if you write test questions, it's not like anyone who understands this shit is making $35k/y to check your work literally all of these answers are wrong no answer can be given without measurements. this is just the literal anus of a horse saying "my average-driven guesswork is the only acceptable answer" don't be sad that you picked the wrong astrological signs
Oh ok I gotcha. Then yes I believe you are correct. A more restrictive IN would override another.
No. If 30sec is legitimately too long for a reason that isn't included in the description, then that needs to be looked at. If it isn't and the OP just arbitrarily wants to know how to make it go faster (which is nonsensical) then the datatype could *possibly* be improved (no information given on table structure), but not by much. For example, you very likely aren't going to improve that query by an order of magnitude, or if you do then it is likely going to involve hardware changes. 20M isn't exactly a "large" table, but it isn't exactly small, either. The time it takes to fully execute a command also takes into consideration the time it takes to transmit data across the network and into your results window --&gt; unless you're inserting into a table, which goes back to the question, "why exactly do you want to improve this?"
Dude, look at his username.
Ha. Sorry, missed that.
I'd give both the MS SQL stack (SSMS and SQL Server) and BigQuery a try. The prior is very popular across businesses and the latter is the new-new hotness in analytics related SQL - they also have fairly different flavors of the SQL language, so that is good to get exposed too. Otherwise, I'd just focus on the language and getting good at transforming data into different levels of granularity and then give OLAP functions a go. For your specific use case, I would also recommend learning how to export your query results as CSVs to upload to R or STATA, and also how to connect R or STATA to a SQL database to query against directly.
So you want to look at your JOIN, WHERE, and ORDER BY clauses, any column in there that isn't indexed means that the server has to scan through every row one by one. (Of course since we are talking about several joins and filters here the optimizer may be able to optimize some of this out, but indexes will help) Adding indexes to every column in the where clause would be a decent start, but realize adding indexes will slow inserts. So you have to think about what you want to prioritize here, reads or writes. Think about adding indexes to: i.message_id i.user_id i.account_id i.deleted_at m.conversation_id m.created_at t.message_id There are a lot of things to think about with indexes, but one of the things to think about is using single or multi field indexes. Multi field ones can be faster but you really need to be optimizing your select queries very specifically in order to utilize them properly. Single field ones will pretty much 'just work'. There is also cardinality, a column in a table with a ton of rows but low cardinality (as in there are lots of rows but few distinct values in the column across all the rows) will not benefit as much from an index, where as high cardinality will benefit more. This is just scratching the surface but yeah, read up on some indexing strategies. The data model itself seems ok from what I can grasp here. But the comment about the GROUP BY clause from the other poster /u/fauxmosexual is spot on. That *is* valid SQL in MySQL -- BUT if that query would actually be returning multiple rows with the same i.id had you not used the GROUP BY then the results returned in every other column would essentially be random data from one of the rows in that group. Not very useful really. It honestly probably shouldn't be allowed but queries like that are due to backward compatibility I guess. Although I believe it would throw an error if strict mode was on.
Here's a function I wrote for MySQL that computes the first Wednesday on or before a given date: drop function if exists last_wednesday; delimiter $$ /* Given a date d, find the date of the first Wednesday on or before d */ create function last_wednesday(d date) returns date begin declare dow smallint; declare days_to_subtract smallint; /* 0=Mon, 1=Tue, 2=Wed, 3=Thu, 4=Fri, 5=Sat, 6=Sun */ set dow = weekday(d); if dow &gt;= 2 then set days_to_subtract = dow - 2; else set days_to_subtract = dow + 7 - 2; end if; return d - interval days_to_subtract day; end $$ delimiter ; Then you can just say select last_wednesday(d1.date) as WeekStart, ...etc... group by last_wednesday(d1.date); Hope that helps. Whatever you end up doing, I would recommend encapsulating the code in a function for readability, maintainability, and all that good stuff.
LOL. You lucky bastard. So first off, your company is clown shoes. Like dumb as fuck. Seriously dumb as a box of rocks. Secondarily, you're basically acting as their data architect. The bad news is that you're gonna suck balls and do a lot of dumb shit. The good news is that you're going to have a great resume entry, and if you work hard &amp; ask advice from people here then you'll probably be able to greatly help their business, which will also look great on your resume, and possibly lead to a full time job where they underpay you for a few years before you jump ship to a new job where you get a 20-40% raise. Welcome to the SQL club.
My expectation is that this takes sub 2 seconds across 20M rows.
Going to be a balance between hardware, database type, and data types then. The query itself isn't "bad," and I really don't see how you're going to get a gain like that from "optimizing it." Indexing *might* get you there, but I suspect you'd also possibly need to partition your table, upgrade your hardware, etc.
I'm still in school and really have a month and a half left to work. I'd love to learn and pull this company up, but I am not interested in being their data architect. And with the time span and conflicting tasks that they give me, do you think I would be able to learn enough to really be their data architect?
Yeah this company is either doing a Truman Show level of experiment to see what you'd do... or they're dumb as a box of fucking rocks. No middle ground. &gt;And with the time span and conflicting tasks that they give me, do you think I would be able to learn enough to really be their data architect? I mean, yes? Depends on you, your ambition, how hard you're willing to work, but more importantly than anything: whether or not they will empower you. Like do you even have a server? Are you just planning on installing a copy of SQL on your local disk and doing this? Are they even interested in doing this?
The query is bad if there are no indexes on the fields queried. The m.* means that even with an index there still has to be a read for the whole data row which may be fast or slow depending on partitioning. The way to correct the query is to logically break down the expensive join first so you determine what is the smallest unit of data you can get and the sub query. I would select the data from messages using conversation_id into a temp table then do the join so your dataset is smaller. But if there is no index it‚Äôs still a full table scan.
&gt;The query is bad if there are no indexes on the fields queried This is a nonsensical statement. The rest of what you're saying doesn't make any sense to me from a MS SQL perspective. I'm not seeing any subquery. It's a simple from &amp; join with a where. The IN() isn't ideal, and the order by is going to cost something. 20M rows in 2seconds isn't going to happen without the appropriate server. 30 seconds seems entirely permissible to me.
Perhaps a better way to phrase it is if your going to join on fields that are not indexed you might need to rethink more than the query itself. It is a simple join but without knowing the table structure and data size for the tables joined you don‚Äôt know if it‚Äôs going to be a one to one or one to many relationship I.e. is it going to be a Cartesian join of significantly larger data set than 20M
Get sqlExpress for free and either connect to your database using Excel and create visualisations that way, or download a free BI tool (power bi, Mstr desktop etc) and do it that way.
No, it doesn't seem like they're interested in SQL or neither do they have a clue on what it is. I would like to think they have a server. I want to use SQL but I have no knowledge on being a data architect, and I am willing to learn but I am not interested in doing it for this company solely because I believe It will take a lot of time to get to that point and by that time I'm just going to look for another internship and focus on school. &amp;#x200B; At first, I was trying to import csv files into MySQLWorkbench, create tables off them, import records then work off of them on localhost but this does not seem to work efficiently as there are a lot of data accuracy problems when importing csv.
&gt; but I have no knowledge on being a data architect Neither do most people who find their way into that job.
I'm not entirely sure of the syntax, but isn't a "JOIN" just a left join? How can that possibly be cartesian? Even something like that was going on, it wouldn't finish in 30 seconds.
&gt; I'm sorry, but 30 seconds across 20M rows and you think thats bad? hey, as long as you're back there in 1982, could I give you some money to invest?
you are super out of your depth all joins are Cartesian
&gt; &gt; The query is bad if there are no indexes on the fields queried &gt; &gt; This is a nonsensical statement. No, it isn't
Ops company are an uncivilised bunch of cave people. OP may get to be the one who shows them what fire is. Now sure, every other tribe has bic lighters and furnaces, and OP will want to join them soon. But for now he can be revered as the person who brought fire from the gods. That can be a pretty sweet gig.
Seriously, install SQL Express or postgres or MySQL somewhere and import all the excel data and start writing queries and putting them into reports. Blow their mind! You may not want to do it, but please recognise how good it will look on your CV to say you single handedly shown a company of barbarians how fire works. When I started my first job as a "data analyst" it involved digging through 6 views that all referenced each other and rewriting it all to make some logical sense without any documentation of what the view was even supposed to do. What you have here is much easier, nice clean slate to get things right!
define all the columns instead of using the asterix and it will magically go faster
i'm pretty sure whereever you pull your excel files from the company's applications (also use csv or txt, never excel, the auto-datatype discovery will mess you up eventually) are in a database, you're just using some reporting GUI meant for primitive raw export. Ideally (and that's going to be a real uphill battle) you need to talk to the application support who in turn needs to talk to their DB support (unless they do both). They most likely will fight tooth and nail to give you even read access to the DB backend. Generally and preferably you shouldn't run analytical queries on an app backend DB, they'll be constantly worried that it'll impact performance if you don't write efficient queries or just trying things. What you need is a staging DB, where the apps can push raw exports db to db whenever it's convenient for them, and then you can go to town on the data with complex analytical queries, you can even connect your excel pivot charts and grapth to pull their source data with an embedded query already pre-formated.
Please start a slightly drunk SQL blog.
b and e are mutually exclusive
This is not true in SQL Server, is it true in MySQL?
I disagree... both are describing the same process of creating a bridge table to support the M:M relationship.
You may have 20M records "in all the tables" but what are the relationships? If there's anything but a 1:1 relationship between tables, you're getting a multiplicative effect. At the very least, the fields you're using to `JOIN` on should be indexed.
Which DBMS are you using?
MSSQL
first, group by Factory_ID, Product_ID, and Sale_Date and with Production_Date, get the count(1)/(Sale_Date - Production_Date) in days) and count(1), then (using the result as the derived table) group by Factory_ID, Product_ID, and Sale_Date, summing up the metrics
1) Why are you using group by here? 2) You can add filter predicates inside the SQL ( I don't know if this tech will work in MySQL) 3) Limit number of columns in m table Try: SELECT m.id,i.inbox\_id, t.to\_id, m.a1, m.a2 etc FROM message m JOIN inbox i ON [m.id](https://m.id/) = i.message\_id AND i.user\_id = 'U901CB3A' AND i.account\_id = "AU210KMx" AND i.deleted IS NULL AND m.conversation\_id IN ('6ac3rf0583cb16affcacffe1643061a4') JOIN to t ON [m.id](https://m.id/) = t.message\_id ORDER BY m.created\_at DESC LIMIT 0 , 30;
getdate() gets you the current date, eomonth( &lt;date&gt;) gets you the end of the month for a given date, dateadd( m, &lt;X&gt;, &lt;date&gt;) gives you a date that's X month from a given date
Don't have access to SQLite 3 but something like this should work &amp;#x200B; SELECT Factory_ID ,Product_ID ,Sale_Date ,Weighted_Count = (COUNT(*) * 1.) / MIN(DATEDIFF(DAY, Production_Date, Sale_Date)) FROM SampleCTE GROUP BY Factory_ID ,Product_ID ,Sale_Date ,DATEDIFF(DAY, Production_Date, Sale_Date)
Thanks! Yeah, computing the metrics then summing them should work just fine.
Rdbms or any sort of database system would make life easier. I would suggest to start with postgres as it is open source, very close to Oracle and ANSI sql in scripting and has a lot of custom analytical functions that can be very handy for a data analyst. If I were you the following steps are what I would follow: 1) setup local postgres DB. 2) whiteboard a schema design for your data. Along with the constraints and indexing on the tables and columns. Make sure you understand the data completely before you start whiteboarding and think about all possible scenarios that you would use the data in. For example, if it is transactional data, you may want indexes on date columns , and partitions as well. 3) load your files into the tables. 4) start working!! Pardon me, if this does not fit exactly with your use case, but just helping out with a direction you could take.
Your point is correct, a staging DB is what op needs. But to understand the concept of it with the experience op has stated would take him time to even get started with. I totally agree with the point on the back end DB. I used to get a lot of shit from my backend team for access!!
No `datediff()` function in sqlite.
Convert your dates to a `yyyy-mm-dd` format (or another one supported by sqlite [date and time functions](https://www.sqlite.org/lang_datefunc.html); Julian date would let you skip the next step) and then use `julianday()` and some subtraction to get the difference in days.
These should pull the right dates for each of your sub-queries. SELECT DATEADD(YEAR,-1,EOMONTH(DATEADD(MONTH,-1,GETDATE()))), EOMONTH(DATEADD(MONTH,-1,GETDATE())) SELECT DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-1,0), DATEADD(DAY,-1,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE()),0)) SELECT DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-2,0), DATEADD(DAY,-1,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-1,0)) SELECT DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-3,0), DATEADD(DAY,-1,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-2,0)) SELECT DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-4,0), DATEADD(DAY,-1,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE())-3,0))
This is a bit abstract to know if it is possible, if so, it should not bog down the system, however changes to how the site gets values. A simple coalesce on the website seems like it would fix your issue.
Quick question. How useful would it be to have access to FK data without having to create a diagram for your database?
Given this table: CREATE TABLE products(Factory_ID TEXT, Product_ID INTEGER, Production_Date TEXT, Sale_Date TEXT); INSERT INTO products VALUES('A',1,'2018-11-01','2018-11-04'); INSERT INTO products VALUES('A',1,'2018-11-01','2018-11-04'); INSERT INTO products VALUES('A',1,'2018-11-03','2018-11-04'); INSERT INTO products VALUES('A',2,'2018-10-05','2018-11-04'); INSERT INTO products VALUES('B',1,'2018-11-03','2018-11-07'); this WITH by_production_day AS (SELECT Factory_ID, Product_ID, Sale_Date , count(*) AS today_count , julianday(Sale_Date) - julianday(Production_Date) AS days_between FROM products GROUP BY Factory_ID, Product_ID, Sale_Date, Production_Date) SELECT Factory_ID, Product_ID, Sale_date , sum(today_count) AS Count , sum(today_count / days_between) AS Weighted_Count FROM by_production_day GROUP BY Factory_ID, Product_ID, Sale_Date ORDER BY Factory_ID, Product_ID, Sale_Date; will give Factory_ID Product_ID Sale_Date Count Weighted_Count ---------- ---------- ---------- ---------- ---------------- A 1 2018-11-04 3 1.66666666666667 A 2 2018-11-04 1 0.03333333333333 B 1 2018-11-07 1 0.25
 Quick question. How useful would it be to have access to FK data without having to create a diagram for your database?
I‚Äôm not an SQL expert by any means, but I don‚Äôt think you can use an aggregate in the where clause. Maybe use a CTE to get around it? (Still learning Sql myself)
Correct, the aggregate would need to be in a Having clause.
The query run by the site has a similar format to this: SELECT DISTINCT TOP 200 Value1,Value2...Value23 from table WITH (NOLOCK) where animalnumber= '123456' and animaltype.table IN ('LAMB') &amp;#x200B; Would coalesce still be appropriate considering the distinct ?
correct, the query would also need a GROUP BY clause plus, you don't need DISTINCT SELECT b.ORDER_NUMBER AS "Order Number" , SUM(ROUND(a.UNIT_PRICE * ORDERED_QTY,0)) AS total FROM SOBOOK b WHERE b.SALES_CATEGORY IN ( 89,90,91,84 ) AND DATE(b.ORDER_DATE) &lt; DATE(:startDate) AND b.TRANSACTION_TYPE = 2 GROUP BY HAVING SUM(ROUND(b.UNIT_PRICE * ORDERED_QTY,0)) &gt;= 1000 AND SUM(ROUND(b.UNIT_PRICE * ORDERED_QTY,0)) &lt;= 25000 note there is no `a` table in the FROM clause so i changed the HAVING references to the `b` table also, not sure you need to round, but if so, it's better to do it outside the SUM rather than on each individual price x qty calculation inside
Here is an example of use of IN under MS SQL Server: https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_in You can see it returns all the records WHERE the record has a value within the specified set. I think the issue is using IF instead of WHERE.
Thanks, unfortunately that's not quite it. (Am also using WHERE in my query. IF was a my mistake) I've updated the post with some more clarification...
Select blah blah from blah where blah in ('type1') Union all Select....... where blah in ('type2')
You could try using a temp table that has the values in it and joining to that. CREATE TABLE #valueTable (myvalues VARCHAR(10)) INSERT INTO #valueTable (myvalues) VALUES ('val1'),('val2'),('val3'),('val1'),('val3') SELECT whatever FROM wherever a JOIN #valueTable b ON A.value = B.myvalues
This question is confusing because (1) no "a" table is mentioned and (2) "ORDERED_QTY" is the only field not coming with a table label, so we don't know where that is coming from. As I see it, it is possible that you have orders with multiple items in records, and the category field may contain lists of categories for the various items. ORDERED_QTY would be ambiguous in this case. However, it would be clearer that you were attempting to sum the item prices and quantities for several different items in an order in this case. It is also possible (and much easier to work with) if you have each item separated out on its own record for an order such that only one item and therefore only one category is possible for each order (or ORDER_NUMBER). Here it would be clear that ORDERED_QTY is coming from table b "SOBOOK" and refers specifically to the sole item associated with the ORDER_NUMBER. What are you summing over? That is totally unclear in this case. I guess the individual copies/instances of an item may fluctuate in price even within an order number (like maybe you sell a gallon of gas each week somebody has a long-term subscription to, and the price fluctuates week-to-week?). Then you could be summing over the units within an order. I think you'd get a better answer if you (1) give an example of what the records look like in the table(s) and (2) tell us what you want from your query in an informal verbal description.
Yeah, there was a group by clause in there at some point, and the query was a lot longer. Looks like a bunch of it got edited away.
You're looking for CAST. If you explicitly cast the substring as an integer it should work for you. Create tableaC as Select tableA.varA, CAST(substr(tableB.varA,1,9) as INTEGER) as varB From tableA Inner join tableB on tableA.varA = tableB.varB Note, this can cause things to fail if that particular substring contains non-numeric characters.
This will work. Thanks y'all!
Thank you for this advice - however, tableB.varA contains numbers and letters. However, the first nine positions in tableB.varA are always numbers. Is it still possible to proceed this way? Thanks
Totally, as long as the bit that's getting returned is always numbers then you should be good.
&gt; my list of values changes from (Product1, Product2) to (Component1, Component1) Huh? Just select the product identifier too in the original query possibly? As in: SELECT Product_Num, whatever FROM wherever WHERE Component IN (Component1, Component2, Component3) Normal SQL behavior will return all the columns matching anything in the IN list, including repeats even if you do not repeat items in the IN list. To be clear: IN (Component1,Component2,Component3,Component1,Component2) is equivalent to: IN (Component1,Component2,Component3) SQL will not guarantee the ordering on the results, so you'd probably need to merge in the result of the query on the Product_Num field/column anyway, unless there is an ORDER BY that isn't being mentioned.
Repeat on u/notasqlstar and u/dodger94 replies. You can check my post history for a Microsoft SQL tutorial on data architecture, but it wont help you if you are new to sql. Pick a db (mssql developer edition is free for non production use, postgres is super friendly and the series functions are powerful, mysql... no comment) Model data, Shine like a golden child, blow people's minds, ask for big money or use your new found skills to get it elsewhere. You have been given a career making opportunity. You're in the end game now.
True, but if indeed stuck with exports or integrations and doesnt want to go full sql server route, for some basic transformation automation i would recommend excel Power Query and create simple models with mostly flat tables. Or use PowerBi which is awesome for displaying csv exports and easily reusable saving lots of manual formatting during updates. Really sounds like PowerBi good fit even if just use yourself.
Not necessarily, usually a menagerie of ftp exports, or imports for certain things, like locations, products, etc.
Yea starting to think this may not be doable in the way we were hoping. &gt;Huh? Just select the product identifier too in the original query possibly? Product num exists in a different table than the component data that I'm after. Might be doable with some JOINery, but your point about the order is quite valid. Going to have to rethink this a bit.
If you use an AFTER INSERT trigger, what is the INSERT that will fire the trigger? I only see a SELECT.
Can you provide the rest of the SQL statement? If it is complete as it is above, I'm going to assume that it is dynamically generated, and that the code that generated it does not accommodate all conditions. `INSERT INTO Data` is not the end of an `INSERT` statement. It should look like `INSERT INTO Data (name, title, salary) VALUES ( 'DevelInTheDetails', 'DBA', '0.00')`, or `INSERT INTO Data (name,title,salary) SELECT (fullname,position,annsal) FROM Employee`.
Oh my god, cue the anti-Excel people ... you heathen, advocating for something other than SQL!
I'm so confused as to what are you trying to do. My best guess: 1) You cannot change a website query that queries table T and has column C in WHERE clause. 2) Something inserts new records in T, but not filling the C column. 3) You want to fetch the C column from another table T2 based on a Key from the table T. The possible solutions: 1) Change the query 2) Change the insert 3) Query a view instead of the table. This view would pull all of the required data 4) If you go the trigger route, use INSTEAD OF trigger. No point in insert and then update.
You'd just need any unique identifier that is common to both tables. I really am curious what the tables look like, an example of records from the involved tables would probably really help. Also, to clarify, you need multiple copies of some records (like the ones with val1 in your OP example because val1 is listed twice)?
Any cheat sheet for functions ? In the process of learning intermediate level T-SQL
Hard to say without seeing the complete statement, but: 1) You are missing the closing parenthesis before BEGINso that would be an error. 2) No point in the ISNULL operator as NULL can't equal your string anyway. 3) Don't use IF COUNT(*) ‚Ä¶ = 0 but use IF NOT EXISTS The difference is that count must find all the matching rows and NOT EXISTS stops on the first matching row. Apart from that, the problem shouldn't be here. There might be other things that could prevent that. For example: 1) There is a different part of the code that prevents it, that you have not shared. 2) It fails silently in TRY CATCH block (for example because of UNIQUE constraint) 3) There is an INSTEAD OF TRIGGER on the table you are updating. 4) The data could be deleted by a different process before you can check. etc.
I got lost in a rabbit hole when I started to write it up. I assume it would be this: &amp;#x200B; create trigger dbo.FillAnimal on dbo.table after insert as begin set nocount on; &amp;#x200B; \--UPDATE \-- table \--SET \-- animaltype.table = animaltype.tableb \--FROM \-- table \--INNER JOIN \-- tableb \--ON \-- table.animal\_id=table2.animal\_id \--where animaltype.table = ' ' &amp;#x200B; end go
Couldn't you count null so the first part before the equal sign is counting all ids that are null.
Yes you've got the gist of the issue. I cannot perform actions 1 to 3 due to politics and other development shenanigans. I thought about instead of but I think that would necessitate me creating a temp table to store the inserted values before the additional value is filled right ?
Nah, WHERE is always evaluated before aggregattion
No unique value common to both tables. The modules share a relation to the components, but not the other way around. Really though, there is a ton of complexity being added by the data about the components being so far removed from the modules. The new table fixes that handily. The data I have on hand is: &gt;module001 &amp; component001 The final product I need to generate is: &gt;red (580nm) 50lm LED Module The new table that we've added looks like this: **SKU | LED | COLOR | TECHCOLOR | LUMENS** &gt;module001 | component001 | red | 580 nm | 50 lm
Because you don't have the rest of the INSERT statement, I'll assume you're tracing db calls and this is not code you've authored and are troubleshooting. If this is the entire statement passed to the database, then that is problematic in and of itself. This will throw an error in a SQL console for reasons shown above - it is not valid SQL. The code that you do have suggests that it's looking to see if a row already exists that represents some parsed (RSS?) data from FlowReader. If it does not, then INSERT the row. I'll assume that the end of the statement looks something like INSERT INTO Data (id, title, descr, uri, pubdate, etc....) VALUES ({ID},{TITLE},{URI},{PUBLISH\_DATE}...), but I'm really going out on a limb here. That limb assumes some things: The FlowReader you speak of is the RSS software of the same name, and the objective is to gather data from RSS feeds into a database. But no, to answer your question, the zero does **not** mean no records should be added. It actually says 'Count the records matching this (value in ID column matches FIELD\_ID value from FlowReader\_cleanedData) condition. If you don't find any, INSERT some data into a table called Data.'
So if they wanted to count null i should use "Where id is null" as this statement actually is replacing nulls with a blank character.
LOL, you aren't wrong, but you're wrong, and I spent a good thirty minutes with my team discussing this topic. In our nomenclature, a left join is not Cartesian, and any insistence that it is one is pedantic. https://community.alteryx.com/t5/Alteryx-Knowledge-Base/Cartesian-Join-Cartesian-Product/ta-p/39009
This code is not counting rows where ID is NULL. It is counting rows where ID = {Field\_ID}. If you compare a NULL to any value, the two both ARE and ARE NOT equal. This is a topic unto itself, but suffice it to say that NULL means 'unknown'. If you do not know my phone number, then it both MIGHT and MIGHT NOT be equal to 704-555-1212. So IsNull prevents ID values of NULL from being compared to {Field\_ID}, because that comparison will always return TRUE. It uses '' instead, which should properly return FALSE when compared to {Field\_ID} (no guarantees, but that's what the author of this code is assuming). If no (zero) rows are returned, then this code wants to INSERT some data. This is a common database INSERT pattern. "Hey, database, do you already have this info? If not, then add it."
/u/AbstractSQLEngineer any thoughts on the execution time here of 30 seconds?
It should be just Set @myVar = ''
You can request one at SQLServer-Tips.com id be happy to write one up when I get time!
Last part to clarify so due to improper formatting of the insert statement it will fail which would cause the table not to update. Also if not exsists would be a better choice as it would do the matching more efficently than this standard if statement.
Whoops, Forgot where I was...please don‚Äôt pitch fork me! Also I thought could use sql language in power query but I guess called ‚ÄòMS Query‚Äô and from what reading can be more efficient than Power Query...also in power query can view ‚Äònative query‚Äô which appears some kind of SQL
Get rid of the group by... Index for table I... on the 3 where statements. Include (the select column) Index on m, on the where include the m.join column. Index on the m.join column Get rid of m.* or index m.id include (.*)
Ha! You're right ... when connecting to a SQL database you can definitely use SQL language using the native database query.
Can connect to an excel file too ...
/u/Achsin gave you the syntax, but &gt;I'm changing it based on conditions in if blocks, is it possible that scope is causing this? Entirely possible; you'd have to show us some code to help you figure it out.
I actually have no idea its just something to try
I'm glad it worked out. It sounds like it was just: SELECT m.sku, c.led, color, techcolor,lumens FROM Components c, Modules m WHERE m.led=c.led; I hope everything is working for you!
WHERE COALESCE(actual.year,goal.year) &gt; 2016
This bit is wrong: &gt;**TIMESTAMP**: Stores number of seconds passed since the Unix epoch (‚Äò1970-01-01 00:00:00‚Äô UTC) Unlike Oracle, the SQL Server TIMESTAMP datatype does not store any datetime information. It's just an auto-incrementing value that's unique throughout the database, and comes in pretty handy for incremental ETLs. [http://www.sqlines.com/sql-server/datatypes/timestamp\_rowversion](http://www.sqlines.com/sql-server/datatypes/timestamp_rowversion)
&gt; when I add in a WHERE statement, there's your problem... in outer joins, those conditions should be in the ON clause, not the WHERE clause show your sql and i can fix it for you
So a non-shared system, with 4 conditions where all 4 conditions are indexed?
ya that's a little more like it skip the unnecessary work, control the row size, get a good ssd, and almost any simple query not being run in bulk on a machine not swamped with writes will be near instant
I'm familiar with creating a local DB with MySQL, if it were between that or postgres should I just switch?
Dunno what you mean about non shared. My relationship table holds all relationships. People to group, group to group, part to people, group to part... direct (direct parent child), descendant (all under the parent), ancestor (all above the parent). And yes, my cluster is on the RelationshipSpecificDataSetNumber (same number for every row), then the Tenant (who owns it), the type (peo 2 peo, ancestor), then the ParentSpeciricDataSetNumber (another tables SpecificDataSetNumber) and the ParentPrimaryKey (another tables pk) Index on tenant, type, parentPK, include(childPK), index on tenant, type, childPk, include(parentpk) Where in() all day.
My personal opinion and preference of SQL db flavors are MSSQL, Postgres, sqllite, access (is sql Express now), no SQL, Oracle, mysql. It is my opinion, that MSSQL and Postgres offer more control over the physical layer (for speed) than mysql, and better overall engine performance than Oracle.
Shared as in multiple users.
Oh, yea. It's the core part of the app. Only about 30-50k peak concurrent, 6 mill user repo.
i was told last year to use datetime2 instead of datetime https://docs.microsoft.com/en-us/sql/t-sql/data-types/datetime2-transact-sql?view=sql-server-2017
Even with heavy writes, you just gotta get that cluster index accurate and in one file, push your non clustered columnars and non clustered indexes to their own database file.. and blobs in a 4th file... then create these files for the primary domains of data... like Reference (rarely changing), Relationship, People, Group, Part, Transaction... w/e.. [https://youtu.be/YfcN6WuHNs8?t=358](https://youtu.be/YfcN6WuHNs8?t=358)
When you say &gt;We exported &amp; imported the entire list of each category, sub category and child category, there's roughly 500 categories. Do you mean you've got just the categorisations themselves, or the products and the categories they belong to? If the latter, and you've got direct access to the ecommerce database, there's very likely a way you can do the whole lot in a single query.
Right so that goes back to one of my earlier statements here asking "why" the OP doesn't want 30 seconds. If you're talking about querying an app, etc., then in my way of thinking that is entirely different than a large web table with dozens of columns and having to search through 20M rows to return your desired results, which often involves less than perfect conditions that aren't/can't be indexed perfectly because of other structural dependencies. If you were designing an absolutely perfect structure for the fastest possible return... using 4 conditions, and no shared users to compete for resources... what would be approximately the minimum threshold for a response? -- And, how would that time scale? So for example if you could design a perfect environment to return in 3 second for 20M rows using 4 conditions, what would you expect it to return when you hit 40M rows? 6 seconds? Trying to use some simple shower math here to keep the numbers simple, but it might be easier to explain the answer if we were talking about 200M rows increasing to 400M rows. I'm assuming your answer is going to be hardware dependent.
Thanks for your response. We uploaded all the categories respective to their parent category (There's a parent category -&gt; child category -&gt; grandchild category -&gt; product), but the 5000 products we uploaded are all lumped into a single default category.
Well the query you posted would kind of work, but if someone is manually going through the products to add the product ID to the where clause and then running the query once for each category it's probably not really saving you time when compared to just assigning the categories via the application.
Who is storing 2gb of text in one row holy crap that's one hell of a json blob
This was exponentially better than any answer I anticipated. Thanks for your time and the laughs.
JSON is not a SQL Server data type. JSON is stored in varchar(max) or nvarchar(max).
Yes, but also you don't want to count the nulls. You want to find the first non/existing.
IMHO It lacks information about storage size for example tinyint 1 byte smallint 2 bytes etc.
Thanks! I‚Äôll get that added
so since it is looking for the nulls then replaces them it would have no results to not match there for there would be no update. i think i hopefully got it.
It sounds like you need a FULL OUTER JOIN, then in the WHERE clause (which is applied as a post-join filter after you've joined together the rows from t1 and t2) have something like &amp;#x200B; WHERE t1.year &gt; 2015 OR t2.year &gt; 2015 &amp;#x200B; ...to restrict to the relevant period. &amp;#x200B; Alternatively, filter the input BEFORE joining with something like: &amp;#x200B; SELECT ... FROM (SELECT \* FROM t1 WHERE year &gt; 2015) t1a FULL OUTER JOIN (SELECT \* FROM t2 WHERE year &gt; 2015) t2a ON ([t1.id](https://t1.id) = [t2.id](https://t2.id) AND t1.year = t2.year AND t1.month = t2.month) ... &amp;#x200B; That will potentially be more performant as you are pushing the filtering to the initial scans of t1 and t2, rather than doing lots of work to perform the join then filtering that (potentially very large) result set.
Turn your single ' into double '' (not a quote).
Single quotes are usually escaped by using another single quote. I don't think that double quotes needs to be escaped, but it could vary based on dbms. So your escaped string would be: 'Rectangle 8''8" x 11''6"'
Thank you!
Thank you!
Try this SELECT rclm.SUBJID, rclm.VISDAT, rclm_dem.SEX, rclm_dem.WEIGHT FROM rclm JOIN rclm_dem ON rclm.SUBJID = rclm_dem.SUBJID
This is such a basic question that, had yu asked it elsewhere (eg stackoverflow) you might have been banned; you need to show you have at least attempted to solve the problem! &amp;#x200B; Anyway: &amp;#x200B; `select a.field1, b.field2` &amp;#x200B; `from a` &amp;#x200B; `inner join b on` [`a.id`](https://a.id) `= b.id`
Use 2 single quotes not \\ &amp;#x200B; `VALUES (82128, 5414, 'Rug Size', 'Rectangle 8''8" x 11''6"', 273.00)`
For further Reading, here is the [SQLZOO Join Page](https://sqlzoo.net/wiki/The_JOIN_operation) that will allow you to learn and practice.
Right here. There's no need to add rclm_dem in the FROM clause.
so what does the "." function do exactly?
Sorry about that, I just started learning and I'm still trying to figure things out, but I was getting stuck on how to do JOINs. Thank you for the help though!
Thanks for the tip! i'll take a look and try to get a better understanding of how this works.
&lt;table&gt;.&lt;column&gt; Also not a function. It's just syntax to refence a column in a particular table. A lot to tables have the same column names (usually where joins take place between the tables). Most DBMS will throw an error if you were to just put SUBJID in a select as it wouldn't know if you're talking about rclm or rclm_dem
Thanks! it worked out. So what does the "." function do exactly? and I noticed that when executed like this that the title name comes out as "rclm SUBJID" I tried to use the AS statement by typing "rclm.subjID AS SubjectNumber" and it came out as "rclm SubjectNumber" . Is there anyway to get rid of the rclm part through Aliasing?
Oh okay that makes a lot more sense now. Thank you for the clarification!
&gt;I would like my table below to hide/NOT show Bic (Shop) if the corresponding item is Ball Tables hold data. Selecting data retrieves data from tables. You want to select only the data that correspond to query. You should also include what type of sql you are working with. SELECT Shop, Item FROM tableName WHERE Shop != 'Bic' OR (Shop = 'Bic' AND Item != 'Ball')
Probably the simplest way is just to concat the strings and compare: SELECT * FROM table1 WHERE Shop + Item &lt;&gt; 'BicBall' You could use NOT EXISTS but I think it's overkill in this case and I believe it won't perform as well anyway. SELECT Shop, Item FROM table1 t WHERE NOT EXISTS (SELECT * FROM table1 t2 WHERE Shop = 'Bic' AND Item = 'Ball' AND t2.Item = t.Item AND t2.Shop = t.Shop)
You forget to specify what field to join on.
The `.` is used to tell the SQL server what table you want that column from. It's best practice to always specifically say what table you want a column from although I believe there are some sql engines that will allow you to not specify the table as long as the column you are selecting is not ambiguous -- as in as long as that column exists on only one table that you are selecting/joining from. What SQL software are you using? What are you using the queries for? (Generating reports? In software?) It typically doesn't matter much what the result column names are because that's not what is literally shown to users -- you can use whatever code is sending the query to the DB to format the output however you want.
Damn dude, no need to be a dick about it. &amp;#x200B; Also, why are you wasting your time writing "inner join"? Everyone knows join = inner join. n00b.
SELECT * FROM Items WHERE NOT (Shop = 'Bic' AND Item = 'Pen')
Can you be more specific? What kind of plsql stuff you would expect to do?
I need a complete replacement of PLSQL in MS SQL server 2014.
there is no PL language support in Microsoft SQL Server. if you wanted completely portable code, you should be using ISO/IEC 9075 based code (https://en.wikipedia.org/wiki/ISO/IEC_9075)... all vendors have proprietary extensions, and they always get use... which is why code is rarely 100% portable.
Someone just asked a similar question yesterday in /r/SQLServer https://www.reddit.com/r/SQLServer/comments/cbo33b/transitioning_from_oracle_to_sqlserver/
Rather than say "I need a complete replacement with 100% feature parity", give some concrete examples of what you actually _need_ to do and perhaps people can offer suggestions.
alternatively to that you can use `(under the tilde) instead of ' so 'Rectangle 8`8" x 11`6"'
It doesn't exist, though everything you can do in PLSQL can be done in MSSQL. Run your stuff until it fails to compile. Then re write the problem section using SQL standard code instead of Oracle shortcuts
the aforementioned CASE expression goes in the SELECT clause SELECT w, x, y, z , x + y as xy , CASE WHEN z &gt; xy THEN xy - w ELSE NULL END AS calc FROM db WHERE w = 100
p.s. technically you don't need `w` in the SELECT clause, because you already know what its value is going to be on every row of the results, but..
Awesome; thanks so much!
What is your sql statement
Thanks! This worked perfectly! If I want a list of items, can I use LIKE? Item LIKE ('Ball', 'Car')
You can, but you should use IN clause, not LIKE. SELECT * FROM Items WHERE NOT (Shop = 'Bic' AND Item IN ('Ball', 'Car'))
Just delete the column from the SELECT statement, or put `--` in front of it and comment it out.
 SELECT varA , MAX(varB) AS max_varB FROM tbl GROUP BY varA HAVING COUNT(*) &gt; 1
Probably wants to do a select * but minus one field from the * - which I believe is not possible
Yep. They're going to have to do it the hard way. List each column they want to be returned.
Hi all, thanks for the help, I ended up selecting each column individually rather than choosing everything using the \* and just leaving out the unneeded column which worked well :) &amp;#x200B; USE \[DB1\] GO SELECT \[COLUMN1\] ,\[COLUMN2\] ,... FROM \[dbo\].\[viewData\] GO &amp;#x200B; Thanks
I am not being a dick. The OP was lazy. He should have least at shown that he tried to solve this himself. It takes a few seconds on google to get the right syntax for a join. That's why I was harsh - not because he didn't know something but because he was lazy and made zero efforts. &amp;#x200B; OP, yes, feel free to ask whatever you want - as long as you show you have at least given it a try
Does your ON statement for the join have the right fields listed? Based on the fields from the rest of the query, it should be RecordID_1 and RecordID_2.
I'd create a staging table with the names exactly the same as the Excel file, and then create a presentation table with the names corrected as you want them to be. I find it helps with trouble shooting.
You just need to learn t-SQL syntax. Fundamentals are the same but the implementation is different.
Wingdings
F in chat
Is it even possible to create a column named "bulk"? I tired it in SSMS and it did work, but it put brackets around the name. "[bulk]" The same with the integers. I'm not sure what the implications are for that? Do I have to include the brackets when I write an SQL query from the back end? In every language? Does it change other things? Does it limit the column in any way? Make it hard to work with SSIS? Or other software? Also, the brackets just look amateurish, imo. It looks hacky.
Like jamming 5 lbs of shit into a 2lb bag.
There‚Äôs a few things to know about transitioning from T-SQL to PL/SQL. Firstly, some negatives. Scalar functions and cursor loops are super non-performant in SQL Server, use stored procedures and table valued functions (much easier to write than Oracle TVFs) instead of scalar UDFs, and use stuff like intermediate temp tables instead of cursors. Now the positives. T-SQL is both the SQL dialect **and** the procedural language. There isn‚Äôt a separate runtime like there is between Oracle SQL and PL/SQL. This means that you can mix procedural SQL in with regular SQL and return result sets. No Anonymous Blocks needed and (perhaps most importantly) there is no performance penalty from switch between pure SQL and procedural SQL.
Yes, you would have to use the brackets, but you wouldn't query the staging table. You would query the 'presentation' table. What dextaUK is describing is common practice in loading data from outside a system. You stage it to the database in a table that pretty closely follows the original data format and then you transform it to your final table using SQL. Like dextaUK said, it helps with troubleshooting.
T-SQL is both the regular language **and** the procedural language. T-SQL is a little weird at first and I missed using cursors, but it‚Äôs pretty nice once you get used to it. It‚Äôs just different and forces you to rely on Pure SQL more, albeit a super-powered pure SQL dialect with the ability to create variables on the fly. Also, fixes some annoying/impossible things about PL/SQL like nonexistent table variables and Oracle‚Äôs tedious syntax for creating table valued functions, etc... You‚Äôll pick it up quickly if you‚Äôre skilled with SQL or PL/SQL already.
How much do you make? How much is the new job?
So sales is like 60k and I work pretty long hours. This would be a big decrease to like 34k....
Is it a 40% decrease in hours?
There‚Äôs a few things to consider: - Can you afford the pay decrease - Do you have a career goal in mind I.e specific role, or do you just want a job that predominantly uses SQL because you enjoy it - With your foot in the door will there be an avenue from this new role to the role you‚Äôre seeking - If yes to the above, could this occur in a reasonable timeframe, or would you get burnt out or disengaged before it was possible - If you didn‚Äôt take this role is there likely going to be another opportunity Not an answer, but some things that may help make up your mind.
Sales=60 hours and I‚Äôm salaries IT= 40 hours and in hourly
IMO, absolutely not; keep your current role. A, "Database Assistant" is ... not a "real" title. Second off, mainly handling large excel files ? Major red flag. I would focus on landing a Jr. DBA role which *should* start around 52-60K (depending on region).
To me if data is your passion then making the jump to the new position is a good move. Yes there is a pay cut but even if you wait it out and find an entry level SQL position somewhere you will be taking a pay cut. No one is going to pay you 60k for self taught with 0 experience. Sorry if that sounds a little harsh. This new position even as a stepping stone (internally or externally) has you at least working with data in some fashion. I get waiting it out for a position that gets you in the door using technologie you are into is fine. But from a hiring standpoint if I've got two guys wit no real world experience and a passion for data one from sales and one from data management I'll pay them both about the same (due to demonstrable experience with the language and tools) but I'd lean towards the guy that is already dealing with data on a day to day basis
Are you comfortable with the new wage? Without comparing to your current salary, would you be okay with it? Or would it result in a significant change to your lifestyle? I had to take a paycut to break into data analysis because my previous industry (cybersecurity) just generally pays more. But I got into an actual analyst role doing the type of work I wanted. And the salary change hasn‚Äôt affected the my day to day living in any way. Is the new role going to be the type of work you want to do? Do you think it will help your career? If the paycut is too much why not try applying to other entry level DBA jobs at other companies actually doing DBA work instead of messing around with excel sheets? It‚Äôs up to you in the end if the money is important or not. If you‚Äôd be happy with the new salary then the new title wouldn‚Äôt hurt. I think if you take the job you should use it to try get your foot in the door with IT so you can actually learn to write queries and work with the DB. Something else that‚Äôs really important is to make sure you have a supportive manager in your new role because that will be more important than a title in terms of getting what you want from your career. A supportive manager will help you learn the skills you want, even if they aren‚Äôt strictly part of the job.
F
If you have sql backup, you can use maximum compression. Otherwise press F.
34K for a "SQL" job is peanuts. I'd pass, personally, but if you want to suck it up for a year, or two, it would be a good way to get your foot in the door for a job making ~60+. Two more years after that you might be shooting for close to 80... two years after that you should be in the 100 club.
Hey, I'm a Senior Software Engineer and I'll provide my two cents. Hopefully, it helps you in making your career decision. I'm not someone who was ever interested in becoming a Database Administrator, and in my honest opinion, they make good money. Usually, and according to Glassdoor, between 59-90k a year. While the role you're talking about makes pennies compared to that, and eventually, you'll catch up in skill and still be making pennies. Since you already have a decent job in sales, I would recommend taking some time do a couple of courses on databases or attempt to go get a few certifications. Example MTA: Database Fundamentals from Microsoft. I didn't go this route, but many who have some simple funds can get these certifications relatively easy and have them on their resume. Having this type of work experience will easily boost you to around the 60k mark as a Jr. Database Administrator. In a month or two, you could have some basics certifications and find a Jr. Database Administrator job. This would not only solve your salary requirements, but also help you build your resume. Within a year, or two you could ask to have the Jr. title dropped when you feel comfortable with what you are doing and ask for a 5k-10k raise. Obviously, this is an opinion, and salaries and job opportunities vary by region.
Thank you so much for the thoughtful response. I‚Äôve actually already passed my MTA and am looking to get my MCSA 70-764 for Database Administration in the next month! I‚Äôve done some basics such as hosting a virtual cluster on my home computer and starting a project regarding hospitals and financial claims. So I feel like I‚Äôm taking steps to becoming adequate with database administration. I feel like this new position would almost be settling even though I technically have no professional experience or IT degree. My Masters in business doesn‚Äôt carry much weight in the IT world.
Gotchya. I think the main attractive thing for me is that I would have an ‚ÄúIT related title‚Äù which is at least my foot in the door in the IT world. Even if my role is miserable and not great pay, is it at least worth it for an IT title?
You are already way ahead of the curve, and trust me that Masters in business carries a lot of weight in the IT world. Especially because end users think that you'll be able to "speak their language". No need to put yourself down, I'm someone with no degrees and no certifications and still have risen to the pinnacle of the Software Engineering world. A little confidence goes a long way, and it may be hard to get past the Human Resources person who is looking for keywords, but if you know your stuff and you speak to the hiring manager. You're a shoo-in. A masters degree carries weight, period. It shows your commitment to long term learning, and dedication to a subject.
[joins diagram ](https://images.app.goo.gl/cpbyqyimJj8snGbG7) - check this out
You can save a reference table so that you can store the differences. Have a column of old name and a column of new
No. I don't think working with large Excel files will get you where you want to be. Without job experience or schooling, sometimes have a certificate can help get you an interview for a Jr dba, developer or analyst.
Do it in chunks of 100GB each and compress each chunk before doing the next one. If it's all text it --&gt;could&lt;-- compress well enough.
I would be working closely with some of the data guys. My thought process was...get an IT related title and do my job. Then try to network with one of the other guys and assist on projects or do any sort of related things that involve SQL. If I can put even a few SQL related tasks on a resume it would be beneficial. Maybe I‚Äôm just being desperate/hopeful?
Presuming this is MS SQL Server, IF the log is mostly empty, you **might** be able to pull this off (SQL only backs up the actual data in the log or the data file, NOT the empty space.) I'd say give it one go, if it fails there's no harm done, then do what ever you have to do to get a drive big enough to back the log up to. You might be able to back up to a network share **IF** the SQL service account (NOT the account running the backup!) has access to the share (if it's a domain account, give it access. If it's a local account, change it to a domain account. If you don't have a domain...) Once you dig out of this hole, figure out *why* the log got so big, then look into preventing it in the future (Full recovery and you're not doing regular, frequent, log backups? Bad. One-off data load? Not so bad.) (Source: I'm a MS SQL DBA and do this for a living.)
You can‚Äôt back up a portion of the transaction log.
How would you actually describe these columns in business terms? What's their purpose - what data do they contain? Naming columns with leading/only integers is bad practice because it requires the column to be qualified. If you can describe their use in more detail, it'll be easier to suggest names.
That's a 33% decrease in hours for a 40% decrease in pay which could, in 6-8 years, put you in a career making well over 100k, which is a 66% increase. A good DBA at the height of their career imo should be making around 150k, which is 150% increase over your current salary. If you are passionate about data and working to showcase your data related skills then I think you're making the right decision to get into the field, but I think you can probably find something better than 34k unless you live in a very small market (so do some research). In addition to that, I would encourage you to look for "analyst" jobs that use SQL on a daily basis, and many of those jobs will pay well over 34k, and many of them will pay for you to get SQL certifications that will help you get a junior DBA role in a few years. Again, this is all market dependent.
Don't take a 40% pay cut to do a job that isn't what you want to do. Jr. DBA jobs are absolutely hard to find and land, but having certs helps and messing around with your own sandbox is a big plus. Wait for a proper DBA position (or database developer, or data analyst, etc - whatever you're interested in).
Better if you just focus on learning SQL! I've used Strata Scratch, HackerRank, and Datacamp to learn SQL. If you're trying to a job, youtube is great to learn how to prepare for technical interviews. I like this one ([https://youtu.be/n6gM265zG68](https://youtu.be/n6gM265zG68))
sounds like you want this role- based on your replies. But I would heed the advice you're given. I was in a totally non IT field, and transitioned to a data engineer role in the past few years and love it. You definitely don't want to be wasting time with large excel when you could find lots of jobs where you learn SQL on the job. SQL knowledge is key...
Yeah you hit it right in he head, recovery mode set to full not simple. What I'm doing is taking care of someone's mess who didn't understand the benefits of simple. Decided to back up the log to be careful, without realizing the obvious. Have been manually moving files to another drive with space, so far so good.
As others mentioned you can create staging tables and then point views at them to name the columns whatever you want for presentation. Another simple thing to do is just name an integer column something like NUM_1, etc., or for BULK something like BULK_COL. All of this comes down to preference and nothing else, and it heavily depends on what you intend to use the data for, how you intend to present it, etc. In my experience naming a column which used to be named `1` into something like `NUM_1` is an easy way for the business to naturally understand what it is, without having to rename it something more significant.
Feels like you are doing the right things. Maybe take a class. Join data meetups in your area or even IT meetups. The thing about working with Excel is you get stuck in that place. Data people don't take you serious enough. It becomes a report job. Which isn't bad but also isn't what you want. I had an internship years and years ago. Promised to work on the data team during my Masters in Data Science. After I started found out that there wasn't a data team. Did Excel reporting for a year before I finally said enough to the HR intern group and then they found me a DBA to work under in a different group.
I really do appreciate the insight. It‚Äôs pretty intimidating changing jobs and especially into an unknown field. I‚Äôve joined my local SQL chapter, PASS, attended SQL Saturday, working towards the MCSA, running home labs etc etc...so I feel like I‚Äôm taking the right steps. As I‚Äôve read through the thread I‚Äôm starting to realize I can be a bit patient and find a new job that lets me work with sql from the get go and not just hope that I‚Äôm allowed to one day. Seriously, thank you for the advice it‚Äôs very much appreciated.
You don't see a lot of people going from sales to junior dba. I think OP getting into a position closer to his goal is appropriate.
Manager: "so it can be done by the end of the day?"
No worries. You're good.
Just keep in mind typically the DBA position is tightly held, because you are responsible for the company's data which in some cases could be the the money for that company. DBAs come into the job by typically two paths. Developer or Networking. Keep heading down those paths if you want a role.
A two pound bag would probably be quite large.
It might look amatureish, but it's great practice for when you need to use source systems like SAP, old financial systems or even As400. Table names and columns in these systems are often just abbreviations of the use case for the table. An example might be Table XCFDDRE and column NSFYPYDAVE (net sales financial year prior year - Dave created it for a test, and it's now been live for 6 years in production). When you have a data issues with these columns, you can just call up the source owner and say "hey, fix NSFYPYDAVE.." they will know exactly what you mean.. and having the presentation table allows your users to have a nicer user experience... which helps new starters as you don't need to memorise all of these table names.
Interestingly enough our team had a meeting about this today... One of the databases we work with has for years had a sequential numbering. Easy, to work with, easy enough to identify when a record was created, etc. No problem at all. Someone made the decision to instead run with a randomly generated ID. No consultation with any areas of the business. Just decided "Fuck it, we're Agile..." The flow on effect has been diabolical. The biggest issue is historical data has kept the original IDs. So if I ran a query to get total counts of the year to date, I need to take into account the same record may be under the old ID as well as the new one.
In general I too, prefer numbers generated by a database sequence for efficiency. However, UUIDs have one big advantage: you can create them anywhere without having to worry about sequences getting out of sync or needing roundtrips to the database to obtain a new value. We recently had a case where we had to clone a large hierarchical structure (a tree of about 50000 interlinked rows) keeping references between the source and the new rows. Doing this in memory took about 200-500 milliseconds because we could generate all IDs in memory (on the client side) and keep the mapping between the source rows and the new rows in memory as well. For fun I tried to implement that cloning that through SQL only, using several chained common table expressions that would build up the mapping of the old and new IDs and then insert the new rows in the right order. That took something around 20-30 seconds (rather than 500 milliseconds). Doing it in a procedural manner (with cursor loops) in the database would have been even slower. As this was something initiated by the end user who would wait for that to finish, the 20-30 seconds would have been unacceptable.
If you insert the rows (1,1),(1,2),(2,1),(2,2) into the table, that query returns (1,2), (2,2), which surely isn't right as there are no distinct varA values in the table: &amp;#x200B; \&gt;select \* from tbl; VARA| VARB 1| 1 1| 2 2| 1 2| 2 Query 2 4 rows ---- 0:00.0 0:00.0 0:00.0 \&gt;select varA, max(varb) as max\_varb from tbl group by varA having count(\*) &gt; 1; VARA| MAX\_VARB 1| 2 2| 2 Query 3 2 rows ---- 0:00.0 0:00.0 0:00.0 \&gt; &amp;#x200B; Might be easier if the OP gave an example data set and the result they want, just to clarify.
&gt; Might be easier if the OP gave an example data set and the result they want ;o)
And what is the error you get?
Look into the lag and lead functions. These functions allow you to query a record that is either before or after your record.
if I understand you correctly, you want to make sure the next reading always equals to previous reading minus consumption? You can apply windows function LEAD to read the value of "reading" next row. &amp;#x200B; SELECT \* FROM ( SELECT date, reading, consumption, LEAD(reading, 0, 1) OVER(ORDER BY date) as next\_reading, (reading - consumption) AS correct\_next\_reading FROM your\_table ) table WHERE next\_reading &lt;&gt; correct\_next\_reading
UUIDs have many advantages over sequences/auto-increment columns. They also have many disadvantages. If the weighted sum from the advantages column outweighs the weighted sum from the disadvantages column for your application, then use them. Otherwise, don't. That's the same for any choice where there are &gt;1 options to achieve the same thing. No need to get annoyed about it. Also keep in mind that decisions will change over time. What was best suited to UUIDs 10yrs ago may be better off with numbers now, but it's not possible to change it without significant effort, so what do you do? Probably leave it as it is...
There is a lot of advice here, and I honestly did not read it all, but will chime in anyway. You have far to much experience to be taking a back channel, entry level approach into IT. What a lot of IT needs is awesome software systems analysts to help bridge the communication between end users and IT. A good analyst can help a developer find the proper solution a lot faster then directly interviewing the end users. You have experience with sales, the sales tools being used and how the users operate the system. If you can get into an Analyst position to work with IT on future development, reporting and other needs you can find pathway entries into IT that will eventually get you where you want to be without the loss of income.
I do think GUID's have been overused lately. I like them as identifiers for PII data.
Don't rule out leaving it in Full, **IF** that's going to be needed. Full will let you restore the DB to a point-in-time (as long as you're backing up the log,) while Simple will only let you restore to your last backup (Full or Differential.) The log backups will help keep the size down, as when the backup completes SQL marks the backed up portion of the log as reusable.
I feel sometimes you have to get a little lucky when moving to a new space. I think landing an interview for an entry level job is probably the hardest part since HR likes to focus on things like degrees and years of experience and may cut your application before you even get a chance. I think if you truly know SQL, you should have no issues killing an interview, you just have to get there. In my experience, companies always prefer to hire internal so if you think there's a chance for you to move up or to learn from the people on your team in that role, it might be worth it to take a pay cut but that's a decision you would have to make for yourself.
Are you sure you want to be on the DBA track? This sounds counter-intuitive, but DBAs aren't exactly a SQL-based role. It's true you can't be a good DBA without SQL, but a lot of their work is around the platform itself: making sure it's properly resourced, backed up, available, secure, etc. DBAs don't actually query the data in their databases all that much. If it's the love of SQL that's got you looking into the career path, look for something in the data analysis or business intelligence or reporting area, with a view to becoming an SQL developer or similar. It sounds like you might already have the skills to take on a junior role in one of those areas and step into a job doing SQL immediately. The pathway a lot of us take isn't actually via IT at all, it's by using a bit of SQL with Excel in the area of the business we were working in anyway, and becoming the go-to person for data queries.
&gt;there haven't been many learning opportunities What's your day to day like?
Usually data fixes, if a etl fail then looking into why and dealing with basic SQL server problems like if it's running slow. They tend not to let me do to much which has been frustrating.
What part of the country are you at? Are you in a large metro? If not, would you be willing to relocate for the next opportunity?
1. Identify the critical databases in your work environment. 2. Take a look at the tables, table structure and the relationship between tables , size , index etc., (this might give you an idea about what are the key tables) 3. Look at the stores procedures and understand what they are doing. (This would give a good idea on the business logics) Some databases are designed in such a way that only the data sits there. All the logics are coded in application itself in c# code. In that case, you can‚Äôt learn much from database itself.
Everytime you get a task assigned that is not unique, then learn to automate it. This will improve your DBA and SQL skills through concrete use cases and you'll be able to apply these techniques over and over.
If your company has a Sales Operations or Sales Analytics team they are more than likely using SQL. Can you keep your current position but request to work closer on a few projects with these teams? Then you could use it as leverage in securing an Analytics or DBA job down the road.
post the link of the specific tutorial or do not write "in this tutorial..."
I actually just relocated down to South Florida! So relocation is unfortunately not an option at this point. I know this limits me to one market, but it is the situation. I would assume S Florida is a large enough market where I‚Äôll have a few different options come up.
How would you go about automating SQL queries?
SELECT \*, IsMatching = CASE WHEN reading = comsumption THEN '1' ELSE '0' END FROM table &amp;#x200B; pretty much using a new column/flag to identify it, if reading = comsumption then 1, else 0. if you would like to query out the matching ones or none matching ones, you can put this into a inner query and then filter by 1 or 0.
you'd create a stored procedure and then schedule it as a job in the SQL Server Agent
oh cool! didn't really even know that SQL went into that kind of stuff
Use stored procedures and a scheduling tool. You can start by scheduling standard DBA scripts and then move up to more complex ones. I was a DBA and really enjoyed it. I managed to automate almost all of my work. I built automated reconciliation procedures that checked daily loads and only sent emails with exceptions. I then moved onto training myself in Data Architecture.
I figured it out for anyone else that comes across this. I needed to select from the CTE itself in order to make the update. In my head what made sense was declare the variable and set it, then as the CTE does it thing on each row, utilize the set variable. Sometimes I need to remind myself that DB is different than code. Here is the updated solution ;WITH CTE_MDReviewIDs (AuthDetailID, UpdatedMDreviewID) AS ( SELECT AuthDetailID , UpdatedMDReviewID FROM @AuthDetailIds ) UPDATE Schema..AuthDetails SET MDReviewID = cte.UpdatedMDReviewID FROM CTE_MDReviewIDs cte WHERE Schema..AuthDetails.AuthDetailID = cte.AuthDetailID I also was not declaring the columns of the CTE itself. Re-reading the Microsoft documentation helped me understand this better.
Thanks! I can definitely see the advantage on being able to create unique records offline (in the DB sense) and still being sure there will be no clash.
I think the concept you're missing is that SQL is an abstract language and you're asking an implementation question. The SQL standard is implemented in products by various vendors (to various degrees of compliance). These include Microsoft SQL Server, Postgesql, MySQL and so on. Each of these will usually have some atomical transaction guarantees that data will be guaranteed to survive a system crash or if not, then guaranteed not to include partial transactions. You can usually be reasonably certain that when the DB driver in your programming language of choice returns then your transaction has been committed.
Get the book, it's well worth a read, get into stored procedures, triggers, backups, cloning and whatnot https://www.amazon.ca/Pro-Server-Internals-Dmitri-Korotkevitch/dp/1484219635/ref=asc_df_1484219635/?tag=googleshopc0c-20&amp;linkCode=df0&amp;hvadid=293033033224&amp;hvpos=1o3&amp;hvnetw=g&amp;hvrand=11365861527217035644&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1002287&amp;hvtargid=pla-332689611737&amp;psc=1
&gt; I know in normal situations, writing to disk constantly can be a great way to make your disk fail early. With modern hardware, this is not worth worrying about. If you have concerns about I/O, they should be around how long it takes to perform a read/write and how long the RDBMS is waiting on access to the disk subsystem. But that's something your DBA would worry about, not developers. Like /u/ijmacd said, you're asking a question that your application code is several layers of abstraction away from. Reading &amp; writing a SQL database is _not_ the same as reading/writing a plain text file from local disk. Not even close.
The nitty gritty details are different per DBMS and your server configuration, but basically when you commit your transaction the data is safe. (Note if you have autocommit enabled then each statement is essentially its own transaction.) There are some cases where this might not be true such as if you are using Oracle with asynchronous commit turned on. The time at which a connection is closed usually has nothing to do with it.
&gt; There are some cases where this might not be true such as if you are using Oracle with asynchronous commit turned on. SQL Server 2016+ has this as well, labeled Delayed Durability
Wel the whole reason I'm flipping to simple is to avoid having these massive transaction logs. I mean I just cleared up 500GB of transaction logs, I don't know if that's normal size but it can't be good.
Completely depends on the company, mine wouldn‚Äôt give me jack squat.
&gt;my professor told the class that it was an industry standard that if you get a certification, your employer should give you a raise. Yeah, professors don't generally operate in industry. Academia's idealism rarely translates to reality. Depends on the company. The last two places I've worked haven't cared at all whether you're certified or not and made no salary adjustments for getting certified. Heck, one of them wouldn't even pay for your exam under the guise of training/continuing education.
Not really. If they want you to have a certification that bad, they‚Äôll require it before you start working there or pay for you to get it after you start. Tech jobs (and most jobs increasingly so) depend upon ability.
sorry for late reply, but I went with COALESCE, and now I have about 6000 rows where the goal amount = 0 but the actual amount is null. I tried the following in my WHERE clause to try and remove them: &amp;#x200B; AND goal.amount &lt;&gt; CASE WHEN actual.amount IS NULL THEN 0 END &amp;#x200B; Somehow it filtered out rows where my goal amount is null but where I have amounts (beginning balances)
That's exactly what happened. Placed example db in correct directory;everything works fine now. Thanks, guys!
Ah okay, checked up on the implementation I was using and yes, it writes each time I close a connection (which is basically a transaction). Plus I did the math and even if I wrote 500kb/sec of changes consistently, it would take 5 years for a standard, small SSD to cap out its TBW limit, so yeah - no need to care about how often it writes Thanks for your help
It might factor into a potential merit increase, but usually no. The raise usually comes after you put it on your resume and get an offer from the next company :-P
I'm glad you managed to figure it out and yes it's almost certainly not worth us wasting our time as developers worrying about this story of thing.
Not worth worrying about, it depends... if you write 5MB/sec consistently, you can cap out a modern SSD's TBW limit in just about a year, and likely kill it. Of course, that's a TON of data and I'll never reach that, so I'm fine - and anyone who is working with that much data is probably working with actual servers with good drives, instead of writing an application to run on their home PC like me ;) &amp;#x200B; But I was able to find my answer thanks to you guys letting me know to check my specific implementation instead of just googling "when does SQL commit to disk", thanks
A certification doesn‚Äôt make you more valuable to the company by default.
Converting '' to Date or Datetime is probably giving you the 0 value (1/1/1900 for sql server). This means that it's getting counted in the 80+ group in both queries and the first query counts it again in the NO DOB group since the empty string value is explicitly called out while the second query doesn't do it because the 80+ case has already been met.
&gt; if you write 5MB/sec consistently, you can cap out a modern SSD's TBW limit in just about a year, and likely kill it. Of course, that's a TON of data and I'll never reach that, so I'm fine Is that a consumer-grade SSD, or an enterprise SSD in a SAN? Big difference in lifetimes. &gt;and anyone who is working with that much data is probably working with actual servers with good drives, instead of writing an application to run on their home PC like me ;) Yep, there we go. On a SAN or even a standalone box with SSDs in a RAID configuration, you'll be protected against the drive failing and get a warning to replace it before it's a problem. Assuming the admin has configured alerts properly and isn't filtering them to trash in Outlook.
&gt; it's almost certainly not worth us wasting our time as developers worrying about this sort of thing. I'm a DBA and _I_ don't think about drive lifetimes. That's the storage team's problem.
It's not normal to be that big, it should only fill up that size if a large transaction runs (eg: inserting 500gb of data in a single transaction). Sounds like the DBs are in full without a regularly scheduled transaction log backup (a full backup does NOT clear the transaction logs). If you don't need point in time restore use simple recovery... If you need to be able to restore to a specific time or cannot deal with losing an entire days data (assuming you run full nightly backups) then stay in full recovery and setup a job to backup the transaction logs every X hours (or minutes, depending on how much data you're willing to lose)
Do you have 1 on 1s with your supervisor? That might be a good time to bring up that you think you've got a good grasp on your current tasks and think you are ready for an additional challenge. Self learning SQL is pretty easy to do, but in my opinion learning everything it takes to be a successful DBA is not as straightforward. Since you're already working as a Junior DBA, that gives you the perfect opportunity to start learning with practical experience. If you have a half-decent manager, he'd at least give you some new challenges for low-priority development and see how you did.
The company has had a lot of turnover since I started so it's been difficult for me to even get a 1 on 1 but I have mentioned it. Even in my job a lot of it has been self taught where I study ELT, other stored procedures and a lot of googling to better understand things. Unfortunately things tend to be ignored a lot at this company which is why I'm looking to other places to learn and if nothing really changes where I'm working at the moment I'd be looking to move elsewhere after a year for better learning opportunities.
It depends on the professor, honestly. My wife is a professor of technical and professional writing, and is deeply embedded in industry -- because her pedagogy would be useless otherwise. That being said, I do agree that OP's professor has some outmoded information. Common wisdom seems to be that focusing on what your individual contributions bring to the company is the strongest leverage for asking about compensation adjustments. So, just getting the certs isn't enough -- using that to contribute more meaningfully in your role, or taking on new and more complex projects will ultimately be a more sound way to legitimize a pay increase. Or, jump ship and use the certs as resume candy to dazzle the empty suits in HR that just care about matching keywords for applicants.
In SQL Server you can query: Select name from sys.columns sys1 where object_id = object_id(‚Äò&lt;table name&gt;‚Äô) To return all column names for a table. Inner join this to a similar query for table2, and you should be all set.
It depends on how the rdbms manages write to disk operations because they are costly. In robust and cutting-edge platforms, it's complicated and was developed over decades. Take Oracle Enterprise, for example. One might think that if you performed a complex dml and then issued a tcl statement such as a commit, that those changes would be set in stone and written to disk. Guaranteed, right? But, nope. Oracle does everything it can in memory and strategically writes to disk when it can, without affecting what's going on in the db. Oracle has several background processes such as "log writer" to accomplish disk writing, and it's anything but black and white. The concept of online redo logs, log switching, and archive logs come into play here. What's in memory vs what's logged to be written to disk vs what's in an archive log vs what is in your actual database files. It's very complex and it isn't as simple as assuming your application, which makes dml changes to table(s), writes to disk constantly. And again, just because it isn't on disk doesn't mean it's lost as soon as the database goes down in an ungraceful manner. With a robust RDBMS, you trade speed for risk, and come up with creative solutions to mitigate risk without losing data. Oracle is complex as fuck and an enigma to most, but enterprise is very good at not losing data.
This, I really like this answer. Makes sense, tells me what I'm worrying about doesn't matter, and also tells me I need to check my specific implementation for the answer to how they handle it
I'd add that it doesn't have to be scheduled either. I have a bunch of scripts with dynamic SQL and cursors (yeah, I know, go ahead and shudder) that I use to make applying permissions for service accounts or create linked servers faster because I find them really boring to do and the GUI is always slow. Learn the system tables / DMVs and a bit about what specialties there are. As a junior DBA, your primary job is data guardian for RTO and RPO, but I know it gets boring, so try taking the lead when something comes in you know you like or if you see something in the system's design you know needs to be addressed. Once you've automated things enough that the days are slower, use downtime to research areas you don't have a ton of experience in and read white papers. I've been a mid level DBA for a while and have been trying really hard to push for the Senior title and figuring out what I lack to get there. If you want more specifics or some links or just want to discuss a little more, feel free to PM me.
I'll take that on board I appreciate the advice and thank you and I'll be sure to take you up on your offer üòÖ
I'm pretty green myself so someone else correct me if I'm wrong but I do remember learning about how temp tables can utilize indices which will improve read performance whereas CTEs cannot. I would imagine they mean doing something like this: &gt;IF OBJECT_ID('tempdb..#dash_lead') IS NOT NULL &gt;DROP TABLE #dash_lead &gt;CREATE TABLE #dash_lead &gt;(user id int, &gt;session_id int, &gt;step smallint, &gt;time datetime) &gt;CREATE NONCLUSTERED INDEX [ix_dash_lead] ON &gt;tempdb..#dash_lead &gt;( &gt; [user_id], &gt; [session_id] &gt;) &gt;INCLUDE ([time]) &gt;insert #dash_lead &gt;SELECT distinct &gt; user_id, &gt; session_id, &gt; 'lead' as stage, &gt; 1 as step, &gt; min(time) as time &gt; from &gt; heap_prod.heap.refinance_click_on_dashboard_refinance_button &gt; group by &gt; user_id, &gt; session_id, &gt; step
How did that ever pass code review and architecture governance?
This hit home. Thank you
I used to work for a guy who thought LESS of people with certifications, believe it or not. He found, in his experience, people with certs tend to be more of the booksmart type who are good in school and less of the get in the trenches and get work done type. Obviously not everyone is like my former manager but that's literally the only commentary I've ever heard on the value of certs from a hiring manager.
Alternatively, if you want to head more into the DevOps way of doing some SQL administrative tasks you could learn to use Powershell to automate some tasks. Take a look at this blog for some examples of things you can do with the SQLSERVER powershell module: [https://port1433.com/2018/02/02/storing-and-applying-sql-server-database-permissions-with-powershell/](https://port1433.com/2018/02/02/storing-and-applying-sql-server-database-permissions-with-powershell/)
I'm not your former manager, at least I think I'm not, and agree with him/her. I'd rather see experience. Some people can do well on tests by temporary cramming information into their brain, but rarely does any of that help with real world experience. I also would not pay or give a raise based on someone getting certifications on my team. Now if it was someone with zero experience trying to understand more, I'd take that into consideration on effort level as part of the job.
This sounds like a textbook case of a company that is going nowhere and is frustrating to work for. I've found that smaller companies tend to let you dabble in places you'd otherwise be locked out of. I'd definitely start networking around on linkedin and find some recruiters.
If data architecture is interesting to you, and you want to see some crazy design patterns... I've been working on a Master Data Managment in SQL tutorial series (ms sql). New video every couple days (every day was too stressful). New video releases in a couple hours. This is a hands on tutorial and if you can follow it and think in these patterns, you'll surpass most of the people in the architecture industry. I have about 30-40 more videos in this series (20 are done/scheduled to release) until I start live Q and A and loading adventure works / wide world importers / fec data etc.. SQL Master Data Management - Complete Tutorial: https://www.youtube.com/playlist?list=PLPI9hmrj2Vd_ntg2HACiHYeYl7iRvrgPb
Pretty much everything /u/ihaxr said. Ideally, find out from the powers-that-be whether the business requires point-in-time recovery or not, then use that as the basis to choose simple or full. If you do need to use full recovery, then get a backup schedule in place to backup the transaction log. NOTE: This is NOT backing up the file on disk, this is setting up your backup solution to perform proper, native SQL transaction log backups. As a further note, these backups **DO NOT** require you to do anything crazy like stop the service, or kick everyone out of the application, or sacrifice a live chicken every time you back them up. The more frequently you backup the TLogs, the smaller your transaction log will stay (within the requirements of what the application requires.)
The answer (and I mean this in the most diplomatic way possible) is that it's none of your business. Think about your question, why do you care if data is written to disk? You only care if the data is available after you commit your transaction. Databases are defined by ACID properties, Atomicity Consistency Isolation and Durability. That last property has nothing to do with whether the data is on disk or in volatile memory. This is a *feature*. Database systems were created to decouple the retrieval of data from dealing with the nuts and bolts of writing to disk. It's sort of like asking "How do I know if this web page is from a server in Montana?" Why would it matter where the machine is? You only want the data. You might answer, but I want to know that the data is getting to me as quickly as possible, but that's a different question. For all you know the data is coming from a satellite. If you need to write the data to a file in a specific spot, then you'll need to access procedures specific to creating files. I mean you could start looking at transaction logs, but they have a very specific purpose, and could conceivably be encrypted anyhow.
I appreciate it and will definitely take a look
In my experience, certs may make it easier to get an interview, especially in larger companies, but once you have a job you probably won't get paid more for getting a cert. On the bright side, many companies will pay for the training and the cert itself, also you can use the cert as evidence of professional growth when negotiating a raise and or promotion.
Here is a small snippet of supporting evidence for what I'm talking about. https://i.imgur.com/AFdbOgN.jpg No idea of that applies to your RDBMs, but for an application to database architecture, my gut instinct says it itdoesn't matter what you do in the app, the DB will go it's own way and use it's own methods to most efficiently process what you throw at it.
The most common paths to become a DBA are system administrators, application administrators, and developers who transition into the role either by becoming accidental DBAs or they discover they like the challenges of the job from working with databases during their career. The advice to look for a Jr. DBA position is fine, but highly dependent on the local market. I can tell you that between about 2009-2017 when I was looking in and around Broward I can't recall seeing more than 2 or 3 Jr. DBA positions. I just checked indeed and found 0 listings, it isn't very common in your market. The best advice I saw was for you was to look at an analyst position to leverage your existing people/sales skills. Business analysts are the translators between IT and business units; they have a foot in both worlds which seems like it would be a better place for you to start. I'd also echo the advice to join the [local SQL user group] (https://sfssug.pass.org/default.aspx) or a developers group (there used to be one that met at NSU in Davie). You can network, learn, and find out about jobs from other members (as well as companies to avoid).
&gt; And again, just because it isn't on disk doesn't mean it's lost as soon as the database goes down in an ungraceful manner. Actually yes. If it's not on disk it's in ram and ram gets wiped if the power goes. So it is indeed lost. &amp;#x200B; But given the context of redo logs etc the transactions are actually on disk.
It should write every transaction commit. Not just when you close the connection. You can have many transactions during the lifetime of a connection. All of them should be written do disk when you issue a commit. If it's not written to disk on commit it's not ACID compliant. &amp;#x200B; That said, transaction commits are not the only time the dbms needs to write to disk. DDL statements need to write to disk too and probably a billion other things. The rule is that a good RDBMS should write as often as it needs but not more often.
Yeah, a cert might get you a job. And you might be able to get a better job after you get your current job to pay for you to get a certification. But it's really unlikely you'd get a raise. As far as any employer is concerned, it's just a bit more education.
When doing an earnings calculation, I think you should think of certifications as a piece of your resume, like education and experience, not a substitute for the others. I've worked for companies where experience and proven track record was everything. There were lots of people at one company with many years tenure and talent. Many of these people had little or no college education and no certifications, but they did well at the company and had the respect of their colleagues. Then the company went bankrupt. Now, many of the jobs they had done for years they can't get an interview for because they have no degree or certifications. A certification might not give you a pay bump now, but will this be the last job of your career?
I think you‚Äôre absolutely right and seriously I appreciate the advice! The more I contemplate my past experience and where I want my career to go, the more I realize my personality better aligns with an analyst position. I really do enjoy the face to face interaction that sales brings but I‚Äôm also extremely passionate about the data side of things as well. SQL has been extremely rewarding and a lot of fun to work with, so finding an analyst role that utilizes SQL seems like a perfect blend of my personality/passion. I‚Äôve actually already joined SFSSUG and recently attended SQL Saturday in Davie. Definitely trying to do my best and network as I‚Äôm both new to the area and the field! Thank you again for your time and advice, it means a lot!
Good you found out. Always remeber SQL is *set* based it loves to do its work in one shot. You dont have to declare the colums in a cte like that. You can use alias in select and it feels more natural.
Your indexes should be: Create non clustered index on table_1(where columns like plan) include (select columns. All columns if *) If one query is where planid and another is where plan.. that's 2 indexes. Your physical layer (database file) should be.. One file for the clustered index, one file for indexes. Your physical layer (for the tables cluster index) You should have one column that is the same number for every row, and never the same number in any other table. Let's call it a SpecificDataSetNumber. Table_1 SpecificDataSetNumber should be 777 for every row. Table_2 could be 7242 for every row, table_3 888, and table_4 whatever you want that's not 777,7242,888... get it? This helps keep the table together on the disk itself. If you cluster an identity.. your file will look like I'd. Table 1. Table1 1. Table 2 1. Table 3 2. Table 1 2. Table 2... etc. Dont use distinct in a where IN() its pointless. And now your return should be under a second.
It's kind of rough to give quality advice. $60k to $34k is a big cut. I took a similar paycut in 2015 from $62k to $42k. I left my position, because the company I was working at had a lot of pressure to relocate for projects without providing compensation for the move or adjusting for cost of living. I got tired of it and left. The position I left for had a non traditional title, but it was a full time SQL reporting position. I tried to look for a better paying job, but I was stuck in that position for another year and a half. I found another job in the same space with a more traditional title of "Programmer Analyst" and a very small salary raise to $46k. I was working on an Operational Data Store and able to actually work with DDL operations. I could create my own tables and views, but the job market wasn't really kind. I found that a lot of recruiters were very skeptical of my experience and very rude at times. I spent a lot of time at this position learning the Oracle database and data modeling. It was very demotivating. I was only able to move on from this position with an internal transfer to Programmer. The bump to $49k. This year I was able to find a position in the $80k range. Its been over 4 years to get back to where I was at when I left my good ole cushy Business Intelligence job with only 1 year and a half of experience. There's a possibility for you to luck out sooner on the next position after "Database Assistant", but keep in mind that can definitely be more than 2 years out. If you're comfortable with that, it might be worth it. Keep in mind that you're likely going to continue to put in a lot of extra work outside of work to push your knowledge and skills further than someone who comes out fresh out of college and is given leeway or someone with the already refined resume.
Yeah I think he meant that it's not necessarily written to the *data files*. But as far as I know (haven't been an Oracle DBA in about 13 years) every transaction is absolutely written to disk in the form of redo logs (online and/or archived) unless you have asynchronous commit enabled.
If we don't see your current execution plan, we can't tell what's wrong.
Yes, that's what I gathered he meant too. I just don't want it to sound like things can magically survive a power loss without being written to disk.
Create indexes on your #tables.
Thanks for the detailed reply. I don't have permissions to alter the subject databases. I think there is a missing index for plan_id in my table_1 example.
The first step is identifying what RDBMS you're using and then identify the native tools supported for that RDBMS. For example microsoft and SSMS. Oracle and SQL developer, etc. Once you know that and can establish a connection, find out what kind of users/schemas there are, and find out the name of the data dictionary tables. That will tell you a lot about what tables are seeded, what tables are custom, and then you can start looking at the data dictionary view that shows the sizes of the tables so you can see which tables have the most data. From there you can go exploring into tables that matter, taking note of which values are unique, and how they might relate to other tables.
Literally no one does, hardware is so cheap just spin up another box, ship it to the cloud, ect. If anyone cares it procurement or your AWS liaison
Thank you so much!!! Do you have a cashapp tag? I can at least send money to treat you to coffee or something? I really appreciate the help.
They add to your value. You might win over someone else for any number of reasons.
Certs mean the most to contracting firms. Certs are peddled as proof of expertise within that space. Other companies give varying degrees of importance on certs, typically they are much more tuned in on what you are doing FOR them than they are into what you are learning or tests you are passing.
Escalate! Send a request to someone who can.
It‚Äôs built into the suite.
Your compensation is 100% related to your interpersonal/negotiating skills. Certs are like a sub-degree differentiation because any asshole can say they are a dba/analyst/programmer, so people want gold stars on the res to look like they ~~deserve more money~~ know what they are talking about. "Life is what you can get away with."
Certifications might equal pay raises. It shows you are trying. That‚Äôs important to a lot of people.
I use it pretty much everyday. I like that I can connect to SQL Server and Oracle with the same client. The smart query boxing is also cool - no need to select individual queries to have them executed selectively. And it saves previous query results which is very useful. There are some drawbacks. Since it uses JDBC connections, you are limited to actual SQL. The extra non-ansi enhancements won't always work. And it doesn't handle xmltype very well. Canceling long-running queries sometimes crashes the client, and most of the time just disconnects your session. These are fairly minor points, though. For the most part, it works really well.
I'd recommend looking at DBeaver. We used to use SQuirrel SQL Client till the team found out about DBeaver. I personally use JetBrain's DataGrip because I have a subscription.
Current company usually wont' give a shit. Just think "oh, you have more value.. get back to work, slave". But, certs will help when you go look for a new job.. b/c often when you deep-dive into the DBA role your certs will help add more value to you, get you into interviews, and justify asking for more money. I had this happen at one job. I got some certs; they didn't care. But, teh certs got me another job quickly that I got a fat "pay raise" at. You have to think of this from your current company's position as a business. They're trying to pay the least amount of money they can for someone. If you were able to get that job w/o the certs, then get certs, then chances are good they'll just watch you walk off to a new job while they just hire a new person w/o certs. And, if you've been there for a while getting pay raises, it just means they can reset the clock and pay a new-hire squat once again. It sounds cold-hearted, but it's two different views from each side of the management desk.
16 seconds for 18 rows doing a simple select on an indexed column in one table seems pretty slow, to be honest. Do you really need select * to begin with? The table sounds super wide. And as already noted, the index is probably missing a lot of columns--resulting in slow key lookups. Someone also mentioned indexing the temp table. Which would help. The overall rule is, if the query seems extremely slow when it shouldn't, something is likely off with estimates vs. actual rows. So possibly stale statistics, or a number of other issues could be contributing. This is all speculation without seeing the actual structure &amp; plan. Can you www.pastetheplan.com ?
It all depends on what you want to do. The first query will convert the left outer join to an inner join because the filter is in the where clause. This is because the where clause is applied after the virtual table created by the join has been set up in SQL Server. The second query implements what‚Äôs known as a predicate. You‚Äôll get all farms and only those matching record from the produce table that meet the filter condition. This allows you to see which farms didn‚Äôt produce any produce during the period. I would google predicates in SQL Server and read up on how queries are logically processed. And don‚Äôt feel bad either...this tripped me up all the time when I was younger (I‚Äôve been coding SQL for about ten years solid now).
To add to this, if you're dealing with a 3rd party software, sometimes they will be *very* reluctant to provide schema information. Some companies consider this trade secret information. They will also use encrypted stored procedures sometimes. There are some programs (SSMS in version 18 even) which will automatically document and/or diagram the database for you as well. That can certainly help get a handle on the relationships. Eventually you'll have to get in there and start "kicking the tires". Just not too hard, until you know if they're going to fall off.
It depends. It depends. It depends. However, using a WHERE statement with a LEFT nullifies the LEFT and creates an INNER.
A few things to address here: 1) I think OUTER LEFT JOIN is wrong, at least I've always seen it as LEFT OUTER JOIN or LEFT JOIN, but maybe this is just a typo you made while writing this post. If it is valid then I stand corrected, but in any event this is not too relevant to your questions it would seem... 2) I agree with you that the extra condition in the left join is a bit weird, however if the test was to prove that you're a MASTER of sql (I don't know if that's the case) it seems like a reasonable thing to test. I've looked into this kind of thing and experimented with it pretty much just to satisfy my curiosity, but you're right in the real world it's weird to do this. It would be a weird thing to write and if I felt I had to do it for whatever reason I would leave a comment on that line so as to avoid tripping up future developers who read my code. 3) Numbers in the group by and order by - eh, I don't usually do this but it can be a useful style. Say you're selecting a large expression and you want to group by it. In some databases you only have the choice of either A) repeating the whole expression found in the select verbatim or B) using an ordinal number for group by/order by. In other words say you have "SELECT CASE WHEN ... END AS my\_case" and that "..." is about five lines long - some databases would force you to either repeat the entire five-line CASE expression, or use a number, and will NOT let you simply use the alias, i.e. "GROUP BY my\_case". So in this kind of situation I would rather use "GROUP BY 1". In the given example I'd probably just use "GROUP BY farm, farmer", but you shouldn't be thrown off too much by "GROUP BY 1, 2". It's valid. I've also seen authors of SQL generators prefer numerals as it can be easier to code sometimes. 4) I've been on the interviewer side of "but that's not how I do it and it's a bad practice!" In my case, I was testing SQL and the interviewee did not like our naming convention of MY\_TABLE.ID for primary key columns - he much preferred MY\_TABLE.MY\_TABLE\_ID. He started ranting about how it's a bad practice but couldn't really provide reasons - that's just what he preferred I guess - it's a little off-putting and it's best to roll with what the interviewer gives you. If there are real solid reasons for something to be considered a bad practice it's fair to point that out, definitely needs to be something you can back up with facts rather than just criticizing style.
Yea the group by is weird, typically I see this in Union statements where you must group by 1, 2 ... Weird to make it harder to read easily.
Not sure what you're doing, can you provide the code you're using and an example of data?
It doesn‚Äôt convert anything from a left outer to an inner join. What it does is applies conditional logic at a different level, which materially impacts your result set. It‚Äôs the different between filtering an end result vs creating a filtered view that‚Äôs being joined to.
LIKE search...won't work with commas in-text. Imagine searching a book... "Blah blah" is different than "blah, blah" or "blah: blah"
To understand this you need to wrap your mind around what the sql engine is doing. In the first example, your creating a result set by joining two tables then filtering that result set. In the second, your filtering a table, then doing a join with that table. It‚Äôs the difference between mixing things then filtering vs filtering then mixing, where the goal is to keep all of one side of a record.
So you can use multiple AND conditions... where field like '%blah%' and field like '%blah2%'. Do you know about wild cards?
I may be grossly misunderstanding your question, but, a %wildcard% only takes into account what's between the %'s irregardless. Do you have an example(s)?
You can double down on this and ... field like '%blah%blah%'
Good point.
Yup. You have to when which part of the statement is going to be executed.
Getting a raise is about telling a story, certs can help you tell a story Imagine I work at XYZ enterprise and I maintain COBOL mainframe jobs. My Boss would love for these jobs to be rewritten by our .Net developers but the have higher priority projects to develop on. I get a my MTA in .NET development and pitch my boss to let me begin rewriting the jobs, and simply incorporate other devs on peer review and unit testing while I continue to maintain the COBOL jobs in the meantime. Boss agrees and I begin work. During my next performance review I note the MTA cert I have and my success with the rewrite process and how COBOL maintenance has decreased over time. My boss is impressed and I get a raise. It‚Äôs hard to do new stuff without getting approval in a lot of corporate settings. This means you have to pitch for things and know how to market your skills. A cert can be an avenue to do that.
Visio is what I used and school and my organization uses along with some open source stuff like draw.io.
Yes you essentially wrote it backwards. A temp table might also be a bit more efficient with a join in the situation than a cte.
DbSchema is pretty solid for creating ERDs. It can reverse engineer a schema, generate SQL, and even has automation capabilities via Groovy scripts. It‚Äôs not free, but it doesn‚Äôt cost much for the value.
Oh dang, you're behind the Master Data Management videos. That's really cool! I am interested in enterprise architect which led me to better understand database modeling and to your tutorial. Thank you for posting!
no probs my ninja. i recommend you follow it / build it at home... because im going to store r/datasets in here. as much as i can. live stream q and a soon... not sure when ill start, but i hope to do a sql saturday type item.
Looks pretty solid!
I am using a temp table initially. How would I get my same solution utlizing a temp table?
Totally agree with GROUP BY where you have crazy CASE statemetns and such. Most of my career has been SQL Server, and I always found it odd that I can make aliases in the SELECT statement, and I can use the alias in the ORDER BY statement, but if I want to use GROUP BY, I have to put the CASE or whatever other funky column switcheroo stuff down there, too. Usually when I have oddities like that I'll just make a pre-query / sub-query, and then have that do all the case stuff without any GROUP BY, then I can join to the sub-query and use GROUP BY on the aliases the sub-query provides. But, it depends on what the end goal is. It may not performance out well in query optimizer, and may performance out better being all written out as a single query with hair-ball GROUP BY.
This to me looks like someone who's been coding since the ANSI-86 standard, which had joins in the WHERE. I've run into this in a few places, especially where there's legacy software involved- the first time I saw it, my reaction was the same as yours. "It's wrong, no one does it this way", etc. However, it's still valid SQL. It took me some time to learn what it was doing under the hood, and why it might be better or worse, but that learning experience made me a better SQL programmer. Overall I think you did 1 thing correct and 1 thing wrong. The correct thing was admitting that you didn't see code like this much and weren't familiar. The wrong thing was questioning who codes like this, which to me implies a judgement that it's not as good. If you had asked questions about the style and demonstrated an interest and willingness to learn, I think you might have been hired.
Yeah, now that folks are mentioning that... it makes perfect sense when looking at the query... JOIN blah ON blah AND blah the join has the conditional and is limiting that table.. but it's still an outer join. The WHERE clause is going to only limit how many things come back with the conditional met and hooking both tables.. which leaves stuff out of the other table. I'm coming back into the working world after years in college for IS, so my mind has been pulled twenty different directions with analytics, python data science, java programming, business classes, etc... and now to get dropped into SQL again... it's one of my main skills, but I'm a little rusty. But, I just figured there were some best practices being broken just to seem "cute". I really hate coding tests where they're trying to act purposfully tricky for the sake of being tricky. EG: It pissed me off in college how Java professor said "our coding convention is CamelCase vars with real names that explain what they're doing." Then on the first test he has code like "x = 2 / y + 3, y = z\^5 \* 10, z = 3" ... then some lines of code that do more calculatins on it all and "what does x, y &amp; z end up being. It's like "DUDE! YOU'RE BREAKING THE CODING CONVENTIONS YOU TAUGHT US JUST TO BE TRICKY!" I've spent all of my career trying to make peoples' lives easier, so it irks me when I find someone trying to make shit harder then it needs to be.
`SELECT ISC.COLUMN_NAME AS COLUMN1, ISC2.COLUMN_NAME AS COLUMN2 FROM INFORMATION_SCHEMA.COLUMNS AS ISC FULL OUTER JOIN INFORMATION_SCHEMA.COLUMNS AS ISC2 ON ISC.COLUMN_NAME = ISC2.COLUMN_NAME AND ISC.TABLE_NAME = ‚ÄòTABLE1_NAME‚Äô AND ISC2.TABLE_NAME = ‚ÄòTABLE2_NAME‚Äô ORDER BY COALESCE(ISC.COLUMN_NAME, ISC2.COLUMN_NAME) ` This will give you a NULL in the first field if the column is only in the second table, a NULL in the second column if it‚Äôs only in the first table, and both values if the column matches in both tables. Change to an INNER JOIN if you really only want the matches.
I agree with you and hate it when I see code with numbers (1, 2, 3) in the group/order; irks me, esp if I want to add fields in the SELECT I then have to adjust the numbers in the group/order too - dumb. Also - missing the preface of the alias in front of the fields...I hate that; it's not always clear which fields are part of which table - prefacing with alias. makes it super clear. And...I agree, I'm a fan of using "as" - it makes it easier to read imo b/c otherwise it's easy to miss the alias, esp if just single letters. &amp;#x200B; I'm in a new role and no longer do any sql coding (sad) but I'm looking into getting into Python...just gotta make sure I know the best practices there. :)
If they presented you with those two statements and specifically asked which one would include farmers with no produce, it should be clear which of them achieved that - getting that wrong is on you. The way it is written is perfectly acceptable, and the option to go with a subquery is not universally more performant for that - I'd actually guess the opposite for a simple two-table query like that. I'm on the interviewers side - it was a pretty straightforward question and regardless of what is "best practice", there was enough information there to be clear about which statement would behave the way the question wanted for. That question could have been a springboard for you to talk about the pros and cons of your chosen approach, but it definitely wasn't a "cute" trick question or a tricky non-standard way of approaching the problem. You're right about ordinals in the ORDER BY. I don't know why, but it always seems to be Oracle devs who do this.
It's not a style thing: the predicate being in the WHERE vs the ON clause of an outer join changes what the query does.
Well said. I think it‚Äôs just counterintuitive to see a conditional in a left join. I personally would explicitly filter the right table in a sub query and then join, but at the end of the day it doesn‚Äôt matter. I actually think this is a decent interview question, although I do agree with OP that grouping and ordering by numbers is not a best practice.
I always use table alias and column name in group by. Order by I will occasionally use the column index, it‚Äôs more of a quick and dirty approach tho if I have only a few columns whose names I‚Äôm not super familiar with or care to write it. More of an ad box thing for me.
That's not the case at all... Whenever you do an outer join you always have to think about what you want to do with null values. In OPs example, the where clause can be fixed to handle nulls and meet the requirements they gave, but it doesn't magically makes it an inner join all of a sudden...
Seriously don‚Äôt worry about it.üëç The appreciation is payment enough! If only my boss understood that lol.
Based on the grammar in your post, maybe IT isn't the field you should be in.
We do the latter. Filter on the join rather than join then filter.
Did you tell them they should name that fucking aggregation something...
I am not a very experienced SQL writer, but did you consider using a CTE instead of a temp table?
you should have been like.. NEITHER! Ordinals in a Group by / order by is for chumps.
He meant the functional equivalent of an inner join. The where clause will remove all rows where there was no match in p - when the only reason to do a LEFT join is to retain rows where there was no match in p. True or False: Replacing LEFT with INNER in query 1 will return the same result.
I used it for a while and switched to DBeaver and DB Forge Studio
I solved this by creating a postStart.sh script that modified the file before it was ran but after the container started up
If you want a complete free full courses from Udemy (it‚Äôs like codecademy) let me know
You should spend some time looking at the SQL order of operations. The engine will execute a statement in this order: FROM WHERE GROUP BY HAVING SELECT ORDER LIMIT You can see from here that you can order by an alias because that alias exists before the ordering happens. The alias doesn't exist at the point your data is grouped.
Order by 1 desc FTW! üòÜ
Also edx, specifically t-SQL but there may be others.
I see plenty of bad practices. But the join in question is a difference between LEFT JOIN and JOIN. If you have LEFT join and condition in the WHERE clause from the LEFT JOIN, it's effectively inner join. Other problems: 1) GROUP BY and ORDER BY as numbers (as you have mentioned) 2) Not using closed-open interval when working with a date range. 3) Using a country specific date format instead of 'YYYYMMDD' Others are preferences like an AS keyword in select aliases, typing out OUTER or not writing multiple selected columns in a list format.
Not OP but I'd definitely be interested
It depends. Truncate is generally faster but it is less safe (deletion triggers do not occur, rolling back may not be possible). Deletion is slower but safer (triggers occur, rollback is possible, deletion criteria can be specified).
If you're doing something manually and have a backup strategy in place, by all means, truncate. One thing the article doesn't touch on that is important is that truncate is a DDL function and not DML, so it requires greater permissions to run it. (db_ddladmin for default roles) If you're trying to run it in code, that's another risk to consider in addition to the lack of logging.
It's bullshit. Of course you can rollback truncation if it's in a transaction. It also skimms over important details like the other users said. Trigger, permissions. The fact that you can't truncate table with foreign keys. Or that you can delete in batches that won't cause lock escalation.
They named it produce_count...
It's no bullshit, but it may depend your engine I suppose. In MS SQL server, a truncate only record the cleaning of the pages in the transaction log, not the data truncated. So you cannot roll it back. You also cannot run it inside an explicit transaction. If you mess up, you will need to restore from a backup.
I've done this for a long time and I didn't even know you \*could\* group by with column numbers. No sure why someone would want to
I see everyone saying that the different joins produce a different result. This site explains why: https://blog.sqlauthority.com/2009/03/15/sql-server-interesting-observation-of-on-clause-on-left-join-how-on-clause-effects-resultset-in-left-join/
Another issue I've seen is not using time-stamps with dates. I pointed this out in the interview. 1/1/01 to 1/1/02 = 1/1/01 00:00:00 to 1/1/02 00:00:00 .. which any activity on 1/1/02 is effectively ignored. I got bit in the ass on this early in my SQL career when pulling "completes" for a project by month. Figured out if you don't give specific time-stamps, then they default to midnight..but midnight in computer terms is "start of day". Have seen this bite IT folks (that don't know IS well) in the ass. At my last job, the IT guys would put in the date of someone's termination to have their login shut off. But, it would default to start-of-day. So, person shows up on their last day (without knowing it was their last day), and can't figure out why they can't login. So, it created VERY awkward conversations with their manager early on in the day when they're told it's they're last day mid-morning. From reading everyone's comments, I've learned that I do need to tone down my "best practices" preachiness some. And, when handed an example at an interview, just look at it as an example not as a sample of work they do. So, I think I'll be less of a jack-ass at my next interview. However, now that I have a masters degree, I'm at a cross-roads... I have to figure out where I want to go... Deep Dive Analytics Deep Dive Data Science Deep Dive DBA Try to go into Management I've always been a jack-of-all-trades / master-of-none, and my IS education has focused a lot on Management. But, it's hard to get into management when a) positions are limited, b) nobody wants to hire a manager w/o experience (and you usually get experience from being promoted to manager at a current job). I really love DBA work, but I also like Analytics and being able to have an impact on things. I'm just facing a hard cross-roads now. Being older, having lots of work experience, and coming out of college with a Masters degree perhaps made me a bit more cock-sure then I am. So, I've got to tone that down.
They dodged a bullet, you sound like a terrible co-worker.
Then use the appropriate expression to determine the difference in days
That's not what was written or how it came across. True or false: adding "OR p.date is null" to the where clause in query 1 will produce the same results as query 2?
I see our confusion now, I interpreted the OP with an implied statement of "for this particular query" and you understood it as "for all LEFT JOIN queries." You are of course right that adding a WHERE clause doesn't always turn it into an INNER join equivalent.
I was a bad fit for that job. I left knowing I didn't have it, and knowing that even if they contacted me I didn't want it. Doesn't mean I'm a terrible co-worker in general.
This. I asked them if this was their coding convention. I wanted to make it clear that I understand some places have a coding convention.. so, I prompted them about that in "please explain to me why you do it like this and not that." They never explained. So, I just assumed it was someone off doing their own thing. When they told me nobody had gotten that question right, but they had been interviewing nothing but folks that had SQL experience... maybe they just got the luck of the draw that preferred putting conditionals in where instead of join. But, I was trying to be polite about it. I was trying to have a discussion to figure out why they did that, but when I asked "does someone code like that here?" and the one hiring manager said "yeah, I do" I immediately realized they were taking it as me challenging / threatening their coding style. So, def not getting the job after that.
I came from VBA to VB to HLSL / Java in college as my "intro to programming".. and then took Python in order to go into Data Science. The Python prof was also the Data Sci prof, so he taught it as an "intro to data sci" class instead of Python. He tossed everyone into the deep end. I was spending my off-hours writing up sample code to send to the class to help demonstrate things he taught in easier fashion (b/c for a lot of them.. it was their intro programming for bach IS degree.. but others, like me, were in it for masters degree and just needed Python to get into Data Sci.. so the beginner bach students were REALLY struggling). The "best practices" I saw neglected in Python was just basic stuff that most folks have already learned with any other programming language. I'd help some noob with their code.. and it was just a hot mess. They wouldn't name their vars well. They wouldn't make functions out of code that was repeated over and over.. Some of the more advanced mistakes they were making was mixing UI &amp; Functional code together. So, I sent out an FYI email about having certain functions be UI only and segregating your functional stuff out some place else. That way, if you have to debug your code, you're not sifting through UI junk to fix a functional problem, and vice-versa. The BIGGEST issue I had with the class was the professor was using some function (cant' remember the name of it.. eval() I think) but it was basically "take this string and execute it like code". This had "injection attack" written all over it, especially as we started creating UI's that would take in user input and just immeidately run their input through that statement to convert text into numeric or what-not for calculations. I just cringed really hard. I sent out an email to fellow students asking them that if they ever have this situaiton, setup some validations first to make sure someone isn't runing an injection attack from a simple UI.. or to simply avoid using that function by doing things a different way. Colleges are trying to play catch-up by quickly shifting IS degrees over to Data Sci degrees, and Python is a big part of that.
Also not OP and interested
https://sqlperformance.com/2017/04/sql-performance/performance-myths-truncate-rollback
I‚Äôm interested in that, could you send me a PM?
The only way you or anyone else can even *begin* to answer your question is by first answering, "Just what in the fuck am I using this data for?" Along with, "What is the source data and how does it look?"
Truncates are still logged even if they're minimally logged. If the transaction is still open, it can be rolled back.
My guess is optimization. The parser and compiler probably have some improvement when using indexes rather than a column name/ alias.
I use geometry data types in my Postges database. Postgis has geography types, but I find them to be a bit overkill in most cases. The main advantage over separate columns, is efficient indexing for bigger queries. You can easily use those types with EF core, the only drawback for me is that I can't get scaffolding to work, so that's a bit more work.
Copy-Pasta my answer because... why not. Additional Explanation: You can store a PostalCodeCityId next to a Group, People, Product... w/e... to link back to all or some of the vaguer groupings (Country, State, County) Storing a geo dimension... Table(parentFK) Country() State(Country) County(State) City(state) PostalCode(country) PostalCodeCity(PostalCode,City) 1. Postal code without state knowledge is possible. 2. If all tables have a TenantId (owner), trusted data can be stored under a tenantId of 1.. unrecognized data can be stored under the customers TenantId. This allows for capturing all data, and cleaning (rerouting) later. 3. Each table can have columns for ISO codes and Lat/Long 4. unknown is a valid record in each table. If a piece of an entered address is missing. Unknown will help bridge the gap.. 5. The below process allows you to build your own dataset if one cannot be found &amp;#x200B; If @name is null Begin select id from country where name = unknown End else Begin Select id from country where name = @name if @Id is null Begin insert and select End --if End --else &amp;#x200B; blatant self-promotion: learn more about modeling and self-repairing data architecture /master data management at [www.youtube.com/elricsims-dataarchitect](http://www.youtube.com/elricsims-dataarchitect)
When you work in groups, sometimes people disagree on the best way to do things. Some of those cases, there's a right and wrong answer. In your example, not only did you get the wrong answer, but you elected to insult the other person's work in general. Then you got so fired up that someone didn't agree with you, that you decided to come post about their "cute" code and grind your teeth. Try having more patience and humility. People like problem solvers, but you have to have tact too. You could have turned that into a productive discussion, even if you ultimately didn't want the job. If this is how you act when putting your best foot forward, I can probably safely assume you are no treat on a bad day during crunch time.
First of all, can you establish a connection to the database using sqlplus? For example: sqlplus username/password@host:port/service sqlplus scott/tiger@oracleservername:1521/PROD Once you get in, we can tell you how to create a table. But I feel like maybe you can't even figure out how to get in.
This is a demo of an online tool ([https://www.sqlbymarcell.com](https://www.sqlbymarcell.com)) for learning SQL. Here you can see the logical deconstruction of how SQL queries turn one table into another table, step by step. Check out this youtube playlist for the entire educational video set: [https://www.youtube.com/watch?v=IzY91daHm4g&amp;list=PLbsKaEcmovRdwpg1GEuzMLFFsNjcTSr5m](https://www.youtube.com/watch?v=IzY91daHm4g&amp;list=PLbsKaEcmovRdwpg1GEuzMLFFsNjcTSr5m)
NVARCHAR(10) will require 1+2\*LEN(value) bytes. (NVARCHAR(255) would use the same amount of space. NVARCHAR(256-4000) uses 2 bytes of overhead) NCHAR(10) will require 20 bytes (2\*10). Personally, I almost always use NVARCHAR unless I know it to be fixed length (like US ZIP codes).
What do you need from NVARCHAR that VARCHAR can't satisfy? Are you storing data from non Latin alphabets? (if you don't mind me asking)
Okay, yeah,my bad... I read the doc first ([https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017)) , especially the part that says &gt;TRUNCATE TABLE is not allowed within the EXPLAIN statement. &gt; &gt;TRUNCATE TABLE cannot be ran inside of a transaction. but missed the important part just before it that says &gt;In Azure SQL Data Warehouse and Parallel Data Warehouse: &amp;#x200B; Next time, I'll fire ssms before making a fool of myself.
See that‚Äôs my issue. I don‚Äôt think I‚Äôm logging in correctly. At all. My first assignment was to walk through the steps of logging in and changing our password. Which I did and he said I did it correctly. But I don‚Äôt think I did and I don‚Äôt think I am logging in correctly. Because everything that I‚Äôm reading and researching says I should just be able to type in Crete table and then things get started
See that‚Äôs my issue. I don‚Äôt think I‚Äôm logging in correctly. At all. My first assignment was to walk through the steps of logging in and changing our password. Which I did and he said I did it correctly. But I don‚Äôt think I did and I don‚Äôt think I am logging in correctly. Because everything that I‚Äôm reading and researching says I should just be able to type in Crete table and then things get started
Generally to connect to oracle, you need a database user(schema), a password, the name of the host (the server name that the database is running on), and you need the database identifier which is called the SID. The SID is the "name" of the database. The port is usually 1521 for Oracle, but it can be set to something else. There is also a service for that, called the "listener". But assuming you can gather those pieces of info, you should be able to use the connect string above.
Would love that please, thank you!
Always plan for the future because migrations are gonna hurt.
Nice work man, breaking down how the engine handles the data in the background is very helpful to a beginner like me!
If you‚Äôre sending payments, it‚Äôs not safe if it‚Äôs not https.
Super cool!
Looks like order by is out of place- it should be: from where group by having select order by I always remember it by imagining a very aroused tree alien asking itself ‚ÄúFrom Where Groot Having Sexy Orgasm?‚Äù
Brilliant!
While the engine might handle it in that order, from the perspective of the table "building out" it makes more sense to do it this way. I say that, because you may not ultimately select the field that is being ordered by, therefor, if you do "select" first, you won't \*see\* what you're ordering by. So, again, this is the logical order, not the computational order.
Thanks! Glad you like it. If you know others that are learning with you perhaps, this could be helpful for them too!
I'll check it out!
Also interested!
Ok guys. Send me your email I‚Äôll send it to you since it is in my Googl drive
Does that mean you also use BIGINT instead of INT by default?
If scoped correctly, yes. Obviously you're trying to troll but getting a global market is much more likely than hitting 4 billion of whatever it is.
Ok man, looks like you're sensitive about this. No worries :)
Not at all, you brought up it.
Yeah I did, but telling me I'm trolling when I'm trying to figure out why you are ok with taking the performance and storage hit of a type that is much wider than what it seems you need?
https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus sorry for the late response
https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus sorry for the delay
https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus Here
[https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus](https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus) &amp;#x200B; sorry for the late response... I was a little busy
[https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus](https://drive.google.com/open?id=1jZQRQ2lMu--EtpxZrVvNktCEllRMWOus)
Click the google drive link to get to the full course and you can download it
Oh sorry, genuinely thought you were being obtuse. To me, storage size and performance are not an issue compared with global compatibility. However, I have worked in the travel and finance industries for the last 15 years so that's just the way I'm trained I guess.
&gt; We looked at the custom dashboards, but couldn't really see a way to get started. If you can talk about *what* "custom dashboards" you looked at, people here can probably help you. Will 7 days really be enough time to collect a valid baseline? The environments I've most recently worked in are very cyclical in system performance demands, so if you were to pick the wrong 7-day period you'd get results that don't reflect peak requirements - and if you based a purchase on those results, you'd have a major problem during peak demand. https://dba.stackexchange.com/questions/111258/creating-sql-server-performance-baseline-monitoring https://www.sentryone.com/white-papers/performance-baselines-and-benchmarks-for-microsoft-sql-server
Thanks so much! I'll be checking this out today.
NoSQL data bases were designed to solve this exact problem. They provide the flexibility to store any type of info without an exhaustive data model beforehand.
Hey, I'm a Senior Software Engineer, and I hope that I can give you some helpful advice! If you're interested on going the route of certificates, Microsoft has done great certificate programs that are recognized and highly regarded in the "enterprise" world. Personally, the jobs I've regarded most in my career have looked for experience not certificates. You can easily learn a whole lot more by taking online classes from Udemy, or some other online video training course website. If you're looking for work experience be careful on titles and what is expected of you when you do take a job. In most of the United States a Jr. Database Admin is going to make between 59-70k. This means that there should be someone or several someone's on the team who can help you build your resume. Make sure you're not joining a company as a single guy who is Jr level and going to do all the work. As for the educational aspect, formal that is, I cant help much. I do not have any formal education. I have heard that a lot of the formal education isnt useful in many aspects of IT, and when and if you do need something formal you can read a book on it. However I also hear that formal education does teach some "how to learn" I hope my two cents helped! Any questions, let me know.
I'm not saying dont pursue a degree... but I am saying that you will find more information about all aspects of SQL (analysis/science, DBA, architecture) and more current takes on those aspects than you will at most universities. SQL isnt a small thing, there are a few specializations with varying salaries... and the salaries vary more depending on what flavor of SQL you use. I would recommend you do some local market research on indeed, careerbuilder, monster, LinkedIn... all job posting sites to see what flavor is in demand in your area. Read the roles and what is asked in that position. Then I would recommend you load version on you personal system, and do some personal projects. Figure out if you like the analysis side, building data models, scripting, ETL, maintenance, etc. If you find an aspect that starts to make sense, without you needing to look up answers, that's your thing.
They are the same, except, example one you are explicitly naming the PK as "PK\_Employees" whereas example 2 will auto generate the PK's name.
Ok thank you
Instead of thinking about querying additional tables for new features... Think about querying one, datetime based, table that has references to as many tables as you want. You will need a good Typing system to separate the types of rows (post, photo,video,audio,repost).. but you wouldnt have to worry about combined disparate sets and waste time on reordering... you just have to identify the table source of this row and dynamically get that data.
Agreed ^^
Yeah I believe that's what I was leaning with with option 2. Maybe call it "Activity" and insert when the user does something Two quick questions before I start hacking at it and doing things the hard way: would I still use a foreign key or just use an int? And would I: - Query "left join 'photo' if reference=1, left join 'club' if reference=2" - Select from 'Activity', then do separate queries for the different reference types in my controller code
This is how my relationship table looks. https://youtu.be/R6OCMpPs70U The SpecificDataSetNumber is a tableID that is redundant in the destination table so, if you join, it's always SpecificDataSetNumber = SpecificDataSetNumber_implied and xId = intPrimaryKey_implied. That's how you can relate any row in any table to any row in any table. Using the same relationship table you can define a table and its join key (using your own information schema) to automate everything. I would have a table structure, that I haven't done a video on, for Posts(30+tables)... but a very limited version would be a SpecificDataSetNumber, intPrimaryKey, time, type.
My post structure would be RCFTPost (5tables) RCFTReference(5tables) RCFTEvent(5tables) RCFTRelationship(5tables) PostEvent (contains time in date,datetime,dateid and event type (created,updated, edited, deleted) PostReference (postId to another tables SpecificDataSetNumber and primary key) RCFTMedia(5tables) typeid for audio, video,photos... etc. And it just keeps going...
Also, one of them is applied to the table dbo.Empployees and the other to dbo.Employees.
Anytime
You want to explicitly name your constraints. While the second example is valid, the first is preferable.
the first one has more letters than the second one.
I saw that as well, but, chalked it up to a typo. ¬Ø\\\_(„ÉÑ)\_/¬Ø
I have done this for my own portfolio. There are many open source softwares you can use to accomplish this. Personally I built a Postgres database that I house within the google cloud and I use python jobs to ping api‚Äôs to pull down data and push it in the database. It was actually super easy to set up. I use dBeaver as my sql interface but realistically you can use any that you like. The hardest part is getting the data, like what api or interface would you use to get the data then writing some sort of ETL to put it where you want it.
&gt;If you're interested on going the route of certificates, Microsoft has done great certificate programs that are recognized and highly regarded in the "enterprise" world. Personally, the jobs I've regarded most in my career have looked for experience not certificates. You can easily learn a whole lot more by taking online classes from Udemy, or some other online video training course website. There's an adage that *"those who can do and those who can't get certified"* but I'm not sure how fair that analogy is. Certifications are not a substitute for experience. However, they can help validate that the experience you claim to have is experience you actually do have. When looking to make career moves it's inevitable that there's going to be limits to what you really should be talking about with regards to the trade secrets of your former employers. There's going to be a lot of things you say you can do that the recruiter and your new manager(s) cannot really verify the validity of. Having a relevant certification on your resume can help dispel doubts someone might have about your skillset, which can be particularly helpful if you're coming from a non-technical education background.
I wouldn't be the best person to provide career advice, but wanted to respond to this point: &gt;It is really my first experience doing any sort of coding (if SQL counts). SQL is absolutely coding. While the most common kinds of programming follow the imperative paradigm (e.g. procedural and object-oriented code), SQL is one of several types of programming in the other major paradigm family--declarative programming. Imperative programming focuses on giving the computer a set of explicit directions that will lead the computer to the desired result. Declarative programming instead focuses on giving the computer a detailed technical description of the desired result and leaving the actual mechanism of how to find upon that desired result up to the computer. The fact that SQL is declarative rather than imperative doesn't mean it isn't coding. Often, complex manipulation of data sets can be done much more efficiently by an RDBMS via SQL than would be possible via an imperative language... The imperative language usually takes longer to write the code to do it and will often be computationally inefficient at doing it as the RDBMS is already heavily optimized for doing the vectorized arithmetic. The following article is a really valuable piece I read last year that I strongly advise looking into if you're not sure if SQL should be considered "coding": [https://dev.to/geshan/you-can-do-it-in-sql-stop-writing-extra-code-for-it-lok](https://dev.to/geshan/you-can-do-it-in-sql-stop-writing-extra-code-for-it-lok) Properly used, SQL can replace a lot of completely unnecessary imperative code when working on data sets and will do so much more efficiently than anything a non-genius coder is likely ever to write.
Well said. Also if you need a manager to verify experience I would definitely bring a portfolio to show him or her. I also provide github links. :)
SQL is a programming language for the definition and manipulation of content in a database. Given the size of your data set, you definitely should have a database to store your data, but SQL is the communication mechanism between your datamart front-end and your database backend, not the platform for the user interface itself. What you would need to do instead is build the user interface in some other language (e.g. Python) or via an off-the-shelf option (PowerPivot for Excel, Power BI, etc.) that would then send SQL queries to the database in order to retrieve the desired information and then present it in a way the end-user can interact with.
Thanks for the answer. I am moving on that direction. How do you approach querying the day-to-day progression of your portfolio?
I ping at the end of the day a snapshot of my portfolio each day, I load this to a dbo.portfolio table. I use my database as a management system so I have a static table that lists what stocks I want to watch or purchase, then every 15 minutes I load a dbo.watchlist table with the share price. So the portfolio table is a daily snapshot and the watchlist is a 15 minute progression (every stock in the portfolio table is in the watch list table because I don‚Äôt own stocks I wouldn‚Äôt still buy). I‚Äôm currently working on monthly reporting in excel, I‚Äôm trying to not look at my stocks daily with the hopes that my energy focused on the experience I‚Äôm gaining working on my database has better returns than the market.
You definitely need SQL, but you also need /r/businessintelligence. There are lots of free databases and free reporting platforms you can get started with. Don't be another IT/programmer that thinks this is something easy you can just bang out in a week. There are companies that have been making BI solutions for decades and they still can't meet every need for every business group. But there are a lot of them that are highly customizable to meet all you basic needs.
That makes a lot of sense. Since I am trying to develop a desktop application with local processing, maybe this approach won't suit me. But I will start with data acquisition and later on I think about processing.
Some problems I noticed for each of your answers, note there could be additional problems: For Question 1, I think you need to rethink what you're grouping by. With "GROUP BY O.orderid, price" you're saying that you want to get the "SUM(qty)" for every combination of orderid and price. Think about what that means. :) For Question 2, you're doing a cross join (probably accidentally) by writing "menu as M, order\_detail as O". You need to add "WHERE M.id=O.menuid" like you did in Question 1. Personally I prefer to use the syntax like "table1 JOIN table2 ON ..." syntax but your style should work too. Question 3 says to order by total sales amount and you're ordering by "qty, price". Question 4: Umm here it looks like your professor (or whoever provided these questions) made a mistake. I don't see how the total qty of a few-to-several hundred per day in the expected answer makes sense, given that in Question 3 there are 3+ thousand each for each of the top 10 items. It's possible I'm missing something. Question 5: There's no year column in any of your tables. You need to extract it from your order\_date.
Thank you! That‚Äôs helpful
PowerBI and SSIS, sounds like a Microsoft shop...so why are you building the DW on Postgres?
&gt; I‚Äôm trying to not look at my stocks daily with the hopes that my energy focused on the experience I‚Äôm gaining working on my database has better returns than the market. This is a very good way to look at this. Out of curiosity, are you hoping to leverage the experience for?
Already did tbh. Just turned 25 and I started a data architect position on the 26th completely work from home making 6 figures. Got a 35% raise from my last position which was an ETL role. In the interview process my database came up twice, I included it on my resume. After getting hired I asked if that helped because I was curious and they said they love seeing the initiative, made them think I was more passionate than had I not had something like that.
I've solved 1-3 so far thanks to your input!
Awesome dude, thank you! I have to buckle down and learn this so I can further my career. This is a big help!
I'm in a very similar position and recently started a Masters in Information Systems and Business Analytics. I have found that the people I see advancing in my company in the data world are completing graduate degrees in analytics/statistics/IS/CS etc and my company pays for quite a bit of the program. I am (slowly) learning Python as part of the MS program and can see some practical applications.
For question 4, your query said give me the quantity of all menu item where it has been ordered and group by their day of the week. My suggestion would be joining all three tables using their keys, and then in the where clause sort item\_name = "Nescafe Expresso". Lastly, order and group by dayofweek(order\_date) aggregated function.
Yeah, I got number 4, now I'm working on 5.
anytime
Because the company doesn't have anything on MS and I don't think they are willing to start with it, since MS SQL server is not for free in the production environment. I would surely like to grab MS SQL server, as it is something I am mostly profecient with, but looking for minimum costs here.
Try Datasette and some SQLite views have pre built filters. I think you can give URLs to JSON or CSV endpoints with the data for individual tables if I remember right. A bunch of users can read concurrently, and you can download stuff in CSV and JSON. python3 -m datasette example.db And you have yourself a website with shareable data. https://datasette.readthedocs.io/en/stable/
Are you looking for Postgres hosting **outside** the network (environment) of your client? I assume you made sure with them, that it's OK to store their (possibly confidential) data outside of their network. There are several hosting (cloud) providers for Postgres: * [https://www.elephantsql.com/](https://www.elephantsql.com/) * [https://www.citusdata.com/](https://www.citusdata.com/) * [https://www.heroku.com/](https://www.heroku.com/) Amazon also offers managed Postgres instances. If your reports don't need a different table structure than the production database has, the usual approach is to create a hot-standby that is open for reading and run the reports on that standby. Maintaining a hot-standby based on [streaming replication](https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION) has an extremely low impact on the primary server. And the queries on the standby will not affect the performance of the primary. Obviously that standby would need to be located in the network of your client.
How about an SQL certification? [https://www.microsoft.com/en-us/learning/exam-70-761.aspx](https://www.microsoft.com/en-us/learning/exam-70-761.aspx)
If you have the tables in place, try an ETL tool to bring across the data. Check for differing data types first and some features in the SQL may not exist in MySQL or are done differently. Might be best to bring the data out into a common format first, like CSV, if that's practical.
Yeah exporting the tables into CSV then converting the CSV tables in SQL helped but I could only get 1000 rows because above it MSSQL started lagging and crashing
&gt;Don't be another IT/programmer that thinks this is something easy you can just bang out in a week. That is exactly what I was afraid of :D Thanks!
&gt;https://datasette.readthedocs.io/en/stable/ I will check it out! Thanks
Thanks!
hackerrank
&gt;MS SQL server is not for free in the production environment. Then how is SSIS a viable solution? You can only get it with a SQL Server license.
Yes I figured out later that I will probably have to use a different platform, probably some free Python based
read from sitws like oracle-base, techonnet, oracletutorials.
Three main routes 1. DBA/DBE - manage sql tables and servers 2. Data Engineer - create ETLs to move data between systems (will need another languages) 3. Data Analyst - analyze and report data.
SSIS - Data flow - ADO.net connection manager using SQL command as the data access mode then writing a select query and then putting that in an OLE DB Destination.
No problem üëç
Hi there. You can use ScaiPlatform for querying the database and you can create tables through the UI and filter data. It is free for 2 users in the cloud on all major cloud providers, like AWS, Azure, or GCP.
Hi, thanks for replying. When I say looked at, I was looking for information on the web of people who had done similar, I didn't really find anything that was similar to what we needed. August is my company's heaviest month in production, so the 7 day snapshot we take gives us enough data to make a prediction for the rest of the year. I ran this command to see how many were being used ***select scheduler_id,cpu_id, status, is_online from sys.dm_os_schedulers where status='VISIBLE ONLINE'*** If I remove the where part, then I see the status of all of the cores. What I want to be able to do is something similar to the above, but on a per database basis and have it monitor constantly for 7 days and report back an min, average and max number of cores used.
I've used SSMA a few times and it has always worked for me. From memory its a pain and really unintuative what to do. By the sound of it you have run the convert schema part but not the migrate data. Once you have a schema synced you would need to click on the MySQL DB and pick Migrate Data. This will then populate the tables with the data. This is the quick guide I wrote for one of our juniors when I wanted them to do it: 1. Create DB First in SQLServer 2. SQL Server Migration Assistant for MySQL 3. Click the MySQL DB: Convert Schema MAKE SURE YOU SET THE SCHEMA TO dbo NOT WHATEVER THE FUCK IT WANTS 4. Right Click SQL DB: Sync with Database (This will create all the tables, Refresh SSMS once done to check they all got created) 5. Click the MySQL DB: Migrate Data (Hope there are no errors) 6. Save Report (I don't know if its possible to get this data back if you dont save it now so just do it) 7. Fix all the errors you come across 8. Right click the table that failed in MySQL: Migrate the data from the tables that gave errors 9. Hope
For analysis, check out https://www.datacamp.com/courses/intro-to-sql-for-data-science
https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about
Big time fan of Udemy. They have literally anything you want to learn on that. You can learn MSSQL Server there. You can learn PostgreSQL. Check out the highest rated with the most ratings. Complete SQL Bootcamp is a good one I think. I personally believe these little $10-$20 courses are the way to go before you get into more detailed classes like below. I'm doing a Python class with them now and I love it.
replace and case when @sourceFK = 115 then (wot.sourceFK = 115) else (wot.sourceFK != 115) end with and ( @sourceFK = 115 and wot.sourceFK = 115 or @sourceFK &lt;&gt; 115 and wot.sourceFK &lt;&gt; 115 )
I don't believe a case is necessary. Try where t.serviceFK in ... w.status = 2 and ((@sourceFK = 115 and wot.sourceFK = 115) or (@sourceFK != 115 and wot.sourceFK != 115))
else doesn‚Äôt take a boolean, it handles whatever doesn‚Äôt fall i to your prior booleans. If you want another boolean you use another when: case when thing = criteria1 then do_this1 when thing = criteria2 then do_this2 else do_something_else end I‚Äôm not super clear on what you‚Äôre trying to accomplish. maybe a sample of your data would help clarify your ask?
/u/r3pr0b8 and /u/offx1 gave me the answer I need. It is so obvious now I can't believe I didn't think of it. I guess I was over complicating things. Thank you very much!
It may work to set a flag and just do the comparison on that, as below. The cross apply may not be necessary but helps for readability I think. declare @sourceFK int SET @sourceFK = 115 select w.id, wot.sourceFK from workorders w inner join tickets t on t.workorderfk = w.id inner join workOrderTypes wot on wot.id = w.typefk cross apply (select case when wot.sourceFK = 115 then 1 else 0 end sourceflag) ca where t.serviceFK in (4420, 4419, 4421) and (w.workdate) &lt;= '15 Jul 2019 23:59' and w.id not in (select wid from commercialManualBillingPOItems) and t.status = 1 and w.status = 2 and ca.sourceflag = case when @sourceFK = 115 then 1 else 0 end
Thank you. I have never seen cross apply before and it definitely is more readable. I wish I knew about this earlier. Cheers!
Try these two queries... &amp;#x200B; declare @sourceFK int; declare @excudelFK int; SET @sourceFK = 115; SET @excludeFK = NULL; select w.id, wot.sourceFK from workorders w inner join tickets t on t.workorderfk = w.id inner join workOrderTypes wot on wot.id = w.typefk and (wot.sourceFK = @sourceFK or (@sourceFK is null and wot.sourceFK &lt;&gt; @excludeFK)) where t.serviceFK in (4420, 4419, 4421) and (w.workdate) &lt;= '15 Jul 2019 23:59' and w.id not in (select wid from commercialManualBillingPOItems) and t.status = 1 and w.status = 2 declare @sourceFK int; declare @excudelFK int; SET @sourceFK = NULL; SET @excludeFK = 115; select w.id, wot.sourceFK from workorders w inner join tickets t on t.workorderfk = w.id inner join workOrderTypes wot on wot.id = w.typefk and (wot.sourceFK = @sourceFK or (@sourceFK is null and wot.sourceFK &lt;&gt; @excludeFK)) where t.serviceFK in (4420, 4419, 4421) and (w.workdate) &lt;= '15 Jul 2019 23:59' and w.id not in (select wid from commercialManualBillingPOItems) and t.status = 1 and w.status = 2
&gt; `else do_something_else` this is problematic because you aren't allowed to "do" anything in a CASE expression, merely provide **values** to be returned as the result of evaluating the expression
good point! I‚Äôve changed it to return_this. I think that‚Äôs more accurate
Thank you so much! This works magically !!
Thank you! Great help!
Thank you, I was struggling and could not get good returns from the google search.
W3schools.com
Unfortunately you'll have to buckle down and write aliases unless you know of some external tool that can do it for you. You may be able to whip up a script in another language like Python or even Bash (to run sed/awk and such, perhaps) to help you generate the SQL. But there's nothing I'm aware of within any dialect of SQL itself that can help you.
Are you trying to store these values in the same table? If so when you create the table just have whatever column names you want. I guess my question is how are you storing your query?
Select name from table where id &lt; 3
Is there a reason it needs to be a select \* that returns everything? This is possible, but it's worth stepping back and critically evaluating why you need to do it this way. If you have huge tables with hundreds of columns with redundant names, someone already kind of just did shit without critically evaluating, and just working around it will just keep kicking the can down the road so that the NEXT time this bloat is a problem, it will be even harder still to pull it off. Having said that, you could do this with Dynamic SQL. Start with a metadata temp table where you put in the two tables you want and the aliases you want to addend (ie \_a and \_b). Then, set up a loop for each of the tables where you loop through all of the columns in INFORMATION\_SCHEMA.COLUMNS for that table, and then write the strings concatenated with the alias. So imagine two loops. First, an outer loop where you set @TableName and @AddendumName by going through your metadata table (I usually give it it an INT IDENTITY column to do this). Then, you set @LoopMax = count(\*) from INFORMATION\_SCHEMA.COLUMNS WHERE TABLE\_NAME = @TableName. Set Loop to 0, then do a WHILE @Loop &lt; @LoopMax where you set @Loop = @Loop + 1, then set @ColumnName = SELECT COLUMN\_NAME from INFORMATION\_SCHEMA.COLUMNS WHERE TABLE\_NAME = @TableName and ORDINAL\_POSITION = @Loop. Then just add a line to your @SQL variable of the form @SQL = @SQL + @ColumnName + @AddendumName + ','. There are some other fiddly bits but that's the main thrust of it.
What I'm actually doing is starting with a single large Table X; this table contains information on a bunch of people, where each person has two records (=rows). From X, I am extracting two subsets A and B that meet differing sets of criteria; in my query, A and B are virtual tables. From there, I join A and B to form Table A+B. Where Table X started with each person's data being split across two rows, Table A+B now has each person's data in a single row that is twice the length of the rows in Tables A and B. Following me? Basically I've done this... Step 1. Start with Table X, the original database. Note each person has two entries. |Table X||| |:-|:-|:-| |col\_1|col\_2|col\_3| |person\_1\_a|person\_1\_a|person\_1\_a| |person\_1\_b|person\_1\_b|person\_1\_b| |person\_2\_a|person\_2\_a|person\_2\_a| |person\_2\_b|person\_2\_b|person\_2\_b| &amp;#x200B; Step 2. Extract two subsets (Tables A and B) from Table X. Note each subset contains only one entry per person &amp;#x200B; |Table A||| |:-|:-|:-| |col\_1|col\_2|col\_3| |person\_1\_a|person\_1\_a|person\_1\_a| |person\_2\_a|person\_2\_a|person\_2\_a| &amp;#x200B; |Table B||| |:-|:-|:-| |col\_1|col\_2|col\_3| |person\_1\_b|person\_1\_b|person\_1\_b| |person\_2\_b|person\_2\_b|person\_2\_b| &amp;#x200B; Step 3. Combine Tables A and B. Note that all data for each person is now on a single row. Note the duplicated column names. &amp;#x200B; |Table A + B|||||| |:-|:-|:-|:-|:-|:-| |col\_1|col\_2|col\_3|col\_1|col\_2|col\_3| |person\_1\_a|person\_1\_a|person\_1\_a|person\_1\_b|person\_1\_b|person\_1\_b| |person\_2\_a|person\_2\_a|person\_2\_a|person\_2\_b|person\_2\_b|person\_2\_b| &amp;#x200B; Steps 2 and 3 are virtual tables within my query. It seems that the duplicated column names in Table A+B are fine through that step. However, from there I'd like to perform some additional selections and manipulations on Table A+B, but my script breaks. I'm pretty sure the problem is the duplicated column names, which confuses subsequent SELECT statements. So if there were some way at Step 3 to basically say "rename the columns in Table B", I think that would fix the problem. Disclaimer: I am pretty new to SQL.
you need to use substring - For the first parameter you use 1 as you want the substring to start at the first character. For the last parameter you need to use ascii(number of the space character). I can't write the code from memory. Just Google "selecting substring before a character"
This gets into murky territory when you start dealing with real world data where people have multiple first/last names, so it's best to just store them separately in the first place. However, you can select just portions of the name by using SUBSTRING and CHARINDEX SELECT SUBSTRING(name,1,CHARINDEX(' ',name)-1) This example finds the position where the space character is in the CHARINDEX function, and then uses that to determine how many characters it should take in the substring. This particular example breaks if there is no space in the string though.
do you need this only to save time from hand-typing these names in? if so you can do a query from metadata that produces relevant names then copy/paste the result into your query. on ms sql this will look something like this: select 'table_a.'+name + ' as ' + name + '_a,' from syscolumns where id = 457104719 on other platforms you might need to use INFORMATION_SCHEMA.
The best answer varies per database - which are you using? But yeah basically as others it will be something like SUBSTRING. And I agree with /u/Achsin that it's better to have separate columns for first and last. What happens when "Bob Jones, Jr." comes up, and you parsed out just the "Jr." to get the last name? So unless you can guarantee the format is *always* "&lt;first&gt; &lt;last&gt;" (I bet you can't guarantee that, though) then whatever parsing you do it subject to error.
SQL Server doesn't really track CPU usage on a per-database level. You may be able to get a ballpark idea by going through the plan cache, but it's not pretty. I'd suggest starting with [Glenn Berry's diagnostic query suite](https://www.sqlskills.com/blogs/glenn/category/dmv-queries/). Thing is though, SQL Server isn't going to use more or less cores depending on load. It's multi-threaded and it schedules threads across all CPUs in an attempt to keep things balanced. You aren't going to see "we're lightly loaded right now, so CPUs 2 &amp; 3 aren't active" on a 4-CPU box. You're just going to see an overall lower utilization across all 4 CPUs. For "constant" monitoring (which is really more of a sampling on an interval), you're going to be in the realm of a dedicated monitoring tool that watches both SQL Server (via DMVs) and the host OS (via Perfmon). OpServer is free and used by Stack Exchange (they created it). Or you could take the opportunity to invest in a monitoring suite long-term because if you don't have one right now (which you don't, otherwise your post is moot), you're flying with one eye shut. I'm an unabashed SentryOne fanboy but SolarWinds and Red Gate are good as well.
Subtraction isn‚Äôt the right word here, but there are a few ways to get what you need. I would go with SELECT SPLIT(name, ‚Äú ‚Äú)[0] as first_name FROM table This splits the name field at each space into a list, and uses the 1st (which is [0]) item in the list. Also check out the function SUBSTRING_INDEX()
Hm, thjat second bit does sound like it would do what I want, if I'm understanding you correctly. Unfortunately, I've only just started learning SQL so implementing that is pretty above my head :/ To your first paragraph, it's not that the database is crappily organized. The duplicate column names are the product of my somewhat hacky query. I provided much more context in [this earlier reply](https://www.reddit.com/r/SQL/comments/cdjsc2/how_to_efficiently_rename_large_number_of_columns/etufa7q?utm_source=share&amp;utm_medium=web2x).
&gt;do you need this only to save time from hand-typing these names in? That's part of it. The other part of it is just to avoid having a super long and bloated script, because I am handing it off to other people who, as an aside, I'd like to avoid giving the impression that my SQL skills are meh.
This is what Excel is made for. Copy the column list to Excel, use =A1+"_a", or similar. Copy back.
By virtual table, are you referring to a temp table, one that has the # symbol before the name? Is so, when creating the table, rename the columns. If you are trying to be fancy and using a select * into, then lastly it seems like the only way to do what you want, is to write a dynamic loop to rename the columns. But honestly the time it will take to do this and debug it, it would be easier to create table b with _b after each column. Sometimes in SQL the simplest answer is to just buckle down and do a mindless task once for 10 minutes, rather than spend 4 hours thinking of the ‚Äúperfect‚Äù way to do and then another 2 building it and the rest of your life maintaining it. In certain cases creating a function to do this might be a fun and challenging project, but is this a reusable process you do with multiple tables with an unknown number of matching columns? Or is this a one off that you just need to stop your code from breaking?
[removed]
The other answers provide a good way how to solve your problem, but you should think about splitting the name into first name and last name. Especially if you are going to use it in any kind of search, the computations will kill your performance. You can have extra column as display name.
You will have to write the aliases - preferably automatically from table definition. Then you can create a view that will use these aliases so you don't have to do it every time.
Thanks I‚Äôm gonna try that one more time
There's no CREATE DOMAIN in MySQL. Nor is there a CHECK yet. (Though I heard the next version will have CHECK.) However you do have enum, so maybe use that.
I would try: ALTER TABLE employee2 ADD CONSTRAINT type_constraint CHECK (TYPE IN ('python', 'javascript', 'html')); Failing that you could create a separate table and put in a foreign key constraint.
&gt; However you do have enum, so maybe use that. **NOOO**OOOoooo....
You really should store them separately. I say this as someone who deals with an RPG system written in the 1970's that did it this way. Generations of software developers have cursed that person's name, even though we have no idea who it was.
&gt; Failing that you could create a separate table and put in a foreign key constraint. this is the correct answer if you cannot upgrade to [MySQL 8.0.16](https://dev.mysql.com/doc/refman/8.0/en/create-table-check-constraints.html)
No, you can only SELECT * and get all of the columns, including a and b.
OP neglected to mention the platform as recommended by the /r/SQL sidebar which seems to have been removed (anybody know where it went?) in MySQL, you would use `SELECT SUBSTRING_INDEX(name,' ',1)`
Dynamic SQL could do it.
Assuming that field_b is a text column and not a date column you could use something like this: SELECT CONCAT(field_a,'_',REPLACE(field_b,'/','')) Otherwise you would need to convert field_b to a text string (and possibly do some other manipulation) first.
To get in the door, sure. That's basically all I knew coming out of college. I got an interview at a software company for a "data conversion specialist" position and kinda just learned the rest once I was in. Few years later and now I'm managing a small team now at a much larger company. If you can comfortably talk queries and reports in an interview then there are plenty of ground-level opportunities out there.
Did you get imposter syndrome while you were starting out? What were some of your daily task
Is there a column called 'Service' in both the Incident and ServiceReq tables?
Yes and no. The basics are pretty easy to pick up but the rabbit hole can get pretty deep.
There is. Confirmed.
It really is that simple to get in the door. Most people know less about their work than the job description purports. 'Imposter Syndrome' is one way to put it, but really you don't need fluency in any programming/QL - just the means to get shit done. &amp;#x200B; You don't need to know *what* the answer is to every question, you just need to know *how* to find the answer. That's the biggest takeaway to data field. Most people aren't well-versed in every language syntax or most-efficient execution of a given request, and truthfully most requests just need to get done "at any cost". You'd be surprised how much duct-taped code sits behind large systems. Having said that, just be a good problem-solver and yes, have some basic backend (SQL) and front-end (data vis. like Tablea or Power BI) and you'll be on your way.
Oh man I still get impostor syndrome lol. But it's not rational. It's not like I lied in my interview, I told them I had a firm grasp of SQL basics, a strong work ethic and an eagerness to learn. My daily tasks were primarily writing data transformation queries at first. My project managers would provide me with data mapping documents which would kinda loosely outline how data needed to move from the legacy system into the new system and I would write queries to make it happen, sequence them in SSIS and then work through any problems we found.
Okay, I've read the other replies. So it sounds like technically all you \*need\* is to rename the aliases in your WITH clause, which (as others have pointed you) you could do with Excel, regex, metadata queries, etc. But what you \*want\* is a dynamic renaming that doesn't need to specify every column so your query looks more professional. The bad news is that you really do need Dynamic SQL to do it this way. Normally I would bill for this but you stumbled upon one of my personal crusades (ie, that Dynamic SQL gets a bad rap and a lot of times can keep stuff cleaner and safer than doing it all longhand if there's a lot of redundancy/copy-pasting/etc), so here's some example code to prove my point: DECLARE @SQL nvarchar(max), @TableName nvarchar(100), @ColumnName nvarchar(100), @Suffix nvarchar(50), @LoopMax int, @Loop int SET @TableName = 'YOURTABLE' SET @Suffix = '_a' SET @SQL = 'SELECT ' SELECT @LoopMax = COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = @TableName SET @Loop = 0 WHILE @Loop &lt; @LoopMax BEGIN SET @Loop = @Loop + 1 SELECT @ColumnName = COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = @TableName AND ORDINAL_POSITION = @Loop SET @SQL = @SQL + @ColumnName + ' as ' + @ColumnName + @Suffix IF @Loop &lt; @LoopMax SET @SQL = @SQL + ', ' --comma and newline for everything but final column END SET @SQL = @SQL + ' FROM ' + @TableName --if you add any more to this query, put it here above the executesql call exec sp_executesql @SQL When you run this, it will select every column from YOURTABLE, but with \_a at the end of each column names. Just like in your regular code, you can add stuff to the @SQL string before this stuff so that it's a WITH statement instead (make sure you change that first @SQL = SELECT to a @SQL = SQL + SELECT if you go this route). Use this code, then change @Suffix to \_b, reset @LOOP, and do it again. Then you can just put the rest of your query in a single text block to concat with @SQL at the end. Remember that you need to put double ticks around every string you want to use as a string within the query, since a single tick closes the string. It's harder than regular SQL writing because debugging stuff within the string is trickier, but if you want to avoid the mess of copy-pasting all the column names, this is what it looks like.
If we were hiring and you knew SQL and Tableau, you'd be a worthy candidate. I wouldn't even care which SQL dialect. I'd want more than just "Basic SQL 101" though. You should at least be able to do some data aggregation, cleanup, transformation and hopefully understand window functions. None of which is that hard. You should also have some Excel skills. Again, nothing super hard, but more than just "I can add two cells together!!" If you had experience with SAP Business Objects and/or VBA... even better. But I could overlook not knowing those as long as you seemed willing and able to learn.
Yup, left some example code in a different reply thread. Dynamic SQL owns
If all you did was add that line and the column exists on both tables then I'm not sure what would be causing the error. It looks like the actual error is being logged somewhere for you, would you be able to find that error and post it here (or potentially run the query on the instance outside of the report and see what error it gives you)?
thank you both! some great info here. Definitely will look into the certifications as others have mentioned. Can't hurt. I've also done a ton of the online courses including the online one. It makes me feel better knowing there are people in the field without formal education
Thanks for this! I kinda feel the same about formal education but wanted to make sure it wasn't a definite requirement. This is all great advice!
Thanks for your reply. I'm an idiot and realized I was trying to use postgresql on mysql db.
Thanks for your reply. I'm an idiot and realized I was trying to use postgresql on mysql db.
Good to know! I chopped off a ton of time from my normal processes just learning basic SQL and more advanced Sheets so this totally makes sense - thanks!
this has been mentioned a few times so I will definitely check it out - thanks!
Interesting thanks! I have some friends in the coding world who have recommended Python. Something I want to look into the future once I'm more comfortable in SQL
I have seen all of these when searching around and couldn't quite figure out what each did haha. Thanks for simplifying it!
if you are someone who can learn without video, I really enjoyed the Mode one ([https://mode.com/sql-tutorial/introduction-to-sql/](https://mode.com/sql-tutorial/introduction-to-sql/)). I believe you can sign up for their editor for free as well. If you prefer video, I enjoyed Udemy ([https://www.udemy.com/sql-for-newbs/](https://www.udemy.com/sql-for-newbs/)) a lot. I know one (and maybe both) of the instructors had no SQL experience before starting at Udemy so they are very easy to follow
Thank you! This post made me feel better :)
Thank you 1000 times. It finally worked. You‚Äôre my savior
Thank you! This was helpful
I didn't know sql before I landed a job using sql. I just knew enough to talk about it in the interview, then learn in the job.
It is that simple. The hardest part is getting the first opportunity to work in SQL once you know it. Once you have one gig under your belt the rest become progressively easier.
If you were hiring, what would the payscale be?
I just start my Data Analytics job and that is off of about 8 months of SQL queries running out of some Power BI dashboards that I built for my old team. It helps that the person I replaced helped get my started and she talked me up enough that I got the job. 1 week on the job and I am just trying to get up to speed with SSRS and SSIS as I had never used either.
I'd probably be interested. I don't have any specific topics off hand. I write some quieres (self taught) at work, and just seeing possibilities would be good for me.
Thank you so much!! This worked
Thats cool. I work with TSQL at work and i do everything from ad hoc requests to data warehousing. I was in the same boat as you when I got into working with data.
Thanks worked
I would ABSOLUTELY be interested! Something like this would be incredible. I‚Äôm self-taught SQL and it‚Äôs really hard to get a grasp on things that you don‚Äôt know...that you don‚Äôt know...please keep me in mind for anything that you come up with!
Sorry I couldn't help directly (I'm on holiday with only my phone), but I'm glad I could help. I've been using SQL for about 15 years in multiple roles. Always feel free to DM me with other questions like this.
When I get more comfortable I hope to be as generous lol
Teaching others is always the best way to learn yourself :)
This could be really beneficial for anyone who is looking to gain valuable insights into the many applications of SQL.
Will do. I‚Äôll keep up with all the users that comment and invite them to a stream if enough people are interested.
Thanks for the feedback. I know I would have loved a place to get help when I got started.
For me it is MSSQL for our database, and I also pull from a MySQL database that is one of our partners. I do a lot of pulls and then manipulate in excel, but I know I could probably do more to start my manipulation in my queries if I was better. I keep thinking I need to buckle down and do something like a SQL Udemy boot camp or something. But then think I should just do an excel and power query one so I‚Äôd be better on that side. Instead, I just keep sitting on my laurels and doing nothing.
Would appreciate some PIVOT &amp; UNPIVOT work being done. The idea is simple enough, but in practice it doesn't. Thanks.
Sure i can cook something up. I haven‚Äôt used this in about a year so I‚Äôll need to take a bit to get a good example set up. If there is other stuff just let me know.
Yeap. It is that simple. You can get an entry level data job with those skills and then you can ramp all the way up to expert level with them. Go for it. It's a fun field.
We ALL get imposter syndrome. If you don't have imposter syndrome then it only means that you aren't aware of how much more you still have to learn.
I'd be interested. I'm taking a Udemy course right now but I'd prefer something live as it commands my attention better.
I‚Äôd also be interested! Lmk!
Please sign me up OP, thanks!
I would, I can write basic queries and join tables, but would love to know more about how to use SQL more. I'm not sure how to do loops or anything other than simple table joins
Hi everyone. Looks like there is plenty of interest. I picked up a microphone that was a prime deal earlier to use. I‚Äôll set up a local server to use with dummy data to use as well. What time and day would work for everyone? I should have my microphone Thursday night so anytime after Thursday would work. I‚Äôm in the US and live in central time FYI.
I just made a comment asking what would be a good time to stream. Let me know what you are struggling with as well.
Check out the comment I just made asking what would be a good time for everyone. Let me know if there is something you need help with in particular.
Check out my comment and let me know what you need help with.
I'm not really struggling per-se I'd just prefer to learn in a live environment. I'm on Eastern time so anytime after 5pm EST is fine w/ me.
You are on the right track. SQL is very powerful and will manipulate data better than excel for the most part. However, excel skills are still useful to present data to a user or to produce visualizations. You would probably do well to learn both.
Loops are easy to write, its the crazy stuff us analysts and developers try to do in the loops that make it hard. I would be happy to help show the basics and some real life examples.
I am interested.
I do ETL work in Java and I would be interested in seeing the data warehousing that other people do. Really anything beyond begginer topics would be great!
üëå
Do you do any data warehousing now?
The ETLs I work on currently are more of a way to bring the data from multiple tables in Oracle, into one table in MS SQL. By constantly bringing in the data, it avoids needing to do complex joins on around 30million records. After filtering it comes down to around 1.3 million. And when running every seconds it's usually less than 10 per cycle.
I self taught querying, but now I'd like to branch into database and data warehouse design from a more seasoned point of view, not just the basic foreign keys and normal forms. Thanks!
"Alright, now to run this query on ten million rows of data..." :P
No problem. Although data warehousing, in my opinion, is very simple at its core. I can write a warehouse on stream and talk my way through it though.
The points have spoken.
Anytime after 8:00 pm central for me.
IIRC, you'll need one liscense per server that has SQL Server installed on it. Price for that server will vary depending on what version and how many cores that server has.
Any time after 7pm EST is great for me!
Thanks for the info..
I would love to learn about the crazy stuff!
A good example would be API calls in SSMS. When the API returns data, it also returns a field that says if its done(no more data coming, this is the last of it) or if there is more on the way. So if the API says its not done, you parse out the JSON string, stick it into a table and then keep going. When the API indicates that its done, then you load that last batch of data into a table and then move on or stop. The loop would be based on this status field indicating that its done or it still has more data to go.
Thanks so much for offering. In case I can‚Äôt make it, i don‚Äôt know how hard this would be, but can you upload a recording of the livestream?
Absolutely.
I‚Äôm interested!
Absolutely! Thank you!
Absolutely!
After 630pm est is good for me! I‚Äôd like to learn How to use LOOPS. Also DECLARE and SET what‚Äôs the difference and when to use? And any other tips to transform data sets for top best performance. Also, is there an order of operations to follow when writing custom queries?
I would absolutely watch a live stream. It would be really beneficial to see workflow and real life scenarios.
No problem, we can definitely cover those. In fact, we would need to use declare and set to do all the cool stuff. üòé
I thought so too then I was shown a diagram of my work‚Äôs data warehouse, like a spiderweb and the design makes very little sense. I suppose I‚Äôd benefit from hearing a theoretical perspective, like your thoughts on it.
I'd be interested but I don't know anything about SQL and I don't want you to have to dumb it down if everyone's a little more advanced. But I'd love to watch.
I saw your code, I don't think you'd need a loop though. You could just dynamically build the column list from each table and name it however you wanted, then assemble everything into an `@SQL varchar(max)` and execute it.
Speed and efficiency are why you would want to warehouse data. One of the most simple reasons would be to avoid storing the same data in multiple places. Think about customer info for a business. You wouldn‚Äôt want it stored many times over many different tables.
Follow up with me after Im able to stream. If you didn‚Äôt get your questions answered or are still confused, I can do a stream going over SQL 101. Happy to help out.
!RemindMe 3 days
I will be messaging you on [**2019-07-19 03:33:20 UTC**](http://www.wolframalpha.com/input/?i=2019-07-19%2003:33:20%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/SQL/comments/cdnv2s/would_anyone_be_interested_in_a_live_stream_on/etvyb44/) [**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FSQL%2Fcomments%2Fcdnv2s%2Fwould_anyone_be_interested_in_a_live_stream_on%2Fetvyb44%2F%5D%0A%0ARemindMe%21%202019-07-19%2003%3A33%3A20) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20cdnv2s) ***** |[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/c5l9ie/remindmebot_info_v20/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=Feedback)| |-|-|-|-|
Yes, it is that simple.
I didn‚Äôt even know you could write loops in sql. Impressive. I think CTEs are the extent of my knowledge
I think you need to buy in packs of two if buying by core
FRIDAY 7PM right?) Will you be streaming on youtube? Twitch?
Why don‚Äôt you wanna download the original SQL server software and add some plugins by APEXSQL?
Thanks for the answer. I'm very ignorant about the subject at this point, so I didn't know that was an option. When I looked for versions of SQL it seemed to require windows, IOS, or Linux. My only computer is a chromebook laptop. I know my questions are dumb but the hold list for the basic SQL books at the library is long so I haven't got one yet. I seem to be able to get a microsoft Azure account and can access it through that website. Does that seem like a reasonable way to start?
Oh yeah I see. So you have a chromebook. I heard that chrome has some sql extensions but I don‚Äôt know if you can actually write and execute queries with some of those extensions but there is actually some online website lol phpadmin that you can use to write and execute queries and i don‚Äôt think the Microsoft azure account will help but create an oracle account they also have an online sql server software
Which DBMS product are you using? As this will require dynamic SQL, it will be a highly vendor specific solution. ("SQL" is just a query language, **not** the name of a specific database product or vendor)
This is a very good example on why you should normalize your data model - especially the rule about "atomic" values (first normal form). Firstname and lastname should be two separate columns in your table. If you have a chance to fix this, do it now.
As a young European, eager to learn, I'd love a link to a recording as well. Have you decided where you'll be steaming? If you go with Twitch I'd love to give you a follow right now!
I second this. One of the simplest and shortest I've ever encountered.
Call your MS licensing rep or reseller. Microsoft has _buildings_ filled with people handling licensing questions, it's not simple and it's not something you want to just trust the internet on in case you get caught with the wrong license setup.
I cant fix it, the group who does this just uploads the data with remote access every hour automatically. The actual issue is that I have measurements like this: 10.23x8.21x0.00 I dont record depth in my measurements but everything else. When I refer to my data I dont want the unrecorded data to show. Its not pretty
 I'd be interested.
I'd love to see your live stream I've only recently started a job where I use SQL and it'll be nice to see new things it'll give me more of an idea as to what I should be learning. My knowledge is very basic right now.
I don't think I'm explaining things clearly enough. Let me try again. So, I had a report that had neither of the highlighted lines, which worked fine. I added one the line regarding Sites. It works great, pulling the data without issue. I add the second line regarding Service and it throws the error. Since I don't know a lot about SQL, I automatically think it has something to do with my scripting or terms, which is why I'm here. Do you see something?
Thank you.
Thank you.
How would i be able to join this? I am 100% interested.
Just use this website : https://www.w3schools.com/sql/ It should cover everything you need.
I would love to. How about recording the session and uploading somewhere so people in the different time zone / occupied at the said time can utilize that too.
Thats a great idea. A few other commenters have also made that suggestion and I definitely will.
I didn't go past the screening stage but I would still would like to familiarize myself with the queries. Below is my attempt to answer most of the questions (Some of my queries may very well be wrong). I could not wrap my head around Q6, Q9 and Q11. Feel free to critique or add anything. Thanks. &amp;#x200B; &amp;#x200B; \--1. Total Employees and total companies SELECT COUNT(DISTINCT('id')),COUNT(DISTINCT('company id')) FROM employees; &amp;#x200B; \--2. Total employees per payment SELECT COUNT (employee\_id), (payment\_id) FROM contributions AS c JOIN payment AS p ON c.payment\_id=[p.id](https://p.id) WHERE payment\_state= 'paid' GROUP BY (payment\_id); \--3.Average employee Contributions per month SELECT AVG(employee\_amount\_pennies) AS a, EXTRACT(Month from contribution\_created\_at) AS Month from contributions GROUP BY Month; &amp;#x200B; \-- 4. Average company Contributions per month &amp;#x200B; SELECT AVG(company\_amount\_pennies) AS a, EXTRACT(Month from contribution\_created\_at) AS Month from contributions GROUP BY Month; &amp;#x200B; \--5.Number of missing payments per company &amp;#x200B; SELECT COUNT(payment\_state), comp\_id FROM payment AS p JOIN contributions as c ON p.id=c.payment\_id JOIN employees AS e ON c.employee\_id=[e.id](https://e.id) WHERE payment\_state ='not\_paid' GROUP BY comp\_id; &amp;#x200B; \--7.Calculate employee salary SELECT ((employee\_amount\_pennies\*100)/employee\_percentage)/100 AS average\_salary\_per\_month\_in\_pounds FROM contributions; &amp;#x200B; \--8.Average number of days between contribution created at and payment paid day SELECT AVG((EXTRACT(day from contribution\_created\_at) - EXTRACT(day from paid\_at))) FROM contributions as c JOIN payment as p ON c.payment\_id=[p.id](https://p.id) WHERE payment\_state ='paid'; \--OR SELECT AVG(AGE(contribution\_created\_at,paid\_at)) FROM contributions as c JOIN payment as p ON c.payment\_id=[p.id](https://p.id) WHERE payment\_state ='paid'; \--10. Create a query that shows the difference between RANK() and DENSE\_RANK() &amp;#x200B; SELECT employee\_id, employee\_amount\_pennies, RANK()OVER (ORDER BY employee\_amount\_pennies DESC), DENSE\_RANK()OVER (ORDER BY employee\_amount\_pennies DESC) FROM contributions;
I‚Äôd love to watch your stream! When are You planning to do it? Could You please link your channel?
I'd love to join your stream. I suppose it's going to be important to me getting some skills in SQL language.
 select distinct foo.id from foo left outer join bar on bar.id = foo.id where bar.id is null
I would watch
While more elegant, this is essentially saying the same thing, no? (not the same as *returning* the same result) My thought was to get away from a Join, hence the WHERE statement, tho a nested SELECT isn't much different than joining another table. Not to be a pain, but is there another way?
Do you think you can go over dynamic execution plan optimization?
SELECT DISTINCT [foo.id](https://foo.id) FROM foo EXCEPT SELECT DISTINCT [bar.id](https://bar.id) FROM bar
Now that is a thing of beauty, thank you.
I don't think it's work this way, you can CONCAT but not sure of the opposite. Put Forst Name and Last Name separetly :)
Look up cursors, they're like for loops but for SQL
&gt; (not the same as returning the same result) but it does return the same result
The index will be updated.
By column headers do you mean column names? If so you could create one table then create the rest with CREATE TABLE AS SELECT, maybe. That should be available in all major SQL databases (you didn't mention which one you're using). Another way could simply be to copy/paste the CREATE TABLE statements in your editor and rename the tables. The requirement seems a little weird though to be honest. Makes me wonder if this might be a hacky approach to table partitioning.
This might be dependent on your database, but I suspect all MVCC databases will update the index, as explained in the accepted answer to this stack exchange question on Postgres: [https://dba.stackexchange.com/questions/118178/does-updating-a-row-with-the-same-value-actually-update-the-row](https://dba.stackexchange.com/questions/118178/does-updating-a-row-with-the-same-value-actually-update-the-row) If you have control over the SQL being run, you could add a WHERE clause to only do the update if the value has changed.
Install Ubuntu via crouton and set up postgresql or whatever in it.
I set up a youtube account. I‚Äôm open to twitch too.
what is the motivation for this? if you are looking for possible performance improvement, a better bet would be to get rid of unnecessary distincts (also, designing tables and writing queries in such a way that distinct is not needed).
Thank you for the answer.
Define "very large" datasets. Why are you doing 100 at a time? SQL Server does not do well with loops. You're _far_ better off performing one set-based operation against 1000 records than the same operation 10 times against 100 records each. A `TOP X` without an `ORDER BY` is non-deterministic. You are not guaranteed to get the same results every time. Table variables generally perform poorly, and an `IN` as you've constructed it here may as well. I can't help but think that this would work better as a larger-scale set-based operation, assuming you've indexed properly. If you're going to stay with this model, I would suggest an `UPDATE` against 100 records with a non-`null` `worked_date` and `output` the `uniqueid`s to a temp table. create table #MyTempTable (UniqueId varchar(25) primary key not null); update top 100 tablefullofdata set worked-date = getdate() OUTPUT inserted.uniqueid into #MyTempTable But batches of 100 may be way too small to be worthwhile; consider going much larger.
Exists is the only time select * is appropriate. As long as you are correlating on the correct field, it should get you the fastest result. Distinct requires the whole result set be gathered in memory, then removing duplicates.
Our dataset could have 100 million+ rows and I might just working the latest 200k that have been added. &gt; You're *far* better off performing one set-based operation against 1000 records than the same operation 10 times against 100 records each. I agree, and that is what I found when I ran some performance testing against real data with my other application. With that one I pull back 1k at a time. &gt; A TOP X without an ORDER BY is non-deterministic. You are not guaranteed to get the same results every time. In my scenario that is ok. I just need another batch of records to work. It could be any 1000 IDs. **I do wonder if there is a better way to get the next set of records.** I really do not care which ones are worked next, but that they are not being already worked by another thread. &amp;#x200B; &gt; Table variables generally perform poorly, and an IN as you've constructed it here may as well. In this case it should be faster with a variable table as the tables are tiny and having them in memory is very quick. I could try to run some tests to prove that. &gt; create table #MyTempTable (UniqueId varchar(25) primary key not null); update top 100 tablefullofdata set worked-date = getdate() OUTPUT inserted.uniqueid into #MyTempTable This is interesting. I've not used the output before, and looks to be much more streamlined than mine. I will most certainly try this out. Thanks!
Yes, because it's just changing the structure of the query I already had, using a JOIN in place of a nested SELECT. /u/MaRa0303hs had what I was thinking of, I knew there had to be a function somewhere that was cleaner &amp; more concise.
You‚Äôre going about this the wrong way. You should create another table and use a foreign key constraint. It‚Äôs much more maintainable
&gt; In this case it should be faster with a variable table as the tables are tiny and having them in memory is very quick. This is a common misconception. Temp tables are cached in the buffer pool just like any other table, and table variables will spill to disk if the memory manager decides it needs to. Table variables also cause problems for cardinality estimation. Under the legacy CE, SQL Server assumes that any table variable has only a single record in it. Under the new CE (2014+, `LEGACY_CARDINALITY_ESTIMATION = OFF`), table variables are always assumed to be 100 records. So, if your table variable isn't close to these, you will get bad estimates and eventually a bad query plan.
The "output" clause is your friend in this case. This should work for MS SQL Server. DECLARE @MyTableVar table(UniqueID varchar(25) NOT NULL ); SET ROWCOUNT 100 UPDATE [TableFullofData] SET worked_date = getdate() OUTPUT INSERTED.UniqueID INTO @MyTableVar WHERE worked_date IS NULL SET ROWCOUNT 0 The "set rowcount 100" means you only will touch 100 records. The output part takes the UniqueID values and writes them to @MyTableVar. Finally, the "set rowcount 0" makes it so queries affect all the rows they would normally. This avoids the transactional locking you're doing currently.
Ah I did not know that. I'll move to a temp table and see if I see the same results.
I am most certainly going to try this out. I've only thought to use inserted when I had triggers being used and this seems to be a slick way to use them.
Youtube. I left a link in the post edit.
In SQL Server there is a feature called non-updating updates, which can dynamically detect when the value changes and doesn't perform update. You can see this in execution plan.
You don't need DISTINCT when using except.
It doesn't matter as EXISTS doesn't even access the column list. You can select 1/0 if you want.
Which flavour of Sql are you using? I ask as Sql server has lag() but has some nuances which you need to watch out for.
Correct. It returns as soon as the match is found.
Fetch and offset. If you can assign your thread an Id, 1-100 you can do FETCH 100 rows offset 100 * @id rows.
I am using SQL server management studio 17
&gt; Exists is the only time select * is appropriate not the only time... another is when you're selecting columns using alias names created in a subquery -- SELECT * FROM ( SELECT 12 * foo - bar AS one , qux * fap / 5 AS two FROM t ) AS d WHERE one &gt; two
I only glanced at this but I think the problem is your CASE expression only handles the condition "(b.PositiveScore - b.NegativeScore) &gt;= 0.4". This means for all rows where that is NOT true, the name will be null.
So what would be the proper way to write that where its only showing me the rows where the condition is met?
My boss told me when managing, to focus on goals, not tasks. That plus unlimited time off has quadrupled team output. I think of that quote daily.
In this instance you are probably looking at filtering the rows using the WHERE clause instead of a CASE statement. Something like this: Select a.[ID], a.FirstName [Name] FROM table1 as a Join table2 as b ON a.ID= b.ID Where a.CountyName='Count1' AND (b.PositiveScore - b.NegativeScore) &gt;= 0.4 Order by a.LastName
That worked perfectly, thank you so much
I sent you a PM. Not spamming my current current solution in comments bc we are very unhappy with them.
I tend to enjoy the things on [Brent Ozar's blog](https://www.brentozar.com/blog/).
I should clarify that the query engine is optimized for select * in an existence check.
Not a blog per se, but the daily feed on sqlservercentral.com are awesome.
Sounds good. Replied. Thanks :)
[removed]
Sorry I‚Äôm on my phone and beers but a lazy way: Crate a table with the dates and an identity (1,1) which will give you your 42 rows and a 3rd column of abc Left join the payments for abc table to this on date = date A Case statement column on the output. Sql / English : case when amount on joined table is null then $0 + lag([payments]) else joined amount + lag([payments] end as [payments] I‚Äôm missing something but beeeeeers
I was today years old when I learned there are replication "enthusiasts."
Also you could do a windowed function which rolls along per row but I‚Äôm sorry to say not in a position to give you an example of that.
lol, i can't think of anything more exciting than moving massive amounts of data from one place to another. If one were to speed that process up in a scaleable way, it could save energy, money, and even lives. I would put a replication enthusiast badge on my account, if I knew how.
Make sure you have an index on worked_date as a leading column, or even better a filtered index (even if you create it just for the update operations). Don't select and update. Just update TOP (X) and use ORDER BY clause. Either PK or other unique key or a constant. Select a batch size that won't do lock escalation (you might test that with Extended events and half the size) Use ROWLOCK, XLOCK and READPAST hints for the concurrency. https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table?view=sql-server-2017
SELECT TOP 0 * INTO New_Table FROM Old_Table Or you could just copy the entire database and run a bunch of TRUNCATE TABLE statements for all of the tables. Depends on how many tables are in your database. Otherwise I think either option is pretty straightforward.
[https://blog.sqlauthority.com](https://blog.sqlauthority.com)
Markus Winand's [use-the-index-luke.com](http://use-the-index-luke.com) and [modern-sql.com](http://modern-sql.com) Denis Gobo's [sqlservercode.blogspot.com](http://sqlservercode.blogspot.com)
What‚Äôs protobuff, out of interest?
https://blog.jooq.org/
lol, came to say same
[Without Round Brackets](https://i.imgur.com/VQM1tcu.png)
it's called a pivot and really, it is best achieved via client reporting/data visualization tools.
[https://planet.postgresql.org/](https://planet.postgresql.org/)
Could you expand or provide me a link with further reading? Thanks!
SQL is pretty intuitive compared to something like Python or R, and you can go pretty far with knowing just a little. Udemy is good, although I learned everything from [w3schools](https://w3schools.com) and sqlzoo. It's easy to learn the syntax, but it's a little more challenging when you're actually using it on the job (though that's where you'll learn the most). If you want to get into analytics, learn SQL. &amp;#x200B; Learn Excel for pivot tables, vlookups, and logical operators. You probably already know this if you're in finance though haha. &amp;#x200B; The fact that you know Tableau is great and you can probably score an entry level analyst job if you pick up SQL well enough to run some basic queries. That said, learning Python and/or R would be a huge advantage for you down the road if you want to move up in the field.
Pinal Dave is the fucking man
You'll have to wrap it in QUOTENAME()
A correlated select statement (a subquery referring to the outer select) can act very differently from a join, with the subquery executed once per foo.id. In your example I'd expect the optimiser to avoid this anyway, but in general you should prefer the left join method over a correlated subquery for this kind of thing.
A few more to add to the list (that haven't been mentioned, all good ones so far): https://www.scarydba.com/ https://littlekendra.com/ https://www.sqlskills.com/blogs/paul/ (most of sqlskills is good really) https://erikdarlingdata.com/blog/ https://orderbyselectnull.com/author/jobbish1/ (although he moved to Erik Darling's blog recently)
[removed]
[removed]
[removed]
`DROP TABLE IF EXISTS #Values` &amp;#x200B; `CREATE TABLE #Values ( (AuthDetailID INT, UpdatedMDreviewID INT)` &amp;#x200B; `INSERT INTO #Values ( (AuthDetailID, UpdatedMDreviewID)` `SELECT AuthDetailID, UpdatedMDReviewID` `FROM @AuthDetailIds` &amp;#x200B; `UPDATE SA` `SET SA.MDReviewID = V..UpdatedMDReviewID` `FROM Schema..AuthDetails SA` `INNER JOIN #Values V ON V. AuthDetailID = S.AuthDetailID` &amp;#x200B; `DROP TABLE IF EXISTS #Values`
&gt;`DROP TABLE IF EXISTS #Values` &gt; &gt; &gt; &gt;`CREATE TABLE #Values ( (AuthDetailID INT, UpdatedMDreviewID INT)` &gt; &gt; &gt; &gt;`INSERT INTO #Values ( (AuthDetailID, UpdatedMDreviewID)` &gt; &gt;`SELECT AuthDetailID, UpdatedMDReviewID` &gt; &gt;`FROM @AuthDetailIds` &gt; &gt; &gt; &gt;`UPDATE SA` &gt; &gt;`SET SA.MDReviewID = V..UpdatedMDReviewID` &gt; &gt;`FROM Schema..AuthDetails SA` &gt; &gt;`INNER JOIN #Values V ON V. AuthDetailID = S.AuthDetailID` &gt; &gt; &gt; &gt;`DROP TABLE IF EXISTS #Values` &amp;#x200B; &amp;#x200B; Wrote this a bit fast so I hope it meshes, but I would assume it would go something like this. Instead of creating a CTE as the values, you would just insert them into a temp table, then join on the table in the UPDATE.
For SQL Server - https://www.sqlservercentral.com/ https://www.mssqltips.com/
[removed]
off the top of my head (excuse the formatting.. I'm on my phone) select A. columnnames from table A inner join ( SELECT column names FROM table WHERE store_id = '123' GROUP BY date, customerID HAVING SKU LIKE '2771') b on a. customerId = b. customerID and a. date = b. date
Many great suggestions alredy, but I'll throw a couple into the mix as well: [SQL with Bert](https://bertwagner.com/) [And performance-tuning guru Eric Darling,](https://erikdarlingdata.com/) who also does free, live office hours on YouTube every Friday if you want to ask him questions.
https://www.sqlshack.com/ I really like this blog. They take deeper dives into various functions, commands, and concepts, like indexing strategies.
https://www.SQLServer-Tips.com is a brand new SQL Server based community for help, advice, tips and trick
I will give this a try tomorrow!
Brent Ozar is the fucking man